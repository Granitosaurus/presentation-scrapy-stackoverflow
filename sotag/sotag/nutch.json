[
{"title": "Adding custom fields and types in nutch elastic indexer", "view_count": 4, "is_answered": false, "question_id": 40418712, "tags": ["elasticsearch", "nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/40418712/adding-custom-fields-and-types-in-nutch-elastic-indexer", "last_activity_date": 1478250209, "owner": {"user_id": 4327283, "answer_count": 40, "creation_date": 1417755361, "accept_rate": 100, "view_count": 69, "location": "Cochin,Kerala,India", "reputation": 651}, "body": "<p>I was running test runs on nutch elastic indexer.I would like to add some custom fields and custom typenames(instead of \"doc\") that can be given as arguments to the indexing job. I understand <strong><code>NutchDocument</code></strong> is the class which is responsible for setting field names and metadata but couldn't figure out where nutch create instance of this and sets values. Or Is there any other way for this. Please help</p>\n", "creation_date": 1478250209, "score": 0},
{"title": "Obtain the raw html of pages fetched by Nutch 2.3.1", "view_count": 19, "owner": {"user_id": 1786165, "answer_count": 7, "creation_date": 1351613216, "accept_rate": 71, "view_count": 42, "location": "London, United Kingdom", "reputation": 245}, "is_answered": true, "answers": [{"question_id": 40362627, "owner": {"user_id": 432844, "link": "http://stackoverflow.com/users/432844/julien-nioche", "user_type": "registered", "reputation": 939}, "body": "<p>glad you found the video useful. If you just need web pages to train a NLP model, why don't you use the CommonCrawl dataset? It contains billions of pages, is free and would save you the hassle of large scale web crawling?</p>\n\n<p>Now to answer your question, you could write a custom IndexWriter and write the content of the pages to whatever you want. I don't use Nutch 2.x as I prefer 1.x as it is faster, has more functionalities and is easier to use (to be honest I actually prefer StormCrawler even more but I am biased). Nutch 1.x has a WARCExporter class which can generate a data dump at the same WARC format used by CommonCrawl; there is also another class for exporting at various formats.</p>\n", "creation_date": 1478098968, "is_accepted": true, "score": 1, "last_activity_date": 1478098968, "answer_id": 40383233}], "question_id": 40362627, "tags": ["html", "web-scraping", "web-crawler", "nutch", "hadoop2"], "answer_count": 1, "link": "http://stackoverflow.com/questions/40362627/obtain-the-raw-html-of-pages-fetched-by-nutch-2-3-1", "last_activity_date": 1478098968, "accepted_answer_id": 40383233, "body": "<p>I'd like to train an NLP model using several Web pages to obtain a good precision.\nSince I don't have the Web pages, I'm considering to use a Web crawler on Amazon EMR.\nI'd like to use a distributed, extensible and scalable Open Source solution that is respectful of robots.txt rules. After some research, I decided to adopt Apache Nutch.</p>\n\n<p>I've found <a href=\"https://www.youtube.com/watch?v=v9zjcTjjjyU\" rel=\"nofollow noreferrer\">this video</a> by Nutch's main contributor Julien Nioche particularly useful to get started.\nAlthough I used the latest available version of Hadoop (Amazon 2.7.3) and Nutch (2.3.1), I managed to successfully complete a small example job.</p>\n\n<p>Unfortunately, though, I couldn't find an easy way to retrieve the raw html files from Nutch's output. While looking for a solution to this problem, I've found a few other useful resources (in addition to Nutch's own <a href=\"http://wiki.apache.org/nutch/\" rel=\"nofollow noreferrer\">wiki</a> and <a href=\"http://wiki.apache.org/nutch/FrontPage#Nutch_2.X_tutorial.28s.29\" rel=\"nofollow noreferrer\">tutorial</a> pages).</p>\n\n<p>Some of them (like <a href=\"https://florianhartl.com/nutch-plugin-tutorial.html\" rel=\"nofollow noreferrer\">this answer</a> or <a href=\"https://stackoverflow.com/questions/10007178/how-do-i-save-the-origin-html-file-with-apache-nutch\">this page</a>) suggest to implement a new <em>plugin</em> (or modify an existing one): the overall idea is to to add a few lines of code that actually save to a file the content of any fetched html page before it is sent to a segment.</p>\n\n<p>Others (like <a href=\"http://stackoverflow.com/questions/5123757/how-to-get-the-html-content-from-nutch\">this answer</a>) suggest to implement a simple post-processing tool that access the segments, goes through all the records that are included there and saves the content of any them that appears to be an html page to a file.</p>\n\n<p>These resources all contain (more or less precise) instructions and code examples, but I had no luck when I tried to run them because they refer to very old versions of Nutch. Also, all my attempts to adapt them to Nuth 2.3.1 have failed due to lack of resources/documentation.</p>\n\n<p>For instance, I appended the following code to the end of the <code>HtmlParser</code> (core of the <code>parse-html</code> plugin), but all the files that get saved on the specified folder are empty:</p>\n\n<pre><code>String html = root.toString();\nif (html == null) {\n    byte[] bytes = content.getContent();\n    try {\n      html = new String(bytes, encoding);\n    } catch (UnsupportedEncodingException e) {\n        LOG.trace(e.getMessage(), e);\n    }\n}\nif (html != null) {\n    html = html.trim();\n    if (!html.isEmpty()) {\n        if (dumpFolder == null) {\n            String currentUsersHomeFolder = System.getProperty(\"user.home\");\n            currentUsersHomeFolder = \"/Users/stefano\";\n            dumpFolder = currentUsersHomeFolder + File.separator + \"nutch_dump\";\n            new File(dumpFolder).mkdir();\n        }\n        try {\n            String filename = base.toString().replaceAll(\"\\\\P{LD}\", \"_\");\n            if (!filename.toLowerCase().endsWith(\".htm\") &amp;&amp; !filename.toLowerCase().endsWith(\".html\")) {\n                filename += \".html\";\n            }\n            System.out.println(\"&gt;&gt; \" + dumpFolder+ File.separator +filename);\n            PrintWriter writer = new PrintWriter(dumpFolder + File.separator + filename, encoding);\n            writer.write(html);\n            writer.close();\n        } catch (Exception e) {\n            LOG.trace(e.getMessage(), e);\n        }\n    }\n}\n</code></pre>\n\n<p>In the other case, instead, I got the following error (which I like because it mentions prolog but it also puzzles me):</p>\n\n<pre><code>[Fatal Error] data:1:1: Content is not allowed in prolog.\n</code></pre>\n\n<p>So, before considering to downgrade my setup to Nutch 1.x, my question is: <strong>have any of you had to face this problem with a recent version of Nutch and successfully solved it?</strong></p>\n\n<p>If so, <strong>can you share it with the community or at least provide some useful pointers to the solution?</strong></p>\n\n<p><em>Many thanks in advance!</em></p>\n\n<hr>\n\n<p>PS: If you wonder how to properly open Nutch sources into IntelliJ, <a href=\"http://stackoverflow.com/questions/15357462/how-to-open-an-ant-project-nutch-source-at-intellij-idea\">this answer</a> might actually point you towards the right direction.</p>\n", "creation_date": 1478012109, "score": 0},
{"title": "Apache Nutch 1.12 with Apache Solr 6.2.1 give an error", "view_count": 35, "is_answered": false, "question_id": 40354313, "tags": ["apache", "solr", "lucene", "nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/40354313/apache-nutch-1-12-with-apache-solr-6-2-1-give-an-error", "last_activity_date": 1478067328, "owner": {"user_id": 893028, "answer_count": 4, "creation_date": 1313237860, "accept_rate": 50, "view_count": 15, "reputation": 82}, "body": "<p>I am using Apache Nutch 1.12 and Apache Solr 6.2.1 to crawl data on the internet and index them, and the combination gives an error: <strong>java.lang.Exception: java.lang.IllegalStateException: Connection pool shut down</strong></p>\n\n<p>I have done the following as I have learned from the Nutch tutorial: <a href=\"https://wiki.apache.org/nutch/NutchTutorial\" rel=\"nofollow noreferrer\">https://wiki.apache.org/nutch/NutchTutorial</a></p>\n\n<ul>\n<li>copied schema.xml of Nutch and placed it in Solr's config folder</li>\n<li>Placed a seed url (of a newspaper company) in urls/seed.txt of Nutch</li>\n<li>changed http.content.limit value to \"-1\" in nutch-site.xml. Since the seed url is the one of newspaper company, I just had to elimiate the http content download size limit</li>\n</ul>\n\n<p>When I run the following command, I get an error:</p>\n\n<pre><code>bin/crawl -i -D solr.server.url=http://localhost:8983/solr/TSolr urls/ TestCrawl/ 2\n</code></pre>\n\n<p>Above, TSolr is just the name of the Solr Core as you can probably guess already.</p>\n\n<p>I am pasting the error log in hadoop.log below:</p>\n\n<pre><code>    2016-10-28 16:21:20,982 INFO  indexer.IndexerMapReduce - IndexerMapReduce: crawldb: TestCrawl/crawldb\n2016-10-28 16:21:20,982 INFO  indexer.IndexerMapReduce - IndexerMapReduce: linkdb: TestCrawl/linkdb\n2016-10-28 16:21:20,982 INFO  indexer.IndexerMapReduce - IndexerMapReduces: adding segment: TestCrawl/segments/20161028161642\n2016-10-28 16:21:46,353 WARN  conf.Configuration - file:/tmp/hadoop-btaek/mapred/staging/btaek1281422650/.staging/job_local1281422650_0001/job.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.\n2016-10-28 16:21:46,355 WARN  conf.Configuration - file:/tmp/hadoop-btaek/mapred/staging/btaek1281422650/.staging/job_local1281422650_0001/job.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.\n2016-10-28 16:21:46,415 WARN  conf.Configuration - file:/tmp/hadoop-btaek/mapred/local/localRunner/btaek/job_local1281422650_0001/job_local1281422650_0001.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.\n2016-10-28 16:21:46,416 WARN  conf.Configuration - file:/tmp/hadoop-btaek/mapred/local/localRunner/btaek/job_local1281422650_0001/job_local1281422650_0001.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.\n2016-10-28 16:21:46,565 INFO  anchor.AnchorIndexingFilter - Anchor deduplication is: off\n2016-10-28 16:21:52,308 INFO  indexer.IndexWriters - Adding org.apache.nutch.indexwriter.solr.SolrIndexWriter\n2016-10-28 16:21:52,383 INFO  solr.SolrMappingReader - source: content dest: content\n2016-10-28 16:21:52,383 INFO  solr.SolrMappingReader - source: title dest: title\n2016-10-28 16:21:52,383 INFO  solr.SolrMappingReader - source: host dest: host\n2016-10-28 16:21:52,383 INFO  solr.SolrMappingReader - source: segment dest: segment\n2016-10-28 16:21:52,383 INFO  solr.SolrMappingReader - source: boost dest: boost\n2016-10-28 16:21:52,383 INFO  solr.SolrMappingReader - source: digest dest: digest\n2016-10-28 16:21:52,383 INFO  solr.SolrMappingReader - source: tstamp dest: tstamp\n2016-10-28 16:21:52,424 INFO  solr.SolrIndexWriter - Indexing 42/42 documents\n2016-10-28 16:21:52,424 INFO  solr.SolrIndexWriter - Deleting 0 documents\n2016-10-28 16:21:53,468 INFO  solr.SolrMappingReader - source: content dest: content\n2016-10-28 16:21:53,468 INFO  solr.SolrMappingReader - source: title dest: title\n2016-10-28 16:21:53,468 INFO  solr.SolrMappingReader - source: host dest: host\n2016-10-28 16:21:53,468 INFO  solr.SolrMappingReader - source: segment dest: segment\n2016-10-28 16:21:53,468 INFO  solr.SolrMappingReader - source: boost dest: boost\n2016-10-28 16:21:53,468 INFO  solr.SolrMappingReader - source: digest dest: digest\n2016-10-28 16:21:53,469 INFO  solr.SolrMappingReader - source: tstamp dest: tstamp\n2016-10-28 16:21:53,472 INFO  indexer.IndexingJob - Indexer: number of documents indexed, deleted, or skipped:\n2016-10-28 16:21:53,476 INFO  indexer.IndexingJob - Indexer:     42  indexed (add/update)\n2016-10-28 16:21:53,477 INFO  indexer.IndexingJob - Indexer: finished at 2016-10-28 16:21:53, elapsed: 00:00:32\n2016-10-28 16:21:54,199 INFO  indexer.CleaningJob - CleaningJob: starting at 2016-10-28 16:21:54\n2016-10-28 16:21:54,344 WARN  util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n2016-10-28 16:22:19,739 WARN  conf.Configuration - file:/tmp/hadoop-btaek/mapred/staging/btaek1653313730/.staging/job_local1653313730_0001/job.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.\n2016-10-28 16:22:19,741 WARN  conf.Configuration - file:/tmp/hadoop-btaek/mapred/staging/btaek1653313730/.staging/job_local1653313730_0001/job.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.\n2016-10-28 16:22:19,797 WARN  conf.Configuration - file:/tmp/hadoop-btaek/mapred/local/localRunner/btaek/job_local1653313730_0001/job_local1653313730_0001.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.\n2016-10-28 16:22:19,799 WARN  conf.Configuration - file:/tmp/hadoop-btaek/mapred/local/localRunner/btaek/job_local1653313730_0001/job_local1653313730_0001.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.\n2016-10-28 16:22:19,807 WARN  output.FileOutputCommitter - Output Path is null in setupJob()\n2016-10-28 16:22:25,113 INFO  indexer.IndexWriters - Adding org.apache.nutch.indexwriter.solr.SolrIndexWriter\n2016-10-28 16:22:25,188 INFO  solr.SolrMappingReader - source: content dest: content\n2016-10-28 16:22:25,188 INFO  solr.SolrMappingReader - source: title dest: title\n2016-10-28 16:22:25,188 INFO  solr.SolrMappingReader - source: host dest: host\n2016-10-28 16:22:25,188 INFO  solr.SolrMappingReader - source: segment dest: segment\n2016-10-28 16:22:25,188 INFO  solr.SolrMappingReader - source: boost dest: boost\n2016-10-28 16:22:25,188 INFO  solr.SolrMappingReader - source: digest dest: digest\n2016-10-28 16:22:25,188 INFO  solr.SolrMappingReader - source: tstamp dest: tstamp\n2016-10-28 16:22:25,191 INFO  solr.SolrIndexWriter - SolrIndexer: deleting 6/6 documents\n2016-10-28 16:22:25,300 WARN  output.FileOutputCommitter - Output Path is null in cleanupJob()\n2016-10-28 16:22:25,301 WARN  mapred.LocalJobRunner - job_local1653313730_0001\njava.lang.Exception: java.lang.IllegalStateException: Connection pool shut down\n    at org.apache.hadoop.mapred.LocalJobRunner$Job.runTasks(LocalJobRunner.java:462)\n    at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:529)\nCaused by: java.lang.IllegalStateException: Connection pool shut down\n    at org.apache.http.util.Asserts.check(Asserts.java:34)\n    at org.apache.http.pool.AbstractConnPool.lease(AbstractConnPool.java:169)\n    at org.apache.http.pool.AbstractConnPool.lease(AbstractConnPool.java:202)\n    at org.apache.http.impl.conn.PoolingClientConnectionManager.requestConnection(PoolingClientConnectionManager.java:184)\n    at org.apache.http.impl.client.DefaultRequestDirector.execute(DefaultRequestDirector.java:415)\n    at org.apache.http.impl.client.AbstractHttpClient.doExecute(AbstractHttpClient.java:863)\n    at org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:82)\n    at org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:106)\n    at org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:57)\n    at org.apache.solr.client.solrj.impl.HttpSolrClient.executeMethod(HttpSolrClient.java:480)\n    at org.apache.solr.client.solrj.impl.HttpSolrClient.request(HttpSolrClient.java:241)\n    at org.apache.solr.client.solrj.impl.HttpSolrClient.request(HttpSolrClient.java:230)\n    at org.apache.solr.client.solrj.SolrRequest.process(SolrRequest.java:150)\n    at org.apache.solr.client.solrj.SolrClient.commit(SolrClient.java:483)\n    at org.apache.solr.client.solrj.SolrClient.commit(SolrClient.java:464)\n    at org.apache.nutch.indexwriter.solr.SolrIndexWriter.commit(SolrIndexWriter.java:190)\n    at org.apache.nutch.indexwriter.solr.SolrIndexWriter.close(SolrIndexWriter.java:178)\n    at org.apache.nutch.indexer.IndexWriters.close(IndexWriters.java:115)\n    at org.apache.nutch.indexer.CleaningJob$DeleterReducer.close(CleaningJob.java:120)\n    at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)\n    at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)\n    at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)\n    at org.apache.hadoop.mapred.LocalJobRunner$Job$ReduceTaskRunnable.run(LocalJobRunner.java:319)\n    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n    at java.util.concurrent.FutureTask.run(FutureTask.java:266)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n    at java.lang.Thread.run(Thread.java:745)\n2016-10-28 16:22:25,841 ERROR indexer.CleaningJob - CleaningJob: java.io.IOException: Job failed!\n    at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:836)\n    at org.apache.nutch.indexer.CleaningJob.delete(CleaningJob.java:172)\n    at org.apache.nutch.indexer.CleaningJob.run(CleaningJob.java:195)\n    at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)\n    at org.apache.nutch.indexer.CleaningJob.main(CleaningJob.java:206)\n</code></pre>\n\n<p>As you can see in the bin/crawl command above, I had Nutch run crawl for 2 rounds. The thing is that the error above only occurs on the second round (1 level deeper of the seed site). So, indexing works successfully on the first round, but after the second crawl and parse for the second round, it spits out the error and stops.</p>\n\n<p>To try things a bit differently from the first run as I have done above, I did the following on the second run:</p>\n\n<ul>\n<li>Deleted TestCrawl folder to start crawl and index fresh new</li>\n<li>ran: <code>bin/crawl -i -D solr.server.url=http://localhost:8983/solr/TSolr urls/ TestCrawl/ 1</code> ==> note that I have changed the number of round for Nutch to \"1\". And, this executes crawling and indexing successfully</li>\n<li>Then, ran the same command again for the second round to crawl 1 level deeper: <code>bin/crawl -i -D solr.server.url=http://localhost:8983/solr/TSolr urls/ TestCrawl/ 1</code> ==> which gives me the same error as I have pasted the hadoop.log above!!</li>\n</ul>\n\n<p>Therefore, for my Solr is NOT able to successfully index what Nutch crawled for the second round or 1 level deeper of the seed site.</p>\n\n<p>Could the error be due to the parsed contents size of the seed site? The seed site is a newspaper company's website, so I am sure that the second round (1 level deeper) would contain a hugh amount of data parsed to index. If the issue is parseed content size, how can I configure my Solr to fix the problem?</p>\n\n<p>If the error is from something else, can someone please help me identify what it is and how to fix it?</p>\n", "creation_date": 1477975729, "score": 0},
{"title": "Nutch 1.12 on Cygwin on Windows 7 - NullPointerException", "view_count": 17, "is_answered": false, "question_id": 40322294, "tags": ["nullpointerexception", "cygwin", "nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/40322294/nutch-1-12-on-cygwin-on-windows-7-nullpointerexception", "last_activity_date": 1477765795, "owner": {"age": 55, "answer_count": 0, "creation_date": 1477400162, "user_id": 7069685, "view_count": 0, "location": "Fairfax, VA", "reputation": 3}, "body": "<p>I'm working to get nutch running for the first time for a work project. At this time, the plan is to run nutch from a single machine (Windows 7) to scrape context from a dozen or so web sites. Below is the command line output from cygwin. </p>\n\n<pre><code>$ bin/nutch inject crawl/crawldb urls\nInjector: starting at 2016-10-29 09:16:37\nInjector: crawlDb: crawl/crawldb\nInjector: urlDir: urls\nInjector: Converting injected urls to crawl db entries.\nInjector: java.lang.NullPointerException\n        at java.lang.ProcessBuilder.start(ProcessBuilder.java:1012)\n        at org.apache.hadoop.util.Shell.runCommand(Shell.java:445)\n        at org.apache.hadoop.util.Shell.run(Shell.java:418)\n        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:650)\n        at org.apache.hadoop.util.Shell.execCommand(Shell.java:739)\n        at org.apache.hadoop.util.Shell.execCommand(Shell.java:722)\n        at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:633)\n        at org.apache.hadoop.fs.FilterFileSystem.setPermission(FilterFileSystem.java:467)\n        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:456)\n        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:424)\n        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:906)\n        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:887)\n        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:849)\n        at org.apache.hadoop.fs.FileSystem.createNewFile(FileSystem.java:1149)\n        at org.apache.nutch.util.LockUtil.createLockFile(LockUtil.java:58)\n        at org.apache.nutch.crawl.Injector.inject(Injector.java:357)\n        at org.apache.nutch.crawl.Injector.run(Injector.java:467)\n        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)\n        at org.apache.nutch.crawl.Injector.main(Injector.java:441)\n</code></pre>\n\n<p>Looking through the source, here are lines 440 thru 443 of org.apache.nutch.crawl.Injector:</p>\n\n<pre><code>  public static void main(String[] args) throws Exception {\n    int res = ToolRunner.run(NutchConfiguration.create(), new Injector(), args);\n    System.exit(res);\n  }\n</code></pre>\n\n<p>It's not clear exactly whether it is the NutchConfiguration.create() or the new Injector() which is failing there. I setup my installation from the tutorial on the nutch site. I put a list of 3 urls, 1 per line, in the file ./urls/seed.txt; and edited ./conf/nutch-site.xml.</p>\n\n<p>Any suggestions for investigation/debugging this would be appreciated.\nThank you!</p>\n", "creation_date": 1477765795, "score": 0},
{"title": "Configure MongoDB with Nutch2.3, some error about indexerJob?", "view_count": 167, "is_answered": false, "answers": [{"question_id": 36076742, "owner": {"user_id": 5907364, "link": "http://stackoverflow.com/users/5907364/j%c3%a9r%c3%a9mie", "user_type": "registered", "reputation": 46}, "body": "<p>If all the others steps was running, it would be not a problem with mongodb but with solr (your nutch-site.xml suggests that you wanted index your data in solr). As far that i remember, when i used solr, i precised the core name, it would be something like that : </p>\n\n<pre><code>http://localhost:8983/solr/mycore/\n</code></pre>\n", "creation_date": 1458310805, "is_accepted": false, "score": 0, "last_activity_date": 1458310805, "answer_id": 36086754}], "question_id": 36076742, "tags": ["mongodb", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/36076742/configure-mongodb-with-nutch2-3-some-error-about-indexerjob", "last_activity_date": 1477631071, "owner": {"user_id": 4983473, "answer_count": 0, "creation_date": 1433685399, "accept_rate": 33, "view_count": 6, "reputation": 16}, "body": "<p>I had successfully configure MongoDB(5.3.1) and Nutch(2.3), when I run the command \"./bin/nutch index -all\" some errors printed after inject/generate/fetch/parse/updatedb commands work,the error details like:</p>\n\n<pre><code>SolrIndexerJob: java.lang.RuntimeException: job failed: name=apache-nutch-2.3.1.jar, jobid=job_local140530148_0001\n    at org.apache.nutch.util.NutchJob.waitForCompletion(NutchJob.java:120)\n    at org.apache.nutch.indexer.IndexingJob.run(IndexingJob.java:154)\n    at org.apache.nutch.indexer.IndexingJob.index(IndexingJob.java:176)\n    at org.apache.nutch.indexer.IndexingJob.run(IndexingJob.java:202)\n    at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)\n    at org.apache.nutch.indexer.IndexingJob.main(IndexingJob.java:211)\n</code></pre>\n\n<p>I had configure the file in $NUTCH_HOME/runtime/local/conf/nutch-site.xml\ndetails:</p>\n\n<p><a href=\"http://i.stack.imgur.com/B1EXa.png\" rel=\"nofollow\"><img src=\"http://i.stack.imgur.com/B1EXa.png\" alt=\"enter image description here\"></a></p>\n", "creation_date": 1458278670, "score": 0},
{"title": "Use Nutch 1.x to crawl Hadoop URL&#39;s using kerboros/spnego authentication in Java", "view_count": 9, "is_answered": false, "question_id": 40295268, "tags": ["authentication", "kerberos", "nutch", "spnego"], "answer_count": 0, "link": "http://stackoverflow.com/questions/40295268/use-nutch-1-x-to-crawl-hadoop-urls-using-kerboros-spnego-authentication-in-java", "last_activity_date": 1477607732, "owner": {"user_id": 4721646, "view_count": 3, "answer_count": 0, "creation_date": 1427475249, "reputation": 1}, "body": "<p>I am currently using Spnego to access Hadoop jobtracker URL to retrieve page info using Java and Authentication using alfredo jar. I am able to successfully retrieve page source using a Java file.</p>\n\n<p>I want to extend this logic to Nutch so that Nutch does this for all urls using this authentication.</p>\n\n<p>What are the possible solutions ? Does Nutch give you authentication using this ?</p>\n\n<p>Can I override Nutch authentication to do this ?</p>\n\n<p>Here is the snippet of code from Alfredo Client \n<a href=\"https://github.com/cloudera/alfredo/blob/0.1/examples/src/main/java/com/cloudera/alfredo/examples/WhoClient.java\" rel=\"nofollow\">https://github.com/cloudera/alfredo/blob/0.1/examples/src/main/java/com/cloudera/alfredo/examples/WhoClient.java</a></p>\n\n<pre><code>String url =\"http://&lt;host&gt;/jobtracker.jsp\";\nAuthenticatedURL.Token token = new AuthenticatedURL.Token();\nURL ur = new URL(url);\nHttpsURLConnection con = (HttpsURLConnection) new   AuthenticatedURL().openConnection(ur, token);\n</code></pre>\n", "creation_date": 1477607732, "score": 0},
{"title": "How can I use Apache Spark with Apache Nutch", "view_count": 94, "is_answered": false, "answers": [{"question_id": 38766387, "owner": {"user_id": 2704277, "link": "http://stackoverflow.com/users/2704277/chris-mattmann", "user_type": "registered", "reputation": 51}, "body": "<p>Check out Sparkler, we are working on it as a prototype for Nutch on Spark, <a href=\"http://github.com/USCDataScience/sparkler.git\" rel=\"nofollow\">http://github.com/USCDataScience/sparkler.git</a></p>\n", "creation_date": 1470328539, "is_accepted": false, "score": 0, "last_activity_date": 1470328539, "answer_id": 38772919}], "question_id": 38766387, "tags": ["apache-spark", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/38766387/how-can-i-use-apache-spark-with-apache-nutch", "last_activity_date": 1477576697, "owner": {"user_id": 3454410, "answer_count": 20, "creation_date": 1395646154, "accept_rate": 48, "view_count": 174, "reputation": 832}, "body": "<p>I have to crawl some data from web and perform some analytics. For crawling, I have decided to use Apache Nutch 2.3.1 and for analytics, I have decided to use Apache Spark. Now How can I integrate Nutch with solr. According to Nutch <a href=\"http://nutch.apache.org/\" rel=\"nofollow\">website</a>, I have come to know that it can support spark 1.4.1. Can some one confirm that my design desicion is correct and second I have not found tutorial for integration of these two. Can soneone guide</p>\n", "creation_date": 1470310961, "score": 0},
{"title": "Getting Exception While crawling my first website Through Nutch On Windows", "view_count": 22, "is_answered": false, "question_id": 40282934, "tags": ["java", "apache", "nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/40282934/getting-exception-while-crawling-my-first-website-through-nutch-on-windows", "last_activity_date": 1477566035, "owner": {"user_id": 7068579, "view_count": 0, "answer_count": 0, "creation_date": 1477385303, "reputation": 1}, "body": "<p>Hi I am trying to learn Apache nutch and following the below link.\n<a href=\"https://wiki.apache.org/nutch/NutchTutorial\" rel=\"nofollow\">https://wiki.apache.org/nutch/NutchTutorial</a> . After making changes in the config files when I am executing the below command  <strong>bin/nutch inject crawl/crawldb urls</strong> I am getting the below exception</p>\n\n<pre><code>Injector: starting at 2016-10-27 16:24:08\nInjector: crawlDb: crawl/crawldb\nInjector: urlDir: urls\nInjector: Converting injected urls to crawl db entries.\nInjector: java.lang.NullPointerException\n        at java.lang.ProcessBuilder.start(ProcessBuilder.java:1012)\n        at org.apache.hadoop.util.Shell.runCommand(Shell.java:445)\n        at org.apache.hadoop.util.Shell.run(Shell.java:418)\n        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:650)\n        at org.apache.hadoop.util.Shell.execCommand(Shell.java:739)\n        at org.apache.hadoop.util.Shell.execCommand(Shell.java:722)\n        at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:633)\n        at org.apache.hadoop.fs.FilterFileSystem.setPermission(FilterFileSystem.java:467)\n        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:456)\n        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:424)\n        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:906)\n        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:887)\n        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:849)\n        at org.apache.hadoop.fs.FileSystem.createNewFile(FileSystem.java:1149)\n        at org.apache.nutch.util.LockUtil.createLockFile(LockUtil.java:58)\n        at org.apache.nutch.crawl.Injector.inject(Injector.java:357)\n        at org.apache.nutch.crawl.Injector.run(Injector.java:467)\n        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)\n        at org.apache.nutch.crawl.Injector.main(Injector.java:441)\n</code></pre>\n\n<p>I tried to find the issue but not able to get it.Can some one who has worked on Apache nutch please help me .</p>\n", "creation_date": 1477566035, "score": 0},
{"title": "Is the class org.apache.nutch.crawl.Crawler in apache-nutch-1.12?", "view_count": 19, "owner": {"age": 55, "answer_count": 0, "creation_date": 1477400162, "user_id": 7069685, "view_count": 0, "location": "Fairfax, VA", "reputation": 3}, "is_answered": true, "answers": [{"question_id": 40252914, "owner": {"user_id": 5953351, "link": "http://stackoverflow.com/users/5953351/sebastian-nagel", "user_type": "registered", "reputation": 121}, "body": "<p>The class o.a.n.crawl.Crawl (in Nutch 2.x \"Crawler\") has been replaced by a shell script (bin/crawl) which is more adaptable, see <a href=\"https://issues.apache.org/jira/browse/NUTCH-1087\" rel=\"nofollow\">NUTCH-1087</a>.</p>\n", "creation_date": 1477489194, "is_accepted": true, "score": 0, "last_activity_date": 1477489194, "answer_id": 40263866}], "question_id": 40252914, "tags": ["nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/40252914/is-the-class-org-apache-nutch-crawl-crawler-in-apache-nutch-1-12", "last_activity_date": 1477489194, "accepted_answer_id": 40263866, "body": "<p>I've installed nutch for the first time. Installation and set up seemed pretty smooth. I have it running on Windows 7. I setup the classpath for the nutch installation. After seeing the error shown below (missing main class), I trouble shot the setup for some time.</p>\n\n<pre><code>C:\\Users\\Public\\PublicApps\\apache-nutch-1.12&gt;nutch.bat crawl urls -dir crawl -depth 1 &gt; crawl.log Error: Could not find or load main class org.apache.nutch.crawl.Crawler\n</code></pre>\n\n<p>Finally, I searched through the nutch jar files. I don't see the org.apache.nutch.crawl.Crawler class in the jars anywhere. Checked several times. I'm stumped. </p>\n\n<p>I am assuming that class should be in the download distribution found at <a href=\"http://www.apache.org/dyn/closer.lua/nutch/1.12/apache-nutch-1.12-bin.zip\" rel=\"nofollow\">http://www.apache.org/dyn/closer.lua/nutch/1.12/apache-nutch-1.12-bin.zip</a></p>\n\n<p>Perhaps there is something simple that I've overlooked.\nAny suggestions?\nThank you</p>\n", "creation_date": 1477450210, "score": 0},
{"title": "Dump data from a Nutch crawl into multiple warc files", "view_count": 17, "is_answered": false, "answers": [{"question_id": 40221231, "owner": {"user_id": 432844, "link": "http://stackoverflow.com/users/432844/julien-nioche", "user_type": "registered", "reputation": 939}, "body": "<p>Sounds a bit wasteful to have one WARC per doc but here you go :\nyou could specify a low value for 'warc.output.segment.size' so that the files get rotated every time a new document is written. WarcExporter uses [<a href=\"https://github.com/ept/warc-hadoop]\" rel=\"nofollow\">https://github.com/ept/warc-hadoop]</a> under the bonnet, the config is used there.</p>\n", "creation_date": 1477321256, "is_accepted": false, "score": 0, "last_activity_date": 1477321256, "answer_id": 40221568}, {"question_id": 40221231, "owner": {"user_id": 5406729, "link": "http://stackoverflow.com/users/5406729/chronus", "user_type": "registered", "reputation": 6}, "body": "<p>After quite a few attempts, I managed to find out that\n<code>./bin/nutch commoncrawldump -outputDir nameOfOutputDir -segment crawl/segments/segmentDir -warc</code>\nDoes exactly what I needed, a full dump of the segment into individual WARC files.</p>\n", "creation_date": 1477486421, "is_accepted": false, "score": 0, "last_activity_date": 1477486421, "answer_id": 40262851}], "question_id": 40221231, "tags": ["web-crawler", "nutch", "warc"], "answer_count": 2, "link": "http://stackoverflow.com/questions/40221231/dump-data-from-a-nutch-crawl-into-multiple-warc-files", "last_activity_date": 1477486421, "owner": {"user_id": 5406729, "view_count": 0, "answer_count": 1, "creation_date": 1443963623, "reputation": 6}, "body": "<p>I have crawled a list of websites using Nutch 1.12. I can dump the crawl data into separate HTML files by using:\n<code>./bin/nutch dump -segment crawl/segments/ -o outputDir nameOfDir</code></p>\n\n<p>And into a single WARC file by using:\n<code>./bin/nutch warc crawl/warcs crawl/segment/nameOfSegment</code></p>\n\n<p>But how can I dump the collected data into multiple WARC files, one for each webpage crawled?</p>\n", "creation_date": 1477320085, "score": 1},
{"title": "Nutch does not Index on Elasticsearch correctly using Mongodb", "view_count": 69, "owner": {"user_id": 2176092, "view_count": 3, "answer_count": 2, "creation_date": 1363402255, "reputation": 8}, "is_answered": true, "answers": [{"last_edit_date": 1476911491, "owner": {"user_id": 2176092, "link": "http://stackoverflow.com/users/2176092/joe-crane", "user_type": "registered", "reputation": 8}, "body": "<p>After a lot of trouble I got it working. I ended up using ES 1.4.4, nutch 2.3.1, mongodb 3.10, and JDK 8. </p>\n\n<p>Many of the issues I went through that remained unanswered in a number of other threads:</p>\n\n<ul>\n<li>(this is an easy one but...) MAKE SURE EVERYTHING IS RUNNING. Make\nsure elasticsearch is running on the correct machine with the\ncorrect port. Make sure you can talk to it. Make sure MongoDB is up\nand running on the correct port, make sure you can talk to it.</li>\n<li>Use the correct index command. for Nutch 3.2.1 it's: \n<code>./bin/nutch index -all</code> (after you fetch and parse). If you run into a solr error, you do not have the correct index funtion in your nutch-site.xml. </li>\n<li>Name your crawler engine the SAME THING in your elasticsearch.yml and your nutch-site.xml. This was huge. This is the main reason I had any error thrown in my index function.</li>\n<li>Versioning. I tried to do this with the newer versions of Elasticsearch and frequently ran into problems. I am going to attempt to build this on the newest version of Elasticsearch and Mongo and get back to this thread. Try to use the same build I did first, then attempt the other builds. Elasticsearch versioning with nutch seems to be the most important part because of the dependencies regarding gora in the ivy/ivy.xml settings as well as the indexer-elastic/plugin.xml settings.  </li>\n</ul>\n\n<p>Please, please, please, let me know if you're having any trouble with this. It took me close to 2 full weeks to figure this build out and I know it can be incredibly frustrating. PM me or post on this if you're running into issues, I'm sure I can help you work through them. </p>\n\n<p>Joe</p>\n", "question_id": 39885025, "creation_date": 1476903606, "is_accepted": true, "score": 0, "last_activity_date": 1476911491, "answer_id": 40139507}], "question_id": 39885025, "tags": ["mongodb", "elasticsearch", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/39885025/nutch-does-not-index-on-elasticsearch-correctly-using-mongodb", "last_activity_date": 1476911491, "accepted_answer_id": 40139507, "body": "<p>I am running Nutch 2.3.1, Mongodb 3.2.9, and Elasticsearch 2.4.1. I have followed a mix of this tutorial: </p>\n\n<p><a href=\"https://qbox.io/blog/scraping-the-web-with-nutch-for-elasticsearch\" rel=\"nofollow\">https://qbox.io/blog/scraping-the-web-with-nutch-for-elasticsearch</a></p>\n\n<p>and this tutorial:</p>\n\n<p><a href=\"http://www.aossama.com/search-engine-with-apache-nutch-mongodb-and-elasticsearch/\" rel=\"nofollow\">http://www.aossama.com/search-engine-with-apache-nutch-mongodb-and-elasticsearch/</a></p>\n\n<p>In order to create a web crawling tool using those aforementioned 3 pieces of software. </p>\n\n<p>Everything works great until it comes down to indexing... as soon as I use the index command from nutch:</p>\n\n<pre><code># bin/nutch index elasticsearch -all\n</code></pre>\n\n<p>this happens:</p>\n\n<pre><code>IndexingJob: starting\nActive IndexWriters :\nElasticIndexWriter\n        elastic.cluster : elastic prefix cluster\n        elastic.host : hostname\n        elastic.port : port (default 9300)\n        elastic.index : elastic index command\n        elastic.max.bulk.docs : ealstic bulk index doc counts. (default 250)\n        elastic.max.bulk.size : elastic bulk index length. (default 2500500 ~2.5MB)\n\nIndexingJob: done.\n</code></pre>\n\n<p>My nutch-site.xml:</p>\n\n<pre><code>&lt;configuration&gt;\n  &lt;property&gt;\n    &lt;name&gt;storage.data.store.class&lt;/name&gt;\n    &lt;value&gt;org.apache.gora.mongodb.store.MongoStore&lt;/value&gt;\n    &lt;description&gt;Default class for storing data&lt;/description&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;http.agent.name&lt;/name&gt;\n    &lt;value&gt;AOssama Crawler&lt;/value&gt;\n  &lt;/property&gt;\n\n  &lt;property&gt;\n    &lt;name&gt;plugin.includes&lt;/name&gt;\n    &lt;value&gt;protocol-(http|httpclient)|urlfilter-regex|index-(basic|more)|query-(basic|site|url|lang)|indexer-elastic|nutch-extensionpoints|parse-(text|html|msexcel|msword|mspowerpoint|pdf)|summary-basic|scoring-opic|urlnormalizer-(pass|regex|basic)|parse-(html|tika|metatags)|index-(basic|anchor|more|metadata)&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;elastic.host&lt;/name&gt;\n    &lt;value&gt;localhost&lt;/value&gt;\n  &lt;/property&gt;\n\n  &lt;property&gt;\n    &lt;name&gt;elastic.cluster&lt;/name&gt;\n    &lt;value&gt;aossama&lt;/value&gt;\n  &lt;/property&gt;\n\n  &lt;property&gt;\n    &lt;name&gt;elastic.index&lt;/name&gt;\n    &lt;value&gt;nutch&lt;/value&gt;\n  &lt;/property&gt;\n\n  &lt;property&gt;\n    &lt;name&gt;parser.character.encoding.default&lt;/name&gt;\n    &lt;value&gt;utf-8&lt;/value&gt;\n  &lt;/property&gt;\n\n  &lt;property&gt;\n    &lt;name&gt;http.content.limit&lt;/name&gt;\n    &lt;value&gt;6553600&lt;/value&gt;\n  &lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre>\n\n<p>I also looked into the ElasticIndexWriter.java code and noticed near line 250 the class that calls the ElasticIndexWriter. I'm digging into that further now, but I'm completely lost as to why this isn't working with Mongo. I'm about to give up and try with Hbase as much as I dislike it.</p>\n\n<p>Thanks!</p>\n\n<p>Joe</p>\n", "creation_date": 1475708811, "score": 0},
{"title": "How to crawl images through nutch 2.3.1 and how to parse and index them through solr?", "view_count": 12, "is_answered": false, "question_id": 40129343, "tags": ["nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/40129343/how-to-crawl-images-through-nutch-2-3-1-and-how-to-parse-and-index-them-through", "last_activity_date": 1476874368, "owner": {"user_id": 6857329, "view_count": 1, "answer_count": 0, "creation_date": 1474446857, "reputation": 1}, "body": "<p>Can anyone let me know how to crawl images through nutch(2.3.1) and how to parse an index them through solr(5.4.1).</p>\n", "creation_date": 1476874368, "score": -1},
{"title": "Solr Highlighting - Display Snippet", "view_count": 1268, "is_answered": true, "answers": [{"last_edit_date": 1377628954, "owner": {"user_id": 2695269, "link": "http://stackoverflow.com/users/2695269/mgs", "user_type": "registered", "reputation": 709}, "body": "<p>Use <code>fl=content</code> parameter with your query. If no highlighted <strong>content</strong> returned then generate snippet from <strong>content (fl=content)</strong> field returned with each document in result set. </p>\n", "question_id": 18469175, "creation_date": 1377627972, "is_accepted": false, "score": 0, "last_activity_date": 1377628954, "answer_id": 18472854}, {"last_edit_date": 1476811172, "owner": {"user_id": 977919, "accept_rate": 93, "link": "http://stackoverflow.com/users/977919/sidgate", "user_type": "registered", "reputation": 3400}, "body": "<p>I guess your query URL looks like <code>q=(title:ABC OR content:ABC)&amp;hl=true&amp;hl.fl=title,content</code></p>\n\n<p>Try adding <a href=\"http://wiki.apache.org/solr/HighlightingParameters#hl.alternateField\" rel=\"nofollow\"><code>hl.alternateField</code></a><code>=content</code> to the query</p>\n", "question_id": 18469175, "creation_date": 1377696429, "is_accepted": false, "score": 2, "last_activity_date": 1476811172, "answer_id": 18489308}], "question_id": 18469175, "tags": ["solr", "code-snippets", "highlighting", "nutch", "solr4"], "answer_count": 2, "link": "http://stackoverflow.com/questions/18469175/solr-highlighting-display-snippet", "last_activity_date": 1476811172, "owner": {"age": 24, "answer_count": 43, "creation_date": 1374847004, "user_id": 2623052, "accept_rate": 45, "view_count": 206, "location": "Scotland", "reputation": 866}, "body": "<p>I have successfully set up highlighting in Solr4, I am indexing docx, xlsx &amp; pdf's mainly so just have fields like url, title &amp; content.  </p>\n\n<p>I have Solr highlighting the content field and it displays the small snippet of text, but sometimes the matched word is in the title as opposed to the content and therefore it will not return me a snippet of text</p>\n\n<p>Is there any way of returning even just the first line or two from the content field so that it is not left blank.</p>\n", "creation_date": 1377615963, "score": 1},
{"title": "Apache Nutch 2.3.1 is giving SolrDeleteDuplicates class not found exception", "view_count": 8, "is_answered": false, "question_id": 40081179, "tags": ["solr", "duplicates", "nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/40081179/apache-nutch-2-3-1-is-giving-solrdeleteduplicates-class-not-found-exception", "last_activity_date": 1476691093, "owner": {"user_id": 3454410, "answer_count": 20, "creation_date": 1395646154, "accept_rate": 48, "view_count": 174, "reputation": 832}, "body": "<p>I have a small cluster with Nutch 2.3.1. When I run a job to crawl few websites, it throws an error at depup stage. </p>\n\n<pre><code>16/10/17 12:50:54 INFO mapred.JobClient: Task Id : attempt_201608250836_0726_m_000000_0, Status : FAILED\njava.lang.RuntimeException: java.lang.ClassNotFoundException: org.apache.nutch.indexer.solr.SolrDeleteDuplicates$SolrInputFormat\n    at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:857)\n    at org.apache.hadoop.mapreduce.JobContext.getInputFormatClass(JobContext.java:187)\n    at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:722)\n    at org.apache.hadoop.mapred.MapTask.run(MapTask.java:364)\n    at org.apache.hadoop.mapred.Child$4.run(Child.java:255)\n    at java.security.AccessController.doPrivileged(Native Method)\n    at javax.security.auth.Subject.doAs(Subject.java:422)\n    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)\n    at org.apache.hadoop.mapred.Child.main(Child.java:249)\nCaused by: java.lang.ClassNotFoundException: org.apache.nutch.indexer.solr.SolrDeleteDuplicates$SolrInputFormat\n    at java.net.URLClassLoader$1.run(URLClassLoader.java:372)\n    at java.net.URLClassLoader$1.run(URLClassLoader.java:361)\n    at java.security.AccessController.doPrivileged(Native Method)\n    at java.net.URLClassLoader.findClass(URLClassLoader.java:360)\n    at java.lang.ClassLoader.loadClass(ClassLoader.java:424)\n    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)\n    at java.lang.ClassLoader.loadClass(ClassLoader.java:357)\n    at java.lang.Class.forName0(Native Method)\n    at java.lang.Class.forName(Class.java:344)\n    at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:810)\n    at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:855)\n    ... 8 more\n</code></pre>\n\n<p>A similar question was posted <a href=\"http://stackoverflow.com/questions/17913085/nutch-failing-on-deleting-duplicates-on-one-solr-core-but-not-another\">here</a> about Nutch 1.7 version. But it does not working for 2.x version.</p>\n", "creation_date": 1476691093, "score": 0},
{"title": "Prioritizing recursive crawl in Storm Crawler", "view_count": 19, "owner": {"user_id": 3143538, "answer_count": 13, "creation_date": 1388307186, "accept_rate": 20, "view_count": 12, "reputation": 113}, "is_answered": true, "answers": [{"question_id": 40018790, "owner": {"user_id": 432844, "link": "http://stackoverflow.com/users/432844/julien-nioche", "user_type": "registered", "reputation": 939}, "body": "<p>StormCrawler can handle recursive crawls, and the way URLs are prioritized depends on the backend used for storing the URLs.</p>\n\n<p>For instance the <a href=\"https://github.com/DigitalPebble/storm-crawler/tree/master/external/elasticsearch\" rel=\"nofollow\">Elasticsearch module</a> can be used for that, see the README for a short tutorial and the <a href=\"https://github.com/DigitalPebble/storm-crawler/blob/master/external/elasticsearch/es-conf.yaml\" rel=\"nofollow\">sample config file</a>, where by default the spouts will sort URLs based on their nextFetchDate (**.sort.field*).</p>\n\n<p>In Nutch, the -topN argument only specifies the max number of URLs to put in the next segment (based on the scores provided by whichever scoring plugin is used). With StormCrawler we don't really need an equivalent as things are not processed by batches, the crawl runs continuously.</p>\n", "creation_date": 1476357587, "is_accepted": true, "score": 1, "last_activity_date": 1476357587, "answer_id": 40019552}], "question_id": 40018790, "tags": ["web-crawler", "nutch", "stormcrawler"], "answer_count": 1, "link": "http://stackoverflow.com/questions/40018790/prioritizing-recursive-crawl-in-storm-crawler", "last_activity_date": 1476357719, "accepted_answer_id": 40019552, "body": "<p>When crawling the world wide web, I would want to give my crawler an initial seed list of URLs - and would expect my crawler to automatically  'discover' new seed URLs from internet during it's crawling.</p>\n\n<p>I see such option in Apach Nutch (see topN parameter in <a href=\"https://wiki.apache.org/nutch/bin/nutch%20generate\" rel=\"nofollow\">generate command of nutch</a>). Is there any such option in <a href=\"https://github.com/DigitalPebble/storm-crawler\" rel=\"nofollow\">Storm Crawler</a> as well?</p>\n", "creation_date": 1476355406, "score": 1},
{"title": "Apache Nutch 2.3.1 is not going to reindex all Hbase documents to solr 6.2", "view_count": 16, "is_answered": false, "question_id": 40014787, "tags": ["solr", "lucene", "nutch", "solr6"], "answer_count": 0, "link": "http://stackoverflow.com/questions/40014787/apache-nutch-2-3-1-is-not-going-to-reindex-all-hbase-documents-to-solr-6-2", "last_activity_date": 1476343620, "owner": {"user_id": 3454410, "answer_count": 20, "creation_date": 1395646154, "accept_rate": 48, "view_count": 174, "reputation": 832}, "body": "<p>I have some crawled data via Apache Nutch 2.3.1. After crawling in few weeks, there were about 7 million docs indexed in solr 4.10.3. Now I have to upgrade solr to its latest version that is 6.2.1. I have changed schema file correctly and ran a simple nutch job successfully that index documents to solr 6.2. Now I have to reindex all old crawed data to solr new version. When I run Nutch job to reindex via following command</p>\n\n<pre><code>bin/nutch solrindex http://10.10.11.91:8983/solr/collection1 -all -crawlId hbase_table\n</code></pre>\n\n<p>Job was finished successfully but there were only 2.2 million documents in Solr core. I have checked logs also but there was not error at all. So now why Nutch is not indexing all documents to Solr? Where is the problem ?</p>\n", "creation_date": 1476343620, "score": 0},
{"title": "Unknown issue in Nutch elastic indexer with nutch REST api", "view_count": 32, "owner": {"user_id": 4327283, "answer_count": 40, "creation_date": 1417755361, "accept_rate": 100, "view_count": 69, "location": "Cochin,Kerala,India", "reputation": 651}, "is_answered": true, "answers": [{"question_id": 39916877, "owner": {"user_id": 4327283, "accept_rate": 100, "link": "http://stackoverflow.com/users/4327283/sachin", "user_type": "registered", "reputation": 651}, "body": "<p>I have found solution for this issue. This is due to the version compatibility of guava dependency. Hadoop uses guava-11.0.2.jar as dependency. But the elastic indexer plugin in nutch requires 18.0 version of guava. That's why it is throwing an exception when trying to run in distributed hadoop. So we just need to update guava version to 18.0 in hadoop libs(can be found at <em>$HADOOP_HOME/share/hadoop/common/libs/</em>).</p>\n", "creation_date": 1476247753, "is_accepted": true, "score": 2, "last_activity_date": 1476247753, "answer_id": 39990721}], "question_id": 39916877, "tags": ["elasticsearch", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/39916877/unknown-issue-in-nutch-elastic-indexer-with-nutch-rest-api", "last_activity_date": 1476247753, "accepted_answer_id": 39990721, "body": "<p>I was trying to expose nutch using REST endpoints and ran into an issue in indexer phase. I'm using elasticsearch index writer to index docs to ES. I've used $NUTCH_HOME/runtime/deploy/bin/nutch startserver command. While indexing an unknown exception is thrown.</p>\n\n<blockquote>\n  <p>Error:\n  com.google.common.util.concurrent.MoreExecutors.directExecutor()Ljava/util/concurrent/Executor;\n  16/10/07 16:01:47 INFO mapreduce.Job:  map 100% reduce 0% 16/10/07\n  16:01:49 INFO mapreduce.Job: Task Id :\n  attempt_1475748314769_0107_r_000000_1, Status : FAILED Error:\n  com.google.common.util.concurrent.MoreExecutors.directExecutor()Ljava/util/concurrent/Executor;\n  16/10/07 16:01:53 INFO mapreduce.Job: Task Id :\n  attempt_1475748314769_0107_r_000000_2, Status : FAILED Error:\n  com.google.common.util.concurrent.MoreExecutors.directExecutor()Ljava/util/concurrent/Executor;\n  16/10/07 16:01:58 INFO mapreduce.Job:  map 100% reduce 100% 16/10/07\n  16:01:59 INFO mapreduce.Job: Job job_1475748314769_0107 failed with\n  state FAILED due to: Task failed task_1475748314769_0107_r_000000 Job\n  failed as tasks failed. failedMaps:0 failedReduces:1</p>\n  \n  <p>ERROR indexer.IndexingJob: Indexer: java.io.IOException: Job failed!\n    at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:865)    at\n  org.apache.nutch.indexer.IndexingJob.index(IndexingJob.java:145)  at\n  org.apache.nutch.indexer.IndexingJob.run(IndexingJob.java:228)    at\n  org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)     at\n  org.apache.nutch.indexer.IndexingJob.main(IndexingJob.java:237)</p>\n  \n  <p>Failed with exit code 255.</p>\n</blockquote>\n\n<p>Any help would be appreciated.</p>\n\n<p>PS : After debugging using stack trace I think the issue is due to mismatch in guava version. I've tried changing build.xml of plugins(parse-tika and parsefilter-naivebayes) but it didn't work.</p>\n", "creation_date": 1475841977, "score": 3},
{"title": "With nutch crawl, if I use smaller values for -topN and -depth, will it still crawl all the same pages?", "view_count": 22, "is_answered": false, "answers": [{"question_id": 39977193, "owner": {"user_id": 432844, "link": "http://stackoverflow.com/users/432844/julien-nioche", "user_type": "registered", "reputation": 939}, "body": "<p>Changing depth (should have a different name really, it's the number of iterations which is often the same as depth but not necessarily) won't make much of a difference as the crawl will stop iterating as soon as there aren't any more URLs to fetch. The topN limits the total number of URLs per segment : if you put a lower value more iteration will be done but as a whole it shouldn't affect how long your crawl takes. </p>\n\n<p>There are many factors affecting the speed of a crawl <a href=\"https://wiki.apache.org/nutch/OptimizingCrawls\" rel=\"nofollow\">see WIKI</a> but it's merely a matter of host diversity and politeness. I'd recommend that you run Nutch in pseudo distributed mode and use the Hadoop UI to understand which steps take time and take it from there.</p>\n\n<p>PS: that's a very old version of Nutch. Maybe time to upgrade to a more recent one? </p>\n", "creation_date": 1476196660, "is_accepted": false, "score": 0, "last_activity_date": 1476196660, "answer_id": 39979829}], "question_id": 39977193, "tags": ["nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/39977193/with-nutch-crawl-if-i-use-smaller-values-for-topn-and-depth-will-it-still-cr", "last_activity_date": 1476208265, "owner": {"user_id": 835589, "answer_count": 4, "creation_date": 1310136962, "accept_rate": 75, "view_count": 19, "location": "Massachusetts", "reputation": 187}, "body": "<p>I am running Nutch 1.4/Solr 4.10 to index a number of sites.  My crawl includes a number of seed pages with several hundred links.  I am currently running with </p>\n\n<pre><code>-topN 400 -depth 20\n</code></pre>\n\n<p>With these settings it takes 5-7 hours to complete the crawl.  I would like to have each individual iteration of \"nutch crawl\" take less time, but I need to ensure all pages are crawled eventually.\nCan I reduce either my -topN or -depth values and still be sure all pages will be crawled?</p>\n", "creation_date": 1476188689, "score": 0},
{"title": "Nutch &amp; Solr; remove pages blocked by regex-urlfilter", "view_count": 13, "is_answered": false, "question_id": 39960834, "tags": ["solr", "nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/39960834/nutch-solr-remove-pages-blocked-by-regex-urlfilter", "last_activity_date": 1476110390, "owner": {"user_id": 835589, "answer_count": 4, "creation_date": 1310136962, "accept_rate": 75, "view_count": 19, "location": "Massachusetts", "reputation": 187}, "body": "<p>I am using nutch 1.4 with Solr 4.10.3 to index a series of sites for search.  I am using the solrdelete command to remove any old items marked DB_GONE.</p>\n\n<p>After the initial crawl, I added a new pattern to my regex-urlfilter.txt designed to prevent indexing of some content:</p>\n\n<pre><code>-^http://www.example.com/nofollow/.*\n</code></pre>\n\n<p>I also made changes to my seeds, triggering its re-crawl.  When the crawl was finished, I was still seeing my <a href=\"http://www.example.com/nofollow/\" rel=\"nofollow\">http://www.example.com/nofollow/</a>* pages in my index.</p>\n\n<p>How do I get Nutch to see these pages as 'missing' so they will be deleted from the index?</p>\n", "creation_date": 1476110390, "score": 0},
{"title": "fetching a latest page from the site", "view_count": 20, "is_answered": false, "question_id": 39920580, "tags": ["nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/39920580/fetching-a-latest-page-from-the-site", "last_activity_date": 1475853176, "owner": {"user_id": 5962670, "view_count": 1, "answer_count": 0, "creation_date": 1456140319, "reputation": 1}, "body": "<p>I am trying to configure nutch to pick the latest page form the site.</p>\n\n<p>we do continuous nutch crawling to crawl the site, I want to setup an event when ever some one updates an existing page, nutch crawl should replace old page with the new updated one.</p>\n\n<p>that is not happening as a workaround we are deleting old crawl data and doing a re-crawling to pick new changes.</p>\n\n<p>We tried by putting the below things in nutch-site.xml file but didnot help.</p>\n\n<pre><code>&lt;name&gt;db.fetch.schedule.class&lt;/name&gt;\n  &lt;value&gt;org.apache.nutch.crawl.AdaptiveFetchSchedule&lt;/value&gt;\n  &lt;description&gt;The implementation of fetch schedule. DefaultFetchSchedule simply\n  adds the original fetchInterval to the last fetch time, regardless of\n  page changes.&lt;/description&gt;\n&lt;/property&gt;\n\n&lt;property&gt;\n  &lt;name&gt;db.fetch.schedule.adaptive.inc_rate&lt;/name&gt;\n  &lt;value&gt;0.4&lt;/value&gt;\n  &lt;description&gt;If a page is unmodified, its fetchInterval will be\n  increased by this rate. This value should not\n  exceed 0.5, otherwise the algorithm becomes unstable.&lt;/description&gt;\n&lt;/property&gt;\n\n&lt;property&gt;\n  &lt;name&gt;db.fetch.schedule.adaptive.dec_rate&lt;/name&gt;\n  &lt;value&gt;0.2&lt;/value&gt;\n  &lt;description&gt;If a page is modified, its fetchInterval will be\n  decreased by this rate. This value should not\n  exceed 0.5, otherwise the algorithm becomes unstable.&lt;/description&gt;\n&lt;/property&gt;\n\n&lt;property&gt;\n  &lt;name&gt;db.fetch.schedule.adaptive.min_interval&lt;/name&gt;\n  &lt;value&gt;60.0&lt;/value&gt;\n  &lt;description&gt;Minimum fetchInterval, in seconds.&lt;/description&gt;\n&lt;/property&gt;\n\n&lt;property&gt;\n  &lt;name&gt;db.fetch.schedule.adaptive.max_interval&lt;/name&gt;\n  &lt;value&gt;31536000.0&lt;/value&gt;\n  &lt;description&gt;Maximum fetchInterval, in seconds (365 days).\n  NOTE: this is limited by db.fetch.interval.max. Pages with\n  fetchInterval larger than db.fetch.interval.max\n  will be fetched anyway.&lt;/description&gt;\n&lt;/property&gt;\n</code></pre>\n", "creation_date": 1475853176, "score": 0},
{"title": "SolrIndexerJob: runtime error", "view_count": 45, "is_answered": false, "question_id": 39916308, "tags": ["java", "mongodb", "elasticsearch", "solr", "nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/39916308/solrindexerjob-runtime-error", "last_activity_date": 1475843200, "owner": {"user_id": 5152507, "view_count": 0, "answer_count": 0, "creation_date": 1437743998, "reputation": 11}, "body": "<p>I'm trying to build a web crawler using Nutch 2.3 + Mongodb+ elasticsearch 1.7. I've configured mongodb store in nutch and it works perfectly. However when I run</p>\n\n<pre><code>./bin/nutch index -all\n</code></pre>\n\n<p>I get </p>\n\n<pre><code> IndexingJob: starting\n SolrIndexerJob: java.lang.RuntimeException: job failed: name=apache-nutch-2.3.1.jar, jobid=job_local2085212843_0001\nat org.apache.nutch.util.NutchJob.waitForCompletion(NutchJob.java:119)\nat org.apache.nutch.indexer.IndexingJob.run(IndexingJob.java:154)\nat org.apache.nutch.indexer.IndexingJob.index(IndexingJob.java:176)\nat org.apache.nutch.indexer.IndexingJob.run(IndexingJob.java:202)\nat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)\nat org.apache.nutch.indexer.IndexingJob.main(IndexingJob.java:211)\n</code></pre>\n\n<p>But I'm  not even using Solr. My nutch-site.xml is configured for elastic search. \nnutch-site.xml</p>\n\n<pre><code>&lt;?xml version=\"1.0\"?&gt;\n&lt;?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?&gt;\n&lt;!-- Put site-specific property overrides in this file. --&gt;\n\n&lt;configuration&gt;\n&lt;property&gt;\n    &lt;name&gt;http.agent.name&lt;/name&gt; \n    &lt;value&gt;Nofrets Cwawler&lt;/value&gt;\n&lt;/property&gt;\n&lt;property&gt;\n    &lt;name&gt;plugin.includes&lt;/name&gt;\n    &lt;value&gt;protocol-http|urlfilter-regex|parse-(html|tika)|index-       (basic|anchor)|urlnormalizer-(pass|regex|basic)|scoring-opic|indexer-elastic&lt;/value&gt;\n&lt;/property&gt;\n&lt;property&gt;\n    &lt;name&gt;storage.data.store.class&lt;/name&gt;\n    &lt;value&gt;org.apache.gora.mongodb.store.MongoStore&lt;/value&gt;\n&lt;/property&gt;\n\n&lt;!--elastic search properties --&gt;\n&lt;property&gt;\n&lt;name&gt;elastic.host&lt;/name&gt;\n&lt;value&gt;localhost&lt;/value&gt;\n&lt;description&gt;The hostname to send documents to using TransportClient.\nEither host and port must be defined or cluster.\n&lt;/description&gt;\n&lt;/property&gt;\n&lt;property&gt;\n&lt;name&gt;elastic.port&lt;/name&gt;\n&lt;value&gt;9200&lt;/value&gt;\n&lt;description&gt;\nThe port to connect to using TransportClient.\n&lt;/description&gt;\n&lt;/property&gt;\n\n&lt;property&gt;\n&lt;name&gt;elastic.cluster&lt;/name&gt;\n&lt;value&gt;elasticsearch&lt;/value&gt;\n&lt;description&gt;The cluster name to discover. Either host and potr must\n be defined or cluster.\n&lt;/description&gt;\n&lt;/property&gt;\n&lt;property&gt;\n&lt;name&gt;elastic.index&lt;/name&gt;\n&lt;value&gt;nutch&lt;/value&gt;\n&lt;description&gt;\nThe name of the elasticsearch index. Will normally be autocreated if it\ndoesn't exist.\n&lt;/description&gt;\n&lt;/property&gt;\n\n&lt;property&gt;\n&lt;name&gt;elastic.max.bulk.docs&lt;/name&gt;\n&lt;value&gt;10&lt;/value&gt;\n&lt;description&gt;\n  The number of docs in the batch that will trigger a flush to\n  elasticsearch.\n  &lt;/description&gt;\n&lt;/property&gt;\n&lt;property&gt;\n&lt;name&gt;elastic.max.bulk.size&lt;/name&gt;\n&lt;value&gt;2500&lt;/value&gt;\n&lt;description&gt;\nThe total length of all indexed text in a batch that will trigger a\nflush to elasticsearch, by checking after every document for excess \nof this amount.\n&lt;/description&gt;\n&lt;/property&gt;\n</code></pre>\n\n<p></p>\n\n<p>Any help will be appreciated. Thanks.</p>\n", "creation_date": 1475840249, "score": 0},
{"title": "How to read data from nutch crawled data", "view_count": 373, "is_answered": false, "answers": [{"question_id": 26099908, "owner": {"user_id": 3230293, "link": "http://stackoverflow.com/users/3230293/tienbm", "user_type": "registered", "reputation": 1}, "body": "<p>By default, Hbase don't understand what type of data is stored. I think you can use <em>readdb</em> command (<a href=\"http://wiki.apache.org/nutch/bin/nutch%20readdb\" rel=\"nofollow\">http://wiki.apache.org/nutch/bin/nutch%20readdb</a> ) in  HBase shell to extract data from webpage table.</p>\n", "creation_date": 1412301771, "is_accepted": false, "score": 0, "last_activity_date": 1412301771, "answer_id": 26172281}, {"question_id": 26099908, "owner": {"user_id": 600009, "accept_rate": 43, "link": "http://stackoverflow.com/users/600009/manohar-tn", "user_type": "registered", "reputation": 41}, "body": "<p>You can use nutch dump command</p>\n\n<p>Here is the syntax:</p>\n\n<p>bin/nutch dump -outputDir  -segment </p>\n\n<p>Example: bin/nutch dump -outputDir /tmp/tt03 -segment crawl/crawldb/seg \nments</p>\n", "creation_date": 1444007695, "is_accepted": false, "score": 0, "last_activity_date": 1444007695, "answer_id": 32940047}, {"question_id": 26099908, "owner": {"user_id": 6678568, "link": "http://stackoverflow.com/users/6678568/dmitry-belokur", "user_type": "registered", "reputation": 27}, "body": "<p>As you certainly know, crawled data are stored in segments. You can actually extract those data using <b>readseg</b> command (for example):</p>\n\n<pre><code>bin/nutch readseg -dump /work/apache-nutch-1.12/crawl/segments/20161005134205 my_dump_dir\n</code></pre>\n\n<p>where <b>my_dump_dir</b> is your directory, which will be created and contain dump.</p>\n\n<p>Then in your my_dump_dir you'll find two files: <b>dump</b> (contains crawled and parsed data - non_encoded), and  <b>.dump.crc</b> (i guess some binary). They view <b>dump</b> with any text editor and view the structure. You can also parse it, if you need.</p>\n", "creation_date": 1475838938, "is_accepted": false, "score": 0, "last_activity_date": 1475838938, "answer_id": 39915889}], "question_id": 26099908, "tags": ["hbase", "nutch"], "answer_count": 3, "link": "http://stackoverflow.com/questions/26099908/how-to-read-data-from-nutch-crawled-data", "last_activity_date": 1475838938, "owner": {"user_id": 3454410, "answer_count": 20, "creation_date": 1395646154, "accept_rate": 48, "view_count": 174, "reputation": 832}, "body": "<p>Apache nutch crawled some data that was saved in hbase. Those data contains some pdfs files. Now I want to extract those file. how I can do that it</p>\n", "creation_date": 1411993569, "score": 0},
{"title": "How to run nutch in production enviornment", "view_count": 37, "owner": {"user_id": 4327283, "answer_count": 40, "creation_date": 1417755361, "accept_rate": 100, "view_count": 69, "location": "Cochin,Kerala,India", "reputation": 651}, "is_answered": true, "answers": [{"question_id": 39652894, "owner": {"user_id": 4327283, "accept_rate": 100, "link": "http://stackoverflow.com/users/4327283/sachin", "user_type": "registered", "reputation": 651}, "body": "<p>As sujen stated, there are two options for this :-</p>\n\n<ol>\n<li>Use REST api if you want to submit crawl requests to nutch remotely.\nSteps to get this running is described here :-</li>\n</ol>\n\n<p><a href=\"https://stackoverflow.com/questions/39761712/how-to-run-nutch-server-on-distributed-environment\">How to run nutch server on distributed environment</a></p>\n\n<ol start=\"2\">\n<li>Otherwise you can run bin/crawl script from runtime/deploy to launch requests to nutch distributed using hadoop.</li>\n</ol>\n", "creation_date": 1475833969, "is_accepted": true, "score": 1, "last_activity_date": 1475833969, "answer_id": 39914386}], "question_id": 39652894, "tags": ["nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/39652894/how-to-run-nutch-in-production-enviornment", "last_activity_date": 1475833969, "accepted_answer_id": 39914386, "body": "<p>I was experimenting some crawl cycles with nutch and would like to setup a distributed crawl environment. But I wonder how can I trigger nutch for incoming crawl requests in a production system. I read about nutch REST api. Is that the real option that I have ? Or can I run nutch as a continuously running distributed server by any other option ? </p>\n\n<p>My preferred nutch version is nutch 1.12.</p>\n", "creation_date": 1474606664, "score": 2},
{"title": "How to run nutch server on distributed environment", "view_count": 37, "owner": {"user_id": 4327283, "answer_count": 40, "creation_date": 1417755361, "accept_rate": 100, "view_count": 69, "location": "Cochin,Kerala,India", "reputation": 651}, "is_answered": true, "answers": [{"question_id": 39761712, "owner": {"user_id": 4327283, "accept_rate": 100, "link": "http://stackoverflow.com/users/4327283/sachin", "user_type": "registered", "reputation": 651}, "body": "<p>From further research I've got nutch server working on distributed mode.</p>\n\n<p>Steps :-</p>\n\n<ol>\n<li>Assume hadoop is configured in all slave nodes. Then setup nutch in all nodes. This can help : <a href=\"http://wiki.apache.org/nutch/NutchHadoopTutorial\" rel=\"nofollow\">http://wiki.apache.org/nutch/NutchHadoopTutorial</a></li>\n<li>On your namenode, <code>cd $NUTCH_HOME/runtime/deploy</code></li>\n<li><code>bin/nutch startserver -port &lt;port&gt; -host &lt;host&gt;</code>\nNote :Port and host are optional.</li>\n<li>Then you can submit requests from nutch using REST. The requests you submit will be accepted by nutch server started on step 3.</li>\n</ol>\n\n<p>Happy crawling :)</p>\n", "creation_date": 1475833307, "is_accepted": true, "score": 1, "last_activity_date": 1475833307, "answer_id": 39914166}], "question_id": 39761712, "tags": ["nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/39761712/how-to-run-nutch-server-on-distributed-environment", "last_activity_date": 1475833307, "accepted_answer_id": 39914166, "body": "<p>I have tested running of nutch in server mode by starting it using bin/nutch startserver command <em>locally</em>. Now I wonder whether I can start nutch in <em>server mode</em> on top of a hadoop cluster(in distributed environment) and submit crawl requests to server using nutch REST api ?\nPlease help.</p>\n", "creation_date": 1475124986, "score": 1},
{"title": "Custom options in nutch crawl script not working", "view_count": 25, "owner": {"user_id": 4327283, "answer_count": 40, "creation_date": 1417755361, "accept_rate": 100, "view_count": 69, "location": "Cochin,Kerala,India", "reputation": 651}, "is_answered": true, "answers": [{"question_id": 39768621, "owner": {"user_id": 4327283, "accept_rate": 100, "link": "http://stackoverflow.com/users/4327283/sachin", "user_type": "registered", "reputation": 651}, "body": "<p>From further study I got the answer myselves. First command didn't work because hadoop doesn't detect it as an argument. It should be read from nutch config itselves.</p>\n", "creation_date": 1475832740, "is_accepted": true, "score": 2, "last_activity_date": 1475832740, "answer_id": 39913995}], "question_id": 39768621, "tags": ["elasticsearch", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/39768621/custom-options-in-nutch-crawl-script-not-working", "last_activity_date": 1475832740, "accepted_answer_id": 39913995, "body": "<p>I was trying to give custom options in <strong><em>bin/crawl</em></strong> script and encountered an issue. I gave a custom config in nutch to ignore external outlinks in my crawl command like :-</p>\n\n<p><strong><em>bin/crawl -i -D elastic.index=test -D db.ignore.external.links=true urls/ CrawlTest/ 3</em></strong></p>\n\n<p>But this is not working. Then I set this property in nutch-site.xml then it is working. </p>\n\n<p>Then I tried to set a custom config to index data to a specific elastic index other than what is given in nutch-site.xml as java option in bin/crawl. To my surprise it is working.\nThe command I've used :-</p>\n\n<p><strong><em>bin/crawl -i -D elastic.index=test urls/ CrawlTest/ 3</em></strong></p>\n\n<p>So I would like to know why my first command didn't work ?Am I missing anything. Please help.</p>\n", "creation_date": 1475147327, "score": 2},
{"title": "Apache Nutch 2.3.1 plugin invalid UTF-8 character exception", "view_count": 13, "is_answered": false, "question_id": 39886901, "tags": ["java", "exception", "solr", "nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/39886901/apache-nutch-2-3-1-plugin-invalid-utf-8-character-exception", "last_activity_date": 1475723334, "owner": {"user_id": 3454410, "answer_count": 20, "creation_date": 1395646154, "accept_rate": 48, "view_count": 174, "reputation": 832}, "body": "<p>I have written a simple nutch plugin that takes text as input and if unicodes are not in a certain range then remove those character. This is the respective plugin code</p>\n\n<pre><code>    // content is indexed, so that it's searchable, but not stored in index\n    String cnt = TableUtil.toString(page.getText());\n    String baseURL = TableUtil.toString(page.getBaseUrl());\n    String baseURL2 = TableUtil.toString(page.getReprUrl());\n\n    //LOG.info(\"UrduParser invoked\");\n   // System.out.println(\"Urdu Parser plugin called\");\n   cnt = cnt.replaceAll( \"([\\\\ud800-\\\\udbff\\\\udc00-\\\\udfff])\", \"\");\n    cnt = UrduParser(cnt);\n    int cnt_len = cnt.length();\n\n    if(cnt_len &lt;= MIN_CONTENT_LENGTH)\n        {\n\n           LOG.warn(\"ShortLengthException: skipping the document Length:\" + cnt_len + \" URL:\" + baseURL + \" URL2:\" + baseURL2);\n           //System.out.println(\"ContentShortLengthException: skipping the document\");\n           return null;\n        }\n    doc.add(\"content_urdu\", cnt);\n</code></pre>\n\n<p>When I run the plugin to index some data in apache solr, following exception occured,</p>\n\n<pre><code>org.apache.solr.client.solrj.impl.HttpSolrServer$RemoteSolrException: Invalid UTF-8 character 0xffff at char #1088905, byte #1804287)\n    at org.apache.solr.client.solrj.impl.HttpSolrServer.request(HttpSolrServer.java:491)\n    at org.apache.solr.client.solrj.impl.HttpSolrServer.request(HttpSolrServer.java:197)\n    at org.apache.solr.client.solrj.request.AbstractUpdateRequest.process(AbstractUpdateRequest.java:117)\n    at org.apache.solr.client.solrj.SolrServer.add(SolrServer.java:68)\n    at org.apache.solr.client.solrj.SolrServer.add(SolrServer.java:54)\n    at org.apache.nutch.indexwriter.solr.SolrIndexWriter.write(SolrIndexWriter.java:84)\n    at org.apache.nutch.indexer.IndexWriters.write(IndexWriters.java:84)\n    at org.apache.nutch.indexer.IndexerOutputFormat$1.write(IndexerOutputFormat.java:48)\n    at org.apache.nutch.indexer.IndexerOutputFormat$1.write(IndexerOutputFormat.java:43)\n    at org.apache.hadoop.mapred.MapTask$NewDirectOutputCollector.write(MapTask.java:638)\n    at org.apache.hadoop.mapreduce.TaskInputOutputContext.write(TaskInputOutputContext.java:80)\n    at org.apache.nutch.indexer.IndexingJob$IndexerMapper.map(IndexingJob.java:120)\n    at org.apache.nutch.indexer.IndexingJob$IndexerMapper.map(IndexingJob.java:69)\n    at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:145)\n    at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:764)\n    at org.apache.hadoop.mapred.MapTask.run(MapTask.java:364)\n    at org.apache.hadoop.mapred.Child$4.run(Child.java:255)\n    at java.security.AccessController.doPrivileged(Native Method)\n    at javax.security.auth.Subject.doAs(Subject.java:422)\n    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)\n    at org.apache.hadoop.mapred.Child.main(Child.java:249)\n</code></pre>\n\n<p>Where is the problem is string parsing? I think my check to replace a character in above code not not correct. Where I have to change ?</p>\n", "creation_date": 1475723334, "score": 0},
{"title": "Error : while start Apache nutch with mongodb", "view_count": 25, "is_answered": false, "answers": [{"question_id": 39806842, "owner": {"user_id": 2176092, "link": "http://stackoverflow.com/users/2176092/joe-crane", "user_type": "registered", "reputation": 8}, "body": "<p>I believe the newest version of Nutch uses </p>\n\n<pre><code># bin/nutch inject seedDirectory/\n</code></pre>\n\n<p>This worked for me at least.</p>\n", "creation_date": 1475707390, "is_accepted": false, "score": 0, "last_activity_date": 1475707390, "answer_id": 39884807}], "question_id": 39806842, "tags": ["mongodb", "apache", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/39806842/error-while-start-apache-nutch-with-mongodb", "last_activity_date": 1475707390, "owner": {"user_id": 6282648, "view_count": 9, "answer_count": 0, "creation_date": 1462215063, "reputation": 21}, "body": "<p>when i try to run, following error comes, In mongodb logs one connection accepted and end immediately.how solve it?<br> <br>\n    $ bin/crawl conf/urls/seeds.txt tuto 1\n    No SOLRURL specified. Skipping indexing.\n    Injecting seed URLs\n    /e/apache-nutch/apache-nutch-2.3.1/runtime/local/bin/nutch inject conf/urls/seeds.txt -crawlId tuto\n    InjectorJob: starting at 2016-10-01 18:15:14\n    InjectorJob: Injecting urlDir: conf/urls/seeds.txt\n    InjectorJob: Using class org.apache.gora.mongodb.store.MongoStore as the Gora storage class.\n    InjectorJob: java.lang.NullPointerException\n            at java.lang.ProcessBuilder.start(ProcessBuilder.java:1010)\n            at org.apache.hadoop.util.Shell.runCommand(Shell.java:482)\n            at org.apache.hadoop.util.Shell.run(Shell.java:455)\n            at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:702)\n            at org.apache.hadoop.util.Shell.execCommand(Shell.java:791)\n            at org.apache.hadoop.util.Shell.execCommand(Shell.java:774)\n            at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:646)\n            at org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:434)\n            at org.apache.hadoop.fs.FilterFileSystem.mkdirs(FilterFileSystem.java:281)\n            at org.apache.hadoop.mapreduce.JobSubmissionFiles.getStagingDir(JobSubmissionFiles.java:125)\n            at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:348)\n            at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1285)\n            at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1282)\n            at java.security.AccessController.doPrivileged(Native Method)\n            at javax.security.auth.Subject.doAs(Subject.java:415)\n            at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1614)\n            at org.apache.hadoop.mapreduce.Job.submit(Job.java:1282)\n            at org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:1303)\n            at org.apache.nutch.util.NutchJob.waitForCompletion(NutchJob.java:115)\n            at org.apache.nutch.crawl.InjectorJob.run(InjectorJob.java:231)\n            at org.apache.nutch.crawl.InjectorJob.inject(InjectorJob.java:252)\n            at org.apache.nutch.crawl.InjectorJob.run(InjectorJob.java:275)\n            at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)\n            at org.apache.nutch.crawl.InjectorJob.main(InjectorJob.java:284)</p>\n", "creation_date": 1475326451, "score": 2},
{"title": "Nutch Elasticsearch Integration", "view_count": 219, "is_answered": false, "question_id": 37052674, "tags": ["elasticsearch", "nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/37052674/nutch-elasticsearch-integration", "last_activity_date": 1475706241, "owner": {"user_id": 1162512, "answer_count": 27, "creation_date": 1327163131, "accept_rate": 58, "view_count": 772, "reputation": 2192}, "body": "<p>I'm following <a href=\"http://www.aossama.com/search-engine-with-apache-nutch-mongodb-and-elasticsearch/\" rel=\"nofollow\">this</a> tutorial for setting up nutch alongwith Elasticsearch. Whenever I try to index the data into the ES, it returns an error. Following are the logs:-</p>\n\n<p><strong>Command:-</strong></p>\n\n<pre><code>bin/nutch index elasticsearch -all\n</code></pre>\n\n<p>Logs when I add <code>elastic.port</code>(9200) in <code>conf/nutch-site.xml</code> :-</p>\n\n<pre><code>2016-05-05 13:22:49,903 INFO  basic.BasicIndexingFilter - Maximum title length for indexing set to: 100\n2016-05-05 13:22:49,904 INFO  indexer.IndexingFilters - Adding org.apache.nutch.indexer.basic.BasicIndexingFilter\n2016-05-05 13:22:49,904 INFO  anchor.AnchorIndexingFilter - Anchor deduplication is: off\n2016-05-05 13:22:49,904 INFO  indexer.IndexingFilters - Adding org.apache.nutch.indexer.anchor.AnchorIndexingFilter\n2016-05-05 13:22:49,905 INFO  indexer.IndexingFilters - Adding org.apache.nutch.indexer.metadata.MetadataIndexer\n2016-05-05 13:22:49,906 INFO  indexer.IndexingFilters - Adding org.apache.nutch.indexer.more.MoreIndexingFilter\n2016-05-05 13:22:49,961 INFO  elastic.ElasticIndexWriter - Processing remaining requests [docs = 0, length = 0, total docs = 0]\n2016-05-05 13:22:49,961 INFO  elastic.ElasticIndexWriter - Processing to finalize last execute\n2016-05-05 13:22:54,898 INFO  client.transport - [Peggy Carter] failed to get node info for [#transport#-1][ubuntu][inet[localhost/127.0.0.1:9200]], disconnecting...\norg.elasticsearch.transport.ReceiveTimeoutTransportException: [][inet[localhost/127.0.0.1:9200]][cluster:monitor/nodes/info] request_id [1] timed out after [5000ms]\n        at org.elasticsearch.transport.TransportService$TimeoutHandler.run(TransportService.java:366)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at java.lang.Thread.run(Thread.java:745)\n2016-05-05 13:22:55,682 INFO  indexer.IndexWriters - Adding org.apache.nutch.indexwriter.elastic.ElasticIndexWriter\n2016-05-05 13:22:55,683 INFO  indexer.IndexingJob - Active IndexWriters :\nElasticIndexWriter\n        elastic.cluster : elastic prefix cluster\n        elastic.host : hostname\n        elastic.port : port  (default 9300)\n        elastic.index : elastic index command \n        elastic.max.bulk.docs : elastic bulk index doc counts. (default 250) \n        elastic.max.bulk.size : elastic bulk index length. (default 2500500 ~2.5MB)\n\n\n2016-05-05 13:22:55,711 INFO  elasticsearch.plugins - [Adrian Toomes] loaded [], sites []\n2016-05-05 13:23:00,763 INFO  client.transport - [Adrian Toomes] failed to get node info for [#transport#-1][ubuntu][inet[localhost/127.0.0.1:92$0]], disconnecting...\norg.elasticsearch.transport.ReceiveTimeoutTransportException: [][inet[localhost/127.0.0.1:9200]][cluster:monitor/nodes/info] request_id [0] time$ out after [5000ms]\n        at org.elasticsearch.transport.TransportService$TimeoutHandler.run(TransportService.java:366)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at java.lang.Thread.run(Thread.java:745)\n2016-05-05 13:23:00,766 INFO  indexer.IndexingJob - IndexingJob: done.\n</code></pre>\n\n<p>Logs when default port 9300 is used:-</p>\n\n<pre><code>2016-05-05 13:58:44,584 INFO  elasticsearch.plugins - [Mentallo] loaded [], sites []\n2016-05-05 13:58:44,673 WARN  transport.netty - [Mentallo] Message not fully read (response) for [0] handler future(org.elasticsearch.client.transport.TransportClientNodesService$SimpleNodeSampler$1@3c80f1dd), error [true], resetting\n2016-05-05 13:58:44,674 INFO  client.transport - [Mentallo] failed to get node info for [#transport#-1][ubuntu][inet[localhost/127.0.0.1:9300]], disconnecting...\norg.elasticsearch.transport.RemoteTransportException: Failed to deserialize exception response from stream\nCaused by: org.elasticsearch.transport.TransportSerializationException: Failed to deserialize exception response from stream\n        at org.elasticsearch.transport.netty.MessageChannelHandler.handlerResponseError(MessageChannelHandler.java:173)\n        at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:125)\n        at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)\n        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)\n        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)\n        at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:296)\n        at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)\n        at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)\n        at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)\n        at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)\n        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)\n        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)\n        at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)\n        at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)\n        at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)\n        at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)\n        at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)\n        at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)\n        at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)\n        at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)\n        at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at java.lang.Thread.run(Thread.java:745)\nCaused by: java.io.StreamCorruptedException: Unsupported version: 1\n        at org.elasticsearch.common.io.ThrowableObjectInputStream.readStreamHeader(ThrowableObjectInputStream.java:46)\n        at java.io.ObjectInputStream.&lt;init&gt;(ObjectInputStream.java:301)\n        at org.elasticsearch.common.io.ThrowableObjectInputStream.&lt;init&gt;(ThrowableObjectInputStream.java:38)\n        at org.elasticsearch.transport.netty.MessageChannelHandler.handlerResponseError(MessageChannelHandler.java:170)\n        ... 23 more\n2016-05-05 13:58:44,676 INFO  indexer.IndexingJob - IndexingJob: done.\n</code></pre>\n\n<p>I've configured everything fine. Have had a look at various threads as well but to no avail. Also java version for both ES and JVM is same. Is there a bug in here?</p>\n\n<p>I'm using Nutch 2.3.1 and have tried with both ES 1.4.4 and 2.3.2. I can see data in Mongo but I cannot index data in ES. Why?? </p>\n", "creation_date": 1462456977, "score": 0},
{"title": "Nutch 2.3.1 hangs while running inject", "view_count": 15, "is_answered": false, "question_id": 39859550, "tags": ["hbase", "nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/39859550/nutch-2-3-1-hangs-while-running-inject", "last_activity_date": 1475677499, "owner": {"user_id": 6922645, "view_count": 0, "answer_count": 0, "creation_date": 1475606018, "reputation": 6}, "body": "<p>Nutch 2.3.1 hangs while running inject command.</p>\n\n<pre><code>./nutch inject seed/\n\nInjectorJob: starting at 2016-10-04 11:03:52 \nInjectorJob: Injecting urlDir: seed\n</code></pre>\n\n<p>Nothing happens after this &amp; no exception are available in logs. Please suggest to make it work.</p>\n", "creation_date": 1475606579, "score": 1},
{"title": "Apache Nutch 2.3.1 remote command failed", "view_count": 21, "is_answered": true, "answers": [{"question_id": 39363802, "owner": {"user_id": 5676586, "accept_rate": 100, "link": "http://stackoverflow.com/users/5676586/abhishek", "user_type": "registered", "reputation": 447}, "body": "<p>It's the timeout exception in execute job in <strong>RemoteCommandExecuter</strong>.java</p>\n\n<p>Try increasing the maximum timeout to wait to get result. It waits for at most the given time for the computation to complete, and then retrieves the result.</p>\n\n<pre><code>private static final int DEFAULT_TIMEOUT_SEC = 60;\n\n    public JobInfo executeRemoteJob(RemoteCommand command) {\n        try {\n          String jobId = client.executeJob(command.getJobConfig());\n          Future&lt;JobInfo&gt; chekerFuture = executor\n              .submit(new JobStateChecker(jobId));\n          return chekerFuture.get(getTimeout(command), TimeUnit.MILLISECONDS);\n        } catch (Exception e) {\n          log.error(\"Remote command failed\", e);\n          JobInfo jobInfo = new JobInfo();\n          jobInfo.setState(State.FAILED);\n          jobInfo.setMsg(ExceptionUtils.getStackTrace(e));\n          return jobInfo;\n        }\n      }\n\n    private long getTimeout(RemoteCommand command) {\n        if (command.getTimeout() == null) {\n          return DEFAULT_TIMEOUT_SEC * DateTimeConstants.MILLIS_PER_SECOND;\n        }\n        return command.getTimeout().getMillis();\n      }\n</code></pre>\n\n<p>Change <code>DEFAULT_TIMEOUT_SEC</code> to a higher value.</p>\n", "creation_date": 1475672664, "is_accepted": false, "score": 2, "last_activity_date": 1475672664, "answer_id": 39874772}], "question_id": 39363802, "tags": ["java", "hadoop", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/39363802/apache-nutch-2-3-1-remote-command-failed", "last_activity_date": 1475672664, "owner": {"user_id": 3454410, "answer_count": 20, "creation_date": 1395646154, "accept_rate": 48, "view_count": 174, "reputation": 832}, "body": "<p>I have complete setup of nutch  with hadoop and hbase. If I run a job via command line (terminal) if works successfully. But When I want to run same command via nutch wepapp server following exception occured.</p>\n\n<pre><code>2016-09-07 12:25:31,800 ERROR impl.RemoteCommandExecutor - Remote command failed\njava.util.concurrent.TimeoutException\n    at java.util.concurrent.FutureTask.get(FutureTask.java:205)\n    at org.apache.nutch.webui.client.impl.RemoteCommandExecutor.executeRemoteJob(RemoteCommandExecutor.java:61)\n    at org.apache.nutch.webui.client.impl.CrawlingCycle.executeCrawlCycle(CrawlingCycle.java:58)\n    at org.apache.nutch.webui.service.impl.CrawlServiceImpl.startCrawl(CrawlServiceImpl.java:69)\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n    at java.lang.reflect.Method.invoke(Method.java:483)\n    at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:317)\n    at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:190)\n    at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157)\n    at org.springframework.aop.interceptor.AsyncExecutionInterceptor$1.call(AsyncExecutionInterceptor.java:97)\n    at java.util.concurrent.FutureTask.run(FutureTask.java:266)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n    at java.lang.Thread.run(Thread.java:745)\n2016-09-07 12:25:31,850 INFO  impl.CrawlingCycle - Executed remote command data: INJECT status: FAILED\n</code></pre>\n\n<p>I have started tow services for app server i.e. <code>nutchserver</code> and <code>webapp</code>. I have run these services in both user mode and root user. but same result.</p>\n", "creation_date": 1473233459, "score": 1},
{"title": "Configuring Nutch 2.3 with HSQL 2.3.3 - ClassNotFoundException : org/apache/avro/ipc/ByteBufferOutputStream", "view_count": 189, "is_answered": false, "answers": [{"question_id": 34472778, "owner": {"user_id": 582789, "link": "http://stackoverflow.com/users/582789/alfonso-nishikawa", "user_type": "registered", "reputation": 1263}, "body": "<p>Gora-sql is not supported. Due some licenses issues (if I am not wrong), it became disabled around Gora 0.2.</p>\n\n<p>So I suggest you to use other storage like, for example, HBase.</p>\n\n<p>How to get HBase up&amp;running fast: read answer at <a href=\"http://stackoverflow.com/a/39837926/582789\">http://stackoverflow.com/a/39837926/582789</a></p>\n", "creation_date": 1475616036, "is_accepted": false, "score": 0, "last_activity_date": 1475616036, "answer_id": 39861956}], "question_id": 34472778, "tags": ["hsqldb", "nutch", "avro", "gora"], "answer_count": 1, "link": "http://stackoverflow.com/questions/34472778/configuring-nutch-2-3-with-hsql-2-3-3-classnotfoundexception-org-apache-avro", "last_activity_date": 1475616036, "owner": {"user_id": 3538426, "view_count": 1, "answer_count": 1, "creation_date": 1397616350, "reputation": 6}, "body": "<p>I'm getting ClassNotFoundException : org/apache/avro/ipc/ByteBufferOutputStream when I run apache Nutch with HSQLDB although I have all the avro related jar files under lib</p>\n\n<pre><code>    avro-1.7.6.jar\n    avro-compiler-1.7.6.jar\n    avro-ipc-1.7.6.jar\n    avro-mapred-1.7.6.jar\n</code></pre>\n\n<p>This is what I did:</p>\n\n<ol>\n<li><p>Got HSQLDB up and running</p>\n\n<pre><code>root@elephant hsqldb#  sudo java -cp /home/hsqldb/hsqldb-2.3.3/hsqldb/lib/hsqldb.jar org.hsqldb.server.Server --props /home/hsqldb/hsqldb-2.3.3/hsqldb/conf/server.properties\n[Server@372f7a8d]: [Thread[main,5,main]]: checkRunning(false) entered\n[Server@372f7a8d]: [Thread[main,5,main]]: checkRunning(false) exited\n[Server@372f7a8d]: Startup sequence initiated from main() method\n[Server@372f7a8d]: Loaded properties from [/home/hsqldb/hsqldb-2.3.3/hsqldb/conf/server.properties]\n[Server@372f7a8d]: Initiating startup sequence...\n[Server@372f7a8d]: Server socket opened successfully in 28 ms.\n[Server@372f7a8d]: Database [index=0, id=0, db=file:/home/hsqldb/hsqldb-2.3.3/hsqldb/data/nutch, alias=nutchdb] opened sucessfully in 1406 ms.\n[Server@372f7a8d]: Startup sequence completed in 1438 ms.\n[Server@372f7a8d]: 2015-12-26 18:30:13.841 HSQLDB server 2.3.3 is online on port 9001\n[Server@372f7a8d]: To close normally, connect and execute SHUTDOWN SQL\n[Server@372f7a8d]: From command line, use [Ctrl]+[C] to abort abruptly\n</code></pre></li>\n<li><p>Configured  ivy/ivy.xml</p></li>\n</ol>\n\n<p>uncommented below lines in ivy.xml  </p>\n\n<pre><code> &lt;dependency org=\"org.apache.gora\" name=\"gora-core\" rev=\"0.5\" conf=\"*-&gt;default\"/&gt; \n</code></pre>\n\n<p>and  </p>\n\n<pre><code>&lt;dependency org=\"org.apache.gora\" name=\"gora-sql\" rev=\"0.1.1-incubating\"\n   conf=\"*-&gt;default\" /&gt;\n</code></pre>\n\n<p>uncommented the below lines conf/gora.properites</p>\n\n<pre><code>###############################\n# Default SqlStore properties #\n###############################\n\ngora.sqlstore.jdbc.driver=org.hsqldb.jdbc.JDBCDriver\ngora.sqlstore.jdbc.url=jdbc:hsqldb:hsql://localhost/nutchdb\ngora.sqlstore.jdbc.user=sa\ngora.sqlstore.jdbc.password=\n</code></pre>\n\n<ol start=\"3\">\n<li><p>Ran ant build</p>\n\n<pre><code>ant runtime\n</code></pre></li>\n<li><p>Added configuration for nutch-site.xml</p>\n\n<pre><code>root@elephant conf# cat nutch-site.xml\n&lt;?xml version=\"1.0\"?&gt;\n&lt;?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?&gt;\n\n&lt;!-- Put site-specific property overrides in this file. --&gt;\n\n&lt;configuration&gt;\n\n        &lt;property&gt;\n                &lt;name&gt;storage.data.store.class&lt;/name&gt;\n                &lt;value&gt;org.apache.gora.sql.store.SqlStore&lt;/value&gt;\n        &lt;/property&gt;\n\n        &lt;property&gt;\n                &lt;name&gt;http.agent.name&lt;/name&gt;\n                &lt;value&gt;NutchCrawler&lt;/value&gt;\n        &lt;/property&gt;\n\n        &lt;property&gt;\n                &lt;name&gt;http.robots.agents&lt;/name&gt;\n                &lt;value&gt;NutchCrawler,*&lt;/value&gt;\n        &lt;/property&gt;\n\n&lt;/configuration&gt;\n</code></pre></li>\n<li><p>Created seed.txt under urls folder</p></li>\n<li><p>Executed the nutch by injecting the urls</p>\n\n<pre><code>[root@elephant local]# bin/nutch inject urls/\nInjectorJob: starting at 2015-12-26 19:11:24\nInjectorJob: Injecting urlDir: urls\nException in thread \"main\" java.lang.NoClassDefFoundError: org/apache/avro/ipc/ByteBufferOutputStream\nat java.lang.Class.forName0(Native Method)\nat java.lang.Class.forName(Class.java:259)\nat org.apache.nutch.storage.StorageUtils.getDataStoreClass(StorageUtils.java:93)\nat org.apache.nutch.storage.StorageUtils.createWebStore(StorageUtils.java:77)\nat org.apache.nutch.crawl.InjectorJob.run(InjectorJob.java:218)\nat org.apache.nutch.crawl.InjectorJob.inject(InjectorJob.java:252)\nat org.apache.nutch.crawl.InjectorJob.run(InjectorJob.java:275)\nat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\nat org.apache.nutch.crawl.InjectorJob.main(InjectorJob.java:284)\nCaused by: java.lang.ClassNotFoundException: org.apache.avro.ipc.ByteBufferOutputStream\nat java.net.URLClassLoader$1.run(URLClassLoader.java:372)\nat java.net.URLClassLoader$1.run(URLClassLoader.java:361)\nat java.security.AccessController.doPrivileged(Native Method)\nat java.net.URLClassLoader.findClass(URLClassLoader.java:360)\nat java.lang.ClassLoader.loadClass(ClassLoader.java:424)\nat sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)\nat java.lang.ClassLoader.loadClass(ClassLoader.java:357)\n... 9 more\n</code></pre></li>\n</ol>\n", "creation_date": 1451144465, "score": 1},
{"title": "Nutch 2.3.1 with Hadoop 2.5.2 configuration", "view_count": 20, "is_answered": false, "question_id": 39859604, "tags": ["hadoop", "nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/39859604/nutch-2-3-1-with-hadoop-2-5-2-configuration", "last_activity_date": 1475606783, "owner": {"user_id": 6922549, "view_count": 0, "answer_count": 0, "creation_date": 1475604829, "reputation": 6}, "body": "<ol>\n<li><p>I experienced with Nutch/MongoDB setup. But I cannot find instructions for Nutch 2.3.1 (Hadoop 2.5.2 data structure) configuration on Linux VM. Hadoop has been installed successfully. Please assist me.</p></li>\n<li><p>Can I avoid Hbase installation for Nutch_2.3.1/Hadoop_2.5.2 configuration?</p></li>\n</ol>\n\n<p>Regards,\nVictor</p>\n", "creation_date": 1475606783, "score": 1},
{"title": "Working of nutch server in distributed mode", "view_count": 43, "owner": {"user_id": 4327283, "answer_count": 40, "creation_date": 1417755361, "accept_rate": 100, "view_count": 69, "location": "Cochin,Kerala,India", "reputation": 651}, "is_answered": true, "answers": [{"question_id": 39853492, "owner": {"user_id": 2702341, "link": "http://stackoverflow.com/users/2702341/sujen-shah", "user_type": "registered", "reputation": 183}, "body": "<p>Nutch REST API is built using Apache CXF framework and JAX-RS. The Nutch Server uses an embedded Jetty Server to service the http requests. </p>\n\n<p>You can find out more about CXF and Jetty here (<a href=\"http://cxf.apache.org/docs/overview.html\" rel=\"nofollow\">http://cxf.apache.org/docs/overview.html</a>) </p>\n", "creation_date": 1475598345, "is_accepted": true, "score": 2, "last_activity_date": 1475598345, "answer_id": 39857389}], "question_id": 39853492, "tags": ["nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/39853492/working-of-nutch-server-in-distributed-mode", "last_activity_date": 1475598345, "accepted_answer_id": 39857389, "body": "<p>I would like to know how nutch server works actually in a distributed environment? Whether it use a listener for incoming crawl requests or it is a continuously running server?  </p>\n", "creation_date": 1475587286, "score": 2},
{"title": "Apache Nutch: FetcherJob throws NoSuchElementException deep in Gora", "view_count": 71, "owner": {"age": 43, "answer_count": 97, "creation_date": 1283955173, "user_id": 442512, "accept_rate": 93, "view_count": 648, "location": "Canada", "reputation": 10767}, "is_answered": true, "answers": [{"last_edit_date": 1475589215, "owner": {"user_id": 582789, "link": "http://stackoverflow.com/users/582789/alfonso-nishikawa", "user_type": "registered", "reputation": 1263}, "body": "<p>I confirm the problem is in MemStore.</p>\n\n<p>In 0.6.1 there is a bug:\n<a href=\"https://github.com/apache/gora/blob/apache-gora-0.6.1/gora-core/src/main/java/org/apache/gora/memory/store/MemStore.java#L128\" rel=\"nofollow\">https://github.com/apache/gora/blob/apache-gora-0.6.1/gora-core/src/main/java/org/apache/gora/memory/store/MemStore.java#L128</a></p>\n\n<p>That is already solved in master: <a href=\"https://github.com/apache/gora/blob/master/gora-core/src/main/java/org/apache/gora/memory/store/MemStore.java#L155\" rel=\"nofollow\">https://github.com/apache/gora/blob/master/gora-core/src/main/java/org/apache/gora/memory/store/MemStore.java#L155</a> , the access to #firstKey() has a guard #isEmpty()</p>\n\n<p><strike>BUT, don't try to update to Gora 0.7-SNAPSHOT because Nutch is not adapted to it by now.</strike></p>\n\n<h2>Edit</h2>\n\n<p>If you want to use Gora-0.7-SNAPSHOT with Nutch 2.x, maybe you could have it working doing this:</p>\n\n<ol>\n<li>Download Gora's master branch with version 0.7-SNAPSHOT</li>\n<li>Do <code>mvn install</code> in gora/ to install it in maven's local repository</li>\n<li>Apply this patch to Nutch: <a href=\"https://paste.apache.org/jjqz\" rel=\"nofollow\">https://paste.apache.org/jjqz</a> so Nutch 2.3.1 will work with Gora 0.7-SNAPSHOT</li>\n<li>Do Nutch's tutorial stuff</li>\n</ol>\n\n<p>I hope it works :)</p>\n\n<h2>Edit 2</h2>\n\n<p>About using HBase, it is quite easy to do a local installation for experimenting.</p>\n\n<ol>\n<li>As stated in <a href=\"https://wiki.apache.org/nutch/Nutch2Tutorial\" rel=\"nofollow\">Nutch2Tutorial</a>, download <a href=\"http://archive.apache.org/dist/hbase/hbase-0.98.8/hbase-0.98.8-hadoop2-bin.tar.gz\" rel=\"nofollow\">HBase 0.98.8-hadoop2</a></li>\n<li>Inflate the tar.gz file in a directory, for example: <code>/home/you/hbase</code></li>\n<li><code>cd /home/you/hbase/bin</code></li>\n<li><code>./start-hbase.sh</code></li>\n</ol>\n\n<p>Now you have HBase up&amp;running.\nConfigure Nutch:</p>\n\n<p>ivy/ivy.xml:\n  Look at @Emmanuel's comment about HBase's ivy dependence configuration.</p>\n\n<p>gora.properties:</p>\n\n<pre><code>gora.datastore.default=org.apache.gora.hbase.store.HBaseStore\ngora.datastore.autocreateschema=true\ngora.datastore.scanner.caching=100\n</code></pre>\n\n<p>nutch-site.xml:</p>\n\n<pre><code>&lt;configuration&gt;\n&lt;property&gt;\n &lt;name&gt;storage.data.store.class&lt;/name&gt;\n &lt;value&gt;org.apache.gora.hbase.store.HBaseStore&lt;/value&gt;\n &lt;description&gt;Default class for storing data&lt;/description&gt;\n&lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre>\n\n<p>Done. It will take all the default configurations for HBase: localhost, /tmp/..., blablabla</p>\n", "question_id": 39834423, "creation_date": 1475518585, "is_accepted": true, "score": 1, "last_activity_date": 1475589215, "answer_id": 39837926}], "question_id": 39834423, "tags": ["java", "apache", "nutch", "gora"], "answer_count": 1, "link": "http://stackoverflow.com/questions/39834423/apache-nutch-fetcherjob-throws-nosuchelementexception-deep-in-gora", "last_activity_date": 1475589215, "accepted_answer_id": 39837926, "body": "<p>I'm running Apache Nutch 2.3.1 out of the box, which uses Gora 0.6.1. I've followed the instructions here: <a href=\"http://wiki.apache.org/nutch/RunNutchInEclipse\" rel=\"nofollow\">http://wiki.apache.org/nutch/RunNutchInEclipse</a></p>\n\n<p>It ran fine with the <code>InjectorJob</code>.</p>\n\n<p>Now I'm running the <code>FetcherJob</code>, and Gora uses <code>MemStore</code> as a data store. I have <code>gora.properties</code> containing</p>\n\n<pre><code>gora.datastore.default=org.apache.gora.memory.store.MemStore\n</code></pre>\n\n<p>This throws:</p>\n\n<pre><code>2016-10-02 22:55:54,605 ERROR mapreduce.GoraRecordReader (GoraRecordReader.java:nextKeyValue(121)) - Error reading Gora records: null\n2016-10-02 22:55:54,605 INFO  mapred.MapTask (MapTask.java:flush(1460)) - Starting flush of map output\n2016-10-02 22:55:54,614 INFO  mapred.LocalJobRunner (LocalJobRunner.java:runTasks(456)) - map task executor complete.\n2016-10-02 22:55:54,615 WARN  mapred.LocalJobRunner (LocalJobRunner.java:run(560)) - job_local874667143_0001\njava.lang.Exception: java.lang.RuntimeException: java.util.NoSuchElementException\n    at org.apache.hadoop.mapred.LocalJobRunner$Job.runTasks(LocalJobRunner.java:462)\n    at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:522)\nCaused by: java.lang.RuntimeException: java.util.NoSuchElementException\n    at org.apache.gora.mapreduce.GoraRecordReader.nextKeyValue(GoraRecordReader.java:122)\n    at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:556)\n    at org.apache.hadoop.mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80)\n    at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91)\n    at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:145)\n    at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:787)\n    at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)\n    at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:243)\n    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n    at java.util.concurrent.FutureTask.run(FutureTask.java:266)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n    at java.lang.Thread.run(Thread.java:745)\nCaused by: java.util.NoSuchElementException\n    at java.util.concurrent.ConcurrentSkipListMap.firstKey(ConcurrentSkipListMap.java:2036)\n    at org.apache.gora.memory.store.MemStore.execute(MemStore.java:128)\n    at org.apache.gora.query.impl.QueryBase.execute(QueryBase.java:73)\n    at org.apache.gora.mapreduce.GoraRecordReader.executeQuery(GoraRecordReader.java:67)\n    at org.apache.gora.mapreduce.GoraRecordReader.nextKeyValue(GoraRecordReader.java:109)\n    ... 12 more\n2016-10-02 22:55:55,383 INFO  mapreduce.Job (Job.java:monitorAndPrintJob(1360)) - Job job_local874667143_0001 running in uber mode : false\n2016-10-02 22:55:55,385 INFO  mapreduce.Job (Job.java:monitorAndPrintJob(1367)) -  map 0% reduce 0%\n2016-10-02 22:55:55,387 INFO  mapreduce.Job (Job.java:monitorAndPrintJob(1380)) - Job job_local874667143_0001 failed with state FAILED due to: NA\n2016-10-02 22:55:55,396 INFO  mapreduce.Job (Job.java:monitorAndPrintJob(1385)) - Counters: 0\nException in thread \"main\" java.lang.RuntimeException: job failed: name=, jobid=job_local874667143_0001\n    at org.apache.nutch.util.NutchJob.waitForCompletion(NutchJob.java:119)\n    at org.apache.nutch.fetcher.FetcherJob.run(FetcherJob.java:205)\n    at org.apache.nutch.fetcher.FetcherJob.fetch(FetcherJob.java:251)\n    at org.apache.nutch.fetcher.FetcherJob.run(FetcherJob.java:314)\n    at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)\n    at org.apache.nutch.fetcher.FetcherJob.main(FetcherJob.java:321)\n</code></pre>\n\n<p>This happens so deep into Nutch and Gora that I have no idea why it's happening. I tried upgrading to Gora 0.8 but same problem. I tried downgrading Gora to 0.6, same problem. I wanted to switch to another data store like hBase but that's a bit overkill for what I need at this moment.</p>\n\n<p>Please help me figure this out.</p>\n", "creation_date": 1475506155, "score": 3},
{"title": "Search for the latest rows in terms of timestamp", "view_count": 45, "is_answered": true, "answers": [{"last_edit_date": 1475330946, "owner": {"user_id": 647053, "link": "http://stackoverflow.com/users/647053/ramprasad-g", "user_type": "registered", "reputation": 3192}, "body": "<p>I dont know python so Im explaining this in hbase shell..\nMore or less you should be able to do this in python.</p>\n\n<h3>Python way : <a href=\"http://stackoverflow.com/questions/12458595/convert-timestamp-since-epoch-to-datetime-datetime\">Convert timestamp since epoch to datetime.datetime</a></h3>\n\n<blockquote>\n  <h3>How to get latest timestamp to pass in the filter ?</h3>\n  \n  <p>LOG data to timestamp\n  To convert the date '08/08/16 20:56:29' from an hbase log into a\n  timestamp, do:</p>\n</blockquote>\n\n<pre><code>     hbase(main):021:0&gt; import java.text.SimpleDateFormat\n                   hbase(main):022:0&gt; import java.text.ParsePosition\n                    hbase(main):023:0&gt; SimpleDateFormat.new(\"yy/MM/dd HH:mm:ss\").parse(\"08/08/16 20:56:29\", ParsePosition.new(0)).getTime()\n =&gt; 1218920189000\n</code></pre>\n\n<p>after above you can try something like this:</p>\n\n<pre><code>scan 't1', {COLUMNS =&gt; 'c1', TIMERANGE =&gt; [1303668804, 1303668904]}\n\n\nhbase(main):001:0&gt; scan\n</code></pre>\n\n<p>Here is some help for this command:\nScan a table; pass table name and optionally a dictionary of scanner\nspecifications.  Scanner specifications may include one or more of:\nTIMERANGE, FILTER, LIMIT, STARTROW, STOPROW, TIMESTAMP, MAXLENGTH,\nor COLUMNS, CACHE</p>\n\n<h3>Some examples:</h3>\n\n<pre><code>  hbase&gt; scan '.META.'\n  hbase&gt; scan '.META.', {COLUMNS =&gt; 'info:regioninfo'}\n  hbase&gt; scan 't1', {COLUMNS =&gt; ['c1', 'c2'], LIMIT =&gt; 10, STARTROW =&gt; 'xyz'}\n  hbase&gt; scan 't1', {COLUMNS =&gt; 'c1', TIMERANGE =&gt; [1303668804, 1303668904]}\n  hbase&gt; scan 't1', {FILTER =&gt; \"(PrefixFilter ('row2') AND (QualifierFilter (&gt;=, 'binary:xyz'))) AND (TimestampsFilter ( 123, 456))\"}\n  hbase&gt; scan 't1', {FILTER =&gt; org.apache.hadoop.hbase.filter.ColumnPaginationFilter.new(1, 0)}\n</code></pre>\n", "question_id": 39807389, "creation_date": 1475329951, "is_accepted": false, "score": 1, "last_activity_date": 1475330946, "answer_id": 39807432}], "question_id": 39807389, "tags": ["python", "hadoop", "hbase", "nutch", "happybase"], "answer_count": 1, "link": "http://stackoverflow.com/questions/39807389/search-for-the-latest-rows-in-terms-of-timestamp", "last_activity_date": 1475343723, "owner": {"age": 36, "answer_count": 0, "creation_date": 1317028726, "user_id": 964731, "view_count": 14, "reputation": 11}, "body": "<p>I am looking for how to search for the latest rows in hbase table that is loaded by Nutch 2.3. </p>\n\n<p>I use happybase and thrift, the only example I have found is in this link <a href=\"https://happybase.readthedocs.io/en/happybase-0.4/tutorial.html#using-table-namespaces\" rel=\"nofollow\">https://happybase.readthedocs.io/en/happybase-0.4/tutorial.html#using-table-namespaces</a></p>\n", "creation_date": 1475329652, "score": 2},
{"title": "How to get job status of crawl tasks in nutch", "view_count": 47, "owner": {"user_id": 4327283, "answer_count": 40, "creation_date": 1417755361, "accept_rate": 100, "view_count": 69, "location": "Cochin,Kerala,India", "reputation": 651}, "is_answered": true, "answers": [{"question_id": 39746440, "owner": {"user_id": 432844, "link": "http://stackoverflow.com/users/432844/julien-nioche", "user_type": "registered", "reputation": 939}, "body": "<p>You should always run Nutch with Hadoop in pseudo or fully distributed mode, this way you'll be able to use the Hadoop UI to track the progress of your crawls, see the logs for each step, access the counters (extremely useful!). </p>\n", "creation_date": 1475064332, "is_accepted": true, "score": 3, "last_activity_date": 1475064332, "answer_id": 39746985}], "question_id": 39746440, "tags": ["nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/39746440/how-to-get-job-status-of-crawl-tasks-in-nutch", "last_activity_date": 1475127120, "accepted_answer_id": 39746985, "body": "<p>In a crawl cycle, we have many tasks/phases like <em>inject,generate,fetch,parse,updatedb,invertlinks,dedup</em> and an <em>index</em> job.\nNow I would like to know is there any methodologies to get <strong><em>status</em></strong> of a crawl task(whether it is running or failed) by any means other than referring to hadoop.log file ? \nTo be more precise I would like to know whether I can track status of a generate/fetch/parse phase ? Any help would be appreciated.</p>\n", "creation_date": 1475062888, "score": 2},
{"title": "Add properties file for custom plugin in Nutch", "view_count": 28, "owner": {"age": 22, "answer_count": 16, "creation_date": 1450071134, "user_id": 5676586, "accept_rate": 100, "view_count": 46, "location": "Alappuzha", "reputation": 447}, "is_answered": true, "answers": [{"last_edit_date": 1475120064, "owner": {"user_id": 5676586, "accept_rate": 100, "link": "http://stackoverflow.com/users/5676586/abhishek", "user_type": "registered", "reputation": 447}, "body": "<p>I just solved by adding a copy task in plugin's <code>build.xml</code> as :</p>\n\n<pre><code>&lt;copy todir=\"${build.classes}\"&gt;\n    &lt;fileset dir=\"${src.dir}\" includes=\"**/*.properties\"/&gt;\n&lt;/copy&gt;\n</code></pre>\n\n<p>It copies the properties file to the compiled jar and issue was solved. Cheers !!</p>\n\n<p>EDIT :</p>\n\n<p>I just used another method also. Moved the properties file to conf directory and get the input in Parsefilter by,</p>\n\n<pre><code>Properties property = new Properties();\nInputStream input = ClassLoader.getSystemResourceAsStream(\"sample.properties\");\nproperty.load(input);\n</code></pre>\n", "question_id": 39462658, "creation_date": 1473938625, "is_accepted": true, "score": 1, "last_activity_date": 1475120064, "answer_id": 39509870}], "question_id": 39462658, "tags": ["java", "ant", "properties", "web-scraping", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/39462658/add-properties-file-for-custom-plugin-in-nutch", "last_activity_date": 1475120064, "accepted_answer_id": 39509870, "body": "<p>I'm a beginner in Nutch. I have done crawling, created a custom plugin based on different tutorials. For a particular task, my Java class have to use a properties file named <code>sample.properties</code> for some tasks. I've getting a <strong>NullPointerException</strong> on the following code.</p>\n\n<pre><code>Properties property = new Properties();\n        InputStream input = getClass().getResourceAsStream(\"sample.properties\");\n        property.load(input);\n</code></pre>\n\n<p>I don't know where to place this properties file, because it doesn't moves  to the compiled jar after compiling with ant. Currently I'm placing at the same directory of java class. Any help will be appreciated.</p>\n", "creation_date": 1473744116, "score": 1},
{"title": "how to extract the value of specefic div in html with crawling in apache nutch?", "view_count": 58, "is_answered": true, "answers": [{"question_id": 39730069, "owner": {"user_id": 5676586, "accept_rate": 100, "link": "http://stackoverflow.com/users/5676586/abhishek", "user_type": "registered", "reputation": 447}, "body": "<p>You need to override the <strong>parsefilter</strong> and use <strong>Jsoup selector</strong> to select particular div.</p>\n", "creation_date": 1475038350, "is_accepted": false, "score": 2, "last_activity_date": 1475038350, "answer_id": 39738306}, {"question_id": 39730069, "owner": {"user_id": 4327283, "accept_rate": 100, "link": "http://stackoverflow.com/users/4327283/sachin", "user_type": "registered", "reputation": 651}, "body": "<p>You will have to write a plugin that will extend <em>HtmlParseFilter</em> to achieve your goal.</p>\n\n<p>You can use some html parser like Jsoup for this and extract URLs that you want and add them as outlinks.</p>\n\n<p>Sample HtmlParseFilter implementation:-  </p>\n\n<pre><code>        public ParseResult filter(Content content, ParseResult parseResult,\n              HTMLMetaTags metaTags, DocumentFragment doc) {\n                // get html content\n                String htmlContent = new String(content.getContent(), StandardCharsets.UTF_8);\n                // parse html using jsoup or any other library.\n                Document document = Jsoup.parse(content.toString(),content.getUrl());\n                Elements elements = document.select(&lt;your_css_selector_query);\n                // modify/select only required outlinks\n                if (elements != null) {\n                    Outlink outlink;\n                    List&lt;String&gt; newLinks=new ArrayList&lt;String&gt;();\n                    List&lt;Outlink&gt; outLinks=new ArrayList&lt;Outlink&gt;();\n                    String absoluteUrl;\n                    Outlink outLink;\n                    for (Element element : elements){\n                     absoluteUrl=element.absUrl(\"href\");\n                     if(includeLinks(absoluteUrl,value)) {\n                        if(!newLinks.contains(absoluteUrl)){\n                          newLinks.add(absoluteUrl);\n                          outLink=new Outlink(absoluteUrl,element.text());\n                          outLinks.add(outLink);\n                          }\n                        }\n                      }\n                    Parse parse = parseResult.get(content.getUrl());\n                    ParseStatus status = parse.getData().getStatus();\n                    Title title = document.title();\n                    Outlink[] newOutLinks = (Outlink[])outLinks.toArray(new Outlink[outLinks.size()]);\n                    ParseData parseData = new ParseData(status, title, newOutLinks, parse.getData().getContentMeta(), parse.getData().getParseMeta());\n                    parseResult.put(content.getUrl(), new ParseText(elements.text()), parseData);\n                    }\n                   //return parseResult with modified outlinks\n                   return parseResult;\n            }\n</code></pre>\n\n<p>Build new plugin using ant and add plugin in nutch-site.xml.</p>\n\n<pre><code>&lt;property&gt;\n    &lt;name&gt;plugin.includes&lt;/name&gt;\n    &lt;value&gt;protocol-httpclient|&lt;custom_plugin&gt;|urlfilter-regex|parse-(tika|html|js|css)|index-(basic|anchor)|query-(basic|site|url)|response-(json|xml)|summary-basic|scoring-opic|urlnormalizer-(pass|regex|basic)|indexer-elastic&lt;/value&gt;\n  &lt;/property&gt;\n</code></pre>\n\n<p>And in <strong><em>parser-plugins.xml</em></strong> you can use your custom plugin instead of default plugin used by tika by something like this :-</p>\n\n<pre><code>&lt;!--\n    &lt;mimeType name=\"text/html\"&gt;\n        &lt;plugin id=\"parse-html\" /&gt;\n    &lt;/mimeType&gt;\n\n        &lt;mimeType name=\"application/xhtml+xml\"&gt;\n        &lt;plugin id=\"parse-html\" /&gt;\n    &lt;/mimeType&gt;\n--&gt;\n\n    &lt;mimeType name=\"text/xml\"&gt;\n        &lt;plugin id=\"parse-tika\" /&gt;\n        &lt;plugin id=\"feed\" /&gt;\n    &lt;/mimeType&gt;\n\n    &lt;mimeType name=\"text/html\"&gt;\n        &lt;plugin id=\"&lt;custom_plugin&gt;\" /&gt;\n    &lt;/mimeType&gt;\n\n                &lt;mimeType name=\"application/xhtml+xml\"&gt;\n        &lt;plugin id=\"&lt;custom_plugin&gt;\" /&gt;\n    &lt;/mimeType&gt;\n</code></pre>\n", "creation_date": 1475038462, "is_accepted": false, "score": 2, "last_activity_date": 1475038462, "answer_id": 39738326}], "question_id": 39730069, "tags": ["parsing", "solr", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/39730069/how-to-extract-the-value-of-specefic-div-in-html-with-crawling-in-apache-nutch", "last_activity_date": 1475074996, "owner": {"user_id": 6365111, "view_count": 4, "answer_count": 0, "creation_date": 1463844705, "reputation": 16}, "body": "<p>I do crawling with nutch 2.2 and the data that i retrieve is the metatag,how to extract the value of specefic div in html with crawling in apache nutch</p>\n", "creation_date": 1474994444, "score": 2},
{"title": "Elasticsearch 2.4.0 compatibilty with nutch 1.12", "view_count": 156, "owner": {"user_id": 2354150, "answer_count": 0, "creation_date": 1367833616, "accept_rate": 25, "view_count": 13, "reputation": 25}, "is_answered": true, "answers": [{"question_id": 39741315, "owner": {"user_id": 432844, "link": "http://stackoverflow.com/users/432844/julien-nioche", "user_type": "registered", "reputation": 939}, "body": "<p>The master branch of Nutch 1.x works with <a href=\"https://github.com/apache/nutch/blob/master/src/plugin/indexer-elastic/ivy.xml\" rel=\"nofollow\">ES 2.3.3</a>. It will be in the next release but for now you can download or clone it from <a href=\"https://github.com/apache/nutch\" rel=\"nofollow\">https://github.com/apache/nutch</a>.</p>\n\n<p>There might be some minor changes to do to the ivy.xml and build.xml files, see <a href=\"https://github.com/apache/nutch/blob/master/src/plugin/indexer-elastic/howto_upgrade_es.txt\" rel=\"nofollow\">how to upgrade</a> for details.</p>\n", "creation_date": 1475055026, "is_accepted": true, "score": 2, "last_activity_date": 1475055026, "answer_id": 39743383}], "question_id": 39741315, "tags": ["elasticsearch", "ant", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/39741315/elasticsearch-2-4-0-compatibilty-with-nutch-1-12", "last_activity_date": 1475056035, "accepted_answer_id": 39743383, "body": "<p>Can we use elasticsearch 2.4.0 with nutch 1.12. I used it directly and it threw error while indexing. Error is something like:</p>\n\n<p>java.lang.IllegalStateException: Received message from unsupported version: [1.0.0] minimal compatible version is: [2.0.0].</p>\n\n<p>Previously I was using elasticsearch 1.7. I have also used elastic-indexer2 plugin : <a href=\"https://github.com/ptorrestr/indexer-elastic2/\" rel=\"nofollow\">https://github.com/ptorrestr/indexer-elastic2/</a>  but the ant build failed.</p>\n", "creation_date": 1475049315, "score": 1},
{"title": "apache nutch - how to change configuration of plugins via REST", "view_count": 23, "is_answered": true, "answers": [{"question_id": 37308636, "owner": {"user_id": 4327283, "accept_rate": 100, "link": "http://stackoverflow.com/users/4327283/sachin", "user_type": "registered", "reputation": 651}, "body": "<p>Use create config service of nutch REST api.\nYou can do something like this :-</p>\n\n<p><strong>POST</strong> config/create</p>\n\n<pre><code>    {\n    \"configId\":\"myconfig\",\n    \"force\":\"true\",\n    \"params\":{\n              \"urlfilter.domain.file\": \"domain-urlfilter.txt\",\n             }\n    }\n</code></pre>\n\n<p>And use this config for that specific job.</p>\n", "creation_date": 1475041872, "is_accepted": false, "score": 2, "last_activity_date": 1475041872, "answer_id": 39738982}], "question_id": 37308636, "tags": ["apache", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/37308636/apache-nutch-how-to-change-configuration-of-plugins-via-rest", "last_activity_date": 1475041872, "owner": {"user_id": 1025852, "answer_count": 2, "creation_date": 1320244878, "accept_rate": 63, "view_count": 201, "reputation": 903}, "body": "<p>I'd like to change dynamically the domain filter configuration per job (for example - on seed url crawl only urls from its domain).</p>\n\n<p>(I'm doing GET /config/default to see what's there to change) </p>\n\n<p>Thing is that the main configuration only point to a file on the server - which I can't reach (in this case</p>\n\n<pre><code>\"urlfilter.domain.file\": \"domain-urlfilter.txt\",\n</code></pre>\n\n<p>so - how can plugins configuration can be changed via REST dynamically? or in more general how can I overrides those \"pointed to file\" configuraiton</p>\n", "creation_date": 1463600110, "score": 0},
{"title": "Inject urls into Apache Nutch from mysql instead of seed.txt", "view_count": 48, "is_answered": true, "answers": [{"question_id": 39528420, "owner": {"user_id": 432844, "link": "http://stackoverflow.com/users/432844/julien-nioche", "user_type": "registered", "reputation": 939}, "body": "<p>Nutch 1.x => Not out of the box. You'd have to modify the Injector code so that it reads from MySQL but this is certainly doable. I did that for a customer ages ago.</p>\n\n<p>Alternatively, you could use <a href=\"http://stormcrawler.net\" rel=\"nofollow\">StormCrawler</a>, it has a MySQL module and there should be no extra work to get it to work. The <a href=\"http://digitalpebble.blogspot.co.uk/2015/09/index-web-with-aws-cloudsearch.html\" rel=\"nofollow\">Cloudsearch tutorial</a> on our blog shows how to use MySQL with SC.</p>\n\n<p>Nutch 2.x uses GORA as an intermediate layer and IIRC there was a SQL plugin for it. Not sure of its status and whether this would be suitable. </p>\n", "creation_date": 1474035274, "is_accepted": false, "score": 1, "last_activity_date": 1474035274, "answer_id": 39533718}, {"question_id": 39528420, "owner": {"user_id": 4327283, "accept_rate": 100, "link": "http://stackoverflow.com/users/4327283/sachin", "user_type": "registered", "reputation": 651}, "body": "<p>As Julien said you should modify INJECTOR code to achieve this. However, I can suggest a workaround for this. You can use NUTCH in server mode using command <em>bin/nutch startserver</em> and then load your seed urls from database. Then you can use Nutch REST API to create a seed list using urls loaded from database and point that created seed file to INJECT job creation service call.</p>\n\n<p>You can find more details regarding REST api here :-</p>\n\n<p><a href=\"http://nutch.apache.org/miredot/1.12/index.html#1153761698\" rel=\"nofollow\">http://nutch.apache.org/miredot/1.12/index.html#1153761698</a> \nor\n<a href=\"https://docs.google.com/document/d/1OGg22ATohapP2ycewIaTcUnENc2FeyYzni0ED_Jjxz8/edit\" rel=\"nofollow\">https://docs.google.com/document/d/1OGg22ATohapP2ycewIaTcUnENc2FeyYzni0ED_Jjxz8/edit</a>\n<a href=\"https://wiki.apache.org/nutch/NutchRESTAPI\" rel=\"nofollow\">https://wiki.apache.org/nutch/NutchRESTAPI</a></p>\n", "creation_date": 1475040758, "is_accepted": false, "score": 3, "last_activity_date": 1475040758, "answer_id": 39738759}], "question_id": 39528420, "tags": ["web-crawler", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/39528420/inject-urls-into-apache-nutch-from-mysql-instead-of-seed-txt", "last_activity_date": 1475040758, "owner": {"user_id": 6013994, "answer_count": 1, "creation_date": 1457017944, "accept_rate": 60, "view_count": 7, "reputation": 48}, "body": "<p>I am new to Apache Nutch and I want to inject urls dynamically from a mysql database. Does Apache Nutch offer such possiblity? If not, is there any similar experiment that i can learn from? Or any suggestions?</p>\n", "creation_date": 1474019232, "score": 2},
{"title": "Metatags are not indexing for some websites", "view_count": 51, "owner": {"age": 22, "answer_count": 16, "creation_date": 1450071134, "user_id": 5676586, "accept_rate": 100, "view_count": 46, "location": "Alappuzha", "reputation": 447}, "is_answered": true, "answers": [{"last_edit_date": 1474972762, "owner": {"user_id": 432844, "link": "http://stackoverflow.com/users/432844/julien-nioche", "user_type": "registered", "reputation": 939}, "body": "<p>It could be because the names are in uppercase</p>\n\n<pre><code>&lt;meta name=\"Description\" content=\"...\"&gt;\n&lt;meta name=\"Keywords\" content=\"...\"&gt;\n</code></pre>\n\n<p>Maybe try the case variants in your config. </p>\n\n<p>BTW you can use './nutch indexchecker ...' to test the extraction and field generation on a given URL.</p>\n\n<p>EDIT : <a href=\"https://github.com/apache/nutch/blob/master/src/plugin/index-metadata/src/java/org/apache/nutch/indexer/metadata/MetadataIndexer.java#L92\" rel=\"nofollow\">https://github.com/apache/nutch/blob/master/src/plugin/index-metadata/src/java/org/apache/nutch/indexer/metadata/MetadataIndexer.java#L92</a> lowercases the keys we are looking for but the key names in the parse metadata might be in the original casing i.e. with an uppercase.</p>\n\n<p>Until this is resolved, you could add some custom code to your own plugin to lowercase the keys or alternatively modify the MetadataIndexer so that it preserves the casing or change the logic so that it can handle variants in the case.</p>\n", "question_id": 39717194, "creation_date": 1474969885, "is_accepted": false, "score": 3, "last_activity_date": 1474972762, "answer_id": 39721536}, {"question_id": 39717194, "owner": {"user_id": 4327283, "accept_rate": 100, "link": "http://stackoverflow.com/users/4327283/sachin", "user_type": "registered", "reputation": 651}, "body": "<p>From the answer and hints provided by <a href=\"http://stackoverflow.com/users/432844/julien-nioche\">Julien nioche</a> you can change your <strong><em>parser-filter plugin</em></strong> to something like this to lowercase all meta names inside which will solve the issue for now.</p>\n\n<pre><code>        Metadata newMeta=new Metadata();\n        Metadata oldMeta=parse.getData().getParseMeta();\n        String metaValue;\n        for(String metaName:oldMeta.names()){\n          metaValue=oldMeta.get(metaName);\n          newMeta.add(metaName.toLowerCase(),metaValue);\n        }\n\n        parseData = new ParseData(status, title, parse.getData().getOutlinks(), \n                                      parse.getData().getContentMeta(), newMeta);\n        parseResult.put(content.getUrl(), new ParseText(text), parseData);\n        return parseResult;\n</code></pre>\n\n<p>HTH</p>\n", "creation_date": 1474982148, "is_accepted": true, "score": 4, "last_activity_date": 1474982148, "answer_id": 39725673}], "question_id": 39717194, "tags": ["elasticsearch", "web-crawler", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/39717194/metatags-are-not-indexing-for-some-websites", "last_activity_date": 1474982148, "accepted_answer_id": 39725673, "body": "<p>I am using Nutch to crawl some websites and index the data to Elastic Search by the help of a custom plugin (myplugin).</p>\n\n<p>I need information stored in meta tags from the crawled sites. So in order to achieve this I just added properties in nutch-site.xml as follows :</p>\n\n<pre><code>&lt;property&gt;\n    &lt;name&gt;plugin.includes&lt;/name&gt;\n    &lt;value&gt;protocol-httpclient|myplugin|urlfilter-regex|parse-(tika|html|js|css|metatags)|index-(basic|anchor|metadata)|query-(basic|site|url)|response-(json|xml)|summary-basic|scoring-opic|urlnormalizer-(pass|regex|basic)|indexer-elastic&lt;/value&gt;\n  &lt;/property&gt;\n\n  &lt;property&gt;\n        &lt;name&gt;metatags.names&lt;/name&gt;\n        &lt;value&gt;*&lt;/value&gt;\n  &lt;/property&gt;\n\n  &lt;property&gt;\n        &lt;name&gt;index.parse.md&lt;/name&gt;\n        &lt;value&gt;keywords,description&lt;/value&gt;\n  &lt;/property&gt;\n\n  &lt;property&gt;\n        &lt;name&gt;index.content.md&lt;/name&gt;\n        &lt;value&gt;keywords,description&lt;/value&gt;\n  &lt;/property&gt;\n</code></pre>\n\n<p>It works fine with some sites, but didn't worked with site like <a href=\"https://www.oracle.com/corporate/pressrelease/database-benchmarking-092016.html\" rel=\"nofollow\">this</a></p>\n\n<p>Any help will be appreciated.</p>\n", "creation_date": 1474956637, "score": 3},
{"title": "Facing issue in elasticsearch mapping of nutch crawled document", "view_count": 64, "owner": {"user_id": 2354150, "answer_count": 0, "creation_date": 1367833616, "accept_rate": 25, "view_count": 13, "reputation": 25}, "is_answered": true, "answers": [{"question_id": 39697398, "owner": {"user_id": 432844, "link": "http://stackoverflow.com/users/432844/julien-nioche", "user_type": "registered", "reputation": 939}, "body": "<p>You should pass the url_id as an additional metadata in your seed list and use the urlmeta and index-metadata plugins so that the Key/Value gets passed to the outlinks (if necessary) or at least be available for the indexing.</p>\n\n<p>See <a href=\"https://wiki.apache.org/nutch/IndexMetatags\" rel=\"nofollow\">Nutch WIKI</a> for an explanation of how to index metatags.</p>\n", "creation_date": 1474881975, "is_accepted": true, "score": 2, "last_activity_date": 1474881975, "answer_id": 39699127}], "question_id": 39697398, "tags": ["mysql", "elasticsearch", "web-crawler", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/39697398/facing-issue-in-elasticsearch-mapping-of-nutch-crawled-document", "last_activity_date": 1474881975, "accepted_answer_id": 39699127, "body": "<p>Facing some serious issues while using nutch and elasticsearch for crawling purpose.</p>\n\n<p>We have two data storage engines in our App.</p>\n\n<ol>\n<li><p>MySql</p></li>\n<li><p>Elasticsearch</p></li>\n</ol>\n\n<p>Lets say I have 10 urls   stored in urls table of mysql db. Now I want to fetch these urls from table in run time and write these into seed,txt for crawling. I have written all these urls into need,txt at one go. Now my crawling starts and then I index these docs inside elasticsearch in an index(lets say url index).  But I want to maintain a reference  inside elasticsearch index  so that I can fetch a particular url's crawled details for analytics purpose as elasticsearch index only contains crawled data.  For ex.</p>\n\n<p>My table structure in mysql is :</p>\n\n<p>Table Url:</p>\n\n<p>id  url</p>\n\n<hr>\n\n<p>1  www.google.com</p>\n\n<p>Elasticsearch index mapping I want is :</p>\n\n<p>Index  url:</p>\n\n<p>{\n_id: \"www.google.com\",\ntype: \"doc\",\ncontent : \"Hello world\"\nurl_id : 1 ,\n.\n.\n.\n}</p>\n\n<p>Here url_id is the field value of id column of the crawled url inside urls table.</p>\n\n<p>I can create separate index for each url but that solution is not ideal because at the end of day I will be having multiple indices. So how to achieve this after crawling.  Do I have to modify the elastic search indexer. I am using nutch 1.12 and elastichsearch 1.7.1 .Any help would be  greatly appreciated.</p>\n", "creation_date": 1474876403, "score": 1},
{"title": "How to make nutch index only pages with certain text?", "view_count": 1462, "is_answered": true, "answers": [{"question_id": 17509795, "owner": {"user_id": 81194, "link": "http://stackoverflow.com/users/81194/claude", "user_type": "registered", "reputation": 827}, "body": "<p><a href=\"http://stackoverflow.com/questions/13884249/nutch-regex-urlfilter-syntax\">Start here</a> to setup your desired URL pattern. Then <a href=\"http://stackoverflow.com/questions/17341072/using-solr-for-indexing-html-tags-with-attributes\">look into plugins</a> to parse your content and decide what should be indexed.</p>\n", "creation_date": 1373297560, "is_accepted": false, "score": 2, "last_activity_date": 1373297560, "answer_id": 17530514}, {"question_id": 17509795, "owner": {"user_id": 5676586, "accept_rate": 100, "link": "http://stackoverflow.com/users/5676586/abhishek", "user_type": "registered", "reputation": 447}, "body": "<p>It shows Injector rejects your url in seed file</p>\n\n<pre><code>Injector: total number of urls rejected by filters: 1\n</code></pre>\n\n<p>Your regex is not working or there will be any other patterns which rejects your url like <code>-.*(/[^/]+)/[^/]+\\1/[^/]+\\1/</code> or <code>-[?*!@=]</code></p>\n", "creation_date": 1474633374, "is_accepted": false, "score": 0, "last_activity_date": 1474633374, "answer_id": 39660681}], "question_id": 17509795, "tags": ["nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/17509795/how-to-make-nutch-index-only-pages-with-certain-text", "last_activity_date": 1474633374, "owner": {"user_id": 319297, "answer_count": 41, "creation_date": 1271520246, "accept_rate": 62, "view_count": 75, "reputation": 861}, "body": "<p>I have 2 requirements.</p>\n\n<p>First one is that I want Nutch to index only pages that contain certain words in the html. For example I only want nutch to index pages that contain \"wounderful\" word in the html.\nSecond one is that I want nutch to index certain URLs from the site. For example I want nutch to index URLs that are similar to \"mywebsite.com/XXXX/ABC/XXXX\" or \"mywebsite.com/grow.php/ABC/XXXX\", where \"XXXX\" can be any word of any length.</p>\n\n<p>This is the content of my seed.txt file</p>\n\n<pre><code>http://mysite.org/\n</code></pre>\n\n<p>this is the content of my regex-urlfilter.txt</p>\n\n<pre><code>+^http://mysite.org/work/.*?/text/\n</code></pre>\n\n<p>I have commented</p>\n\n<pre><code>#+.\n</code></pre>\n\n<p>By I am still getting below error</p>\n\n<pre><code>crawl started in: crawl\nrootUrlDir = bin/urls\nthreads = 10\ndepth = 3\nsolrUrl=http://localhost:8983/solr/\ntopN = 5\nInjector: starting at 2013-07-09 11:05:51\nInjector: crawlDb: crawl/crawldb\nInjector: urlDir: bin/urls\nInjector: Converting injected urls to crawl db entries.\nInjector: total number of urls rejected by filters: 1\nInjector: total number of urls injected after normalization and filtering: 0\nInjector: Merging injected urls into crawl db.\nInjector: finished at 2013-07-09 11:06:08, elapsed: 00:00:17\nGenerator: starting at 2013-07-09 11:06:08\nGenerator: Selecting best-scoring urls due for fetch.\nGenerator: filtering: true\nGenerator: normalizing: true\nGenerator: topN: 5\nGenerator: jobtracker is 'local', generating exactly one partition.\nGenerator: 0 records selected for fetching, exiting ...\nStopping at depth=0 - no more URLs to fetch.\nNo URLs to fetch - check your seed list and URL filters.\ncrawl finished: crawl\n</code></pre>\n", "creation_date": 1373178395, "score": 1},
{"title": "Crawl and index specific html tags with NUTCH and Solr", "view_count": 50, "is_answered": false, "answers": [{"question_id": 38613594, "owner": {"user_id": 5676586, "accept_rate": 100, "link": "http://stackoverflow.com/users/5676586/abhishek", "user_type": "registered", "reputation": 447}, "body": "<p>You have to override the parsefilter and use Jsoup selector to select the portion you want to crawl and index.\nHave a look at this <a href=\"http://stackoverflow.com/a/38854219/5676586\">http://stackoverflow.com/a/38854219/5676586</a></p>\n", "creation_date": 1474630062, "is_accepted": false, "score": 0, "last_activity_date": 1474630062, "answer_id": 39659660}], "question_id": 38613594, "tags": ["html", "solr", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/38613594/crawl-and-index-specific-html-tags-with-nutch-and-solr", "last_activity_date": 1474630062, "owner": {"user_id": 5635244, "view_count": 3, "answer_count": 0, "creation_date": 1449149962, "reputation": 1}, "body": "<p>Right now im crawling a website like ebay for cars, www.standvirtual.com</p>\n\n<p>in my nutch regex-urlfilter.txt +^<a href=\"http://([a-z0-9]\" rel=\"nofollow\">http://([a-z0-9]</a>*.)*standvirtual.com/carros/anuncios/ like this just crawls the ads of cars, but like this nutch will index the whole content of the page, and i just wanted to index specific parts of that page, like title of the add, description, etc...</p>\n\n<p>example:\nlets imagine a scen\u00e1rio that title is a Audi a3 2.0cc\nand Audi a3 2000cc with ac, diesel...</p></p>\n", "creation_date": 1469624062, "score": -1},
{"title": "Nutch 1.2 - Why won&#39;t nutch crawl url with query strings?", "view_count": 1740, "is_answered": true, "answers": [{"question_id": 7045716, "owner": {"user_id": 12016, "accept_rate": 77, "link": "http://stackoverflow.com/users/12016/mana", "user_type": "registered", "reputation": 2720}, "body": "<p>See my previous question here <a href=\"http://stackoverflow.com/questions/6495468/adding-url-parameter-to-nutch-solr-index-and-search-results\">Adding URL parameter to Nutch/Solr index and search results</a></p>\n\n<p>The first 'Edit' should answer your question.</p>\n", "creation_date": 1313417813, "is_accepted": false, "score": 2, "last_activity_date": 1313417813, "answer_id": 7065976}, {"question_id": 7045716, "owner": {"user_id": 585874, "accept_rate": 62, "link": "http://stackoverflow.com/users/585874/emab", "user_type": "registered", "reputation": 990}, "body": "<p>By default, crawlers shouldn't crawl links with query strings to avoid spams and fake search engines.</p>\n", "creation_date": 1377002998, "is_accepted": false, "score": 0, "last_activity_date": 1377002998, "answer_id": 18335536}, {"question_id": 7045716, "owner": {"user_id": 5676586, "accept_rate": 100, "link": "http://stackoverflow.com/users/5676586/abhishek", "user_type": "registered", "reputation": 447}, "body": "<pre><code># skip URLs containing certain characters as probable queries, etc.\n#-[?*!@=]\n</code></pre>\n\n<p>You have to comment it or modify it as :</p>\n\n<pre><code># skip URLs containing certain characters as probable queries, etc.\n-[*!@]\n</code></pre>\n", "creation_date": 1474629434, "is_accepted": false, "score": 0, "last_activity_date": 1474629434, "answer_id": 39659458}], "question_id": 7045716, "tags": ["nutch"], "answer_count": 3, "link": "http://stackoverflow.com/questions/7045716/nutch-1-2-why-wont-nutch-crawl-url-with-query-strings", "last_activity_date": 1474629434, "owner": {"age": 43, "answer_count": 13, "creation_date": 1301510168, "user_id": 684514, "accept_rate": 59, "view_count": 78, "location": "Maryland", "reputation": 157}, "body": "<p>I'm new to Nutch and not really sure what is going on here.  I run nutch and it crawl my website, but it seems to ignore URLs that contain query strings.  I've commented out the filter in the crawl-urlfilter.txt page so it look like this now:</p>\n\n<pre><code># skip urls with these characters\n#-[]\n\n#skip urls with slash delimited segment that repeats 3+ times\n#-.*(/[^/]+)/[^/]+\\1/[^/]+\\1/\n</code></pre>\n\n<p>So, i think i've effectively removed any filter so I'm telling nutch to accept all urls it finds on my website.</p>\n\n<p>Does anyone have any suggestions? Or is this a bug in nutch 1.2? Should i upgrade to 1.3 and will this fix this issue i am having?  OR am i doing something wrong?</p>\n", "creation_date": 1313179539, "score": 0},
{"title": "how to crawl data on few topics using apache nutch?", "view_count": 24, "is_answered": true, "answers": [{"question_id": 39021887, "owner": {"user_id": 5676586, "accept_rate": 100, "link": "http://stackoverflow.com/users/5676586/abhishek", "user_type": "registered", "reputation": 447}, "body": "<p>The url you giving is actually rejecting at injecting phase.</p>\n\n<p>You have to specify the regex that accepts the url in regex-urlfilter.txt or leave it as <code>+.</code> which means it accept all urls.</p>\n\n<pre><code>-[?*!@=]\n</code></pre>\n\n<p>The above pattern rejects your url. Since, it contains <strong>?</strong> and <strong>=</strong></p>\n", "creation_date": 1474629007, "is_accepted": false, "score": 1, "last_activity_date": 1474629007, "answer_id": 39659313}], "question_id": 39021887, "tags": ["apache", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/39021887/how-to-crawl-data-on-few-topics-using-apache-nutch", "last_activity_date": 1474629007, "owner": {"user_id": 6722737, "view_count": 1, "answer_count": 0, "creation_date": 1471362792, "reputation": 6}, "body": "<p>I am using apache nutch to perform crawling on rosettacode. I dont want to crawl through entire website, i just want to crawl on selected topics(eg.<a href=\"http://www.rosettacode.org/mw/index.php?title=Special%3ASearch&amp;search=Optimization+algorithms&amp;go=Go\" rel=\"nofollow\">http://www.rosettacode.org/mw/index.php?title=Special%3ASearch&amp;search=Optimization+algorithms&amp;go=Go</a>). But i am unable to perform crawl and it is throwing me error saying \"no urls to fetch.. check ur seed list and url filters\". Can anyone help me to solve this problem?? </p>\n", "creation_date": 1471533457, "score": 0},
{"title": "how to get anchorText of each URL in apache nutch for writing a new scoringFilter plugin?", "view_count": 26, "is_answered": true, "answers": [{"question_id": 39205955, "owner": {"user_id": 5676586, "accept_rate": 100, "link": "http://stackoverflow.com/users/5676586/abhishek", "user_type": "registered", "reputation": 447}, "body": "<p>You will get the anchor text from the outlinks. <code>outlink.getToUrl()</code> gives the url as String and <code>outlink.getAnchor()</code> will gives the anchor text.</p>\n", "creation_date": 1474628446, "is_accepted": false, "score": 2, "last_activity_date": 1474628446, "answer_id": 39659139}], "question_id": 39205955, "tags": ["apache", "web-crawler", "anchor", "nutch", "scoring"], "answer_count": 1, "link": "http://stackoverflow.com/questions/39205955/how-to-get-anchortext-of-each-url-in-apache-nutch-for-writing-a-new-scoringfilte", "last_activity_date": 1474628446, "owner": {"user_id": 6770051, "view_count": 1, "answer_count": 0, "creation_date": 1472471185, "reputation": 1}, "body": "<p>I am newbie to Apache Nutch so I spend a lot of time to search about it. <strong>I need to get anchors of parent pages of each url in Apache Nutch</strong>. I read about LinkDatum,LinkDB and Inlink that save data about each URL but I don't know exactly how to use these classes for adding a plugin for a new ScoringFilter.\nAny help would be appreciated.</p>\n", "creation_date": 1472472394, "score": -1},
{"title": "External links are not getting crawled", "view_count": 24, "owner": {"user_id": 4327283, "answer_count": 40, "creation_date": 1417755361, "accept_rate": 100, "view_count": 69, "location": "Cochin,Kerala,India", "reputation": 651}, "is_answered": true, "answers": [{"question_id": 39593734, "owner": {"user_id": 5676586, "accept_rate": 100, "link": "http://stackoverflow.com/users/5676586/abhishek", "user_type": "registered", "reputation": 447}, "body": "<p>Try this,  I found a workaround for this :</p>\n\n<ul>\n<li>Add some additional code snippets to your parsefilter. </li>\n<li>Select the links that you wish to crawl using Jsoup selector elements.\nFor Example, I am selecting links in a particular div</li>\n<li>Set this newOutLinks to your parse result and Nutch will accept this links and starts crawling.</li>\n</ul>\n\n<p>Code sample for step 2: </p>\n\n<pre><code>List&lt;Outlink&gt; outLinks=new ArrayList&lt;Outlink&gt;(); \nOutlink outLink;\nString link; \nElements elements = document.select(\"div.show a[href]\"); \nfor (Element element : elements) {\nlink=element.absUrl(\"href\"); \noutLink=new Outlink(absoluteUrl,element.text()); \noutLinks.add(outLink); \n}\nOutlink[] newOutLinks = (Outlink[])outLinks.toArray(new Outlink[outLinks.size()]);\n</code></pre>\n", "creation_date": 1474612916, "is_accepted": true, "score": 2, "last_activity_date": 1474612916, "answer_id": 39654208}], "question_id": 39593734, "tags": ["nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/39593734/external-links-are-not-getting-crawled", "last_activity_date": 1474612916, "accepted_answer_id": 39654208, "body": "<p>I'm working on a custom parse plugin for nutch and ran into an issue. I want to crawl all URLs in a specific area of seed url. But nutch ignores external links and these links are not getting crawled.</p>\n\n<p>eg : seed url - <code>https://in.news.yahoo.com</code>\nIt contains links to sites like timesofindia.com,thehindu.com etc but these links are not getting crawled.</p>\n\n<p>My nutch-site.xml contains :-</p>\n\n<pre><code>&lt;property&gt;\n  &lt;name&gt;db.ignore.external.links&lt;/name&gt;\n  &lt;value&gt;false&lt;/value&gt;\n&lt;/property&gt;\n</code></pre>\n\n<p>And regex-url filter accepts anything else.(with regex +.).\nI wonder why nutch is ignoring certain external links in the url given as seed. Please help</p>\n", "creation_date": 1474373427, "score": 2},
{"title": "Nutch Crawler doesn&#39;t retrieve news article content", "view_count": 59, "owner": {"user_id": 4327283, "answer_count": 40, "creation_date": 1417755361, "accept_rate": 100, "view_count": 69, "location": "Cochin,Kerala,India", "reputation": 651}, "is_answered": true, "answers": [{"question_id": 38761035, "owner": {"user_id": 4193280, "accept_rate": 27, "link": "http://stackoverflow.com/users/4193280/multi10ant", "user_type": "registered", "reputation": 1229}, "body": "<p>Just an out of context answer, but try using Apache ManifoldCF . It provides inbuilt connector to elastic search, and a better logged history to figure out why the data wasn't indexed .The connector section in ManifoldCF allows you to specify , that in which field your content should be indexed . It's a good open source alternative to try your hands on . </p>\n", "creation_date": 1470694774, "is_accepted": false, "score": 1, "last_activity_date": 1470694774, "answer_id": 38839350}, {"last_edit_date": 1474612031, "owner": {"user_id": 4327283, "accept_rate": 100, "link": "http://stackoverflow.com/users/4327283/sachin", "user_type": "registered", "reputation": 651}, "body": "<p>Don't know why nutch couldn't extract article contents. But I found a workaround for this using Jsoup. I have developed a custom parse-filter plugin which parses the whole doc and sets the parse text in ParseResult returned from parser filter. And used my custom-parse filter by replacing parse-html plugin in <code>parse-plugins.xml</code></p>\n\n<p>It will be something like :-</p>\n\n<pre><code>   document = Jsoup.parse(new String(content.getContent(),\"UTF-8\"),content.getUrl());\n   parse = parseResult.get(content.getUrl());\n   status = parse.getData().getStatus();\n   title = document.title();\n   parseData = new ParseData(status, title,parse.getData().getOutlinks(), parse.getData().getContentMeta(), parse.getData().getParseMeta());\n   parseResult.put(content.getUrl(), new ParseText(document.body().text()), parseData);\n</code></pre>\n", "question_id": 38761035, "creation_date": 1474611673, "is_accepted": true, "score": 3, "last_activity_date": 1474612031, "answer_id": 39653868}], "question_id": 38761035, "tags": ["web-crawler", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/38761035/nutch-crawler-doesnt-retrieve-news-article-content", "last_activity_date": 1474612031, "accepted_answer_id": 39653868, "body": "<p>I was trying to crawl news articles from the links :-</p>\n\n<p><a href=\"http://www.bloomberg.com/press-releases/2016-07-05/apple-donate-life-america-bring-national-organ-donor-registration-to-iphone\" rel=\"nofollow\">Article 1</a></p>\n\n<p><a href=\"http://www.bloomberg.com/press-releases/2016-07-08/network-1-announces-settlement-of-patent-litigation-with-apple-inc\" rel=\"nofollow\">Article 2</a></p>\n\n<p>But I'm not getting the text out of the page to the content field in index(elasticsearch).</p>\n\n<p>Outcome of the crawling is :-</p>\n\n<pre><code>{\n  \"took\": 2,\n  \"timed_out\": false,\n  \"_shards\": {\n    \"total\": 5,\n    \"successful\": 5,\n    \"failed\": 0\n  },\n  \"hits\": {\n    \"total\": 2,\n    \"max_score\": 0.09492774,\n    \"hits\": [\n      {\n        \"_index\": \"news\",\n        \"_type\": \"doc\",\n        \"_id\": \"http://www.bloomberg.com/press-releases/2016-07-08/network-1-announces-settlement-of-patent-litigation-with-apple-inc\",\n        \"_score\": 0.09492774,\n        \"_source\": {\n          \"tstamp\": \"2016-08-04T07:21:59.614Z\",\n          \"segment\": \"20160804125156\",\n          \"digest\": \"d583a81c0c4c7510f5c842ea3b557992\",\n          \"host\": \"www.bloomberg.com\",\n          \"boost\": \"1.0\",\n          \"id\": \"http://www.bloomberg.com/press-releases/2016-07-08/network-1-announces-settlement-of-patent-litigation-with-apple-inc\",\n          \"url\": \"http://www.bloomberg.com/press-releases/2016-07-08/network-1-announces-settlement-of-patent-litigation-with-apple-inc\",\n          \"content\": \"\"\n        }\n      },\n      {\n        \"_index\": \"news\",\n        \"_type\": \"doc\",\n        \"_id\": \"http://www.bloomberg.com/press-releases/2016-07-05/apple-donate-life-america-bring-national-organ-donor-registration-to-iphone\",\n        \"_score\": 0.009845509,\n        \"_source\": {\n          \"tstamp\": \"2016-08-04T07:22:05.708Z\",\n          \"segment\": \"20160804125156\",\n          \"digest\": \"2a94a32ffffd0e03647928755e055e30\",\n          \"host\": \"www.bloomberg.com\",\n          \"boost\": \"1.0\",\n          \"id\": \"http://www.bloomberg.com/press-releases/2016-07-05/apple-donate-life-america-bring-national-organ-donor-registration-to-iphone\",\n          \"url\": \"http://www.bloomberg.com/press-releases/2016-07-05/apple-donate-life-america-bring-national-organ-donor-registration-to-iphone\",\n          \"content\": \"\"\n        }\n      }\n    ]\n  }\n}\n</code></pre>\n\n<p>in which we can notice that content field is empty. I've tried with different options in nutch-site.txt. But still the outcome remains the same. Please help me on this.</p>\n", "creation_date": 1470295719, "score": 2},
{"title": "bin/nutch inject crawl/crawldb urls not working", "view_count": 63, "owner": {"age": 22, "answer_count": 16, "creation_date": 1450071134, "user_id": 5676586, "accept_rate": 100, "view_count": 46, "location": "Alappuzha", "reputation": 447}, "is_answered": true, "answers": [{"question_id": 39360980, "owner": {"user_id": 5998715, "link": "http://stackoverflow.com/users/5998715/mohammed-hashim-p-a", "user_type": "registered", "reputation": 36}, "body": "<p>I was going through the same problem. The documentation seems to be outdated. It is for 1.x .</p>\n\n<p>For 2.x I have tried the following and it worked for me.</p>\n\n<pre><code>bin/nutch inject urls\n</code></pre>\n\n<p>Hope it helps.</p>\n", "creation_date": 1474437995, "is_accepted": true, "score": 2, "last_activity_date": 1474437995, "answer_id": 39608653}], "question_id": 39360980, "tags": ["apache", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/39360980/bin-nutch-inject-crawl-crawldb-urls-not-working", "last_activity_date": 1474437995, "accepted_answer_id": 39608653, "body": "<p>I just followed the tutorial to setup Nutch from <a href=\"http://wiki.apache.org/nutch/NutchTutorial\" rel=\"nofollow\">NutchWiki</a>.</p>\n\n<p>Downloaded Nutch 2.x src and set all configurations.\nThe problem occurs when I just started to crawl. \nWhen I run this code : <code>bin/nutch inject crawl/crawldb urls</code> I am getting an error message like this : <code>Unrecognized arg urls</code>\nI just followed all steps in the tutorial, created directories, made changes to configuration files etc. And I also have a query that there is no crawldb directory in the apache-nutch-2.x/runtime/local/ Is it automatically generated or need to manually generate it ?\nAny help to this problem will be appreciated.</p>\n", "creation_date": 1473220567, "score": 1},
{"title": "How to search tamil word in indexed content in solr", "view_count": 34, "owner": {"user_id": 6742044, "answer_count": 2, "creation_date": 1471841898, "accept_rate": 50, "view_count": 1, "reputation": 3}, "is_answered": true, "answers": [{"question_id": 39272959, "owner": {"user_id": 4009944, "link": "http://stackoverflow.com/users/4009944/neechalkaran", "user_type": "registered", "reputation": 91}, "body": "<p>This should be an encoding issue. try this solutions\n<a href=\"http://stackoverflow.com/questions/9825793/utf-8-characters-not-showing-properly\">UTF-8 characters not showing properly</a>\nTamil character spaces in unicode block are in between 2944 to 3071</p>\n", "creation_date": 1474422980, "is_accepted": true, "score": 2, "last_activity_date": 1474422980, "answer_id": 39606343}], "question_id": 39272959, "tags": ["solr", "web-crawler", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/39272959/how-to-search-tamil-word-in-indexed-content-in-solr", "last_activity_date": 1474422980, "accepted_answer_id": 39606343, "body": "<p>I am using nutch and solr for crawling.nutch crawled and indexed the contents from Tamil language website.but during search in solr, i give Tamil words ,the given word was <strong><em>converted into unicode</em></strong>.so,unable to find the particular content in the indexed documents.</p>\n", "creation_date": 1472736714, "score": 0},
{"title": "how to index all metatags in nutch", "view_count": 461, "is_answered": false, "answers": [{"question_id": 26434292, "owner": {"user_id": 1044441, "link": "http://stackoverflow.com/users/1044441/zhur", "user_type": "registered", "reputation": 87}, "body": "<p>Plugin index-metadata doesn't work that way. You have to specify complete name there, e.g. \"metatag.keywords\".</p>\n\n<p>Also \"metatags.names\" value \"<em>\" is not really wildcard. You can't put something like \"meta</em>\" there as well.</p>\n", "creation_date": 1474396009, "is_accepted": false, "score": 0, "last_activity_date": 1474396009, "answer_id": 39601266}], "question_id": 26434292, "tags": ["solr", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/26434292/how-to-index-all-metatags-in-nutch", "last_activity_date": 1474396009, "owner": {"user_id": 3919720, "view_count": 1, "answer_count": 0, "creation_date": 1407438073, "reputation": 6}, "body": "<p>I have installed Nutch 1.9 and configured it to successfully crawl with Solr 4.10.1. I am trying to set Nutch to index metadata as outlined here <a href=\"https://wiki.apache.org/nutch/IndexMetatags\" rel=\"nofollow\">https://wiki.apache.org/nutch/IndexMetatags</a></p>\n\n<p>How do I set it to index ALL of the metadata on a site? I set the value for metatags.names to *  like this </p>\n\n<p><div class=\"snippet\" data-lang=\"js\" data-hide=\"false\">\r\n<div class=\"snippet-code\">\r\n<pre class=\"snippet-code-html lang-html prettyprint-override\"><code>&lt;property&gt;\r\n    &lt;name&gt;metatags.names&lt;/name&gt;\r\n    &lt;value&gt;*&lt;/value&gt;\r\n    &lt;description&gt;Names of the metatags to extract, separated by ','. Use '*' to extract all metatags. Prefixes the names with 'metatag.' in the parse-metadata. For instance to index description and keywords, you need to activate the plugin index-metadata and set the\r\n    value of the parameter 'index.parse.md' to 'metatag.description,metatag.keywords'.\r\n    &lt;/description&gt;\r\n&lt;/property&gt;</code></pre>\r\n</div>\r\n</div>\r\n</p>\n\n<p>but I am unsure of how to set the value for index.parse.md without listing individual metatag names. I tried this </p>\n\n<p><div class=\"snippet\" data-lang=\"js\" data-hide=\"false\">\r\n<div class=\"snippet-code\">\r\n<pre class=\"snippet-code-html lang-html prettyprint-override\"><code>&lt;property&gt;\r\n    &lt;name&gt;index.parse.md&lt;/name&gt;\r\n    &lt;value&gt;meta*&lt;/value&gt;\r\n    &lt;description&gt;Comma-separated list of keys to be taken from the parse metadata to generate fields. Can be used e.g. for 'description' or 'keywords' provided that these values are generated by a parser (see parse-metatags plugin)\r\n    &lt;/description&gt;\r\n&lt;/property&gt;</code></pre>\r\n</div>\r\n</div>\r\n</p>\n\n<p>but that doesn't display any metadata when running </p>\n\n<p><div class=\"snippet\" data-lang=\"js\" data-hide=\"false\">\r\n<div class=\"snippet-code\">\r\n<pre class=\"snippet-code-html lang-html prettyprint-override\"><code>bin/nutch indexchecker http://nutch.apache.org/</code></pre>\r\n</div>\r\n</div>\r\n</p>\n\n<p>and I am sure there is metadata on that site because it returns Parse Metadata when running </p>\n\n<p><div class=\"snippet\" data-lang=\"js\" data-hide=\"false\">\r\n<div class=\"snippet-code\">\r\n<pre class=\"snippet-code-html lang-html prettyprint-override\"><code>bin/nutch parsechecker http://nutch.apache.org/</code></pre>\r\n</div>\r\n</div>\r\n</p>\n\n<p>Any help would be greatly appreciated! Thanks</p>\n", "creation_date": 1413585050, "score": 1},
{"title": "apache nutch 2.3 2.x solr 4.8 hbase", "view_count": 14, "is_answered": false, "question_id": 39594594, "tags": ["apache", "solr", "hbase", "nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/39594594/apache-nutch-2-3-2-x-solr-4-8-hbase", "last_activity_date": 1474375784, "owner": {"user_id": 5528045, "view_count": 5, "answer_count": 0, "creation_date": 1446707527, "reputation": 6}, "body": "<p>I installed apache nutch 2.3,solr 4.8 and hbase 0.9 and crawled website and got content. I want to index crawled content based on HEADER Data, HTML Tag,Tag class name or Tag id Or Crawled such way that easily fetch content HEADER Data, HTML Tag,Tag class name or Tag id .I Googed got some idea also most of them for nutch 1.x. \nPlease Help me.  </p>\n", "creation_date": 1474375784, "score": 1},
{"title": "Apache Nutch 2.3.1 plugin not going to work", "view_count": 31, "is_answered": true, "answers": [{"question_id": 38590343, "owner": {"user_id": 3287636, "accept_rate": 75, "link": "http://stackoverflow.com/users/3287636/anup", "user_type": "registered", "reputation": 86}, "body": "<p>try changing the extension id in plugin.xml. Change it to \"org.apache.nutch.indexer.AddField\" and re-build Nutch</p>\n\n<pre><code>&lt;extension id=\"org.apache.nutch.indexer.AddField\"\n       name=\"Add Field to Index\"\n       point=\"org.apache.nutch.indexer.IndexingFilter\"&gt;\n     &lt;implementation id=\"myPlugin\"\n         class=\"org.apache.nutch.indexer.AddField\"/&gt;\n&lt;/extension&gt;\n</code></pre>\n\n<p>I think that should solve the issue.</p>\n\n<p>Also just to verify that the control is coming to your plugin class or not add some info log in your code like </p>\n\n<p>LOG.info(\"printing from plugin\");<br>\nIf you are able to see these logs in hadoop.log that means control is coming to plugin class.</p>\n", "creation_date": 1474357756, "is_accepted": false, "score": 1, "last_activity_date": 1474357756, "answer_id": 39588504}], "question_id": 38590343, "tags": ["java", "apache", "plugins", "solr", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/38590343/apache-nutch-2-3-1-plugin-not-going-to-work", "last_activity_date": 1474357756, "owner": {"user_id": 3454410, "answer_count": 20, "creation_date": 1395646154, "accept_rate": 48, "view_count": 174, "reputation": 832}, "body": "<p>I have to extract some metadata info of crawled data by Apache Nutch 2.3.1 that is not provided by Nutch at default. For That I have to write a plugin. For learning purpose, I have taken <a href=\"http://florianhartl.com/nutch-plugin-tutorial.html\" rel=\"nofollow\">Nutch tutorial</a> as starting point. I know this tutorial is for 1.x version. I have change all required classed and build it successfully. Following are the steps that I have followed.</p>\n\n<ol>\n<li>Create a directory like $NUTCH_HOME/src/plugin/myPlugin</li>\n<li>Copy index-metadata to my plugina and create a file myField.java\ncp -r index-metadata/* myPlugin/</li>\n<li>Directory listing should be like </li>\n</ol>\n\n<blockquote>\n<pre><code>myPlugin/plugin.xml\nbuild.xml\nivy.xml\nsrc/java/org/apache/nutch/indexer/AddField.java\n</code></pre>\n</blockquote>\n\n<ol start=\"4\">\n<li>plugin/myplgin/plugin.xml should look like this</li>\n</ol>\n\n<blockquote>\n<pre><code>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;plugin id=\"myPlugin\" name=\"Add Field to Index\"\n    version=\"1.0.0\" provider-name=\"your name\"&gt;\n   &lt;runtime&gt;\n     &lt;library name=\"myPlugin.jar\"&gt;\n       &lt;export name=\"*\"/&gt;\n     &lt;/library&gt;\n   &lt;/runtime&gt; \n   &lt;extension id=\"org.apache.nutch.indexer.myPlugin\"\n       name=\"Add Field to Index\"\n       point=\"org.apache.nutch.indexer.IndexingFilter\"&gt;\n     &lt;implementation id=\"myPlugin\"\n         class=\"org.apache.nutch.indexer.AddField\"/&gt;\n   &lt;/extension&gt;\n&lt;/plugin&gt;\n</code></pre>\n</blockquote>\n\n<ol start=\"5\">\n<li>change build.xml like</li>\n</ol>\n\n<blockquote>\n<pre><code>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;project name=\"myPlugin\" default=\"jar\"&gt;\n  &lt;import file=\"../build-plugin.xml\"/&gt;\n&lt;/project&gt;\n</code></pre>\n</blockquote>\n\n<ol start=\"6\">\n<li>Then  </li>\n</ol>\n\n<blockquote>\n  <p><code>&lt;ant dir=\"myPlugin\" target=\"deploy\" /&gt;</code></p>\n</blockquote>\n\n<ol start=\"7\">\n<li><p>edit your ./conf/nutch-site.xml</p>\n\n<blockquote>\n<pre><code>&lt;property&gt;\n  &lt;name&gt;plugin.includes&lt;/name&gt;\n  &lt;value&gt;plugin-1|plugin-2|myPlugin&lt;/value&gt;\n  &lt;description&gt;Added myPlugin&lt;/description&gt;\n&lt;/property&gt;\n</code></pre>\n</blockquote></li>\n<li><p>Add following line in schema.xml and solrindex-mapping.xml respectively</p>\n\n<blockquote>\n<pre><code>&lt;field name=\"pageLength\" type=\"long\" stored=\"true\" indexed=\"true\"/&gt;\n&lt;field dest=\"pageLength\" source=\"pageLength\"/&gt;\n</code></pre>\n</blockquote></li>\n<li><p>Then I have compiled my written code ( similar to given example in URL )</p></li>\n</ol>\n\n<p>When I run Nutch in local mode, Following is indexing to solr step log info</p>\n\n<pre><code>Active IndexWriters :\nSOLRIndexWriter\n    solr.server.url : URL of the SOLR instance (mandatory)\n    solr.commit.size : buffer size when sending to SOLR (default 1000)\n    solr.mapping.file : name of the mapping file for fields (default solrindex-mapping.xml)\n    solr.auth : use authentication (default false)\n    solr.auth.username : username for authentication\n    solr.auth.password : password for authentication    \nIndexingJob: done.\n</code></pre>\n\n<p>I have added field pageLength in solr schema also. According to my expectation, there should be a new field pageLength with proper values but there is no field in solr. </p>\n\n<p>Where is the problem? Its a simple toy example.\nThis is nutch log file (hadoop.log) output for indexing step</p>\n\n<pre><code>2016-07-26 16:53:25,649 INFO  solr.SolrMappingReader - source: content dest: content\n2016-07-26 16:53:25,649 INFO  solr.SolrMappingReader - source: title dest: title\n2016-07-26 16:53:25,649 INFO  solr.SolrMappingReader - source: host dest: host\n2016-07-26 16:53:25,649 INFO  solr.SolrMappingReader - source: batchId dest: batchId\n2016-07-26 16:53:25,649 INFO  solr.SolrMappingReader - source: boost dest: boost\n2016-07-26 16:53:25,649 INFO  solr.SolrMappingReader - source: digest dest: digest\n2016-07-26 16:53:25,649 INFO  solr.SolrMappingReader - source: tstamp dest: tstamp\n2016-07-26 16:53:25,649 INFO  solr.SolrMappingReader - source: pageLength dest: pageLength\n2016-07-26 16:53:26,140 INFO  solr.SolrIndexWriter - Total 1 document is added.\n2016-07-26 16:53:26,140 INFO  indexer.IndexingJob - IndexingJob: done.\n</code></pre>\n\n<p>How I can confirm that plugin is loaded by nutch ?\nSecond, is there any way to test Nutch plugin before I configure it to nutch for crawling?</p>\n", "creation_date": 1469537312, "score": 1},
{"title": "Near Dupliate Document detection using TextProfileSignature fnv-text-profile-signature", "view_count": 19, "is_answered": false, "answers": [{"question_id": 39418334, "owner": {"user_id": 2544747, "accept_rate": 50, "link": "http://stackoverflow.com/users/2544747/andrew", "user_type": "registered", "reputation": 201}, "body": "<p>Turns out that it is very easy to call the code on github. Understand that I did all of this on a Fedora Linux computer. I did the following:</p>\n\n<p>I built the code using \"./gradlew build\"</p>\n\n<p>I added the generated JAR file to my local maven repository: mvn install:install-file -Dfile=/home/andy/fnv-text-profile-signature-1.0.jar -DgroupId=com.casetext -DartifactId=textprofilesignature -Dversion=1.0 -Dpackaging=jar</p>\n\n<p>After setting the dependency in my POM file, I can use the following trivial code:</p>\n\n<pre><code>TextProfileSignature signer = new TextProfileSignature(quantizationRate, minimumTokenLength);\nsigner.addText(s);\nString signature = signer.getSignature();\n</code></pre>\n\n<p>Very easy to do. I found that I could process roughly 70 documents a second. I tested against 5200 documents. For me, this was pretty slow, so I looked deeper. </p>\n\n<p>I found out that I can directly call the SOLR / LUCENE version with no problem.</p>\n\n<p>First, I added this to my POM file:</p>\n\n<pre><code>    &lt;dependency&gt;\n        &lt;groupId&gt;org.apache.solr&lt;/groupId&gt;\n        &lt;artifactId&gt;solr-core&lt;/artifactId&gt;\n        &lt;version&gt;4.6.0&lt;/version&gt;\n    &lt;/dependency&gt;\n</code></pre>\n\n<p>I included some dependencies:</p>\n\n<pre><code>import org.apache.solr.update.processor.TextProfileSignature;\nimport org.apache.solr.common.params.ModifiableSolrParams;\n</code></pre>\n\n<p>The SOLR version returns byte[] rather than string, and it accepts no parameters</p>\n\n<pre><code>    TextProfileSignature signer = new TextProfileSignature();\n    signer.init(params);\n    signer.add(s);\n    byte[] signature = signer.getSignature();\n</code></pre>\n\n<p>If you do not use init, you have the default parameter values, which is probably acceptable for most uses. This is how I initialized the parameter object to use my values:</p>\n\n<pre><code>    ModifiableSolrParams params = new ModifiableSolrParams();\n    params.set(\"minTokenLen\", minimumTokenLength);\n    params.set(\"quantRate\", Float.toString(quantizationRate));\n</code></pre>\n\n<p>I determined this by trial and error, and also by looking at some source code. I have not rigorously verified that I am properly setting the parameters, but the number of unique documents returned is 4691 as opposed to 4690; close enough for me to accept it as correct, especially since the slower version generates a 127-bit FNV-1 hash and the SOLR version seems to be generating an MD5, but again, I did not push too deeply to see.</p>\n\n<p>So ultimately, why do I care about using the SOLR version, because it seems to have processed about 10,000 documents a second rather than 70 documents a second; but I need to run a few more tests to see if I still agree with this number.</p>\n", "creation_date": 1474313830, "is_accepted": false, "score": 0, "last_activity_date": 1474313830, "answer_id": 39580793}], "question_id": 39418334, "tags": ["solr", "duplicates", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/39418334/near-dupliate-document-detection-using-textprofilesignature-fnv-text-profile-sig", "last_activity_date": 1474313830, "owner": {"user_id": 2544747, "answer_count": 29, "creation_date": 1372816104, "accept_rate": 50, "view_count": 37, "location": "Columbus, OH", "reputation": 201}, "body": "<p>I have numerous documents already converted to text. Many of these documents are harvested web pages. Apache Tika is used for some of this (if anyone cares).</p>\n\n<p>I would like a Java Library that I can use to find near duplicates (NDD). I could provide you with links to different methods and documentation on this, but, this question is specifically about the use of TextProfileSignature. That said, if I am missing something obvious from another existing package, I am rather new to Near Duplicate Detection.</p>\n\n<p>I first found the TextProfileSignature class in SOLR</p>\n\n<p><a href=\"https://lucene.apache.org/solr/5_2_1/solr-core/org/apache/solr/update/processor/TextProfileSignature.html\" rel=\"nofollow\">https://lucene.apache.org/solr/5_2_1/solr-core/org/apache/solr/update/processor/TextProfileSignature.html</a> </p>\n\n<p>It is stated that the algorithm was taken from Clutch</p>\n\n<p>org.apache.nutch.crawl.TextProfileSignature</p>\n\n<p>And then to muddle the waters, it looks like the implementation is actually available directly on GitHub</p>\n\n<p><a href=\"https://github.com/casetext/fnv-text-profile-signature\" rel=\"nofollow\">https://github.com/casetext/fnv-text-profile-signature</a></p>\n\n<p>It is clear to me that if I install SOLR/Lucene, when I feed documents into SOLR I can configure it to run NDD and populate the text profile signature. For my uses, I was hoping to not run my documents through SOLR/Lucene, but rather, simply generate the text profile signature.</p>\n\n<p>I was not able to find any example code using any instances of this outside of the provided packages. While preparing to ask this question, I found the GITHUB code, and it appears as though this is probably the best way for me to go since it looks like it will provide a stand-alone package without trying to extract JARS from the much larger SOLR package.</p>\n\n<p>I have followed many trails and this is how far I have gone.... So, any example code to use these classes in your own code?</p>\n", "creation_date": 1473448455, "score": 1},
{"title": "Nutch Fetcher aborting with N hung threads", "view_count": 839, "owner": {"user_id": 1187062, "answer_count": 3, "creation_date": 1328258401, "accept_rate": 64, "view_count": 28, "reputation": 88}, "is_answered": true, "answers": [{"question_id": 10331440, "owner": {"user_id": 1150329, "accept_rate": 89, "link": "http://stackoverflow.com/users/1150329/tejas-patil", "user_type": "registered", "reputation": 4793}, "body": "<p>Some requests seem to hang, despite all intentions. This happens when the Fetcher threads don't perform any activity for a long time. See line 932-936 <a href=\"http://www.docjar.com/html/api/org/apache/nutch/fetcher/Fetcher.java.html\" rel=\"nofollow\">here</a>.</p>\n\n<p><strong>Steps to deal here:</strong></p>\n\n<ol>\n<li>check what urls were been crawled just before this message was logged in log file. (see fetching... statements in the log). </li>\n<li>Are those urls taking lot of time to load ? (try to <code>wget</code> those urls from the same machine. </li>\n<li>Is the content of those pages big ? (check their size))</li>\n<li>The timeout value is typically 600 seconds. Increase the value of configuration <a href=\"http://hadoop.apache.org/common/docs/r0.20.0/mapred-default.html\" rel=\"nofollow\">mapred.task.timeout</a> in mapred-site.xml of hadoop configuration. (For local mode, simply add the value in nutch-site.xml with larger value)</li>\n<li>Are you performing any operation (say parsing) which is taking really lot of time ? Is the application hanging somewhere ?</li>\n</ol>\n\n<p>I think if u work of these things, u can get it fixed.</p>\n\n<p>also read <a href=\"http://lucene.472066.n3.nabble.com/Nutch-1-2-fetcher-aborting-with-N-hung-threads-td2411724.html\" rel=\"nofollow\">this</a> and <a href=\"http://lucene.472066.n3.nabble.com/Nutch-1-1-Crawl-is-slow-hangs-and-aborts-eventually-td1402770.html\" rel=\"nofollow\">this</a>.</p>\n", "creation_date": 1335714519, "is_accepted": true, "score": 2, "last_activity_date": 1335714519, "answer_id": 10373508}], "question_id": 10331440, "tags": ["nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/10331440/nutch-fetcher-aborting-with-n-hung-threads", "last_activity_date": 1474053607, "accepted_answer_id": 10373508, "body": "<p>I am using Nutch-1.4 for crawling websites. the issue i am facing in crawling is fetcher always aborts with N hung threads.\nEntries in log file are,</p>\n\n<blockquote>\n  <p>INFO  fetcher.Fetcher - -activeThreads=1, spinWaiting=0, fetchQueues.totalSize=0<br>\n  INFO  fetcher.Fetcher - -activeThreads=1, spinWaiting=0, fetchQueues.totalSize=0<br>\n  INFO  fetcher.Fetcher - -activeThreads=1, spinWaiting=0, fetchQueues.totalSize=0<br>\n  WARN  fetcher.Fetcher - Aborting with 1 hung threads.</p>\n</blockquote>\n\n<p>How to resolve this issue?</p>\n", "creation_date": 1335435389, "score": 1},
{"title": "run nutch2.3.1 on hadoop2", "view_count": 17, "is_answered": false, "question_id": 39485798, "tags": ["hadoop", "nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/39485798/run-nutch2-3-1-on-hadoop2", "last_activity_date": 1473871679, "owner": {"user_id": 4041798, "view_count": 0, "answer_count": 0, "creation_date": 1410769627, "reputation": 6}, "body": "<p>I want to run nutch2.3.1 to crawl data on hadoop2. I have 3 nodes for hadoop2:</p>\n\n<ul>\n<li>crawler1:master</li>\n<li>crawler2:slave</li>\n<li>crawler3:slave</li>\n</ul>\n\n<p>I deployed nutch2.3.1 to crawler1 and run it with following command:\n/usr/local/nutch/deploy/bin/crawl  hdfs://xxx.xxx.xxx.xxx/urls/seed.txt test 5</p>\n\n<p>It works and can crawl data ,but it looks like the crawl job only run on crawler1, the others nodes did not do any job for nutch.</p>\n\n<p>my questions are:</p>\n\n<ol>\n<li>do I need deploy nutch  to crawler2 and crawler3?</li>\n<li>do I need run crawl command on 3 nodes?</li>\n<li>if my steps are wrong ,what are the right steps?</li>\n</ol>\n\n<p>Sorry for my poor English, I really appreciate any help you can provide.</p>\n", "creation_date": 1473841645, "score": 1},
{"title": "Nutch and Elasticsearch Integration", "view_count": 37, "is_answered": false, "answers": [{"question_id": 39454615, "owner": {"user_id": 3125823, "accept_rate": 78, "link": "http://stackoverflow.com/users/3125823/user3125823", "user_type": "registered", "reputation": 286}, "body": "<p>Ok, so after much searching, reading and watching some videos... its pretty clear that Nutch 2.x (2.3) is a good choice. It seems to be better suited going forward and will work with ES.</p>\n\n<p>-HTH anyone else facing similar situation</p>\n", "creation_date": 1473806947, "is_accepted": false, "score": 0, "last_activity_date": 1473806947, "answer_id": 39480105}], "question_id": 39454615, "tags": ["elasticsearch", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/39454615/nutch-and-elasticsearch-integration", "last_activity_date": 1473806947, "owner": {"user_id": 3125823, "answer_count": 11, "creation_date": 1387649781, "accept_rate": 78, "view_count": 68, "reputation": 286}, "body": "<p>I'd like to know which versions of Nutch and Elasticsearch work well together to create a vertical search application (on AWS)?</p>\n\n<p>If I plan on starting with 500 sites to crawl and increase from there, what are the best versions to use together.</p>\n\n<p>I have Nutch 1.10 and ES 1.5 working together on my local machine for dev and testing purposes but I know as my data gets bigger (more sites crawled) this won't be feasible.</p>\n\n<p>I'd like to use AWS EMR and store the crawled data on S3.</p>\n", "creation_date": 1473696855, "score": 1},
{"title": "nutch on Hadoop on google cloud- gloud dataproc", "view_count": 33, "is_answered": false, "answers": [{"question_id": 39439122, "owner": {"user_id": 3777211, "link": "http://stackoverflow.com/users/3777211/dennis-huo", "user_type": "registered", "reputation": 3700}, "body": "<p>You get this exception because you're running the job as user <code>user</code> who isn't in the <code>hadoop</code> group by default, and so the driver is unable to access the local dir. Try the following:</p>\n\n<pre><code>sudo sudo -u mapred hadoop jar \\\n    /home/user/apache-nutch-1.7/runtime/deploy/apache-nutch-1.7.job \\\n    org.apache.nutch.crawl.Crawl /tmp/testnutch/input/urls.txt \\\n    -solr http://SOLRIP:8080/solr/ -depth 5 -topN2\n</code></pre>\n\n<p>Alternatively, if you want to submit via the Dataproc jobs API without SSH'ing into the cluster, Dataproc will run with sufficient permissions as well:</p>\n\n<pre><code>gcloud dataproc jobs submit hadoop --cluster cluster-1 \\\n    --jar apache-nutch-1.7.jar \\\n    org.apache.nutch.crawl.Crawl /tmp/testnutch/input/urls.txt \\\n    -solr http://SOLRIP:8080/solr/ -depth 5 -topN2\n</code></pre>\n", "creation_date": 1473731626, "is_accepted": false, "score": 0, "last_activity_date": 1473731626, "answer_id": 39461139}], "question_id": 39439122, "tags": ["hadoop", "nutch", "gcloud", "google-cloud-dataproc"], "answer_count": 1, "link": "http://stackoverflow.com/questions/39439122/nutch-on-hadoop-on-google-cloud-gloud-dataproc", "last_activity_date": 1473731744, "owner": {"user_id": 1638675, "answer_count": 10, "creation_date": 1346416692, "accept_rate": 0, "view_count": 8, "reputation": 116}, "body": "<p>I get below error when i try to run nutch on hadoop on google cloud (dataproc). any idea why i would be facing the issue</p>\n\n<pre><code>user@cluster-1-m:~/apache-nutch-1.7/build$ hadoop jar /home/user/apache-nutch-1.7/runtime/deploy/apache-nutch-1.7.job org.apache.nutch.crawl.Crawl /tmp/testnutch/input/urls.txt -solr http://SOLRIP:8080/solr/ -depth 5 -topN2\n</code></pre>\n\n<blockquote>\n  <p>16/09/11 17:57:38 INFO crawl.Crawl: crawl started in:\n  crawl-20160911175737 16/09/11 17:57:38 INFO crawl.Crawl: rootUrlDir =\n  -topN2 16/09/11 17:57:38 INFO crawl.Crawl: threads = 10 16/09/11 17:57:38 INFO crawl.Crawl: depth = 5 16/09/11 17:57:38 INFO\n  crawl.Crawl: solrUrl=<a href=\"http://SOLRIP:8080/solr/\" rel=\"nofollow\">http://SOLRIP:8080/solr/</a> 16/09/11 17:57:38 WARN\n  conf.Configuration: Could not make crawl/20160911175738 in local\n  directories from mapredu ce.cluster.local.dir 16/09/11 17:57:38 WARN\n  conf.Configuration:\n  mapreduce.cluster.local.dir[0]=/hadoop/mapred/local Exception in\n  thread \"main\" java.io.IOException: No valid local directories in\n  property: mapreduce.cluster.local. dir\n          at org.apache.hadoop.conf.Configuration.getLocalPath(Configuration.java:2302)\n          at org.apache.hadoop.mapred.JobConf.getLocalPath(JobConf.java:569)\n          at org.apache.nutch.crawl.Crawl.run(Crawl.java:123)\n          at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)\n          at org.apache.nutch.crawl.Crawl.main(Crawl.java:55)\n          at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n          at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n          at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n          at java.lang.reflect.Method.invoke(Method.java:498)\n          at org.apache.hadoop.util.RunJar.run(RunJar.java:221)\n          at org.apache.hadoop.util.RunJar.main(RunJar.java:136)</p>\n</blockquote>\n", "creation_date": 1473617162, "score": 1},
{"title": "Nutch and Elasticsearch index template alias", "view_count": 18, "is_answered": false, "question_id": 39428243, "tags": ["elasticsearch", "nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/39428243/nutch-and-elasticsearch-index-template-alias", "last_activity_date": 1473525344, "owner": {"user_id": 3125823, "answer_count": 11, "creation_date": 1387649781, "accept_rate": 78, "view_count": 68, "reputation": 286}, "body": "<p>I've been searching and reading and it seems like Nutch 1.10 does not support Elasticsearch 1.5 index template alias? Is that correct and if so, if I need that support, do I need to use Nutch 2.x branch?</p>\n", "creation_date": 1473525344, "score": 1},
{"title": "java.lang.RuntimeException: org.apache.nutch.plugin.PluginRuntimeException: java.lang.ClassNotFoundException when parsing with nutch", "view_count": 46, "is_answered": false, "question_id": 39408556, "tags": ["apache", "plugins", "nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/39408556/java-lang-runtimeexception-org-apache-nutch-plugin-pluginruntimeexception-java", "last_activity_date": 1473414187, "owner": {"age": 22, "answer_count": 16, "creation_date": 1450071134, "user_id": 5676586, "accept_rate": 100, "view_count": 46, "location": "Alappuzha", "reputation": 447}, "body": "<p>I'm beginner in Nutch. Tried out some tutorial to crawl the web from <a href=\"http://wiki.apache.org/nutch/NutchTutorial#Install_Nutch\" rel=\"nofollow\">NutchWiki</a>. Then I try to make a custom plugin for parsing with the help of <a href=\"http://wiki.apache.org/nutch/WritingPluginExample\" rel=\"nofollow\">this</a>.\nAll configurations are made. When I parsed the fetched content, I got the following error.</p>\n\n<pre><code>Error parsing: http://example.com/: java.lang.RuntimeException: org.apache.nutch.plugin.PluginRuntimeException: java.lang.ClassNotFoundException: org.apache.nutch.parsefilter.TagExtractorParseFilter\n    at org.apache.nutch.plugin.PluginRepository.getOrderedPlugins(PluginRepository.java:469)\n    at org.apache.nutch.parse.HtmlParseFilters.&lt;init&gt;(HtmlParseFilters.java:35)\n    at org.apache.nutch.parse.html.HtmlParser.setConf(HtmlParser.java:340)\n    at org.apache.nutch.plugin.Extension.getExtensionInstance(Extension.java:163)\n    at org.apache.nutch.parse.ParserFactory.getParsers(ParserFactory.java:136)\n    at org.apache.nutch.parse.ParseUtil.parse(ParseUtil.java:78)\n    at org.apache.nutch.parse.ParseSegment.map(ParseSegment.java:107)\n    at org.apache.nutch.parse.ParseSegment.map(ParseSegment.java:45)\n    at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)\n    at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:453)\n    at org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)\n    at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:243)\n    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n    at java.util.concurrent.FutureTask.run(FutureTask.java:266)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n    at java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.nutch.plugin.PluginRuntimeException: java.lang.ClassNotFoundException: org.apache.nutch.parsefilter.TagExtractorParseFilter\n    at org.apache.nutch.plugin.Extension.getExtensionInstance(Extension.java:167)\n    at org.apache.nutch.plugin.PluginRepository.getOrderedPlugins(PluginRepository.java:441)\n    ... 16 more\nCaused by: java.lang.ClassNotFoundException: org.apache.nutch.parsefilter.TagExtractorParseFilter\n    at java.net.URLClassLoader.findClass(URLClassLoader.java:381)\n    at java.lang.ClassLoader.loadClass(ClassLoader.java:424)\n    at java.lang.ClassLoader.loadClass(ClassLoader.java:357)\n    at org.apache.nutch.plugin.PluginRepository.getCachedClass(PluginRepository.java:331)\n    at org.apache.nutch.plugin.Extension.getExtensionInstance(Extension.java:156)\n    ... 17 more\n</code></pre>\n\n<p>I can't figure what exactly is the problem and I have done all things specified in the tutorial. \nAny help will be appreciated.</p>\n", "creation_date": 1473414187, "score": 1},
{"title": "Does nutch generator use CrawlDB to for initial links?", "view_count": 19, "is_answered": false, "answers": [{"question_id": 39405881, "owner": {"user_id": 432844, "link": "http://stackoverflow.com/users/432844/julien-nioche", "user_type": "registered", "reputation": 939}, "body": "<p>Yes, the injection turns your flat list of seeds into entries in the crawldb. New links discovered by the parsing step are merged with the existing crawldb during the update step. Nutch won't resume crawling all by itself, if the system shutsdown you'll have to restart the Nutch scripts and the actions to take will depend on where it was when it stopped. </p>\n\n<p>StormCrawler might be a better option if you need a continuous crawler that resumes itself in case of failure. </p>\n\n<p>There are quite a few Nutch tutorial around which should help understand how it works, including <a href=\"http://digitalpebble.blogspot.co.uk/2015/09/index-web-with-aws-cloudsearch.html\" rel=\"nofollow\">on from our blog</a> which described both Nutch and StormCrawler.</p>\n", "creation_date": 1473409075, "is_accepted": false, "score": 0, "last_activity_date": 1473409075, "answer_id": 39406938}], "question_id": 39405881, "tags": ["web-crawler", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/39405881/does-nutch-generator-use-crawldb-to-for-initial-links", "last_activity_date": 1473409075, "owner": {"user_id": 5216499, "view_count": 8, "answer_count": 0, "creation_date": 1439320611, "reputation": 21}, "body": "<p>I know that inital seeds are provided by a flat file. However, where are these urls injected into? Is it crawlDB? And if that is the case, are the new links, fetched by the crawler, stored back into crawlDB? and what happens when the system shuts down, is the crawlDB is refreshed on next nutch startup?</p>\n\n<p>Actually i want nutch to resume crawling where it left off in case of system shutdown.</p>\n", "creation_date": 1473405357, "score": 1},
{"title": "Nutch Elasticsearch indexer plugin", "view_count": 11, "is_answered": false, "question_id": 39293752, "tags": ["elasticsearch", "nutch", "indexwriter"], "answer_count": 0, "link": "http://stackoverflow.com/questions/39293752/nutch-elasticsearch-indexer-plugin", "last_activity_date": 1473093718, "owner": {"user_id": 6078412, "view_count": 0, "answer_count": 0, "creation_date": 1458234083, "reputation": 6}, "body": "<p>I have many urls in nutch seed.txt file,  <a href=\"http://www.foodurl1.com,http://www.foofurl2.com\" rel=\"nofollow\">http://www.foodurl1.com,http://www.foofurl2.com</a> etc..\nand want to index all URL in ES under single index e.g. foodindex and each url as separate type e.g foodindex/foodurl1, foodindex/foodurl2. So i can search each url individually based on type. Is there any out of the box way to pass \ntype in elasticindexwriter.java  in ES indexer plugin(by default its using \"doc\" type). </p>\n\n<p>or any other suggestion to achieve this requirement.</p>\n", "creation_date": 1472823243, "score": 1},
{"title": "Using your own web scrapers and data spiders - how to avoid getting blocked?", "view_count": 10, "is_answered": false, "question_id": 39331513, "tags": ["web-crawler", "nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/39331513/using-your-own-web-scrapers-and-data-spiders-how-to-avoid-getting-blocked", "last_activity_date": 1473081564, "owner": {"user_id": 172359, "answer_count": 107, "creation_date": 1252719253, "accept_rate": 86, "view_count": 580, "reputation": 5984}, "body": "<p>One of our PoC's uses web scrapers/spiders to populate segmented data sets.  In making this a platform, the traffic across these spiders will significantly increase.  I have noticed that some hosts will block data requests from specific IP addresses, so using something like <a href=\"https://qbox.io/blog/scraping-the-web-with-nutch-for-elasticsearch\" rel=\"nofollow\">Nutch</a> or our own spiders can prove difficult.</p>\n\n<p>We have also seen many data aggregation websites like Indeed (for jobs/resumes).  How can we (and do they) get around being blocked when trying to query RSS data from various sources?</p>\n", "creation_date": 1473081564, "score": 1},
{"title": "Injector in nutch does not work", "view_count": 34, "is_answered": false, "question_id": 39287984, "tags": ["hadoop", "elasticsearch", "web-crawler", "nutch", "hortonworks-sandbox"], "answer_count": 0, "link": "http://stackoverflow.com/questions/39287984/injector-in-nutch-does-not-work", "last_activity_date": 1472806318, "owner": {"user_id": 6365111, "view_count": 4, "answer_count": 0, "creation_date": 1463844705, "reputation": 16}, "body": "<p>I try to launch injector in nutch 2.2.1 in Hadoop(hdp sandbox 2.3) with elasticsearch 1.1.1 hbase 1.1.2 gora 0.3  jdk 7 ,but i get this exception :</p>\n\n<pre><code>InjectorJob: org.apache.gora.util.GoraException: java.lang.RuntimeException: org.apache.hadoop.hbase.MasterNotRunningException\nat org.apache.gora.store.DataStoreFactory.createDataStore(DataStoreFactory.java:167)\nat org.apache.gora.store.DataStoreFactory.createDataStore(DataStoreFactory.java:135)\nat org.apache.nutch.storage.StorageUtils.createWebStore(StorageUtils.java:75)\nat org.apache.nutch.crawl.InjectorJob.run(InjectorJob.java:221)\nat org.apache.nutch.crawl.InjectorJob.inject(InjectorJob.java:251)\nat org.apache.nutch.crawl.InjectorJob.run(InjectorJob.java:273)\nat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\nat org.apache.nutch.crawl.InjectorJob.main(InjectorJob.java:282)\nCaused by: java.lang.RuntimeException: org.apache.hadoop.hbase.MasterNotRunningException\nat org.apache.gora.hbase.store.HBaseStore.initialize(HBaseStore.java:127)\nat org.apache.gora.store.DataStoreFactory.initializeDataStore(DataStoreFactory.java:102)\nat org.apache.gora.store.DataStoreFactory.createDataStore(DataStoreFactory.java:161)\n... 7 more\nCaused by: org.apache.hadoop.hbase.MasterNotRunningException\nat org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.getMaster(HConnectionManager.java:394)\nat org.apache.hadoop.hbase.client.HBaseAdmin.&lt;init&gt;(HBaseAdmin.java:94)\nat org.apache.gora.hbase.store.HBaseStore.initialize(HBaseStore.java:109)\n... 9 more\n</code></pre>\n", "creation_date": 1472805620, "score": 1},
{"title": "How to get the crawled pages content and corresponding URL in nutch?", "view_count": 572, "is_answered": false, "answers": [{"question_id": 17802043, "owner": {"user_id": 1965370, "link": "http://stackoverflow.com/users/1965370/do-do", "user_type": "registered", "reputation": 389}, "body": "<p>You can modify Fetch Job of Nutch to get URLs and page content belong to the URLs during the crawling process. In the source code file (src/java/org/apache/nutch/fetcher/FetcherReducer.java):</p>\n\n<pre><code>      case ProtocolStatusCodes.SUCCESS:        // got a page\n          String URL= TableUtil.reverseUrl(fit.url); //URL\n          content = Bytes.toString(ByteBuffer.wrap((content.getContent()))));//URL belong the URL\n          output(fit, content, status, CrawlStatus.STATUS_FETCHED);\n          break;\n</code></pre>\n\n<p>Hope this helps,</p>\n\n<p>Le Quoc Do</p>\n", "creation_date": 1411742362, "is_accepted": false, "score": 0, "last_activity_date": 1411742362, "answer_id": 26062168}, {"question_id": 17802043, "owner": {"user_id": 226568, "accept_rate": 79, "link": "http://stackoverflow.com/users/226568/bob-yoplait", "user_type": "registered", "reputation": 1435}, "body": "<p>With nutch 1, you can do something like:</p>\n\n<pre><code>./bin/nutch readseg -get out-crawl/segments/20160823085007/  \"https://en.wikipedia.org/wiki/Canon\" -nofetch -nogenerate -noparse -noparsedata -noparsetext &gt; Canon.html\n</code></pre>\n\n<p>It still comes with a few line to get rid off at the begining of the file.</p>\n", "creation_date": 1472737284, "is_accepted": false, "score": 0, "last_activity_date": 1472737284, "answer_id": 39273160}], "question_id": 17802043, "tags": ["nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/17802043/how-to-get-the-crawled-pages-content-and-corresponding-url-in-nutch", "last_activity_date": 1472737284, "owner": {"age": 23, "answer_count": 7, "creation_date": 1373650315, "user_id": 2577337, "accept_rate": 73, "view_count": 41, "location": "Mumbai, India", "reputation": 158}, "body": "<p>I want to get the crawled content by nutch in text file. I have used the #readseg commads but output is not fruitful.</p>\n\n<p>Is there is some plugin which can get nutch to crawl and store the url and content in text file.</p>\n", "creation_date": 1374557035, "score": 2},
{"title": "nutch1.8 and solr3.4.0 does not indxing all urls", "view_count": 10, "is_answered": false, "question_id": 39265009, "tags": ["solr", "web-crawler", "nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/39265009/nutch1-8-and-solr3-4-0-does-not-indxing-all-urls", "last_activity_date": 1472714165, "owner": {"user_id": 6742044, "answer_count": 2, "creation_date": 1471841898, "accept_rate": 50, "view_count": 1, "reputation": 3}, "body": "<p>i'm using nutch1.8 and solr3.4.0 ,after fetching the urls using nutch ,the solr does not indexing the all urls.\nFor example:nutch crawl 2000 urls,but no.of indxing urls in solr admin is 54 only.</p>\n", "creation_date": 1472714165, "score": 0},
{"title": "How to crawl a website that has SAML authentication using ManifoldCF or nutch?", "view_count": 167, "is_answered": false, "question_id": 38831413, "tags": ["solr", "saml", "nutch", "full-text-indexing", "manifoldcf"], "answer_count": 0, "link": "http://stackoverflow.com/questions/38831413/how-to-crawl-a-website-that-has-saml-authentication-using-manifoldcf-or-nutch", "last_activity_date": 1472568441, "owner": {"user_id": 4193280, "answer_count": 114, "creation_date": 1414573314, "accept_rate": 27, "view_count": 114, "location": "India", "reputation": 1229}, "body": "<p>I am trying to crawl a website, more specifically a <code>Google Site</code> using <code>ManifoldCF</code> that has SAML authentication and index the crawled data into Apache Solr. But as I crawl the URL, it gives me <code>302</code> redirection to login page and then says <code>RESPONSECODENOTINDEXABLE</code>.</p>\n\n<p>I am not sure if have I authenticated correctly or not. In manifoldCF we have options for <code>HTTP basic</code> authentication, <code>NTLM authentication</code> and <code>Session-based</code> access credentials authentication method. I used <code>Session based</code> authentication method which more looks like a form based authentication rather than <code>SAML</code> authentication.</p>\n\n<p>Has anybody crawled a website using manifoldCF which has <code>SAML</code> authentication? And if not <code>manifoldCF</code>, has anyone been able to accomplish this via Apache Nutch, because I am afraid, it also provides only <code>HTTP</code> basic , <code>Digest</code> and <code>NTLM</code> authentication.</p>\n\n<p>Any insight would be helpful. Can provide more information regarding the issue, if anyone here thinks it can easily be accomplished. Basically when I crawl <a href=\"https://sites.google.com/a/my-sub-domain.com\" rel=\"nofollow\">https://sites.google.com/a/my-sub-domain.com</a>, it redirects to SSO login page and crawler refuses to crawl any more giving a 302 error. It's an intranet based website.</p>\n", "creation_date": 1470665263, "score": 16},
{"title": "Unresolved Dependencies errors When Trying To Build Apache Nutch 2.3.1", "view_count": 63, "is_answered": false, "answers": [{"question_id": 39196799, "owner": {"user_id": 256618, "link": "http://stackoverflow.com/users/256618/mark-oconnor", "user_type": "registered", "reputation": 54719}, "body": "<p>Cannot reproduce your issue.</p>\n\n<p>I was able to successfully compile the code, using the lastest 2.3.1 release candidate:</p>\n\n<pre><code>git clone https://github.com/apache/nutch.git\ncd nutch\ngit checkout -b release-2.3.1rc2 release-2.3.1rc2\nant\n</code></pre>\n\n<p>Could I suggest that if you still have an problem you raise an issue against the project. You will need to quote which version you are trying to compile.</p>\n\n<ul>\n<li><a href=\"https://issues.apache.org/jira/browse/NUTCH\" rel=\"nofollow\">https://issues.apache.org/jira/browse/NUTCH</a></li>\n</ul>\n", "creation_date": 1472452476, "is_accepted": false, "score": 0, "last_activity_date": 1472452476, "answer_id": 39199887}], "question_id": 39196799, "tags": ["apache", "ant", "ivy", "nutch", "avro"], "answer_count": 1, "link": "http://stackoverflow.com/questions/39196799/unresolved-dependencies-errors-when-trying-to-build-apache-nutch-2-3-1", "last_activity_date": 1472452476, "owner": {"user_id": 3077416, "answer_count": 4, "creation_date": 1386415634, "accept_rate": 69, "view_count": 61, "reputation": 137}, "body": "<p>Its my first time to trying setting up and build apache nutch 2.3.1 based on <a href=\"https://www.youtube.com/watch?v=enq1rsnEN1A\" rel=\"nofollow\">this youtube tutorial</a> on Windows 10 got Unresolved Dependencies errors like below:</p>\n\n<blockquote>\n  <p>D:\\apachenutch>ant runtime \n  Buildfile: D:\\apachenutch\\build.xml \n  Trying to override old definition of task javac   [taskdef] Could not load\n  definitions from resource org/sonar/ant/antlib.xml. It could not be\n  found.</p>\n  \n  <p>ivy-probe-antlib:</p>\n  \n  <p>ivy-download:   [taskdef] Could not load definitions from resource\n  org/sonar/ant/antlib.xml. It could not be found.</p>\n  \n  <p>ivy-download-unchecked:</p>\n  \n  <p>ivy-init-antlib:</p>\n  \n  <p>ivy-init:</p>\n  \n  <p>init:\n      [mkdir] Created dir: D:\\apachenutch\\build\n      [mkdir] Created dir: D:\\apachenutch\\build\\classes\n      [mkdir] Created dir: D:\\apachenutch\\build\\release\n      [mkdir] Created dir: D:\\apachenutch\\build\\test\n      [mkdir] Created dir: D:\\apachenutch\\build\\test\\classes</p>\n  \n  <p>clean-lib:</p>\n  \n  <p>resolve-default: [ivy:resolve] :: Apache Ivy 2.3.0 - 20130110142753 ::\n  <a href=\"http://ant.apache.org/ivy/\" rel=\"nofollow\">http://ant.apache.org/ivy/</a> :: [ivy:resolve] :: loading settings ::\n  file = D:\\apachenutch\\ivy\\ivysettings.xml [ivy:resolve] [ivy:resolve]\n  :: problems summary :: [ivy:resolve] :::: WARNINGS [ivy:resolve]<br>\n  :::::::::::::::::::::::::::::::::::::::::::::: [ivy:resolve]<br>\n  ::          UNRESOLVED DEPENDENCIES         :: [ivy:resolve]<br>\n  :::::::::::::::::::::::::::::::::::::::::::::: [ivy:resolve]<br>\n  :: org.apache.avro#avro;1.7.6: configuration not found in\n  org.apache.avro#avro;1.7.6: 'compile'. It was required from\n  org.apache.gora#gora-core;0.6.1 compile [ivy:resolve]<br>\n  :::::::::::::::::::::::::::::::::::::::::::::: [ivy:resolve]\n  [ivy:resolve] [ivy:resolve] :: USE VERBOSE OR DEBUG MESSAGE LEVEL FOR\n  MORE DETAILS</p>\n  \n  <p>BUILD FAILED D:\\apachenutch\\build.xml:468: impossible to resolve\n  dependencies:\n          resolve failed - see output for details</p>\n  \n  <p>Total time: 14 seconds</p>\n  \n  <p>D:\\apachenutch></p>\n</blockquote>\n\n<p>I have been trying with \"Ant Clean\" command and \"Ant Runtime\" command still for many times not luck and got errors like above.</p>\n\n<p>So how I can fix it?</p>\n", "creation_date": 1472427821, "score": 0},
{"title": "How to crawl rss+xml mimetype in nutch", "view_count": 20, "is_answered": false, "question_id": 39180469, "tags": ["java", "plugins", "rss", "feed", "nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/39180469/how-to-crawl-rssxml-mimetype-in-nutch", "last_activity_date": 1472296283, "owner": {"user_id": 6665728, "view_count": 0, "answer_count": 0, "creation_date": 1470106632, "reputation": 1}, "body": "<p>I've been trying to crawl new.google.com/news/feed which returns an application content type application/rss+xml. I've already added all the plugins need in the nutch-site.xml fix the regex-urlfilter.xml in the conf and created my custom plugin. So whenever I do indexchecker and parsechecker I get the data I needed. The problem is whenever I do crawl it doesn't generate segment and crawldb and will return an java.io.IOException : Segment already fetched!</p>\n\n<p>How can I do or is there any workaround that I can do to crawl an rss+xml type? I've already done my search on google still no results found.</p>\n", "creation_date": 1472296283, "score": 0},
{"title": "How to Open an Ant project (Nutch Source) at Intellij Idea?", "view_count": 7386, "is_answered": true, "answers": [{"question_id": 15357462, "owner": {"user_id": 500478, "accept_rate": 75, "link": "http://stackoverflow.com/users/500478/bastien-jansen", "user_type": "registered", "reputation": 5611}, "body": "<p>It seems that dependencies are managed using Ivy (see the <code>ivy</code> folder in the sources archives), so you could try to install the <a href=\"http://plugins.jetbrains.com/plugin/?id=2267\" rel=\"nofollow\">Ivy plugin</a>, which would allow you to fix your classpath issues.</p>\n", "creation_date": 1363082922, "is_accepted": false, "score": 0, "last_activity_date": 1363082922, "answer_id": 15358302}, {"last_edit_date": 1369179185, "owner": {"user_id": 1736002, "accept_rate": 60, "link": "http://stackoverflow.com/users/1736002/cguzel", "user_type": "registered", "reputation": 576}, "body": "<p>I think, you should use ant build for nutch project. Because, pom.xml is problematical for nutch. If you want to use it anyway maven, you check maven dependency in pom.xml .</p>\n\n<p>I think, the problem can be solved with the following: </p>\n\n<ul>\n<li>You create new project via idea and add nutch source. Idea is not\nsupported ivy ant project. You can install ivy plugin for idea, I\nsuppose, Idea12 does not support it. </li>\n</ul>\n\n<p>or</p>\n\n<ul>\n<li>You can create ant project for nutch via eclipse and then save\nproject. Then open the project on idea via eclipse classpath.</li>\n</ul>\n\n<p>or</p>\n\n<ul>\n<li>You can configure classpath. If you use ant build, you should add jars : <code>File-&gt;ProjectStructure</code> as follows: </li>\n</ul>\n\n<p>Create a library\n<img src=\"http://i.stack.imgur.com/VAQ4p.png\" alt=\"click libraries\"></p>\n\n<p>Attach Files\n<img src=\"http://i.stack.imgur.com/F34hP.png\" alt=\"to add jars\"></p>\n\n<p>Then, if you use <strong>ant</strong> build, select jars from NUTCH_HOME/build/lib/* after build.\n<img src=\"http://i.stack.imgur.com/VxWwv.png\" alt=\"select jars\"></p>\n\n<p>If you use <strong>maven</strong> build, select jars from ~/.m2/* (MAVEN_REPO)</p>\n", "question_id": 15357462, "creation_date": 1369178727, "is_accepted": false, "score": 1, "last_activity_date": 1369179185, "answer_id": 16681134}, {"last_edit_date": 1471924903, "owner": {"user_id": 772391, "accept_rate": 100, "link": "http://stackoverflow.com/users/772391/vu-anh", "user_type": "registered", "reputation": 351}, "body": "<p>I finally figure out how to do it. Now our team can dev nutch in IntellIJ</p>\n\n<p>The process we do</p>\n\n<ol>\n<li>Get nutch source from apache.org</li>\n</ol>\n\n<p><code>wget <a href=\"http://www.eu.apache.org/dist/nutch/2.3/apache-nutch-2.3-src.tar.gz\" rel=\"nofollow\">http://www.eu.apache.org/dist/nutch/2.3/apache-nutch-2.3-src.tar.gz</a>\n</code></p>\n\n<ol start=\"2\">\n<li><p>Import nutch source in intellij</p></li>\n<li><p>Get Dependencies by Ant</p>\n\n<ul>\n<li>Run <code>ant runtime</code></li>\n<li>Run <code>ant test</code></li>\n</ul></li>\n<li><p>Import dependencies into Intellij</p>\n\n<ul>\n<li>File > Project Structures</li>\n<li>Library > Ivy</li>\n<li>Click to Plus button</li>\n<li>Select all libraries in <code>apache-nutch-2.3/build/lib</code></li>\n</ul></li>\n</ol>\n\n<p>Now we have a project with nutch source and all dependencies</p>\n", "question_id": 15357462, "creation_date": 1447041696, "is_accepted": false, "score": 1, "last_activity_date": 1471924903, "answer_id": 33602332}], "question_id": 15357462, "tags": ["ant", "intellij-idea", "nutch"], "answer_count": 3, "link": "http://stackoverflow.com/questions/15357462/how-to-open-an-ant-project-nutch-source-at-intellij-idea", "last_activity_date": 1471924903, "owner": {"user_id": 453596, "answer_count": 127, "creation_date": 1285053991, "accept_rate": 77, "view_count": 2042, "reputation": 21412}, "body": "<p>I want to open Nutch 2.1 source file (<a href=\"http://www.eu.apache.org/dist/nutch/2.1/\">http://www.eu.apache.org/dist/nutch/2.1/</a>) at Intellij IDEA. Here is an explanation of how to open it at Eclipse: <a href=\"http://wiki.apache.org/nutch/RunNutchInEclipse\">http://wiki.apache.org/nutch/RunNutchInEclipse</a></p>\n\n<p>However I am not familiar with Ant (I use Maven) and when I open that source file many classes are not known by Intellij. \ni.e.: </p>\n\n<pre><code>org.apache.hadoop.mapreduce.JobContext\norg.apache.gora.mapreduce.GoraMapper\n</code></pre>\n\n<p>How can I add them to library or what should I do?</p>\n", "creation_date": 1363080422, "score": 6},
{"title": "nutch 2.2 generator java Runtime Exception during curl job failed", "view_count": 33, "is_answered": false, "question_id": 39022443, "tags": ["java", "solr", "nutch", "runtimeexception", "solrcloud"], "answer_count": 0, "link": "http://stackoverflow.com/questions/39022443/nutch-2-2-generator-java-runtime-exception-during-curl-job-failed", "last_activity_date": 1471535498, "owner": {"user_id": 1190910, "view_count": 13, "answer_count": 0, "creation_date": 1328462464, "reputation": 33}, "body": "<p>I am getting the below error during curl , sometimes it\u2019s getting fail   .\nI have number of collection ( en,es,it,..etc ) when i check daily several collection have been fail ( not pattern )  , it may be \"en\" collection today but , tomorrow it \u201cen\u201d will curl successfully,\nCannot identify any  partner or issue , have check inodes and disk space , no problem with those ,</p>\n\n<p>Nutch                    : 2.2.1<br>\nJava                       : 1.7<br>\nHbase                   : 0.90.4                  </p>\n\n<p>yesterday for en </p>\n\n<pre><code>InjectorJob: java.lang.RuntimeException: job failed: name=[crawl_en_03_live]inject /software/bea/nutch/live/en_03/seed_03.txt, jobid=job_local406356849_0001\n    at org.apache.nutch.util.NutchJob.waitForCompletion(NutchJob.java:54)\n    at org.apache.nutch.crawl.InjectorJob.run(InjectorJob.java:233)\n    at org.apache.nutch.crawl.InjectorJob.inject(InjectorJob.java:251)\n    at org.apache.nutch.crawl.InjectorJob.run(InjectorJob.java:273)\n    at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n    at org.apache.nutch.crawl.InjectorJob.main(InjectorJob.java:282)\n</code></pre>\n\n<p>another day for norther language </p>\n\n<pre><code>InjectorJob: Injecting urlDir: /software/bea/nutch/live/pt/seed.txt\nInjectorJob: Using class org.apache.gora.hbase.store.HBaseStore as the Gora           storage class.\n  InjectorJob: java.lang.RuntimeException: job failed: name=     [crawl_pt_live]inject /software/bea/nutch/live/pt/seed.txt,     jobid=job_local1327090171_0001\n    at org.apache.nutch.util.NutchJob.waitForCompletion(NutchJob.java:54)\n    at org.apache.nutch.crawl.InjectorJob.run(InjectorJob.java:233)\n    at org.apache.nutch.crawl.InjectorJob.inject(InjectorJob.java:251)\n    at org.apache.nutch.crawl.InjectorJob.run(InjectorJob.java:273)\n    at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n    at org.apache.nutch.crawl.InjectorJob.main(InjectorJob.java:282)\n</code></pre>\n\n<p>any help appreciate </p>\n", "creation_date": 1471535056, "score": 1},
{"title": "Single Crawl script to Crawl website (Nutch) and Index results (Solr)", "view_count": 107, "owner": {"user_id": 1131384, "answer_count": 4, "creation_date": 1325739317, "accept_rate": 68, "view_count": 137, "reputation": 780}, "is_answered": true, "answers": [{"question_id": 38972881, "owner": {"user_id": 4193280, "accept_rate": 27, "link": "http://stackoverflow.com/users/4193280/multi10ant", "user_type": "registered", "reputation": 1229}, "body": "<p>Chillax ! Just Relax !! Have you ever looked into Apache  ManifoldCF project ? It provides a clean interface to crawl web pages , better than Nutch , to lessen the hassle . It is open Source and within a matter of few minutes you can set up a Job with all your parameters and index your data in Server of your choice , be it Solr , Elastic Search , etc . And , once you set up a Job , you can save settings , so that you don't have to intermittently configure things . Also it supports a Rest API that surely allows you to automate your jobs on the fly . Google it . You won't regret . Hope that helps :) .</p>\n", "creation_date": 1471372763, "is_accepted": false, "score": 0, "last_activity_date": 1471372763, "answer_id": 38982221}, {"question_id": 38972881, "owner": {"user_id": 5953351, "link": "http://stackoverflow.com/users/5953351/sebastian-nagel", "user_type": "registered", "reputation": 121}, "body": "<p>There are two possible ways:</p>\n\n<ol>\n<li><p>configure Nutch to re-fetch all previously crawled pages after one week, see the property <code>db.fetch.interval.default</code>. Keep the crawl/ folder and the Solr index as is. Nutch will automatically delete gone pages from Solr. Ev. you should delete old segments after each crawl (<code>rm -rf crawl/segments/*</code>) to avoid that the disk fills up over time.</p></li>\n<li><p>launch each crawl from scratch (just remove the folder <code>crawl/</code> before calling <code>bin/crawl</code>. It's also possible to delete a Solr index from command-line, e.g. by firing:\n<code>\ncurl http://localhost:8983/solr/update --data '&lt;delete&gt;&lt;query&gt;*:*&lt;/query&gt;&lt;/delete&gt;' -H 'Content-type:text/xml; charset=utf-8'\ncurl http://localhost:8983/solr/update --data '&lt;commit/&gt;' -H 'Content-type:text/xml; charset=utf-8'\n</code></p></li>\n</ol>\n\n<p>It's not difficult to combine these commands and including the call of <code>bin/crawl</code> in a short shell script which can be called by cronjob. Of course, it's also easy to modify the script <code>bin/crawl</code> to your own needs.</p>\n", "creation_date": 1471430042, "is_accepted": true, "score": 3, "last_activity_date": 1471430042, "answer_id": 38994204}], "question_id": 38972881, "tags": ["indexing", "solr", "lucene", "web-crawler", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/38972881/single-crawl-script-to-crawl-website-nutch-and-index-results-solr", "last_activity_date": 1471430042, "accepted_answer_id": 38994204, "body": "<p>I am new to Nutch and Solr. I have just taken over the activities and I have to crawl and index my website now.</p>\n\n<p>These are the steps I have been asked to follow.</p>\n\n<ul>\n<li><p>Delete the crawl folders (apache-nutch-1.10\\crawl) </p></li>\n<li><p>Remove the existing indexes:</p></li>\n</ul>\n\n<blockquote>\n  <p>Solr-Admin-> Skyweb->Documents->Document Type (xml) and execute\n  <em>:</em></p>\n</blockquote>\n\n<ul>\n<li>Go to Solr-Admin -> Core Admin -> Click on 'Reload' and then 'Optimize' </li>\n<li>And run the crawl job using the following command:</li>\n</ul>\n\n<blockquote>\n  <p>bin/crawl -i -D solr.server.url=<a href=\"http://IP:8080/solr/website/\">http://IP:8080/solr/website/</a> urls/\n  crawl/ 5</p>\n</blockquote>\n\n<p>I did some research and felt that doing these tasks manually is overwork and the script should take care of all the above tasks.</p>\n\n<p>So my queries\\concerns are:</p>\n\n<p><em>Doesn't the above script take care of the entire process? Do I still need to delete the crawl folders and clear the existing indexes manually?</em> </p>\n\n<p><em>What is the relevance of the Admin tasks - 'Reload' and 'Optimize'?</em> </p>\n\n<p><em>Can I cron schedule the the crawl script to run weekly and will it take care of the entire process?</em> </p>\n\n<p><em>How else can I automate the crawling and indexing to run periodically?</em></p>\n", "creation_date": 1471344427, "score": 1},
{"title": "Create a Nutch 2.x plugin using eclipse", "view_count": 19, "is_answered": false, "answers": [{"question_id": 38968453, "owner": {"user_id": 1977773, "link": "http://stackoverflow.com/users/1977773/jorge-luis", "user_type": "registered", "reputation": 510}, "body": "<p>You can use any editor you want to write your code, as long as you generate a <code>jar</code> that you load in the Nutch plugin system with the right dependencies and configurations in the xml file everything should work. You can check <a href=\"https://wiki.apache.org/nutch/RunNutchInEclipse\" rel=\"nofollow\">https://wiki.apache.org/nutch/RunNutchInEclipse</a> that contains detailed instructions to open and  <strong>run</strong> within eclipse so debugging is easier, but its not required. Specially important is to run <code>ant eclipse</code> in your local copy of the project, so that you can open the Nutch entire source code in Eclipse, once this is done you can create your plugin file structure and start coding. </p>\n\n<p>Hope it helps.</p>\n", "creation_date": 1471390044, "is_accepted": false, "score": 0, "last_activity_date": 1471390044, "answer_id": 38985965}], "question_id": 38968453, "tags": ["eclipse", "plugins", "web-crawler", "hbase", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/38968453/create-a-nutch-2-x-plugin-using-eclipse", "last_activity_date": 1471390044, "owner": {"user_id": 3454410, "answer_count": 20, "creation_date": 1395646154, "accept_rate": 48, "view_count": 174, "reputation": 832}, "body": "<p>I have to write a plugin to parse crawled content vy Nutch 2.3.1. I have decided to use eclipse as Its better than simple editor. Now How can I create a plugin in eclipse and test it via some simple use case ?</p>\n", "creation_date": 1471330777, "score": 0},
{"title": "How to test Apache Nutch plugin via some use cases", "view_count": 21, "is_answered": true, "answers": [{"question_id": 38968525, "owner": {"user_id": 1977773, "link": "http://stackoverflow.com/users/1977773/jorge-luis", "user_type": "registered", "reputation": 510}, "body": "<p>So you wrote an <code>IndexingFilter</code> plugin right? my usual recommendation is to take a look at a plugin somehow \"similar\" to the one you wrote, for instance let's take the <code>index-anchor</code> plugin and checkout how the tests of this plugin is written, take a look at <a href=\"https://github.com/apache/nutch/blob/2.x/src/plugin/index-anchor/src/test/org/apache/nutch/indexer/anchor/TestAnchorIndexingFilter.java\" rel=\"nofollow\">https://github.com/apache/nutch/blob/2.x/src/plugin/index-anchor/src/test/org/apache/nutch/indexer/anchor/TestAnchorIndexingFilter.java</a>. If your plugin read some values from the configuration (a configuration option a path to a file, etc.) you should take a good look at the 36-39 lines (<a href=\"https://github.com/apache/nutch/blob/2.x/src/plugin/index-anchor/src/test/org/apache/nutch/indexer/anchor/TestAnchorIndexingFilter.java#L36-L39\" rel=\"nofollow\">https://github.com/apache/nutch/blob/2.x/src/plugin/index-anchor/src/test/org/apache/nutch/indexer/anchor/TestAnchorIndexingFilter.java#L36-L39</a>).</p>\n\n<p>The general idea es to instantiate the filter that you just wrote, feed it some fake data and assert on the result of the execution of the filter. Although the information on <a href=\"https://wiki.apache.org/nutch/WritingPluginExample-0.9\" rel=\"nofollow\">https://wiki.apache.org/nutch/WritingPluginExample-0.9</a> is specific to the 1.x branch of Nutch it contains general guidelines that could help you get there.</p>\n\n<p>Some plugins also implement the <code>main()</code> method allowing to be invoked using the <code>bin/nutch</code> script directly from the terminal, this is helpful when you want the user to \"play\"/test the configurations values without the need to run a crawl. Once you implement the <code>main()</code> method you can invoke your plugin using the <code>bin/nutch plugin &lt;plugin name&gt; &lt;plugin class&gt; [some additional parameters]</code> command. Keep in mind that the plugin must be activated in the <code>conf/nutch-site.xml</code> file. Take a look at <a href=\"https://github.com/apache/nutch/blob/a3e7420494304bc4de7ee1a0b25a5158108856f5/src/plugin/urlfilter-regex/src/java/org/apache/nutch/urlfilter/regex/RegexURLFilter.java\" rel=\"nofollow\">https://github.com/apache/nutch/blob/a3e7420494304bc4de7ee1a0b25a5158108856f5/src/plugin/urlfilter-regex/src/java/org/apache/nutch/urlfilter/regex/RegexURLFilter.java</a>, this plugin is implemented for the 1.x version of Nutch but it could help you see in more details what I've talked about.</p>\n\n<p>With this you 1) test your implementation in an isolated environment and 2) provide a testing environment for the user/you if for instance your plugin have an additional configuration file with many different options. And my final recommendation: always execute the entire test suite of Nutch before deploying and run a small test crawl to make sure everything is ok.</p>\n", "creation_date": 1471389375, "is_accepted": false, "score": 1, "last_activity_date": 1471389375, "answer_id": 38985881}], "question_id": 38968525, "tags": ["java", "unit-testing", "plugins", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/38968525/how-to-test-apache-nutch-plugin-via-some-use-cases", "last_activity_date": 1471389375, "owner": {"user_id": 3454410, "answer_count": 20, "creation_date": 1395646154, "accept_rate": 48, "view_count": 174, "reputation": 832}, "body": "<p>I have wrote a simple plugin in java for Nutch 2.3.1 using some guide from this <a href=\"http://grokbase.com/t/nutch/user/164c34pq8m/adding-a-new-field-to-nutch-mongodb-datastore-using-plugin\" rel=\"nofollow\">site</a>. Now I have to test it so that I am sure that it is working properly before I integrate it to Nutch. If I simply run by java command then its give classes not found error. How can I test my plugin.</p>\n", "creation_date": 1471331009, "score": 0},
{"title": "Removing menu&#39;s from html during crawl or indexing with nutch and solr", "view_count": 1888, "is_answered": true, "answers": [{"last_edit_date": 1302637059, "owner": {"user_id": 484046, "accept_rate": 81, "link": "http://stackoverflow.com/users/484046/millebii", "user_type": "registered", "reputation": 976}, "body": "<p>If you want to do that I believe you should write a customized parser in nutch, such that the data to index does not contain the data.\nBasically after parsing the text data is raw text without any structure.</p>\n", "question_id": 5617373, "creation_date": 1302549544, "is_accepted": false, "score": 0, "last_activity_date": 1302637059, "answer_id": 5626343}, {"question_id": 5617373, "owner": {"user_id": 259705, "accept_rate": 62, "link": "http://stackoverflow.com/users/259705/mlathe", "user_type": "registered", "reputation": 1917}, "body": "<p>I think you have a few choices:</p>\n\n<ol>\n<li>extend the Nutch HTML parser, and add logic to strip the header out. (There might be better places to do this, like when you have the raw data but before the DOM is parsed)</li>\n<li>make your site smart enough to not draw the header when nutch is crawling. This is pretty easy to do by just checking the User-Agent value in the request header. You might need to do a better job of seeding your crawl since the links in the header won't be there to help nutch find the other pages</li>\n<li>Somehow get Solr to remove the header for the nutch data. I'm not sure how you'd do this, and I <em>think</em> this means you lose some of the Nutch/Solr synergies.</li>\n<li>Somehow edit the Nutch index (just a lucene index). In theory, you could just walk through all documents in the index and do a trimming on the correct property of each Document.</li>\n</ol>\n\n<p>I would think the easiest way to do this, is to do #2 if you have a consistent way of drawing the header (ie a skin or a common include). Then perhaps #1 and #4. I think #3 would be the hardest, but I might be wrong.</p>\n", "creation_date": 1317057415, "is_accepted": false, "score": 1, "last_activity_date": 1317057415, "answer_id": 7558702}, {"question_id": 5617373, "owner": {"user_id": 914705, "accept_rate": 100, "link": "http://stackoverflow.com/users/914705/mike-sokolov", "user_type": "registered", "reputation": 4825}, "body": "<p><a href=\"https://issues.apache.org/jira/browse/SOLR-2597\" rel=\"nofollow\">Here is a patch</a> for SOLR that you can place in your indexing config to ignore the contents of tags you configure.  It will only work with XML, though, so if you can tidy your HTML or you know that it is XHTML, then this would work, but it won't work with just any random HTML.</p>\n", "creation_date": 1317059551, "is_accepted": false, "score": 0, "last_activity_date": 1317059551, "answer_id": 7559100}, {"question_id": 5617373, "owner": {"user_id": 1500564, "link": "http://stackoverflow.com/users/1500564/techguy", "user_type": "registered", "reputation": 172}, "body": "<p>A new feature has been introduced in Nutch 1.12 using apache tika parser which works on boilerpipe algorithm to strip off the header and footer content from html pages in parsing stage itself.</p>\n\n<p>We can use following properties in nutch-site.xml to have this implemented :</p>\n\n<pre><code>&lt;!-- parse-tika plugin properties --&gt;\n&lt;property&gt;\n  &lt;name&gt;tika.extractor&lt;/name&gt;\n  &lt;value&gt;boilerpipe&lt;/value&gt;\n  &lt;description&gt;\n  Which text extraction algorithm to use. Valid values are: boilerpipe or none.\n  &lt;/description&gt;\n&lt;/property&gt;\n&lt;property&gt;\n  &lt;name&gt;tika.extractor.boilerpipe.algorithm&lt;/name&gt;\n  &lt;value&gt;DefaultExtractor&lt;/value&gt;\n  &lt;description&gt;\n  Which Boilerpipe algorithm to use. Valid values are: DefaultExtractor, ArticleExtractor\n  or CanolaExtractor.\n  &lt;/description&gt;\n&lt;/property&gt;\n</code></pre>\n\n<p>Its working for me. Hope it will work for others as well...:)</p>\n\n<p>For detailed overview, you can refer to this ticket : \n<a href=\"https://issues.apache.org/jira/browse/NUTCH-961\" rel=\"nofollow\">https://issues.apache.org/jira/browse/NUTCH-961</a></p>\n", "creation_date": 1471374105, "is_accepted": false, "score": 0, "last_activity_date": 1471374105, "answer_id": 38982576}], "question_id": 5617373, "tags": ["solr", "design-patterns", "nutch"], "answer_count": 4, "link": "http://stackoverflow.com/questions/5617373/removing-menus-from-html-during-crawl-or-indexing-with-nutch-and-solr", "last_activity_date": 1471374105, "owner": {"age": 46, "answer_count": 1, "creation_date": 1280831913, "user_id": 409556, "view_count": 7, "location": "England, United Kingdom", "reputation": 33}, "body": "<p>I am crawling our large website(s) with nutch and then indexing with solr and the results a pretty good. However, there are several menu structures across the site that index and spoil the results of a query. </p>\n\n<p>Each of these menus is clearly defined in a DIV so <code>&lt;div id=\"RHBOX\"&gt; ... &lt;/div&gt; or &lt;div id=\"calendar\"&gt; ...&lt;/div&gt;</code> and several others.</p>\n\n<p>I need to, at some point, delete the content in these DIVS. </p>\n\n<p>I am guessing that the right place is during indexing by solr but cannot work out how.</p>\n\n<p>A pattern would look something like <code>(&lt;div id=\"calendar\"&gt;).*?(&lt;\\/div&gt;)</code> but i cannot get that to work in <code>&lt;tokenizer class=\"solr.PatternTokenizerFactory\" pattern=\"(&lt;div id=\"calendar\"&gt;).*?(&lt;\\/div&gt;)\" /&gt;</code> and I am not really sure where to put it in schema.xml.</p>\n\n<p>When I do put that pattern in schema.xml does not parse.</p>\n", "creation_date": 1302502000, "score": 4},
{"title": "Exception in thread &quot;async-console-appender-1&quot; java.lang.OutOfMemoryError: GC overhead limit exceeded", "view_count": 31, "is_answered": false, "question_id": 38893492, "tags": ["java", "nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/38893492/exception-in-thread-async-console-appender-1-java-lang-outofmemoryerror-gc-ov", "last_activity_date": 1471084986, "owner": {"user_id": 6704342, "view_count": 2, "answer_count": 0, "creation_date": 1470910002, "reputation": 6}, "body": "<p>When I start Web Scrapping by using Apache Nutch,It throws this error: [<code>java.lang.OutOfMemoryError: GC overhead limit exceeded</code>]</p>\n\n<p>I have try to crawl 500 pages of same website and did not get any error while Scrapping 5k-6k pages I received these Error.</p>\n\n<p>Please help</p>\n\n<p><a href=\"http://i.stack.imgur.com/L6CYv.png\" rel=\"nofollow\"><img src=\"http://i.stack.imgur.com/L6CYv.png\" alt=\"Error Page\"></a></p>\n", "creation_date": 1470910483, "score": 0},
{"title": "What is Nutch 1.10 crawl command for elasticsearch", "view_count": 1024, "owner": {"user_id": 3125823, "answer_count": 11, "creation_date": 1387649781, "accept_rate": 78, "view_count": 68, "reputation": 286}, "is_answered": true, "answers": [{"last_edit_date": 1439227435, "owner": {"user_id": 5206685, "link": "http://stackoverflow.com/users/5206685/hubington", "user_type": "registered", "reputation": 36}, "body": "<p>Unfortunately I can't tell you where you're going wrong as I'm in the same boat although from what I can see you are running nutch and elastic on the same box where as I've split it across two.</p>\n\n<p>I've not got it to work but according to a guide I found on integrating nutch 1.7 with elastic it should just be </p>\n\n<pre><code>bin/crawl urls/ TestCrawl -depth 3 -topN 5\n</code></pre>\n\n<p>It may just be it isn't working for me because I've added the extra complication of networking.</p>\n\n<p>I also assume you have created an index called elasticTestIndex in your elastic instance and launched it on the box before trying to run your crawl?</p>\n\n<p>Should it be of help the guide I got that command from is</p>\n\n<p><a href=\"https://www.mind-it.info/integrating-nutch-1-7-elasticsearch/\" rel=\"nofollow\">https://www.mind-it.info/integrating-nutch-1-7-elasticsearch/</a></p>\n\n<p><strong>Update:</strong></p>\n\n<p>I'm not sure I'm quite there yet but using your update I've got further than I had.</p>\n\n<p>You are putting in port 9200 which is the web administartion port but you need to use port 9300 to interact with the service so change the port to 9300 </p>\n\n<p>I'm not sure but I thing the portion after the slash refers to the index so in your example make sure you have \"elastic\" set up as an index. or change</p>\n\n<pre><code>blah (low rep score so can't put in to many urls) blah localhost:9300/[index name]/\n</code></pre>\n\n<p>so that it uses and index you have created. If you haven't created one then you can do so from the putty with the following command.</p>\n\n<pre><code>curl -XPUT 'http://localhost:9200/[index name]/'\n</code></pre>\n\n<p>Using the command you supplied with the alternative port it did run although I've yet to extract the crawl data from elastic.</p>\n\n<p><strong>Supplemental Update:</strong></p>\n\n<p>It's successfully dumping data crawled from nutch into elastic for me and having put a different index in on the command line I can tell you it ignores that and uses what ever is in your nutch-site.xml</p>\n", "question_id": 31885431, "creation_date": 1439056329, "is_accepted": true, "score": 2, "last_activity_date": 1439227435, "answer_id": 31896878}, {"question_id": 31885431, "owner": {"user_id": 3125823, "accept_rate": 78, "link": "http://stackoverflow.com/users/3125823/user3125823", "user_type": "registered", "reputation": 286}, "body": "<p><strong>To help anyone else get it working</strong></p>\n\n<p>Start off by reading this <a href=\"https://www.mind-it.info/2013/09/26/integrating-nutch-1-7-elasticsearch/\" rel=\"nofollow\">blog post</a> to help you get Elasticsearch configured to work with Nutch.</p>\n\n<p>After that read <a href=\"https://wiki.apache.org/nutch/NutchTutorial\" rel=\"nofollow\">this Nutch doc</a> to get familiar with the <strong>NEW</strong> cli command for running the crawl script. (Works for 1.9+)</p>\n\n<p>Follow the example in the new Nutch crawl script command on that page. You have to change it a bit for elasticsearch:\nsolr.server.url=<a href=\"http://localhost:8983/solr/\" rel=\"nofollow\">http://localhost:8983/solr/</a> to something like\nelastic.server.url=<a href=\"http://localhost:9300/yourelasticindex/\" rel=\"nofollow\">http://localhost:9300/yourelasticindex/</a></p>\n\n<p>So basically there are 2 steps:</p>\n\n<ol>\n<li>Configure Elasticsearch to work with Nutch (click on first link above)</li>\n<li>Change the new cli command for solr to work with Elasticsearch (its\ndefault is solr) Hope that helps!</li>\n</ol>\n", "creation_date": 1471006403, "is_accepted": false, "score": 1, "last_activity_date": 1471006403, "answer_id": 38918472}], "question_id": 31885431, "tags": ["elasticsearch", "command", "web-crawler", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/31885431/what-is-nutch-1-10-crawl-command-for-elasticsearch", "last_activity_date": 1471006635, "accepted_answer_id": 31896878, "body": "<p>Using Nutch 1.10 (newbie), I am trying to learn how to crawl using Nutch 1.10 and using ElasticSearch as my indexer. Not sure why, but I can not get this crawl command to work:</p>\n\n<pre><code>bin/crawl -i --elastic -D elastic.server.url=http://localhost:9200/elastic/ urls elasticTestCrawl 1\n</code></pre>\n\n<p>UPDATE: just used</p>\n\n<pre><code>bin/crawl -i -D elastic.server.url=http://localhost:9200/elastic/ urls/ elasticTestCrawl/  2\n</code></pre>\n\n<p>--almost succesfully, received following error when it came to the indexing part of the command:</p>\n\n<pre><code>Error running:\n  /home/david/apache-nutch-1.10/bin/nutch clean -Delastic.server.url=http://localhost:9200/elastic/ elasticTestCrawl//crawldb\nFailed with exit value 255.\n</code></pre>\n\n<p>What is exit value 255 for nutch 1.x? And why does the space get deleted between \"-D and elastic...\"</p>\n\n<p>I have these ElasticSearch Properties from <a href=\"https://www.mind-it.info/integrating-nutch-1-7-elasticsearch/\" rel=\"nofollow\">here</a> in my nutch-site.xml file:</p>\n\n<p>If someone can point my to the error of my ways, that would be great!</p>\n\n<p><strong>Update</strong>\nI just posted my own answer below, its the second one. I had already accepted the first answer months ago when I initially got it working. My answer is simply more clear and concise to make it easier (and quicker) to get started with Nutch.</p>\n", "creation_date": 1438976481, "score": 1},
{"title": "Apache Solr Search API default result filters", "view_count": 14, "owner": {"user_id": 6357863, "answer_count": 0, "creation_date": 1463684440, "view_count": 4, "location": "Warszawa, Polska", "reputation": 25}, "is_answered": true, "answers": [{"question_id": 38892985, "owner": {"user_id": 137650, "link": "http://stackoverflow.com/users/137650/matslindh", "user_type": "registered", "reputation": 13901}, "body": "<p>The <em>cleanest</em> implementation would be to <a href=\"https://cwiki.apache.org/confluence/display/solr/Defining+Fields\" rel=\"nofollow\">add a custom field</a> in your schema, and then <a href=\"https://cwiki.apache.org/confluence/display/solr/Copying+Fields\" rel=\"nofollow\">use <code>copyField</code> to copy</a> the content from <code>url</code> to a <code>url_tokenized</code> field.</p>\n\n<pre><code>&lt;copyField source=\"url\" dest=\"url_tokenized\" /&gt;\n</code></pre>\n\n<p>By <a href=\"https://cwiki.apache.org/confluence/display/solr/Tokenizers#Tokenizers-RegularExpressionPatternTokenizer\" rel=\"nofollow\">using a PatternTokenizer</a> you can tell Solr to split tokens by <code>/</code>, so that you get <code>ru-RU</code> as a token in the <code>url_tokenized</code> field:</p>\n\n<pre><code>&lt;analyzer&gt;\n    &lt;tokenizer class=\"solr.PatternTokenizerFactory\" pattern=\"/\"/&gt;\n&lt;/analyzer&gt;\n</code></pre>\n\n<p>Which should give you something like:</p>\n\n<pre><code>&lt;fieldType name=\"url_tokenized\" class=\"solr.TextField\"&gt;\n  &lt;analyzer&gt;\n    &lt;tokenizer class=\"solr.PatternTokenizerFactory\" pattern=\"/\"/&gt;\n    &lt;filter class=\"solr.LowerCaseFilterFactory\"/&gt;\n  &lt;/analyzer&gt;\n&lt;/fieldType&gt;\n</code></pre>\n\n<p>By adding the LowerCaseFilterFactory we'll make sure that ru-RU and ru-ru both are found regardless of casing used.</p>\n\n<p>Querying would then be done by applying a filter query (<code>fq</code>) to the query string:</p>\n\n<pre><code>...&amp;fq=url_tokenized:ru-ru\n</code></pre>\n\n<p>This will limit the response to documents that contains \"/ru-ru/\" somewhere in its URL.</p>\n", "creation_date": 1470916994, "is_accepted": true, "score": 1, "last_activity_date": 1470916994, "answer_id": 38895858}], "question_id": 38892985, "tags": ["apache", "search", "indexing", "solr", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/38892985/apache-solr-search-api-default-result-filters", "last_activity_date": 1470916994, "accepted_answer_id": 38895858, "body": "<p>I'm using Solr with apache nutch to indexing website</p>\n\n<p>My json result looks like this:</p>\n\n<pre><code>  \"response\": {\n\"numFound\": 0,\n\"start\": 0,\n\"docs\": [\n  {\n    \"id\": \"http://mysite.pl/cl-BR/link/link\",\n    \"url\": \"http://mysite.pl/cl-BR/link/link\",\n    \"content\": [\n      \"content\"\n    ],\n    \"_version_\": 0000\n  },\n  {\n    \"id\": \"http://mysite.pl/ru-RU/link/link\",\n    \"url\": \"http://mysite.pl/ru-RU/link/link\",\n    \"content\": [\n      \"content\"\n    ],\n    \"_version_\": 0000\n  },\n  {\n    \"id\": \"http://mysite.pl/en-EN/link/link\",\n    \"url\": \"http://mysite.pl/en-EN/link/link\",\n    \"content\": [\n      \"content\"\n    ],\n    \"_version_\": 0000\n  },\n</code></pre>\n\n<p>I would like to add parameter to my query, contains information about language into format for example like this: <code>en-EN</code>\nAnd next return only search result where url contains my parameter.</p>\n\n<p>For example:\nMy query is: <code>/solr/CoreName/select?q=you&amp;fl=id,ul,content&amp;urlContains=en-EN</code></p>\n\n<p>My result is:</p>\n\n<pre><code>  \"response\": {\n\"numFound\": 0,\n\"start\": 0,\n\"docs\": [\n  {\n    \"id\": \"http://mysite.pl/en-EN/link/link\",\n    \"url\": \"http://mysite.pl/en-EN/link/link\",\n    \"content\": [\n      \"content\"\n    ],\n    \"_version_\": 0000\n  },\n</code></pre>\n\n<p>And when my query is: <code>/solr/CoreName/select?q=you&amp;fl=id,ul,content&amp;urlContains=ru-RU</code></p>\n\n<p>My result is:</p>\n\n<pre><code>  \"response\": {\n\"numFound\": 0,\n\"start\": 0,\n\"docs\": [\n  {\n    \"id\": \"http://mysite.pl/ru-RU/link/link\",\n    \"url\": \"http://mysite.pl/ru-RU/link/link\",\n    \"content\": [\n      \"content\"\n    ],\n    \"_version_\": 0000\n  },\n</code></pre>\n\n<p>How can i do this?</p>\n", "creation_date": 1470909203, "score": 0},
{"title": "Nutch 1.12 exception java.io.IOException: No FileSystem for scheme: http", "view_count": 97, "is_answered": false, "answers": [{"question_id": 38887056, "owner": {"user_id": 5953351, "link": "http://stackoverflow.com/users/5953351/sebastian-nagel", "user_type": "registered", "reputation": 121}, "body": "<p>The <a href=\"https://wiki.apache.org/nutch/NutchTutorial\" rel=\"nofollow\">tutorial</a> still mentions the deprecated <code>solrindex</code> command. The index command should be\n<code>\nbin/nutch index -Dsolr.server.url=http://.../solr crawldb/ -linkdb linkdb/ segments/*\n</code>\nWithout argument Nutch commands show a command-line help:\n<code>\nbin/nutch index\nUsage: Indexer &lt;crawldb&gt; [-linkdb &lt;linkdb&gt;] [-params k1=v1&amp;k2=v2...] (&lt;segment&gt; ... | -dir &lt;segments&gt;) [-noCommit] [-deleteGone] [-filter] [-normalize] [-addBinaryContent] [-base64]\nActive IndexWriters :\nSOLRIndexWriter\n        solr.server.url : URL of the SOLR instance\n        solr.zookeeper.hosts : URL of the Zookeeper quorum\n        solr.commit.size : buffer size when sending to SOLR (default 1000)\n        solr.mapping.file : name of the mapping file for fields (default solrindex-mapping.xml)\n        solr.auth : use authentication (default false)\n        solr.auth.username : username for authentication\n        solr.auth.password : password for authentication\n</code></p>\n", "creation_date": 1470905305, "is_accepted": false, "score": 0, "last_activity_date": 1470905305, "answer_id": 38891556}], "question_id": 38887056, "tags": ["nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/38887056/nutch-1-12-exception-java-io-ioexception-no-filesystem-for-scheme-http", "last_activity_date": 1470905305, "owner": {"user_id": 1381588, "view_count": 15, "answer_count": 0, "creation_date": 1336467098, "reputation": 3}, "body": "<p>I followed <a href=\"https://wiki.apache.org/nutch/NutchTutorial\" rel=\"nofollow\">https://wiki.apache.org/nutch/NutchTutorial</a> and trying to install and integrate Nutch 1.12 with Solr 5.5.2. I installed Nutch by following steps mentioned in tutorial but when trying to integrate with solr by running below command. It is throwing the below exception.</p>\n\n<p><strong>bin/nutch index <a href=\"http://10.209.18.213:8983/solr\" rel=\"nofollow\">http://10.209.18.213:8983/solr</a> crawl/crawldb/ -linkdb crawl/linkdb/ crawl/segments/* -filter -normalize</strong></p>\n\n<pre><code>Exception \n\n2016-08-11 09:18:40,076 WARN  util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n2016-08-11 09:18:40,383 WARN  segment.SegmentChecker - The input path at crawldb is not a segment... skipping\n2016-08-11 09:18:40,397 INFO  segment.SegmentChecker - Segment dir is complete: crawl/segments/20160810110110.\n2016-08-11 09:18:40,403 INFO  segment.SegmentChecker - Segment dir is complete: crawl/segments/20160810112551.\n2016-08-11 09:18:40,408 INFO  segment.SegmentChecker - Segment dir is complete: crawl/segments/20160810112952.\n2016-08-11 09:18:40,409 INFO  indexer.IndexingJob - Indexer: starting at 2016-08-11 09:18:40\n2016-08-11 09:18:40,415 INFO  indexer.IndexingJob - Indexer: deleting gone documents: false\n2016-08-11 09:18:40,415 INFO  indexer.IndexingJob - Indexer: URL filtering: true\n2016-08-11 09:18:40,415 INFO  indexer.IndexingJob - Indexer: URL normalizing: true\n2016-08-11 09:18:40,672 INFO  indexer.IndexWriters - Adding org.apache.nutch.indexwriter.solr.SolrIndexWriter\n2016-08-11 09:18:40,672 INFO  indexer.IndexingJob - Active IndexWriters :\nSOLRIndexWriter\n        solr.server.url : URL of the SOLR instance\n        solr.zookeeper.hosts : URL of the Zookeeper quorum\n        solr.commit.size : buffer size when sending to SOLR (default 1000)\n        solr.mapping.file : name of the mapping file for fields (default solrindex-mapping.xml)\n        solr.auth : use authentication (default false)\n        solr.auth.username : username for authentication\n        solr.auth.password : password for authentication\n\n\n2016-08-11 09:18:40,677 INFO  indexer.IndexerMapReduce - IndexerMapReduce: crawldb: http://10.209.18.213:8983/solr\n2016-08-11 09:18:40,677 INFO  indexer.IndexerMapReduce - IndexerMapReduce: linkdb: crawl/linkdb\n2016-08-11 09:18:40,677 INFO  indexer.IndexerMapReduce - IndexerMapReduces: adding segment: crawl/segments/20160810110110\n2016-08-11 09:18:40,683 INFO  indexer.IndexerMapReduce - IndexerMapReduces: adding segment: crawl/segments/20160810112551\n2016-08-11 09:18:40,684 INFO  indexer.IndexerMapReduce - IndexerMapReduces: adding segment: crawl/segments/20160810112952\n2016-08-11 09:18:41,362 ERROR indexer.IndexingJob - Indexer: java.io.IOException: No FileSystem for scheme: http\n        at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:2385)\n        at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2392)\n        at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:89)\n        at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2431)\n        at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2413)\n        at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:368)\n        at org.apache.hadoop.fs.Path.getFileSystem(Path.java:296)\n        at org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:256)\n        at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:228)\n        at org.apache.hadoop.mapred.SequenceFileInputFormat.listStatus(SequenceFileInputFormat.java:45)\n        at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:304)\n        at org.apache.hadoop.mapreduce.JobSubmitter.writeOldSplits(JobSubmitter.java:520)\n        at org.apache.hadoop.mapreduce.JobSubmitter.writeSplits(JobSubmitter.java:512)\n        at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:394)\n        at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1285)\n        at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1282)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:415)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\n        at org.apache.hadoop.mapreduce.Job.submit(Job.java:1282)\n        at org.apache.hadoop.mapred.JobClient$1.run(JobClient.java:562)\n        at org.apache.hadoop.mapred.JobClient$1.run(JobClient.java:557)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:415)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\n        at org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:557)\n        at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:548)\n        at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:833)\n        at org.apache.nutch.indexer.IndexingJob.index(IndexingJob.java:145)\n        at org.apache.nutch.indexer.IndexingJob.run(IndexingJob.java:228)\n        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)\n        at org.apache.nutch.indexer.IndexingJob.main(IndexingJob.java:237)\n</code></pre>\n", "creation_date": 1470888092, "score": 0},
{"title": "how to crawl particular website using Apache Nutch?", "view_count": 137, "owner": {"age": 23, "answer_count": 17, "creation_date": 1444830614, "user_id": 5445436, "accept_rate": 80, "view_count": 26, "location": "Pune, India", "reputation": 131}, "is_answered": true, "answers": [{"question_id": 34744072, "owner": {"user_id": 5445436, "accept_rate": 80, "link": "http://stackoverflow.com/users/5445436/lokesh-kakran", "user_type": "registered", "reputation": 131}, "body": "<p>first install the nutch:</p>\n\n<p><strong>under configuration of nutch-site.xml, paste:</strong></p>\n\n<pre><code>&lt;property&gt;\n    &lt;name&gt;http.agent.name&lt;/name&gt;\n    &lt;value&gt;My Nutch Spider&lt;/value&gt;\n&lt;/property&gt;\n</code></pre>\n\n<p><strong>Under your nutch-default.xml: add</strong></p>\n\n<pre><code>&lt;property&gt;\n  &lt;name&gt;http.robot.rules.whitelist&lt;/name&gt;\n  &lt;value&gt;http://nihilent.com/&lt;/value&gt;\n  &lt;description&gt;Comma separated list of hostnames or IP addresses to ignore\n  robot rules parsing for. Use with care and only if you are explicitly\n  allowed by the site owner to ignore the site's robots.txt!\n  &lt;/description&gt;\n&lt;/property&gt;\n</code></pre>\n\n<p><strong>Under regex-urlfilter.txt</strong> :</p>\n\n<pre><code># accept anything else\n+.\n+^http://([a-z0-9]*\\.)*http://nihilent.com/\n</code></pre>\n\n<p><strong>and also comment the</strong></p>\n\n<pre><code># skip URLs containing certain characters as probable queries, etc.\n#-[?*!@=]\n</code></pre>\n\n<p><strong>then run the below commands</strong></p>\n\n<pre><code>bin/nutch inject crawl/crawldb dmoz\nbin/nutch inject crawl/crawldb urls\nbin/nutch generate crawl/crawldb crawl/segments\ns1=`ls -d crawl/segments/2* | tail -1`\necho $s1\nbin/nutch fetch $s1\nbin/nutch parse $s1\nbin/nutch updatedb crawl/crawldb $s1\n\nbin/nutch invertlinks crawl/linkdb -dir crawl/segments\n</code></pre>\n\n<p>Now check your data  in the crawl/crawldb folder &amp; other successfully.</p>\n", "creation_date": 1452687587, "is_accepted": true, "score": 0, "last_activity_date": 1452687587, "answer_id": 34766568}, {"last_edit_date": 1470894716, "owner": {"user_id": 5445436, "accept_rate": 80, "link": "http://stackoverflow.com/users/5445436/lokesh-kakran", "user_type": "registered", "reputation": 131}, "body": "<p>Below are few commands which will help you while doing Nutch in various ways</p>\n\n<ul>\n<li>These command contains direct crwaling on console , big data reading dumpin etc</li>\n<li>I am mentioning all available command which i had done please modify as per your requirements</li>\n</ul>\n\n<h2>Commands Nutch</h2>\n\n<blockquote>\n<pre><code>bin/nutch inject crawl/crawldb dmoz\nbin/nutch inject crawl/crawldb urls\nbin/nutch generate crawl/crawldb crawl/segments\ns4=`ls -d crawl/segments/2* | tail -1`\necho $s1\nbin/nutch fetch $s1\nbin/nutch parse $s1\nbin/nutch updatedb crawl/crawldb $s1\n</code></pre>\n</blockquote>\n\n<pre><code>bin/nutch invertlinks crawl/linkdb -dir crawl/segments\n\nbin/nutch commoncrawldump -outputDir hdfs://localhost:9000/dfs -segment /home/lokesh_Kumar/soft/apache-nutch-1.11/crawl/segments/ -jsonArray -reverseKey -SimpleDateFormat -epochFilename\n\nbin/nutch readseg -dump /home/lokesh_Kumar/soft/apache-nutch-1.11/crawl/segments/ /home/lokesh_Kumar/soft/apache-nutch-1.11/ndeploy/1\n\nbin/nutch readseg -get /home/lokesh_Kumar/soft/apache-nutch-1.11/crawl/segments http://1465212304000.html -nofetch -nogenerate -noparse -noparsedata -noparsetext\n</code></pre>\n\n<blockquote>\n  <p>bin/nutch parsechecker -dumpText <a href=\"http://nihilent.com/\" rel=\"nofollow\">http://nihilent.com/</a></p>\n</blockquote>\n\n<pre><code>bin/nutch readlinkdb /home/lokesh_Kumar/soft/apache-nutch-1.11/crawl/linkdb -dump /home/lokesh_Kumar/soft/apache-nutch-1.11/ndeploy/Data/Team-A/fileLinkedIn/3\n\nbin/nutch readdb crawl/crawldb -dump /home/lokesh_Kumar/soft/apache-nutch-1.11/ndeploy/Data/Team-A/fileLinkedIn\n\nbin/nutch readdb crawl/crawldb -dump /hdfs://localhost:9000/dfs\n\nhadoop fs -copyFromLocal \n\nhadoop fs -copyFromLocal /home/lokesh_Kumar/soft/apache-nutch-1.11/ndeploy/data/commoncrawl/com hdfs://localhost:9000/dfs\n</code></pre>\n\n<hr>\n\n<p>added new answer just because of avoid sandwich data</p>\n", "question_id": 34744072, "creation_date": 1470893035, "is_accepted": false, "score": 0, "last_activity_date": 1470894716, "answer_id": 38887836}], "question_id": 34744072, "tags": ["apache", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/34744072/how-to-crawl-particular-website-using-apache-nutch", "last_activity_date": 1470894716, "accepted_answer_id": 34766568, "body": "<p><strong>i have followed below url</strong> and done successfully till <strong>Step-by-Step: Invertlinks</strong> </p>\n\n<p><a href=\"https://wiki.apache.org/nutch/NutchTutorial#Crawl_your_first_website\" rel=\"nofollow\">https://wiki.apache.org/nutch/NutchTutorial#Crawl_your_first_website</a></p>\n\n<p><strong>But i didn't get any data regarding them</strong></p>\n\n<p>i am new to this techno,</p>\n\n<p>please give <strong>steps/demo/site/example</strong> if someone has done it before successfully.\nAnd\nplease do not give rough steps.</p>\n", "creation_date": 1452602683, "score": 0},
{"title": "Nutch 2 exclude content-type image from crawling", "view_count": 30, "owner": {"age": 25, "answer_count": 25, "creation_date": 1341341131, "user_id": 1499705, "accept_rate": 77, "view_count": 123, "location": "Athens, Greece", "reputation": 1035}, "is_answered": true, "answers": [{"last_edit_date": 1470755353, "owner": {"user_id": 2420102, "accept_rate": 80, "link": "http://stackoverflow.com/users/2420102/mshoaib91", "user_type": "registered", "reputation": 950}, "body": "<p>You can achieve this by writing a <a href=\"https://wiki.apache.org/nutch/AboutPlugins\" rel=\"nofollow\">plugin</a> that will extend <a href=\"http://nutch.apache.org/apidocs/apidocs-1.8/org/apache/nutch/net/URLFilter.html\" rel=\"nofollow\">URLFilter</a> interface.</p>\n\n<p>In <code>String filter(String urlString)</code> method, you can check the url if it has some vague extension then further validate by getting its HTTP header values from the server and check if its content type is an image then return <code>null</code> otherwise return the URL. But I doubt that would not be very efficient method since many useless HTTP calls will be generated for this validation purpose only.</p>\n\n<p>Another thing is to just let it be and Nutch will not going to parse and/or index the image anyway.</p>\n", "question_id": 38844746, "creation_date": 1470742935, "is_accepted": true, "score": 0, "last_activity_date": 1470755353, "answer_id": 38849764}], "question_id": 38844746, "tags": ["nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/38844746/nutch-2-exclude-content-type-image-from-crawling", "last_activity_date": 1470755353, "accepted_answer_id": 38849764, "body": "<p>The problem is that there can be images not with the specific image extensions. For example Nutch2 was crawling a page ending with <code>.ashx</code> but was still an image.</p>\n\n<p>Is there a way to exclude images using an HTML header filter:<code>content-type: images/*</code> or something equivalent but not based on a url pattern (<code>regex-urlfilter.txt</code>)?</p>\n", "creation_date": 1470728189, "score": 0},
{"title": "How to select data from specific tags in nutch", "view_count": 75, "owner": {"user_id": 4327283, "answer_count": 40, "creation_date": 1417755361, "accept_rate": 100, "view_count": 69, "location": "Cochin,Kerala,India", "reputation": 651}, "is_answered": true, "answers": [{"question_id": 38767878, "owner": {"user_id": 2420102, "accept_rate": 80, "link": "http://stackoverflow.com/users/2420102/mshoaib91", "user_type": "registered", "reputation": 950}, "body": "<p>You will have to write a <a href=\"https://wiki.apache.org/nutch/AboutPlugins\" rel=\"nofollow\">plugin</a> that will extend <a href=\"http://nutch.apache.org/apidocs/apidocs-1.8/org/apache/nutch/parse/HtmlParseFilter.html\" rel=\"nofollow\">HtmlParseFilter</a> to achieve your goal.</p>\n\n<p>I reckon you will be doing some of the stuff yourself like parsing the html's specific section, extracting the URLs that you want and add them as outlinks.</p>\n\n<p><strong>HtmlParseFilter implementation:</strong> (Code below gives the general idea)</p>\n\n<pre><code>ParseResult filter(Content content, ParseResult parseResult, HTMLMetaTags metaTags, DocumentFragment doc){\n    // get html content\n    String htmlContent = new String(content.getContent(), StandardCharsets.UTF_8);\n    // parse html using jsoup or any other library.\n    String url = content.getUrl();\n    Parse parse = parseResult.get(url);\n    ParseData parseData = parse.getData();\n    Outlink[] links = parseData.getOutlinks();\n    // modify/select only required outlinks\n    // return ParsePesult with modified outlinks\n    return parseResult;\n}\n</code></pre>\n\n<p>Hope this will be helpful.</p>\n\n<p>If you are new to plugin, I have written a simple plugin \"<a href=\"https://github.com/mshoaib91/nutch-fetch-page\" rel=\"nofollow\">nutch-fetch-page</a>\" which saves html pages and text content on a local drive using <code>HtmlParseFilter</code> interface. You can fork/download and modify the code.</p>\n", "creation_date": 1470754703, "is_accepted": true, "score": 2, "last_activity_date": 1470754703, "answer_id": 38854219}], "question_id": 38767878, "tags": ["web-scraping", "web-crawler", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/38767878/how-to-select-data-from-specific-tags-in-nutch", "last_activity_date": 1470754703, "accepted_answer_id": 38854219, "body": "<p>I am a newbie in Apache Nutch and I would like to know whether it's possible to crawl selected area of a web page. For instance, select a particular <code>div</code> and crawl contents in that <code>div</code> only. Any help would be appreciated. Thanks!</p>\n", "creation_date": 1470315129, "score": 1},
{"title": "How to prevent crawling external links with apache nutch?", "view_count": 120, "is_answered": true, "answers": [{"question_id": 36436093, "owner": {"user_id": 5718513, "link": "http://stackoverflow.com/users/5718513/karsten-r", "user_type": "registered", "reputation": 710}, "body": "<p>You want to fetch only pages from a specific domain.</p>\n\n<p>You already tried <code>db.ignore.external.links</code> but this restrict anything but the seek.txt urls.</p>\n\n<p>You should try <code>conf/regex-urlfilter.txt</code> like in the example of the <a href=\"http://wiki.apache.org/nutch/NutchTutorial#A.28Optional.29_Configure_Regular_Expression_Filters\" rel=\"nofollow\">nutch1 tutorial</a>:</p>\n\n<pre><code>+^http://([a-z0-9]*\\.)*your.specific.domain.org/\n</code></pre>\n", "creation_date": 1459947895, "is_accepted": false, "score": 1, "last_activity_date": 1459947895, "answer_id": 36452053}, {"question_id": 36436093, "owner": {"user_id": 5983993, "accept_rate": 67, "link": "http://stackoverflow.com/users/5983993/avinash", "user_type": "registered", "reputation": 56}, "body": "<p>Are you using \"Crawl\" script? If yes make sure you giving level which is greater than 1. If you run something like this \"bin/crawl seedfoldername crawlDb <a href=\"http://solrIP:solrPort/solr\" rel=\"nofollow\">http://solrIP:solrPort/solr</a> 1\". It will crawl only urls which are listed in the seed.txt</p>\n\n<p>And to crawl specific domain you can use regex-urlfiltee.txt file.</p>\n", "creation_date": 1459958498, "is_accepted": false, "score": 1, "last_activity_date": 1459958498, "answer_id": 36456379}, {"question_id": 36436093, "owner": {"user_id": 3454410, "accept_rate": 48, "link": "http://stackoverflow.com/users/3454410/shafiq", "user_type": "registered", "reputation": 832}, "body": "<p>Add following property in nutch-site.xml</p>\n\n<pre><code>&lt;property&gt; \n&lt;name&gt;db.ignore.external.links&lt;/name&gt; \n&lt;value&gt;true&lt;/value&gt; \n&lt;description&gt;If true, outlinks leading from a page to external hosts will be ignored. This is an effective way to limit the crawl to include only initially injected hosts, without creating complex URLFilters. &lt;/description&gt; \n&lt;/property&gt;\n</code></pre>\n", "creation_date": 1470746871, "is_accepted": false, "score": 0, "last_activity_date": 1470746871, "answer_id": 38851205}], "question_id": 36436093, "tags": ["solr", "web-crawler", "nutch", "information-retrieval", "external-links"], "answer_count": 3, "link": "http://stackoverflow.com/questions/36436093/how-to-prevent-crawling-external-links-with-apache-nutch", "last_activity_date": 1470746871, "owner": {"user_id": 4644454, "view_count": 4, "answer_count": 0, "creation_date": 1425741586, "reputation": 10}, "body": "<p>I want to crawl only specific domains on nutch. For this I set the <code>db.ignore.external.links</code> to <strong>true</strong> as it was said in this <a href=\"http://wiki.apache.org/nutch/FAQ#Is_it_possible_to_fetch_only_pages_from_some_specific_domains.3F\" rel=\"nofollow\">FAQ link</a></p>\n\n<p>The problem is nutch start to crawl only links in the seed list. For example if I put \"nutch.apache.org\" to seed.txt, It only find the same url (nutch.apache.org). </p>\n\n<p>I get the result by running crawl script with 200 depth. And it's finished with one cycle and generate the out put below. </p>\n\n<p>How can I solve this problem ? </p>\n\n<p>I'm using apache nutch 1.11</p>\n\n<pre><code>Generator: starting at 2016-04-05 22:36:16\nGenerator: Selecting best-scoring urls due for fetch.\nGenerator: filtering: false\nGenerator: normalizing: true\nGenerator: topN: 50000\nGenerator: 0 records selected for fetching, exiting ...\nGenerate returned 1 (no new segments created)\nEscaping loop: no more URLs to fetch now\n</code></pre>\n\n<p>Best Regards</p>\n", "creation_date": 1459887027, "score": 0},
{"title": "How to add a new field in Nutch and Hbase data store", "view_count": 7, "is_answered": false, "question_id": 38849021, "tags": ["hbase", "nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/38849021/how-to-add-a-new-field-in-nutch-and-hbase-data-store", "last_activity_date": 1470740706, "owner": {"user_id": 3454410, "answer_count": 20, "creation_date": 1395646154, "accept_rate": 48, "view_count": 174, "reputation": 832}, "body": "<p>I am using Nutch 2.3.1 with hbase 0.94.14. I have to perform some analytics on  crawled data ( hbase data) and then I have to store this info in a new field in Hbase for each document that will be indexed later. How to change Nutch so that it should add a new field in Hbase for each crawled document.</p>\n", "creation_date": 1470740706, "score": 0},
{"title": "\u0130ntegration Nutch Hbase Solr on Eclipse,Eror:Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.hbase.HBaseConfiguration", "view_count": 45, "is_answered": false, "question_id": 38836763, "tags": ["java", "eclipse", "solr", "hbase", "nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/38836763/%c4%b0ntegration-nutch-hbase-solr-on-eclipse-erorcaused-by-java-lang-classnotfounde", "last_activity_date": 1470683257, "owner": {"user_id": 5208936, "answer_count": 0, "creation_date": 1439148600, "view_count": 4, "location": "\u0130stanbul, T&#252;rkiye", "reputation": 6}, "body": "<p><strong>Hi all</strong></p>\n\n<p>I have a problem with integration apache <strong>nutch ,hbase and solr  on Eclipse</strong> </p>\n\n<p>I  have encountering many erroy espacially version mistmatch ,therefore I want ask you which versions is ideal for this integration ?</p>\n\n<p>In addition,the following eror which lastest eror i have encountered on Eclipse with Nutch (2.3 v),hbase (0.94.27)</p>\n\n<pre><code>InjectorJob: starting at 2016-08-08 21:36:30\nInjectorJob: Injecting urlDir: /Users/admn/Documents/2.x/urls\nException in thread \"main\" java.lang.NoClassDefFoundError: org/apache/hadoop/hbase/HBaseConfiguration\n    at org.apache.gora.hbase.store.HBaseStore.initialize(HBaseStore.java:114)\n    at org.apache.gora.store.DataStoreFactory.initializeDataStore(DataStoreFactory.java:102)\n    at org.apache.gora.store.DataStoreFactory.createDataStore(DataStoreFactory.java:161)\n    at org.apache.gora.store.DataStoreFactory.createDataStore(DataStoreFactory.java:135)\n    at org.apache.nutch.storage.StorageUtils.createWebStore(StorageUtils.java:78)\n    at org.apache.nutch.crawl.InjectorJob.run(InjectorJob.java:233)\n    at org.apache.nutch.crawl.InjectorJob.inject(InjectorJob.java:267)\n    at org.apache.nutch.crawl.InjectorJob.run(InjectorJob.java:290)\n    at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)\n    at org.apache.nutch.crawl.InjectorJob.main(InjectorJob.java:299)\nCaused by: java.lang.ClassNotFoundException: org.apache.hadoop.hbase.HBaseConfiguration\n    at java.net.URLClassLoader.findClass(URLClassLoader.java:381)\n    at java.lang.ClassLoader.loadClass(ClassLoader.java:424)\n    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331)\n    at java.lang.ClassLoader.loadClass(ClassLoader.java:357)\n    ... 10 more\n</code></pre>\n\n<p>Thank you so much and really appreciated every help,</p>\n", "creation_date": 1470683257, "score": 0},
{"title": "apache solr 6.x with apache nutch 2.3.1", "view_count": 65, "is_answered": false, "question_id": 38740417, "tags": ["java", "solr", "version-control", "lucene", "nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/38740417/apache-solr-6-x-with-apache-nutch-2-3-1", "last_activity_date": 1470218573, "owner": {"user_id": 3454410, "answer_count": 20, "creation_date": 1395646154, "accept_rate": 48, "view_count": 174, "reputation": 832}, "body": "<p>I have to crawl a some webdata using Apache Nutch 2.3.1 and then provide full text facility. For that, I have decided to use Apache Solr 6.1 that is latest stable release. But according to Apache <a href=\"http://nutch.apache.org/\" rel=\"nofollow\">Nutch site</a>, It is recommended to use Solr 4.10.3 with Apache Nutch 2.3.1. </p>\n\n<p>What is the problem if I use latest Solr version. Nutch have to just index data to solr by REST API. Is there really some problem that can happen if I change the version ?</p>\n", "creation_date": 1470218573, "score": 0},
{"title": "Nutch in deploy mode not indexing data in solr", "view_count": 39, "is_answered": false, "question_id": 36151976, "tags": ["java", "hadoop", "solr", "nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/36151976/nutch-in-deploy-mode-not-indexing-data-in-solr", "last_activity_date": 1470218168, "owner": {"user_id": 3454410, "answer_count": 20, "creation_date": 1395646154, "accept_rate": 48, "view_count": 174, "reputation": 832}, "body": "<p>I am using apache nutch 2.3 with hadoop 1.2.1 and hbase 0.94.18. I have 3 datanodes. I have configured nutch to run in deploy mode and crawled some websites. All jobs run successfully. But when I check solr index, nothing is indexed. One nutch job (solrdedup) is failed with following error</p>\n\n<pre><code>java.lang.RuntimeException: java.lang.ClassNotFoundException: org.apache.nutch.indexer.solr.SolrDeleteDuplicates$SolrInputFormat\n    at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:857)\n    at org.apache.hadoop.mapreduce.JobContext.getInputFormatClass(JobContext.java:187)\n    at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:722)\n    at org.apache.hadoop.mapred.MapTask.run(MapTask.java:364)\n    at org.apache.hadoop.mapred.Child$4.run(Child.java:255)\n    at java.security.AccessController.doPrivileged(Native Method)\n    at javax.security.auth.Subject.doAs(Subject.java:422)\n    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)\n    at org.apache.hadoop.mapred.Child.main(Child.java:249)\nCaused by: java.lang.ClassNotFoundException: org.apache.nutch.indexer.solr.SolrDeleteDuplicates$SolrInputFormat\n    at java.net.URLClassLoader$1.run(URLClassLoader.java:372)\n    at java.net.URLClassLoader$1.run(URLClassLoader.java:361)\n    at java.security.AccessController.doPrivileged(Native Method)\n    at java.net.URLClassLoader.findClass(URLClassLoader.java:360)\n    at java.lang.ClassLoader.loadClass(ClassLoader.java:424)\n    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)\n    at java.lang.ClassLoader.loadClass(ClassLoader.java:357)\n    at java.lang.Class.forName0(Native Method)\n    at java.lang.Class.forName(Class.java:344)\n    at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:810)\n    at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:855)\n</code></pre>\n\n<p>But I expect as solrindex job is finished successfully there should be data in solr index but It is not there. What is the problem with my setup?</p>\n", "creation_date": 1458642697, "score": 0},
{"title": "Indexing a structure in solr with apache nutch", "view_count": 74, "is_answered": false, "answers": [{"question_id": 38723310, "owner": {"user_id": 1977773, "link": "http://stackoverflow.com/users/1977773/jorge-luis", "user_type": "registered", "reputation": 510}, "body": "<p>Take also a look at <a href=\"https://issues.apache.org/jira/browse/NUTCH-1870\" rel=\"nofollow\">https://issues.apache.org/jira/browse/NUTCH-1870</a> which is an XPath plugin for Nutch, this will allow you to extract the desired elements of the webpage and store this in individual fields. </p>\n\n<p>If you're willing to take a look at a different crawler, take a look at <a href=\"https://github.com/DigitalPebble/storm-crawler/\" rel=\"nofollow\">https://github.com/DigitalPebble/storm-crawler/</a> which is a set of resources for building your own crawler based on Apache Storm. The main gain with this approach is that is a NRT crawler.</p>\n", "creation_date": 1470190058, "is_accepted": false, "score": 0, "last_activity_date": 1470190058, "answer_id": 38733222}], "question_id": 38723310, "tags": ["json", "apache", "solr", "web-crawler", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/38723310/indexing-a-structure-in-solr-with-apache-nutch", "last_activity_date": 1470190058, "owner": {"user_id": 5635244, "view_count": 3, "answer_count": 0, "creation_date": 1449149962, "reputation": 1}, "body": "<p>In a 2nd hand car seller website there is thousands of cars ads\nThis is a typical ad -> <a href=\"http://www.standvirtual.com/carros/anuncios/Alfa-Romeo/MiTo/1.3-jtdm-racer/P8806891/\" rel=\"nofollow\">alfa-romeo</a> </p>\n\n<p>If I crawl all these ads pages, all diferent cars, I index all these useless text that I dont want, i would like to just crawl something like</p>\n\n<p>title, description, km of the car, power cv(hp), not the whole page,</p>\n\n<p>Im using nutch since it has good integration with solr but nutch its prepared to crawl everything, and in terms of plugins didnt found a good one to solve my problem.</p>\n\n<p>Already used nutch-custom-search didnt worked.</p>\n\n<p>Do you know something to solve my problem, I just want to crawl the pages of a specific website, and just specific parts of the pages, and index it to solr</p>\n\n<p>maybe another crawler with good integration with solr?</p>\n\n<p>Ty</p>\n", "creation_date": 1470148274, "score": 0},
{"title": "Issue when indexing to elasticsearch from apache nutch", "view_count": 59, "owner": {"user_id": 4327283, "answer_count": 40, "creation_date": 1417755361, "accept_rate": 100, "view_count": 69, "location": "Cochin,Kerala,India", "reputation": 651}, "is_answered": true, "answers": [{"question_id": 38713092, "owner": {"user_id": 4327283, "accept_rate": 100, "link": "http://stackoverflow.com/users/4327283/sachin", "user_type": "registered", "reputation": 651}, "body": "<p>From further research I got the solution. The error is coming up because of the version mismatch in indexer plugin of nutch (which was ES 1.4.1). </p>\n\n<p>One solution to this is to download the source from <strong><a href=\"https://github.com/apache/nutch/blob/master/\" rel=\"nofollow\">https://github.com/apache/nutch/blob/master/</a></strong> and then customize the plugin version with your elastic server version by following these instructions given in <strong><em>src/plugin/indexer-elastic/howto_upgrade_es.txt</em></strong>.</p>\n\n<blockquote>\n  <ol>\n  <li><p>Upgrade elasticsearch dependency in src/plugin/indexer-elastic/ivy.xml</p></li>\n  <li><p>Upgrade the Elasticsearch specific dependencies in src/plugin/indexer-elastic/plugin.xml    To get the list of\n  dependencies and their versions execute:    $ ant -f ./build-ivy.xml<br>\n  $ ls lib/</p></li>\n  <li><p>Build from nutch source folder using ant or any other build tool.</p></li>\n  </ol>\n</blockquote>\n\n<p>Then we can index to ElasticSearch without this issue. \nCheers :)</p>\n", "creation_date": 1470139141, "is_accepted": true, "score": 1, "last_activity_date": 1470139141, "answer_id": 38719835}], "question_id": 38713092, "tags": ["elasticsearch", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/38713092/issue-when-indexing-to-elasticsearch-from-apache-nutch", "last_activity_date": 1470139141, "accepted_answer_id": 38719835, "body": "<p>I was trying to index from apache nutch to single node ES cluster and got this error.</p>\n\n<blockquote>\n  <p>org.elasticsearch.transport.RemoteTransportException: Failed to\n  deserialize exception response from stream Caused by:\n  org.elasticsearch.transport.TransportSerializationException: Failed to\n  deserialize exception response from stream    at\n  org.elasticsearch.transport.netty.MessageChannelHandler.handlerResponseError(MessageChannelHandler.java:173)\n    at\n  org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:125)\n    at\n  org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)\n    at\n  org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)\n    at\n  org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)\n    at\n  org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:296)\n    at\n  org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)\n    at\n  org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)\n    at\n  org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)\n    at\n  org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)\n    at\n  org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)\n    at\n  org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)\n    at\n  org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)\n    at\n  org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)\n    at\n  org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)\n    at\n  org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)\n    at\n  org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)\n    at\n  org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)\n    at\n  org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)\n    at\n  org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)\n    at\n  org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)\n    at\n  java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n    at\n  java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n    at java.lang.Thread.run(Thread.java:745) Caused by:\n  java.io.StreamCorruptedException: Unsupported version: 1  at\n  org.elasticsearch.common.io.ThrowableObjectInputStream.readStreamHeader(ThrowableObjectInputStream.java:46)\n    at java.io.ObjectInputStream.(ObjectInputStream.java:301)     at\n  org.elasticsearch.common.io.ThrowableObjectInputStream.(ThrowableObjectInputStream.java:38)\n    at\n  org.elasticsearch.transport.netty.MessageChannelHandler.handlerResponseError(MessageChannelHandler.java:170)\n    ... 23 more</p>\n</blockquote>\n\n<p>From further research I came to know I should have same jvm version on client and ES server. Reference : <a href=\"http://jontai.me/blog/2013/06/elasticsearch-remotetransportexception-failed-to-deserialize-exception-response-from-stream/\" rel=\"nofollow\">http://jontai.me/blog/2013/06/elasticsearch-remotetransportexception-failed-to-deserialize-exception-response-from-stream/</a></p>\n\n<p>I'm using ES version <strong>2.3.2</strong> and my JVM version is <strong>\"1.8.0_91\"</strong>.\nWhen I checked /plugins/indexer-elastic/plugin.xml,the version specified is <strong>1.4.1</strong>. I would like to know this could be the issue and a possible solution other than to downgrade ES cluster version. I would like to continue with ES 2.3.2. Please help me on this.</p>\n\n<p>PS : The command I've used for indexing is <strong><code>bin/nutch index crawl/crawldb/ -linkdb crawl/linkdb/ crawl/segments/20160801174223/</code></strong></p>\n", "creation_date": 1470118933, "score": 2},
{"title": "Web scraping in Java/Scala", "view_count": 232, "is_answered": true, "answers": [{"last_edit_date": 1469790577, "owner": {"user_id": 1699191, "link": "http://stackoverflow.com/users/1699191/ameertawfik", "user_type": "registered", "reputation": 618}, "body": "<p>I have used Apache Nutch to crawl around 600k urls in 3 hours. I did use nutch in a Hadoop cluster. However, a friend of mine has used his own in house crawler to crawl 1 millions records in 1 hour. </p>\n\n<p>Ideally, with this scale of your records, you will need a distributed solution. As a result, I recommend using nutch as it suits your need. You could also try Storm crawler. It is similar to Nutch but uses storm as its distribution engine.You might think building a new one is a better option, but I do not think so. Nutch is indeed a matured and scalable solution.</p>\n\n<p>Do you need Solr? it all depends in what you want to do with the end result. If you want to search it, then you will need Solr or Elasticsearch. If you want to crawl and push data into a database then you need to build a new indexer that pushes crawled data to your desired data sink. Which is possible in Nutch. </p>\n\n<p>I hope that helps.</p>\n", "question_id": 35273211, "creation_date": 1454957954, "is_accepted": false, "score": 2, "last_activity_date": 1469790577, "answer_id": 35277195}, {"question_id": 35273211, "owner": {"user_id": 432844, "link": "http://stackoverflow.com/users/432844/julien-nioche", "user_type": "registered", "reputation": 939}, "body": "<p>As mentioned by @ameertawfik you could write a custom indexer class in Nutch so that it sends whichever data you need to keep into a DB. Whether you could use SOLR or ES as indexing backends depends on how you need to use the data.</p>\n\n<p>You could do the NLP processing as part of the Nutch parsing or indexing steps by implementing custom HTMLParseFilters or IndexingFilters. Either way would work fine and since it would be running on Hadoop, the NLP part would also scale.</p>\n\n<p>Nutch would work fine but is probably an overkill for what you need to do. If you know that the URLs you want to fetch are new then I'd just go for StormCrawler instead. Your crawls are non-recursive so it could be a matter of having N queues with RabbitMQ (or any other distributed queue) and inject your URLs daily into the queues based e.g. on their hostname. The SC topology would then require a custom Spout to connect to RabbitMQ (there are examples on the web you can use as starting points), most of the other components would be the standard ones, except a couple of bolts for doing the NLP work + another one to send the results to the storage of your choice.</p>\n\n<p>Either Nutch or SC would be good ways of doing it. SC would certainly be faster and probably easier to understand. I wrote <a href=\"http://digitalpebble.blogspot.co.uk/2015/09/index-web-with-aws-cloudsearch.html\" rel=\"nofollow\">a tutorial</a> a few months ago which describes and compares Nutch and SC, this might be useful for understanding how they differ.</p>\n", "creation_date": 1455025645, "is_accepted": false, "score": 1, "last_activity_date": 1455025645, "answer_id": 35293719}, {"question_id": 35273211, "owner": {"user_id": 276093, "accept_rate": 60, "link": "http://stackoverflow.com/users/276093/matt-burns", "user_type": "registered", "reputation": 11794}, "body": "<p>We use <a href=\"http://www.stormcrawler.net\" rel=\"nofollow\">StormCrawler</a> for our search engine, <a href=\"http://www.stolencamerafinder.com\" rel=\"nofollow\">StolenCameraFinder</a>. It's all in java and I've clocked it fetching over 4 million urls per day with a politeness setting of 1 url per second <em>per host</em>. The bottleneck wasn't StormCrawler but my url diversity. The <em>per host</em> part is important, it will never fetch more than 1 url per second for each host (technically it actually leaves a 1 second rest between fetches). For example, if you had 60 URLs for yahoo.com/* and 100 million for flickr.com/* then you will still never exceed 120/min.</p>\n\n<p>You can index the data straight into <a href=\"https://www.elastic.co/products/elasticsearch\" rel=\"nofollow\">ElasticSearch</a> and that works very well. StormCrawler has hooks right into so you should be able to get something running quite easily.</p>\n", "creation_date": 1460365840, "is_accepted": false, "score": 2, "last_activity_date": 1460365840, "answer_id": 36544398}], "question_id": 35273211, "tags": ["scala", "web-scraping", "nutch", "spray", "vert.x"], "answer_count": 3, "link": "http://stackoverflow.com/questions/35273211/web-scraping-in-java-scala", "last_activity_date": 1469790577, "owner": {"user_id": 3510671, "answer_count": 4, "creation_date": 1396955878, "accept_rate": 70, "view_count": 48, "reputation": 395}, "body": "<p>I need to extract keywords, title and description of a long list of URLs (initially ~250,000 URLs per day and eventually ~15,000,000 URLs per day)</p>\n\n<p>How would you recommend executing this? Preferably and solution that could be extended to 15,000,000 events per day. Preferably in Scala or Java</p>\n\n<p>So far I've looked at:</p>\n\n<ul>\n<li>Spray - I'm not very familiar with Spray yet so I can't quite evaluate it. Is it a useful framework for my task?</li>\n<li>Vertx - I've worked with Vertx before, if this is a good framework could you explain how would to be the best way to implement it with Vertx? </li>\n<li>Scala scraper - Not familiar with it at all. Is it a good framework for the use case and the loads I need? </li>\n<li>Nutch - I'm not sure how good it will be if I want to use it inside my code. Also I'm not sure I need Solr for my usecase. Anyone had any experience with it?</li>\n</ul>\n\n<p>I'll be happy to hear of other options if you think they're better</p>\n\n<p>I know I can probably dig in each of these solution and decide whether it's good or not but there seem to be so many options so any direction will be appreciated. </p>\n\n<p>Thanks in advance</p>\n", "creation_date": 1454945196, "score": 2},
{"title": "Nutch 1.7 Crawl - it is killed", "view_count": 14, "is_answered": false, "question_id": 38545610, "tags": ["solr", "centos", "nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/38545610/nutch-1-7-crawl-it-is-killed", "last_activity_date": 1469301572, "owner": {"user_id": 6604123, "view_count": 1, "answer_count": 0, "creation_date": 1468852481, "reputation": 1}, "body": "<p>I had a working system with nutch 1.7 and solr 4.5, using the 'crawl' script.  I updated the seed file containing 30K URLs - It goes zipping through, but gets down to a point in the 'Parsed' portion, then fails with this message:</p>\n\n<p>bin/crawl: line 182: 16594 Killed  $bin/nutch parse $commonOptions $skipRecordsOptions $CRAWL_PATH/segments/$SEGMENT</p>\n\n<p>I cut it down to about 30 URLs in the seed file and it worked.</p>\n\n<p>Could somebody tell me why it is being killed, and is there a way around it?  It must be some URL that it finds particularly distasteful, but it would be good if I could skip over it without killing the whole routine.</p>\n\n<p>I looked at the crawl file and line 182 is a 'done' line.  When it fails, I get no input into solr.  In order to get it running with a small group of URLs, I killed the TestCrawl folder that had my db/segments in it.</p>\n\n<p>This is implemented on a new CENTOS server run by my provider A2 Hosting.</p>\n\n<p>Any help very much appreciated.</p>\n", "creation_date": 1469301572, "score": 0},
{"title": "indexing stops after solr.Solr Mapping Reader", "view_count": 29, "is_answered": false, "question_id": 38536943, "tags": ["indexing", "solr", "web-crawler", "nutch", "retrieve-and-rank"], "answer_count": 0, "link": "http://stackoverflow.com/questions/38536943/indexing-stops-after-solr-solr-mapping-reader", "last_activity_date": 1469233375, "owner": {"age": 24, "answer_count": 8, "creation_date": 1465486803, "user_id": 6446301, "accept_rate": 67, "view_count": 16, "location": "Toronto, ON, Canada", "reputation": 6}, "body": "<p>I am trying to index my Nuch crawled data by the following command :</p>\n\n<pre><code>bin/nutch index -D solr.server.url=\"https://gateway.watsonplatform.net/retrieve-and-rank/api/v1/solr_clusters/sc97b4177a_600f_4040_9309_e632c116443f/solr/localWebCollection/\" -D solr.auth=true -D solr.auth.username=\"USER\" -D solr.auth.password=\"PASS\" final/crawl/crawldb -linkdb final/crawl\n</code></pre>\n\n<p>I get no errors but when I run it, after a few seconds it ends and does not index.\nHere is my log:</p>\n\n<pre><code>2016-07-22 20:03:09,599 INFO  indexer.IndexingJob - Indexer: starting at            2016-07-22 20:03:09\n2016-07-22 20:03:09,707 INFO  indexer.IndexingJob - Indexer: deleting gone documents: false\n2016-07-22 20:03:09,708 INFO  indexer.IndexingJob - Indexer: URL filtering: false\n2016-07-22 20:03:09,708 INFO  indexer.IndexingJob - Indexer: URL normalizing: false\n2016-07-22 20:03:10,216 INFO  indexer.IndexWriters - Adding org.apache.nutch.indexwriter.solr.SolrIndexWriter\n2016-07-22 20:03:10,216 INFO  indexer.IndexingJob - Active IndexWriters :\nSolrIndexWriter\n    solr.server.type : Type of SolrServer to communicate with (default 'http' however options include 'cloud', 'lb' and 'concurrent')\n    solr.server.url : URL of the Solr instance (mandatory)\n    solr.zookeeper.url : URL of the Zookeeper URL (mandatory if 'cloud' value for solr.server.type)\n    solr.loadbalance.urls : Comma-separated string of Solr server strings to be used (madatory if 'lb' value for solr.server.type)\n    solr.mapping.file : name of the mapping file for fields (default solrindex-mapping.xml)\n    solr.commit.size : buffer size when sending to Solr (default 1000)\n    solr.auth : use authentication (default false)\n    solr.auth.username : username for authentication\n    solr.auth.password : password for authentication\n\n2016-07-22 20:03:10,220 INFO  indexer.IndexerMapReduce - IndexerMapReduce: crawldb: final/crawl/crawldb\n2016-07-22 20:03:10,220 INFO  indexer.IndexerMapReduce - IndexerMapReduce: linkdb: final/crawl\n2016-07-22 20:03:10,376 WARN  util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n2016-07-22 20:03:10,495 WARN  indexer.IndexerMapReduce - Ignoring linkDb for indexing, no linkDb found in path: final/crawl\n2016-07-22 20:03:11,381 WARN  conf.Configuration - file:/tmp/hadoop-sdavari/mapred/staging/sdavari1351924025/.staging/job_local1351924025_0001/job.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.\n2016-07-22 20:03:11,385 WARN  conf.Configuration - file:/tmp/hadoop-sdavari/mapred/staging/sdavari1351924025/.staging/job_local1351924025_0001/job.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.\n2016-07-22 20:03:11,551 WARN  conf.Configuration - file:/tmp/hadoop-sdavari/mapred/local/localRunner/sdavari/job_local1351924025_0001/job_local1351924025_0001.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.\n2016-07-22 20:03:11,557 WARN  conf.Configuration - file:/tmp/hadoop-sdavari/mapred/local/localRunner/sdavari/job_local1351924025_0001/job_local1351924025_0001.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.\n2016-07-22 20:03:11,880 INFO  anchor.AnchorIndexingFilter - Anchor deduplication is: off\n2016-07-22 20:03:13,437 INFO  indexer.IndexWriters - Adding org.apache.nutch.indexwriter.solr.SolrIndexWriter\n2016-07-22 20:03:13,448 INFO  solr.SolrUtils - Authenticating as: f4a73627-777b-4d13-af60-df67be41ecb5\n2016-07-22 20:03:13,673 INFO  solr.SolrMappingReader - source: content dest: content\n2016-07-22 20:03:13,673 INFO  solr.SolrMappingReader - source: title dest: title\n2016-07-22 20:03:13,673 INFO  solr.SolrMappingReader - source: host dest: host\n2016-07-22 20:03:13,673 INFO  solr.SolrMappingReader - source: url dest: url\n2016-07-22 20:03:13,673 INFO  solr.SolrMappingReader - source: segment dest: segment\n2016-07-22 20:03:13,673 INFO  solr.SolrMappingReader - source: boost dest: boost\n2016-07-22 20:03:13,673 INFO  solr.SolrMappingReader - source: digest dest: digest\n2016-07-22 20:03:13,673 INFO  solr.SolrMappingReader - source: tstamp dest: tstamp\n2016-07-22 20:03:14,605 INFO  solr.SolrUtils - Authenticating as: f4a73627-777b-4d13-af60-df67be41ecb5\n2016-07-22 20:03:14,613 INFO  solr.SolrMappingReader - source: content dest: content\n2016-07-22 20:03:14,614 INFO  solr.SolrMappingReader - source: title dest: title\n2016-07-22 20:03:14,614 INFO  solr.SolrMappingReader - source: host dest: host\n2016-07-22 20:03:14,614 INFO  solr.SolrMappingReader - source: url dest: url\n2016-07-22 20:03:14,614 INFO  solr.SolrMappingReader - source: segment dest: segment\n2016-07-22 20:03:14,614 INFO  solr.SolrMappingReader - source: boost dest: boost\n2016-07-22 20:03:14,614 INFO  solr.SolrMappingReader - source: digest dest: digest\n2016-07-22 20:03:14,614 INFO  solr.SolrMappingReader - source: tstamp dest: tstamp\n2016-07-22 20:03:15,685 INFO  indexer.IndexingJob - Indexer: number of documents indexed, deleted, or skipped:\n2016-07-22 20:03:15,695 INFO  indexer.IndexingJob - Indexer: finished at            2016-07-22 20:03:15, elapsed: 00:00:06\n</code></pre>\n\n<p>Any ideas, how can I fix this and make it index my data?\nThe URL is for Bluemix Retrieve and Rank Service, but it is Built on top of Apache Solr, So I am guessing I can use it as long as the Schema of my Nutch and Solr match. right?</p>\n", "creation_date": 1469233375, "score": 0},
{"title": "mapping files created by: nutch dump to the URL from which each file has been dumped", "view_count": 18, "is_answered": false, "question_id": 38515541, "tags": ["mapping", "ibm-bluemix", "nutch", "dumpdata", "retrieve-and-rank"], "answer_count": 0, "link": "http://stackoverflow.com/questions/38515541/mapping-files-created-by-nutch-dump-to-the-url-from-which-each-file-has-been-du", "last_activity_date": 1469209277, "owner": {"age": 24, "answer_count": 8, "creation_date": 1465486803, "user_id": 6446301, "accept_rate": 67, "view_count": 16, "location": "Toronto, ON, Canada", "reputation": 6}, "body": "<p>I am trying to crawl some URLs in Apache Nutch, and then index them with Bluemix Retrieve And Rank service. \nTo do so I crawl my data by Nutch and dump the crawled data as files (mostly html files) in a directory:</p>\n\n<blockquote>\n  <p>bin/nutch dump -flatdir -outputDir dumpData/ -segment crawl/segments/</p>\n</blockquote>\n\n<p>Then I send these files to the Bluemix Document Converter Service to create a json file \"compatible with Bluemix R&amp;R Service\", and post this Json file to my R&amp;R Service. \nMy question is: How can I find a way to map each one of these files to their related URL? </p>\n", "creation_date": 1469141095, "score": 0},
{"title": "Solr 6 and Nutch 2.3.1 integration", "view_count": 142, "is_answered": false, "question_id": 38525848, "tags": ["solr", "nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/38525848/solr-6-and-nutch-2-3-1-integration", "last_activity_date": 1469188580, "owner": {"user_id": 6223061, "answer_count": 0, "creation_date": 1461044073, "accept_rate": 50, "view_count": 10, "reputation": 41}, "body": "<p>According to Nutch news the latest version of Nutch is 2.3.1 compatible with Solr 4.10.3 which is very old version of solr. </p>\n\n<p>Can we integrate Solr 6 with Nutch 2.3.1. What will be the drawbacks if solr 6 will be integrated? Anybody tried this?</p>\n", "creation_date": 1469188580, "score": 0},
{"title": "how to get the content of a particular url using nutch database", "view_count": 116, "owner": {"user_id": 2652226, "answer_count": 3, "creation_date": 1375688142, "accept_rate": 75, "view_count": 7, "reputation": 3}, "is_answered": true, "answers": [{"question_id": 22854912, "owner": {"user_id": 2148560, "link": "http://stackoverflow.com/users/2148560/sanjiv", "user_type": "registered", "reputation": 505}, "body": "<p>Use the nokogiri gem <a href=\"http://rubygems.org/gems/nokogiri\" rel=\"nofollow\">http://rubygems.org/gems/nokogiri</a> for parsing the content of webpages and select the link using nokogiri selector </p>\n", "creation_date": 1396593067, "is_accepted": false, "score": 0, "last_activity_date": 1396593067, "answer_id": 22855198}, {"question_id": 22854912, "owner": {"user_id": 2652226, "accept_rate": 75, "link": "http://stackoverflow.com/users/2652226/lussi", "user_type": "registered", "reputation": 3}, "body": "<p>I have seperate the files through a logic..as i am able to get the content for all urls in a single file with a particular pattern repeating for every record or url..i have seperate the content on row field.</p>\n", "creation_date": 1397469955, "is_accepted": true, "score": 0, "last_activity_date": 1397469955, "answer_id": 23057318}], "question_id": 22854912, "tags": ["ruby-on-rails", "solr", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/22854912/how-to-get-the-content-of-a-particular-url-using-nutch-database", "last_activity_date": 1469143992, "accepted_answer_id": 23057318, "body": "<p>I am new with nutch concept.\nI have configured everything properly best of my knowledge.\nAm able to crawl the links, and i can get crawled urls also.</p>\n\n<p>My problem is that, want to fetch content of webpages separate for every link, and am not able to find the solution for it.</p>\n\n<p>Can anyone please help me??</p>\n\n<p>Thank you.</p>\n", "creation_date": 1396592027, "score": 0},
{"title": "Nutch fetches just the URLs that exist in seeds file", "view_count": 102, "is_answered": true, "answers": [{"question_id": 38357814, "owner": {"user_id": 432844, "link": "http://stackoverflow.com/users/432844/julien-nioche", "user_type": "registered", "reputation": 939}, "body": "<p>Generate a crawl dump with the readdb command and check the nextFetchDate for the seeds or try a fresh crawl with a new crawldb and segments dir to see what happens.</p>\n\n<p>Do the logs reveal anything interesting? Are the seed URLs actually fetched and if so how do you know they are?</p>\n\n<p>Is the content of the seeds likely to have yielded different URLs from the previous day?</p>\n\n<p><em>fetcher.max.crawl.delay</em> is not related to the scheduling but is about how to behave when robots.txt files set a value so large that it is impractical.</p>\n\n<p>The config you are after is </p>\n\n<pre><code>&lt;property&gt;\n      &lt;name&gt;db.fetch.interval.default&lt;/name&gt;\n      &lt;value&gt;2592000&lt;/value&gt;\n      &lt;description&gt;The default number of seconds between re-fetches of a page (30 days).\n      &lt;/description&gt;\n    &lt;/property&gt;\n</code></pre>\n\n<p>i.e refetch a month later. Again, a crawldb dump will give you all the details about what happened to your URLs </p>\n", "creation_date": 1468529954, "is_accepted": false, "score": 1, "last_activity_date": 1468529954, "answer_id": 38384129}], "question_id": 38357814, "tags": ["web-crawler", "nutch", "information-retrieval"], "answer_count": 1, "link": "http://stackoverflow.com/questions/38357814/nutch-fetches-just-the-urls-that-exist-in-seeds-file", "last_activity_date": 1468947395, "owner": {"age": 24, "answer_count": 8, "creation_date": 1465486803, "user_id": 6446301, "accept_rate": 67, "view_count": 16, "location": "Toronto, ON, Canada", "reputation": 6}, "body": "<p>I am trying to crawl some URLs with Apache Nutch 1.11.\nThere are 7 URLs in my <code>seeds.txt</code> file, and I run the command :</p>\n\n<p><code>bin crawl -i urls crawl 22</code></p>\n\n<p>My problem is that with depth 22, I expect it to fetch quite a few number of pages. But today, all it does is fetching the exact same URLs that exist in my <code>seeds.txt</code> file and nothing more. And as weird as it sounds, yesterday the exact same files and properties ended up fetching 313 URLs. I didn't change anything since yesterday. Anyone knows what's going on? </p>\n\n<p>The only thing that has changes is that yesterday I was using another computer. But since I am running the crawl command on a  remote computer, I don't think it has anything to do with it. Does it?</p>\n", "creation_date": 1468429648, "score": 0},
{"title": "Solr - Combining automated crawl values and other manually entered data", "view_count": 18, "is_answered": false, "answers": [{"question_id": 38439621, "owner": {"user_id": 1977773, "link": "http://stackoverflow.com/users/1977773/jorge-luis", "user_type": "registered", "reputation": 510}, "body": "<p>Adding some fields to the Solr schema should not cause any trouble at all, keep in mind that this will be true unless you specify one of the new fields as <code>required</code> in which case an exception will be thrown. Otherwise you're fine. Of course since Nutch is unaware of these fields no value will be set during the Nutch crawl.</p>\n", "creation_date": 1468898453, "is_accepted": false, "score": 0, "last_activity_date": 1468898453, "answer_id": 38449051}], "question_id": 38439621, "tags": ["solr", "web-crawler", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/38439621/solr-combining-automated-crawl-values-and-other-manually-entered-data", "last_activity_date": 1468898453, "owner": {"user_id": 6604123, "view_count": 1, "answer_count": 0, "creation_date": 1468852481, "reputation": 1}, "body": "<p>new to Solr and have implemented a crawl for specific URLs with Nutch/Solr.  If I add new fields to the Solr schema, can I still use it to crawl with Nutch and import into Solr without problems, or will the manually entered fields cause the crawl to not work properly?  Thank you for your time.</p>\n", "creation_date": 1468852975, "score": 0},
{"title": "Indexer IOException job fail while Indexing nutch crawled data in \u201cBluemix\u201d solr", "view_count": 106, "owner": {"age": 24, "answer_count": 8, "creation_date": 1465486803, "user_id": 6446301, "accept_rate": 67, "view_count": 16, "location": "Toronto, ON, Canada", "reputation": 6}, "is_answered": true, "answers": [{"question_id": 37869588, "owner": {"user_id": 6446301, "accept_rate": 67, "link": "http://stackoverflow.com/users/6446301/s-doe", "user_type": "registered", "reputation": 6}, "body": "<p>As far as I know, indexing nutch crawled data in Bluemix R&amp;R, by the index command provided in nutch itself(bin/nutch index...) is not possible.\nI realized that for indexing nutch crawled data in Bluemix Retrieve and Rank service one should:</p>\n\n<ol>\n<li>Crawl seeds with nutch e.g\n\n<blockquote>\n  <p>$:bin/crawl -w 5 urls crawl 25</p>\n</blockquote></li>\n</ol>\n\n<p>you can check the status of crawling with:</p>\n\n<blockquote>\n  <p>bin/nutch readdb crawl/crawldb/ -stats</p>\n</blockquote>\n\n<ol start=\"2\">\n<li><p>Dumped the crawled dataas files:</p>\n\n<blockquote>\n  <p>$:bin/nutch dump -flatdir -outputDir dumpData/ -segment crawl/segments/</p>\n</blockquote></li>\n<li><p>Post those that are possible e.g xml files to solr Collection on Retrieve and Rank:</p>\n\n<pre><code>Post_url = '\"https://gateway.watsonplatform.net/retrieve-and-rank/api/v1/solr_clusters/%s/solr/%s/update\"' %(solr_cluster_id, solr_collection_name)\ncmd ='''curl -X POST -H %s -u %s %s --data-binary @%s''' %(Cont_type_xml, solr_credentials, Post_url, myfilename)          \nsubprocess.call(cmd,shell=True)\n</code></pre></li>\n<li><p>Convert the rest to json with Bluemix Doc-Conv service:</p>\n\n<pre><code>doc_conv_url = '\"https://gateway.watsonplatform.net/document-conversion/api/v1/convert_document?version=2015-12-15\"'\ncmd ='''curl -X POST -u %s -F config=\"{\\\\\"conversion_target\\\\\":\\\\\"answer_units\\\\\"}\" -F file=@%s %s''' %(doc_conv_credentials, myfilename, doc_conv_url)\nprocess = subprocess.Popen(cmd, shell= True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n</code></pre>\n\n<p>and then save these Json results in a json file.</p></li>\n<li><p>Post this json file to the collection:</p>\n\n<pre><code>Post_converted_url = '\"https://gateway.watsonplatform.net/retrieve-and-rank/api/v1/solr_clusters/%s/solr/%s/update/json/docs?commit=true&amp;split=/answer_units/id&amp;f=id:/answer_units/id&amp;f=title:/answer_units/title&amp;f=body:/answer_units/content/text\"' %(solr_cluster_id, solr_collection_name)\ncmd ='''curl -X POST -H %s -u %s %s --data-binary @%s''' %(Cont_type_json, solr_credentials, Post_converted_url, Path_jsonFile)\nsubprocess.call(cmd,shell=True)\n</code></pre></li>\n<li><p>Send Queries:</p>\n\n<pre><code>pysolr_client = retrieve_and_rank.get_pysolr_client(solr_cluster_id, solr_collection_name)\nresults = pysolr_client.search(Query_term)\nprint(results.docs)\n</code></pre>\n\n<p>Codes are in python. </p></li>\n</ol>\n\n<p>For beginners: You can use the curl commands directly in you CMD. I hope it helpes others.</p>\n", "creation_date": 1468895914, "is_accepted": true, "score": 0, "last_activity_date": 1468895914, "answer_id": 38448720}], "question_id": 37869588, "tags": ["indexing", "solr", "ibm-bluemix", "nutch", "retrieve-and-rank"], "answer_count": 1, "link": "http://stackoverflow.com/questions/37869588/indexer-ioexception-job-fail-while-indexing-nutch-crawled-data-in-bluemix-solr", "last_activity_date": 1468895914, "accepted_answer_id": 38448720, "body": "<p>I'm trying to index the nutch crawled data by Bluemix solr. I used the following command in my command prompt:</p>\n\n<blockquote>\n  <p>bin/nutch index -D solr.server.url=\"https://gateway.watsonplatform.net/retrieve-and-rank/api/v1/solr_clusters/CLUSTER-ID/solr/admin/collections -D solr.auth=true -D solr.auth.username=\"USERNAME\" -D solr.auth.password=\"PASS\" Crawl/crawldb -linkdb Crawl/linkdb Crawl/segments/2016* </p>\n</blockquote>\n\n<p>But it fails to finish the indexing. The result is as followed:</p>\n\n<pre><code>Indexer: starting at 2016-06-16 16:31:50\nIndexer: deleting gone documents: false\nIndexer: URL filtering: false\nIndexer: URL normalizing: false\nActive IndexWriters :\nSolrIndexWriter\n        solr.server.type : Type of SolrServer to communicate with (default 'http' however options include 'cloud', 'lb' and 'concurrent')\n        solr.server.url : URL of the Solr instance (mandatory)\n        solr.zookeeper.url : URL of the Zookeeper URL (mandatory if 'cloud' value for solr.server.type)\n        solr.loadbalance.urls : Comma-separated string of Solr server strings to be used (madatory if 'lb' value for solr.server.type)\n        solr.mapping.file : name of the mapping file for fields (default solrindex-mapping.xml)\n        solr.commit.size : buffer size when sending to Solr (default 1000)\n        solr.auth : use authentication (default false)\n        solr.auth.username : username for authentication\n        solr.auth.password : password for authentication\nIndexing 153 documents\nIndexing 153 documents\nIndexer: java.io.IOException: Job failed!\n        at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:836)\n        at org.apache.nutch.indexer.IndexingJob.index(IndexingJob.java:145)\n        at org.apache.nutch.indexer.IndexingJob.run(IndexingJob.java:222)\n        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)\n        at org.apache.nutch.indexer.IndexingJob.main(IndexingJob.java:231)\n</code></pre>\n\n<p>I guess it has something to do with the solr.server.url address, maybe the end of it. I changed it in different ways\ne.g </p>\n\n<blockquote>\n  <p>\"<a href=\"https://gateway.watsonplatform.net/retrieve-and-rank/api/v1/solr_clusters/CLUSTER-ID/solr/example_collection/update\" rel=\"nofollow\">https://gateway.watsonplatform.net/retrieve-and-rank/api/v1/solr_clusters/CLUSTER-ID/solr/example_collection/update</a>\".</p>\n</blockquote>\n\n<p>(since it is used for indexing JSON/CSV/... files by the the Bluemix Solr ).\n But no chance to now.</p>\n\n<p>Anyone knows how can I fix it? And if the problem is as I guessed, anyone knows what exactly should the solr.server.url be ?\nBy the way, \"example_collection\" is my collections name, and I'm working with nutch1.11.</p>\n", "creation_date": 1466112335, "score": 0},
{"title": "Indexing nutch crawled data in &quot;Bluemix&quot; solr", "view_count": 87, "owner": {"age": 24, "answer_count": 8, "creation_date": 1465486803, "user_id": 6446301, "accept_rate": 67, "view_count": 16, "location": "Toronto, ON, Canada", "reputation": 6}, "is_answered": true, "answers": [{"question_id": 37731716, "owner": {"user_id": 6446301, "accept_rate": 67, "link": "http://stackoverflow.com/users/6446301/s-doe", "user_type": "registered", "reputation": 6}, "body": "<p>Thanks to Lewis John Mcgibbney I realized that index tool should be used as followed:</p>\n\n<blockquote>\n  <p>bin/nutch index -D solr.server.url=\"https://gateway.watsonplatform.net/retrieve-and-rank/api/v1/solr_clusters/CLUSTER-ID/solr/admin/collections -D solr.auth=true -D solr.auth.username=\"USERNAME\" -D solr.auth.password=\"PASS\" Crawl/crawldb -linkdb Crawl/linkdb Crawl/segments/2016* </p>\n</blockquote>\n\n<p>Means: using -D before each of the auth-parameters and mentioning these parameters right IN FRONT of the Tool arguments.</p>\n", "creation_date": 1466110465, "is_accepted": false, "score": 0, "last_activity_date": 1466110465, "answer_id": 37869176}, {"question_id": 37731716, "owner": {"user_id": 6446301, "accept_rate": 67, "link": "http://stackoverflow.com/users/6446301/s-doe", "user_type": "registered", "reputation": 6}, "body": "<p>For indexing nutch crawled data in Bluemix Retrieve and Rank service one should:</p>\n\n<ol>\n<li>Crawl seeds with nutch e.g\n\n<blockquote>\n  <p>$:bin/crawl -w 5 urls crawl 25</p>\n</blockquote></li>\n</ol>\n\n<p>you can check the status of crawling with:</p>\n\n<blockquote>\n  <p>bin/nutch readdb crawl/crawldb/ -stats</p>\n</blockquote>\n\n<ol start=\"2\">\n<li><p>Dumped the crawled dataas files:</p>\n\n<blockquote>\n  <p>$:bin/nutch dump -flatdir -outputDir dumpData/ -segment crawl/segments/</p>\n</blockquote></li>\n<li><p>Posted those that are possible e.g xml files to solr Collection on Retrieve and Rank:</p>\n\n<p>Post_url = '\"<a href=\"https://gateway.watsonplatform.net/retrieve-and-rank/api/v1/solr_clusters/%s/solr/%s/update\" rel=\"nofollow\">https://gateway.watsonplatform.net/retrieve-and-rank/api/v1/solr_clusters/%s/solr/%s/update</a>\"' %(solr_cluster_id, solr_collection_name)\ncmd ='''curl -X POST -H %s -u %s %s --data-binary @%s''' %(Cont_type_xml, solr_credentials, Post_url, myfilename)\nsubprocess.call(cmd,shell=True)</p></li>\n<li><p>Converted the rest to json with Bluemix Doc-Conv service: </p>\n\n<pre><code>doc_conv_url = '\"https://gateway.watsonplatform.net/document-conversion/api/v1/convert_document?version=2015-12-15\"'\ncmd ='''curl -X POST -u %s -F config=\"{\\\\\"conversion_target\\\\\":\\\\\"answer_units\\\\\"}\" -F file=@%s %s''' %(doc_conv_credentials, myfilename, doc_conv_url)\nprocess = subprocess.Popen(cmd, shell= True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n</code></pre></li>\n</ol>\n\n<p>and then save these Json results in a json file.</p>\n\n<ol start=\"5\">\n<li><p>Post this json file to the collection: </p>\n\n<pre><code>Post_converted_url = '\"https://gateway.watsonplatform.net/retrieve-and-rank/api/v1/solr_clusters/%s/solr/%s/update/json/docs?commit=true&amp;split=/answer_units/id&amp;f=id:/answer_units/id&amp;f=title:/answer_units/title&amp;f=body:/answer_units/content/text\"' %(solr_cluster_id, solr_collection_name)\ncmd ='''curl -X POST -H %s -u %s %s --data-binary @%s''' %(Cont_type_json, solr_credentials, Post_converted_url, Path_jsonFile)\nsubprocess.call(cmd,shell=True)\n</code></pre></li>\n<li><p>Send Queries:</p>\n\n<pre><code>pysolr_client = retrieve_and_rank.get_pysolr_client(solr_cluster_id, solr_collection_name)\nresults = pysolr_client.search(Query_term)\nprint(results.docs)\n</code></pre></li>\n</ol>\n\n<p>Codes are in python. For beginners: You can use the curl commands directly in you CMD.\nI hope it helpes</p>\n", "creation_date": 1468893936, "is_accepted": true, "score": 0, "last_activity_date": 1468893936, "answer_id": 38448478}], "question_id": 37731716, "tags": ["json", "solr", "ibm-bluemix", "nutch", "retrieve-and-rank"], "answer_count": 2, "link": "http://stackoverflow.com/questions/37731716/indexing-nutch-crawled-data-in-bluemix-solr", "last_activity_date": 1468893936, "accepted_answer_id": 38448478, "body": "<p>I'm trying to index the nutch crawled data by Bluemix solr and I cannot find anyway to do it. My main question is: Is there anybody that can help me to do so? what should I do to send the result of my nutch crawled data to my Blumix Solr.\nFor the crawling I used nutch 1.11 and here is a part of what I did to now and the problems I faced:\nI thought there may be two possible solutions: </p>\n\n<ol>\n<li>By nutch command:</li>\n</ol>\n\n<blockquote>\n  <p>\u201cNUTCH_PATH/bin/nutch index crawl/crawldb -linkdb crawl/linkdb crawl/ -Dsolr.server.url=\"OURSOLRURL\"\u201d</p>\n</blockquote>\n\n<p>I can index the nutch crawled data by OURSOLR. However, I found some problem with that.</p>\n\n<p>a-Though it sounds really odd, it could not accept the URL. I could handle it by using the URL\u2019s Encode instead.</p>\n\n<p>b-Since I have to connect to a specific Username and password, nutch could not connect to my solr. Considering this:</p>\n\n<pre><code> Active IndexWriters :\n SolrIndexWriter\n    solr.server.type : Type of SolrServer to communicate with (default 'http' however options include 'cloud', 'lb' and 'concurrent')\n    solr.server.url : URL of the Solr instance (mandatory)\n    solr.zookeeper.url : URL of the Zookeeper URL (mandatory if 'cloud' value for solr.server.type)\n    solr.loadbalance.urls : Comma-separated string of Solr server strings to be used (madatory if 'lb' value for solr.server.type)\n    solr.mapping.file : name of the mapping file for fields (default solrindex-mapping.xml)\n    solr.commit.size : buffer size when sending to Solr (default 1000)\n    solr.auth : use authentication (default false)\n    solr.auth.username : username for authentication\n    solr.auth.password : password for authentication\n</code></pre>\n\n<p>in the command line output,I tried to manage this problem by using authentication parameters of the command \n\"solr.auth=true solr.auth.username=\"SOLR-UserName\" solr.auth.password=\"Pass\" to it.</p>\n\n<p>So up to now I\u2019ve got to a point to use this command:</p>\n\n<blockquote>\n  <p>\u201dbin/nutch index crawl/crawldb -linkdb crawl/linkdb crawl/segments/2016* solr.server.url=\"https%3A%2F%2Fgateway.watsonplatform.net%2Fretrieve-and-rank%2Fapi%2Fv1%2Fsolr_clusters%2FCLUSTER-ID%2Fsolr%2Fadmin%2Fcollections\" solr.auth=true solr.auth.username=\"USERNAME\" solr.auth.password=\"PASS\"\u201c.</p>\n</blockquote>\n\n<p>But for some reason that I couldn\u2019t realize yet, the command considers the authentication parameters as crawled data directory and does not work. So I guess it is not the right way to \"Active IndexWriters\"\ncan anyone tell me then how can I??</p>\n\n<ol start=\"2\">\n<li>By curl command:</li>\n</ol>\n\n<blockquote>\n  <p>\u201ccurl -X POST -H \"Content-Type: application/json\" -u \"BLUEMIXSOLR-USERNAME\":\"BLUEMIXSOLR-PASS\" \"<a href=\"https://gateway.watsonplatform.net/retrieve-and-rank/api/v1/solr_clusters/CLUSTERS-ID/solr/example_collection/update\" rel=\"nofollow\">https://gateway.watsonplatform.net/retrieve-and-rank/api/v1/solr_clusters/CLUSTERS-ID/solr/example_collection/update</a>\" --data-binary @{/path_to_file}/FILE.json\u201d</p>\n</blockquote>\n\n<p>I thought maybe I can feed json files created by this command: </p>\n\n<blockquote>\n  <p>bin/nutch commoncrawldump -outputDir finalcrawlResult/ -segment crawl/segments -gzip -extension json -SimpleDateFormat -epochFilename -jsonArray -reverseKey\n  but there are some problems here.</p>\n</blockquote>\n\n<p>a. this command provides so many files in complicated Paths which will take so much time to manually post all of them.I guess for big cawlings it may be even impossible. \nIs there any way to POST all the files in a directory and its subdirectories at once by just one command??</p>\n\n<p>b. there is a weird name \"\u00d9\u00d9\u00f7y\u0153\" at the start of json files created by commoncrawldump.</p>\n\n<p>c. I removed the name weird name and tried to POST just one of these files but here is the result:</p>\n\n<pre><code> {\"responseHeader\":{\"status\":400,\"QTime\":23},\"error\":{\"metadata\":[\"error-class\",\"org.apache.solr.common.SolrException\",\"root-error-class\",\"org.apache.solr.common.SolrException\"],\"msg\":\"Unknown command 'url' at [9]\",\"code\":400}}\n</code></pre>\n\n<p>Does it mean these files cannot be fed to Bluemix solr and it is all useless for me?</p>\n", "creation_date": 1465490334, "score": 1},
{"title": "Nutch 2.3.1 on OSX does not connect to MongoDB", "view_count": 93, "is_answered": false, "question_id": 38339385, "tags": ["osx", "mongodb", "nutch", "gora"], "answer_count": 0, "link": "http://stackoverflow.com/questions/38339385/nutch-2-3-1-on-osx-does-not-connect-to-mongodb", "last_activity_date": 1468398548, "owner": {"age": 34, "answer_count": 31, "creation_date": 1396857595, "user_id": 3505809, "view_count": 16, "location": "Aachen, Germany", "reputation": 372}, "body": "<p>I configured a local Nutch 2.3.1 instance on MacOS 10.11.5 (El Capitan) running in Eclipse as described here: <a href=\"https://wiki.apache.org/nutch/RunNutchInEclipse\" rel=\"nofollow\">https://wiki.apache.org/nutch/RunNutchInEclipse</a></p>\n\n<p>As data store to use I configured MongoDB 2.6.12 which is also running on my local MacOS machine. I took the Gora config from here: <a href=\"http://www.aossama.com/search-engine-with-apache-nutch-mongodb-and-elasticsearch/\" rel=\"nofollow\">http://www.aossama.com/search-engine-with-apache-nutch-mongodb-and-elasticsearch/</a></p>\n\n<p><strong>ivy.xml</strong></p>\n\n<pre><code>&lt;dependency org=\"org.apache.gora\" name=\"gora-mongodb\" rev=\"0.6.1\" conf=\"*-&gt;default\" /&gt;\n</code></pre>\n\n<p><strong>gora.properties</strong></p>\n\n<pre><code>gora.datastore.default=org.apache.gora.mongodb.store.MongoStore\ngora.mongodb.override_hadoop_configuration=false\ngora.mongodb.mapping.file=/gora-mongodb-mapping.xml\ngora.mongodb.servers=localhost:27017\n# I tried several server settings like localhost, 127.0.0.1, 127.0.0.1:27017, ...\ngora.mongodb.db=nutch\n</code></pre>\n\n<p>I did not change <strong>gora-mongodb-mapping.xml</strong>.</p>\n\n<p><strong>nutch-site.xml</strong></p>\n\n<pre><code>&lt;property&gt;\n &lt;name&gt;storage.data.store.class&lt;/name&gt;\n &lt;value&gt;org.apache.gora.mongodb.store.MongoStore&lt;/value&gt;\n &lt;description&gt;Default class for storing data&lt;/description&gt;\n&lt;/property&gt;\n</code></pre>\n\n<p>If I run the inject command, <strong>hadoop.log</strong> shows this confusing result:</p>\n\n<pre><code>2016-07-12 23:23:16,818 INFO  crawl.InjectorJob - InjectorJob: starting at 2016-07-12 23:23:16\n2016-07-12 23:23:16,819 INFO  crawl.InjectorJob - InjectorJob: Injecting urlDir: /Users/myaccount/Documents/Nutch/urls\n2016-07-12 23:23:17,054 WARN  util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n2016-07-12 23:23:17,416 ERROR store.MongoStore - \n2016-07-12 23:23:17,417 ERROR store.MongoStore - [Ljava.lang.StackTraceElement;@4b5189ac\n2016-07-12 23:23:17,418 ERROR store.MongoStore - Error while initializing MongoDB store: java.lang.NullPointerException\n2016-07-12 23:23:17,419 ERROR crawl.InjectorJob - InjectorJob: org.apache.gora.util.GoraException: java.lang.RuntimeException: java.io.IOException: java.lang.NullPointerException\n    at org.apache.gora.store.DataStoreFactory.createDataStore(DataStoreFactory.java:167)\n    at org.apache.gora.store.DataStoreFactory.createDataStore(DataStoreFactory.java:135)\n    at org.apache.nutch.storage.StorageUtils.createWebStore(StorageUtils.java:78)\n    at org.apache.nutch.crawl.InjectorJob.run(InjectorJob.java:233)\n    at org.apache.nutch.crawl.InjectorJob.inject(InjectorJob.java:267)\n    at org.apache.nutch.crawl.InjectorJob.run(InjectorJob.java:290)\n    at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)\n    at org.apache.nutch.crawl.InjectorJob.main(InjectorJob.java:299)\nCaused by: java.lang.RuntimeException: java.io.IOException: java.lang.NullPointerException\n    at org.apache.gora.mongodb.store.MongoStore.initialize(MongoStore.java:131)\n    at org.apache.gora.store.DataStoreFactory.initializeDataStore(DataStoreFactory.java:102)\n    at org.apache.gora.store.DataStoreFactory.createDataStore(DataStoreFactory.java:161)\n    ... 7 more\nCaused by: java.io.IOException: java.lang.NullPointerException\n    at org.apache.gora.mongodb.store.MongoMappingBuilder.fromFile(MongoMappingBuilder.java:123)\n    at org.apache.gora.mongodb.store.MongoStore.initialize(MongoStore.java:118)\n    ... 9 more\nCaused by: java.lang.NullPointerException\n    at org.apache.gora.mongodb.store.MongoMapping.newDocumentField(MongoMapping.java:109)\n    at org.apache.gora.mongodb.store.MongoMapping.addClassField(MongoMapping.java:169)\n    at org.apache.gora.mongodb.store.MongoMappingBuilder.loadPersistentClass(MongoMappingBuilder.java:169)\n    at org.apache.gora.mongodb.store.MongoMappingBuilder.fromFile(MongoMappingBuilder.java:112)\n    ... 10 more\n</code></pre>\n\n<p>After two days I've run out of ideas. </p>\n\n<p>Within the log file I can't identify any valuable hint. The MongoDB logs don't show any connection attempts (not to mention an active connection). Using <code>mongo</code> I'm able to connect to the database and requesting <a href=\"http://localhost:27017\" rel=\"nofollow\">http://localhost:27017</a> shows the expected message (<em>\"It looks like you are trying to access MongoDB over HTTP on the native driver port.\"</em>) and corresponding log file entries. If I switch the data store to Cassandra, injecting works as expected, so Nutch itself also seems to work.</p>\n\n<p><strong>Does anybody know what I'm missing or understand what the hadoop.log is trying to tell me?</strong> </p>\n\n<p>Any help would be appreciated! Thx.</p>\n\n<p><strong>Update:</strong> I also tried to use this configuration on an Ubuntu 14.04 server - works as expected. So I suppose my issue is related to the connection between Nutch &amp; MongoDB running on a Mac. (If somebody wants to know: I'm trying to get the configuration working on my Mac because I want to do some local development with no need of a server connection.)</p>\n", "creation_date": 1468359993, "score": 1},
{"title": "Integrating nutch 1.11 with solr 6.0.1 cloud", "view_count": 217, "is_answered": false, "answers": [{"question_id": 37731252, "owner": {"user_id": 1977773, "link": "http://stackoverflow.com/users/1977773/jorge-luis", "user_type": "registered", "reputation": 510}, "body": "<p>You're not specifying the protocol to connect to Solr: You need to specify the <code>http://</code> portion of the <code>solr.server.url</code> and you used the wrong syntax to specify the port to connect, the right URL should be: <code>http://localhost:8983/solr/nndcweb/</code>. </p>\n", "creation_date": 1465509049, "is_accepted": false, "score": 0, "last_activity_date": 1465509049, "answer_id": 37736763}, {"question_id": 37731252, "owner": {"user_id": 6446301, "accept_rate": 67, "link": "http://stackoverflow.com/users/6446301/s-doe", "user_type": "registered", "reputation": 6}, "body": "<p>About the problem with URL when using solr index: I had the same problem, and I know it sounds stupid but for some reason that I cannot get, you can fix it by using the URL\u2019s Encode(replace \":\" with \"%3A\", \"/\" with \"%2F\" and... ) instead.(at least for me this fixed that problem.)\nin your case:</p>\n\n<blockquote>\n  <p>bin/nutch solrindex -D solr.server.url=http%3A%2F%2Flocalhost%3A8983%2Fsolr%2Fnndcweb crawl/crawldb -linkdb crawl/linkdb -dir crawl/segments -normalize</p>\n</blockquote>\n\n<p>I hope it helps.\nBTW, now I'm having the exact same problem as you do (Indexer: java.io.IOException: Job failed!)</p>\n", "creation_date": 1466440316, "is_accepted": false, "score": 0, "last_activity_date": 1466440316, "answer_id": 37927538}], "question_id": 37731252, "tags": ["solr", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/37731252/integrating-nutch-1-11-with-solr-6-0-1-cloud", "last_activity_date": 1468337174, "owner": {"user_id": 6441328, "view_count": 0, "answer_count": 0, "creation_date": 1465401295, "reputation": 1}, "body": "<p>This is similar to <a href=\"http://stackoverflow.com/questions/34262293/integration-between-nutch-1-111-x-and-solr-5-3-15-x\">solr5.3.15-nutch here</a>, but with a few extra wrinkles. First, as background, I tried solr 4.9.1 and nutch with no problems. Then moved up to solr 6.0.1.  Integration worked great as a standalone, and got backend code working to parse the json, etc.  However, ultimately, we need security, and don't want to use Kerberos. According to the Solr security documentation, basic auth and rule-based auth (which is what we want) works only in cloud mode (as an aside, if anyone has suggestions for getting non-Kerberos security working in standalone mode, that would work as well).  So, went through the doc at <a href=\"https://cwiki.apache.org/confluence/display/solr/Getting+Started+with+SolrCloud\" rel=\"nofollow\">Solr-Cloud-Ref</a>, using the interactive startup and taking all the defaults, except for the name of the collection which I made as \"nndcweb\" instead of \"gettingstarted\". The configuration I took was <strong>data_driven_schema_configs</strong> .  To integrate nutch, there were many permutations of attempts I made. I'll only give the last 2 that seemed to come closest based on what I've been able to find so far.  From the earlier stack-overflow reference, the last one I tried was (note all urls have http://, but the posting system for Stackoverflow was complaining, so I took them out for the sake of this post):</p>\n\n<blockquote>\n  <p>bin/nutch index  crawl/crawldb -linkdb crawl/linkdb  -D solr.server.url=localhost:8939/solr/nndcweb/ -Dsolr.server.type=cloud -D solr.zookeeper.url=localhost:9983/  -dir crawl/segments/* -normalize</p>\n</blockquote>\n\n<p>I ended up with the same problem noted in the previous thread mentioned: namely,</p>\n\n<blockquote>\n  <p>Exception in thread \"main\" java.lang.IllegalArgumentException: java.net.URISyntaxException: Illegal character in scheme name at index 15: solr.server.url=localhost:8939/solr/nndcweb\n      at org.apache.hadoop.fs.Path.initialize(Path.java:206)\n      at org.apache.hadoop.fs.Path.(Path.java:172)\n      at org.apache.nutch.indexer.IndexingJob.run(IndexingJob.java:217)\n      at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)\n      at org.apache.nutch.indexer.IndexingJob.main(IndexingJob.java:231)\n  Caused by: java.net.URISyntaxException: Illegal character in scheme name at index 15: solr.server.url=localhost:8939/solr/nndcweb\n      at java.net.URI$Parser.fail(URI.java:2848)\n      at java.net.URI$Parser.checkChars(URI.java:3021)\n      at java.net.URI$Parser.parse(URI.java:3048)\n      at java.net.URI.(URI.java:746)\n      at org.apache.hadoop.fs.Path.initialize(Path.java:203)</p>\n</blockquote>\n\n<p>I also tried:</p>\n\n<blockquote>\n  <p>bin/nutch solrindex localhost:8983/solr/nndcweb crawl/crawldb -linkdb crawl/linkdb -Dsolr.server.type=cloud -D solr.zookeeper.url=localhost:9983/  -dir crawl/segments/* -normalize</p>\n</blockquote>\n\n<p>and get same thing.  Doing a help on solrindex indicates using the -params with an \"&amp;\" separating the options (in contrast to using -D). However, this only serves telling my Linux system to try to run some strange things in the background, of course.  </p>\n\n<p>Does anybody have any suggestions on what to try next?  Thanks!</p>\n\n<p><strong>Update</strong>\nI updated the commands used above to reflect the correction to a silly mistake I made.  Note that all url references, in practice, do have the http:// prefix, but I had to take them out to be able to post.  In spite of the fix, I'm still getting the same exception though ( a sample of which I used to replace the original above, again with the http:// cut out..which does make things confusing...sorry about that...).</p>\n\n<p><strong>Yet Another Update</strong>\nSo..this is interesting.  Using the solrindex option, I just took out the port from the zookeeper url ..just localhost (with the http:// prefix).  15 characters.  The URISyntaxException says the problem is at index 18 (from org.apache.hadoop.fs.Path.initialize(Path.java:206)).  This does happen to match the \"=\" in \"solr.zookeeper.url=\".  So, it seems like the hadoop.fs.Path.intialize() is taking the whole string as the url.  So perhaps I am not setting that up correctly?  Or is this a bug in hadoop?  That would be hard to believe.</p>\n\n<p><strong>An Almost There Update</strong>\nAlright..given the results of the last attempt, I decided to put the solr.type of cloud and the zookeeper.url in the nutch-site.xml config file. Then did:</p>\n\n<blockquote>\n  <p>bin/nutch solrindex <a href=\"http://localhost:8983/solr/nndcweb\" rel=\"nofollow\">http://localhost:8983/solr/nndcweb</a> crawl/crawldb -linkdb crawl/linkdb  -dir crawl/segments -normalize</p>\n</blockquote>\n\n<p>(great..no complaints about the url now from StackOverflow).  No uri exception anymore.  Now, the error I get is:</p>\n\n<p>(cutting verbiage at the top)</p>\n\n<pre><code>Indexing 250 documents\nIndexer: java.io.IOException: Job failed!\nat org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:836)\nat org.apache.nutch.indexer.IndexingJob.index(IndexingJob.java:145)\nat org.apache.nutch.indexer.IndexingJob.run(IndexingJob.java:222)\nat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)\nat org.apache.nutch.indexer.IndexingJob.main(IndexingJob.java:231)\n</code></pre>\n\n<p>Digging deeper into the nutch logs, I see the following:</p>\n\n<blockquote>\n  <p>No Collection Param specified on request and no default collection has been set.  </p>\n</blockquote>\n\n<p>Apparently, this has been mentioned at the Nutch Mailing list , in connection with Nutch 1.11 and solr 5 (cloud mode).  There it was mentioned that it was not going to work, but a patch would be uploaded (this was back in January 2016).  Digging around on the nutch development site, I hadn't come across anything on this issue...something a little bit similar for nutch 1.13, which is apparently not officially released.  Still digging around, but if anybody actually has this working somehow, I'd love to hear how you did it..</p>\n\n<p><em>Edit July 12-2016</em></p>\n\n<p>So, after a few weeks diversion on another unrelated project, I'm back to this.  Before seeing S. Doe's response below, I decided to give ElasticSearch a try instead..as this is a completely new project and we're not tied to anything yet.  So far so good.  Nutch is working well with it, although to use the distributed binaries I had to back the Elasticsearch version down to 1.4.1.  Haven't tried the security aspect yet.  Out of curiosity, I will try S. Doe's suggestion with solr eventually and will post how that goes later...</p>\n", "creation_date": 1465488751, "score": 0},
{"title": "Apache Nutch with Lucene", "view_count": 781, "is_answered": false, "answers": [{"question_id": 18374606, "owner": {"user_id": 2695269, "link": "http://stackoverflow.com/users/2695269/mgs", "user_type": "registered", "reputation": 709}, "body": "<p>You can use solr for indexing purpose. Solr is an open-source search server based on the Lucene Java search library and easily configurable with Nutch. <br/></p>\n\n<p>Command:<br/></p>\n\n<ul>\n<li>bin/nutch crawl urls -solr <a href=\"http://solr.server:8983/solr/\" rel=\"nofollow\">http://solr.server:8983/solr/</a> -depth depth -topN topN</li>\n</ul>\n\n<p>It will crawl seed urls list up to specified depth and index them to specified solr server. Solr internally creates lucene indexes..<br/></p>\n\n<p>Reference: <a href=\"http://wiki.apache.org/nutch/NutchTutorial\" rel=\"nofollow\">http://wiki.apache.org/nutch/NutchTutorial</a></p>\n", "creation_date": 1377159956, "is_accepted": false, "score": 0, "last_activity_date": 1377159956, "answer_id": 18375198}, {"question_id": 18374606, "owner": {"user_id": 2623052, "accept_rate": 45, "link": "http://stackoverflow.com/users/2623052/allan-macmillan", "user_type": "registered", "reputation": 866}, "body": "<p>Nutch: This is the web or file crawler that will crawl through web pages or fileshares and fetch and parse the content.  It was designed to be integrated with Apache Solr so has many functions, the most uselful is passing content it has generated over to Solr, but Nutch does not do the indexing.</p>\n\n<p>Solr:  Solr is a search server that will index your data for you using Lucene.  Once Nutch has passed the documents over (via http) Solr will index them and store the Lucene indexes.  There is a nice search interface allowing you to query Solr which will return the results in XML format.</p>\n\n<p>Use Solr and Nutch - they were designed to work together</p>\n\n<p>Check out <a href=\"http://amac4.blogspot.com/2013/07/setting-up-solr-with-apache-tomcat-be.html\" rel=\"nofollow\">Setting Up Solr 4</a> and <a href=\"http://amac4.blogspot.com/2013/07/configuring-nutch-to-crawl-urls.html\" rel=\"nofollow\">Setting Up Nutch</a></p>\n", "creation_date": 1377160536, "is_accepted": false, "score": 0, "last_activity_date": 1377160536, "answer_id": 18375379}], "question_id": 18374606, "tags": ["apache", "lucene", "indexing", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/18374606/apache-nutch-with-lucene", "last_activity_date": 1468302737, "owner": {"user_id": 2116143, "answer_count": 0, "creation_date": 1361980865, "accept_rate": 0, "view_count": 3, "reputation": 16}, "body": "<p>We have a legacy code in Lucene and as a new requirement we need to use Apache Nutch for crawling. It means that Apache Nutch should crawl content and then the existing Lucene analysers should generate indexes.</p>\n\n<p>My problem is that Apache Nutch already generate indexes from which I can't generate the content. We do not want to use Nutch indexes.</p>\n\n<p>Do you advise me to use another crawler or is it still possible to use Apache Nutch for this end?</p>\n", "creation_date": 1377157914, "score": 0},
{"title": "Apache Nutch perks", "view_count": 40, "owner": {"user_id": 6239131, "view_count": 3, "answer_count": 1, "creation_date": 1461306843, "reputation": 1}, "is_answered": true, "answers": [{"question_id": 36787111, "owner": {"user_id": 432844, "link": "http://stackoverflow.com/users/432844/julien-nioche", "user_type": "registered", "reputation": 939}, "body": "<p>Javascript - there is a Selenium-based protocol implementation, this can help with JS sites</p>\n\n<p>Nutch is based on Hadoop and so is batch-driven. If you are after a stream based crawling framework then <a href=\"http://digitalpebble.com/storm-crawler/\" rel=\"nofollow\">StormCrawler</a> would be a better tool.</p>\n", "creation_date": 1461317210, "is_accepted": true, "score": 0, "last_activity_date": 1461317210, "answer_id": 36789992}], "question_id": 36787111, "tags": ["java", "hadoop", "web-crawler", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/36787111/apache-nutch-perks", "last_activity_date": 1467823797, "accepted_answer_id": 36789992, "body": "<p>I came across this very big tool and before diving deep into some <em>not so pretty</em> documentations and/or buying an <a href=\"http://rads.stackoverflow.com/amzn/click/B00HK3VPCC\" rel=\"nofollow\">ebook</a> : I just wanted to ask :</p>\n\n<p>How does Apache Nutch handle javascript heavy sites, AND how does it fetch pages?\nI mean : how does it overcome IP bans?</p>\n", "creation_date": 1461308580, "score": -3},
{"title": "Re-crawl Nutch periodically using cron job", "view_count": 513, "is_answered": true, "answers": [{"last_edit_date": 1467725275, "owner": {"user_id": 2420102, "accept_rate": 80, "link": "http://stackoverflow.com/users/2420102/mshoaib91", "user_type": "registered", "reputation": 950}, "body": "<p>You used <code>bin/crawl</code> command which executes complete crawl cycles. When you executed the command first time, it goes up to depth 5 i.e it executed 5 cycles.</p>\n\n<p>Now when you are running the same command again, providing the same segment folder, in your case <code>crawl</code>, it will try to fetch the pages from the depth 6 because crawl DB already have pages retrieved from the earlier 5 crawls marked as fetched. </p>\n\n<p>There could be certain factors for not fetching more pages. </p>\n\n<p>One reason could be either there are no more links available that you are trying to fetch. If you have restricted the URLs to fetch in <code>NUTCH_HOME/conf/regex-urlfilter.txt</code> this could be the possibility.</p>\n\n<p>There could be other constrains too in your configurations, checkout my answer on <a href=\"http://stackoverflow.com/a/37461417/2420102\">How to increase number of documents fetched by Apache Nutch crawler</a></p>\n\n<hr>\n\n<p>As I percieve from your question's title: <em>\"Re-crawl nutch periodically using cronjob\"</em>. If you want to again re-crawl the pages from scratch, then you should change or remove the folder where all your nutch's <em>crawldb, linkdb and segments</em> are saved. In your case <em>\"crawl\"</em> folder. This will not continue your crawling from the last crawl process but will start from ground zero again.</p>\n\n<p>You can also check out this <a href=\"http://stackoverflow.com/questions/14261586/recrawl-url-with-nutch-just-for-updated-sites\">post</a>.</p>\n", "question_id": 37909123, "creation_date": 1466416472, "is_accepted": false, "score": 1, "last_activity_date": 1467725275, "answer_id": 37919460}], "question_id": 37909123, "tags": ["solr", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/37909123/re-crawl-nutch-periodically-using-cron-job", "last_activity_date": 1467725275, "owner": {"age": 31, "answer_count": 1, "creation_date": 1349978858, "user_id": 1739055, "accept_rate": 67, "view_count": 12, "location": "India", "reputation": 17}, "body": "<p>I've successfully crawled a website using Nutch 1.12 and indexed it in Solr 6.1 using the below command:</p>\n\n<pre><code>[root@2a563cff0511 nutch-latest]# bin/crawl -i \\\n&gt; -D solr.server.url=http://192.168.99.100:8983/solr/test/ urls/ crawl 5\n</code></pre>\n\n<p>When I run the above command again then it says the following:</p>\n\n<pre><code>[root@2a563cff0511 nutch-latest]# bin/crawl -i \\\n&gt; -D solr.server.url=http://192.168.99.100:8983/solr/test/ urls/ crawl 5\nInjecting seed URLs\n/opt/nutch-latest/bin/nutch inject crawl/crawldb urls/\nInjector: starting at 2016-06-19 15:29:08\nInjector: crawlDb: crawl/crawldb\nInjector: urlDir: urls\nInjector: Converting injected urls to crawl db entries.\nInjector: overwrite: false\nInjector: update: false\nInjector: Total urls rejected by filters: 0\nInjector: Total urls injected after normalization and filtering: 1\nInjector: Total urls injected but already in CrawlDb: 1\nInjector: Total new urls injected: 0\nInjector: finished at 2016-06-19 15:29:13, elapsed: 00:00:05\nSun Jun 19 15:29:13 UTC 2016 : Iteration 1 of 1\nGenerating a new segment\n/opt/nutch-latest/bin/nutch generate -D mapreduce.job.reduces=2 -D mapred.child.java.opts=-Xmx1000m -D mapreduce.reduce.\nspeculative=false -D mapreduce.map.speculative=false -D mapreduce.map.output.compress=true crawl/crawldb crawl/segments\n-topN 50000 -numFetchers 1 -noFilter\nGenerator: starting at 2016-06-19 15:29:15\nGenerator: Selecting best-scoring urls due for fetch.\nGenerator: filtering: false\nGenerator: normalizing: true\nGenerator: topN: 50000\nGenerator: 0 records selected for fetching, exiting ...\nGenerate returned 1 (no new segments created)\nEscaping loop: no more URLs to fetch now\n</code></pre>\n\n<p>However, I made some changes i.e. new file is being added and an existing file has been changed.</p>\n", "creation_date": 1466352093, "score": 0},
{"title": "Nutch 1.12, why are the fetched data garbled text?", "view_count": 60, "is_answered": false, "question_id": 38118213, "tags": ["java", "osx", "apache", "nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/38118213/nutch-1-12-why-are-the-fetched-data-garbled-text", "last_activity_date": 1467708076, "owner": {"user_id": 6197131, "view_count": 2, "answer_count": 0, "creation_date": 1460530292, "reputation": 21}, "body": "<p><strong>UPDATE: SOLVED</strong></p>\n\n<p>Turns out that I was being an idiot.\nThe files Nutch created are Hadoop sequence files, they need to be read and written using class SequenceFile.</p>\n\n<hr>\n\n<p>I built Nutch 1.x following <a href=\"https://wiki.apache.org/nutch/RunNutchInEclipse#Running_Nutch_in_Eclipse\" rel=\"nofollow\">this guide</a>, I think it's 1.12.</p>\n\n<p>I'm on OS X, and doing this with JDK 1.8.0_92 (the latest one).</p>\n\n<p>I have added the following codes in nutch-site.xml, and changed the nutch-default.xml with them too.</p>\n\n<pre><code>&lt;property&gt;\n    &lt;name&gt;parser.character.encoding.default&lt;/name&gt;\n    &lt;value&gt;utf-8&lt;/value&gt;\n&lt;/property&gt;\n</code></pre>\n\n<p>I even hardcoded ./src/plugin/parse-html/src/java/org/apache/nutch/parse/html/HtmlParser.java</p>\n\n<pre><code>private final String defaultCharEncoding=\"UTF-8\";\nthis.defaultCharEncoding = getConf().get(\n    \"parser.character.encoding.default\", \"UTF-8\");\n</code></pre>\n\n<p>And then I ran the following main classes (I was crawling reddit):</p>\n\n<pre><code>org.apache.nutch.crawl.Injector\norg.apache.nutch.crawl.Generator\norg.apache.nutch.fetcher.Fetcher\norg.apache.nutch.parse.ParseSegment\n</code></pre>\n\n<p>which completed well.</p>\n\n<p>But the fetched data files are all in garbled text, like this:</p>\n\n<p>\u00a3a\u03c0H\u00c9\"EJ\u00f6II\u00b5\u00ed\u2566\u00f7t\u2551\u00bd\u00fc&amp;\u2551\u2557\u2514>B\u00fbb\u2510\u221e\u2248\u00b2\u00f4g3\u00bd\u00b7\u00dc(\u2556\n\n<p>I've been stuck for hours by now, please help.</p>\n", "creation_date": 1467276622, "score": 0},
{"title": "Nutch on windows: ERROR crawl.Injector", "view_count": 237, "is_answered": false, "question_id": 38147431, "tags": ["windows", "hadoop", "cygwin", "nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/38147431/nutch-on-windows-error-crawl-injector", "last_activity_date": 1467387388, "owner": {"user_id": 6538299, "view_count": 3, "answer_count": 0, "creation_date": 1467377963, "reputation": 6}, "body": "<p>I'm trying to install nutch 1.12 on a windows 2012 server based on cygwin64 2.874. Due to limited skills with java and linux I followed the step by step introduction at <a href=\"https://wiki.apache.org/nutch/NutchTutorial#Step-by-Step:_Seeding_the_crawldb_with_a_list_of_URLs\" rel=\"nofollow\">https://wiki.apache.org/nutch/NutchTutorial#Step-by-Step:_Seeding_the_crawldb_with_a_list_of_URLs</a>. The command </p>\n\n<pre><code> bin/nutch inject crawl/crawldb urls\n</code></pre>\n\n<p>throws an error because winutils.exe couldn't be found. Here is the hadoop log:</p>\n\n<pre><code>2016-07-01 09:22:25,660 ERROR util.Shell - Failed to locate the winutils binary in the hadoop binary path\njava.io.IOException: Could not locate executable null\\bin\\winutils.exe in the Hadoop binaries.\n    at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:318)\n    at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:333)\n    at org.apache.hadoop.util.Shell.&lt;clinit&gt;(Shell.java:326)\n    at org.apache.hadoop.util.GenericOptionsParser.preProcessForWindows(GenericOptionsParser.java:432)\n    at org.apache.hadoop.util.GenericOptionsParser.parseGeneralOptions(GenericOptionsParser.java:478)\n    at org.apache.hadoop.util.GenericOptionsParser.&lt;init&gt;(GenericOptionsParser.java:170)\n    at org.apache.hadoop.util.GenericOptionsParser.&lt;init&gt;(GenericOptionsParser.java:153)\n    at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:64)\n    at org.apache.nutch.crawl.Injector.main(Injector.java:441)\n2016-07-01 09:22:25,832 INFO  crawl.Injector - Injector: starting at 2016-07-01 09:22:25\n2016-07-01 09:22:25,832 INFO  crawl.Injector - Injector: crawlDb: crawl/crawldb\n2016-07-01 09:22:25,832 INFO  crawl.Injector - Injector: urlDir: urls\n2016-07-01 09:22:25,832 INFO  crawl.Injector - Injector: Converting injected urls to crawl db entries.\n2016-07-01 09:22:26,050 WARN  util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n2016-07-01 09:22:26,394 ERROR crawl.Injector - Injector: java.lang.NullPointerException\n    at java.lang.ProcessBuilder.start(Unknown Source)\n    at org.apache.hadoop.util.Shell.runCommand(Shell.java:445)\n    at org.apache.hadoop.util.Shell.run(Shell.java:418)\n    at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:650)\n    at org.apache.hadoop.util.Shell.execCommand(Shell.java:739)\n    at org.apache.hadoop.util.Shell.execCommand(Shell.java:722)\n    at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:633)\n    at org.apache.hadoop.fs.FilterFileSystem.setPermission(FilterFileSystem.java:467)\n    at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:456)\n    at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:424)\n    at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:906)\n    at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:887)\n    at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:849)\n    at org.apache.hadoop.fs.FileSystem.createNewFile(FileSystem.java:1149)\n    at org.apache.nutch.util.LockUtil.createLockFile(LockUtil.java:58)\n    at org.apache.nutch.crawl.Injector.inject(Injector.java:357)\n    at org.apache.nutch.crawl.Injector.run(Injector.java:467)\n    at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)\n    at org.apache.nutch.crawl.Injector.main(Injector.java:441)\n</code></pre>\n\n<p>I found several hints here and downloaded winutils.exe from <a href=\"https://codeload.github.com/srccodes/hadoop-common-2.2.0-bin/zip/master\" rel=\"nofollow\">https://codeload.github.com/srccodes/hadoop-common-2.2.0-bin/zip/master</a>. I unpacked the folder on the server and set the environment variable NUTCH_OPTS=-Dhadoop.home.dir=[winutils_folder]. Now the winutils error is gone but the nutch call fails with a different error:</p>\n\n<pre><code>2016-07-01 13:24:09,697 INFO  crawl.Injector - Injector: starting at 2016-07-01 13:24:09\n2016-07-01 13:24:09,697 INFO  crawl.Injector - Injector: crawlDb: crawl/crawldb\n2016-07-01 13:24:09,697 INFO  crawl.Injector - Injector: urlDir: urls\n2016-07-01 13:24:09,697 INFO  crawl.Injector - Injector: Converting injected urls to crawl db entries.\n2016-07-01 13:24:09,885 WARN  util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n2016-07-01 13:24:10,307 ERROR crawl.Injector - Injector: org.apache.hadoop.util.Shell$ExitCodeException: \n    at org.apache.hadoop.util.Shell.runCommand(Shell.java:505)\n    at org.apache.hadoop.util.Shell.run(Shell.java:418)\n    at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:650)\n    at org.apache.hadoop.util.Shell.execCommand(Shell.java:739)\n    at org.apache.hadoop.util.Shell.execCommand(Shell.java:722)\n    at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:633)\n    at org.apache.hadoop.fs.FilterFileSystem.setPermission(FilterFileSystem.java:467)\n    at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:456)\n    at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:424)\n    at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:906)\n    at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:887)\n    at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:849)\n    at org.apache.hadoop.fs.FileSystem.createNewFile(FileSystem.java:1149)\n    at org.apache.nutch.util.LockUtil.createLockFile(LockUtil.java:58)\n    at org.apache.nutch.crawl.Injector.inject(Injector.java:357)\n    at org.apache.nutch.crawl.Injector.run(Injector.java:467)\n    at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)\n    at org.apache.nutch.crawl.Injector.main(Injector.java:441)\n</code></pre>\n\n<p>After updating .bashrc (added the following lines) the hadoop log only shows warnings.</p>\n\n<pre><code>export JAVA_HOME='/cygdrive/c/Program Files/Java/jre1.8.0_92'\nexport PATH=$PATH:$JAVA_HOME/bin\n</code></pre>\n\n<p>But nutch still throws an error:</p>\n\n<pre><code>Injector: starting at 2016-07-01 17:25:22\nInjector: crawlDb: crawl/crawldb\nInjector: urlDir: urls\nInjector: Converting injected urls to crawl db entries.\nException in thread \"main\" java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z\n        at org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\n        at org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:570)\n        at org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:977)\n        at org.apache.hadoop.util.DiskChecker.checkAccessByFileMethods(DiskChecker.java:173)\n        at org.apache.hadoop.util.DiskChecker.checkDirAccess(DiskChecker.java:160)\n        at org.apache.hadoop.util.DiskChecker.checkDir(DiskChecker.java:94)\n        at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.confChanged(LocalDirAllocator.java:285)\n        at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathForWrite(LocalDirAllocator.java:344)\n        at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:150)\n        at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:131)\n        at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:115)\n        at org.apache.hadoop.mapred.LocalDistributedCacheManager.setup(LocalDistributedCacheManager.java:131)\n        at org.apache.hadoop.mapred.LocalJobRunner$Job.&lt;init&gt;(LocalJobRunner.java:163)\n        at org.apache.hadoop.mapred.LocalJobRunner.submitJob(LocalJobRunner.java:731)\n        at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:432)\n        at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1285)\n        at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1282)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Unknown Source)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\n        at org.apache.hadoop.mapreduce.Job.submit(Job.java:1282)\n        at org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:1303)\n        at org.apache.nutch.crawl.Injector.inject(Injector.java:376)\n        at org.apache.nutch.crawl.Injector.run(Injector.java:467)\n        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)\n        at org.apache.nutch.crawl.Injector.main(Injector.java:441)\n</code></pre>\n\n<p>I need hints what may be wrong configured or isn't it possible to run nutch with windows/cygwin?</p>\n", "creation_date": 1467381759, "score": 1},
{"title": "elastic search on qbox is not accesible through nutch", "view_count": 57, "is_answered": false, "answers": [{"question_id": 38131693, "owner": {"user_id": 2159602, "link": "http://stackoverflow.com/users/2159602/pranav-shukla", "user_type": "registered", "reputation": 766}, "body": "<p>You usually get this error when either cluster host/port or cluster name does not match on client and server.</p>\n\n<p>In your <code>$NUTCH_ROOT/runtime/local/conf/nutch-site.xml</code> please make sure you have correctly configured the host, port and cluster name that matches the cluster name on qbox.io -</p>\n\n<pre><code>&lt;property&gt;\n  &lt;name&gt;elastic.host&lt;/name&gt;\n  &lt;value&gt;&lt;/value&gt;\n  &lt;description&gt;The hostname to send documents to using TransportClient. Either host\n  and port must be defined or cluster.&lt;/description&gt;\n&lt;/property&gt;\n\n&lt;property&gt; \n  &lt;name&gt;elastic.port&lt;/name&gt;\n  &lt;value&gt;9300&lt;/value&gt;\n  &lt;description&gt;The port to connect to using TransportClient.&lt;/description&gt;\n&lt;/property&gt;\n\n&lt;property&gt; \n  &lt;name&gt;elastic.cluster&lt;/name&gt;\n  &lt;value&gt;&lt;/value&gt;\n  &lt;description&gt;The cluster name to discover. Either host and port must be defined\n  or cluster.&lt;/description&gt;\n&lt;/property&gt;\n</code></pre>\n", "creation_date": 1467374870, "is_accepted": false, "score": 0, "last_activity_date": 1467374870, "answer_id": 38145039}], "question_id": 38131693, "tags": ["elasticsearch", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/38131693/elastic-search-on-qbox-is-not-accesible-through-nutch", "last_activity_date": 1467374870, "owner": {"age": 26, "answer_count": 0, "creation_date": 1423729059, "user_id": 4558261, "view_count": 1, "location": "Mumbai, Maharashtra, India", "reputation": 6}, "body": "<p>I have a qbox instance for elastic search (more details about qbox elasticsearch can be found at <a href=\"http://qbox.io/\" rel=\"nofollow\">http://qbox.io/</a>) with a custom tcp port.\nWhen I try to access the instance through nutch for indexing I get following error:</p>\n\n<pre>java.lang.Exception: org.elasticsearch.client.transport.NoNodeAvailableException: None of the configured nodes are available: []\n    at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:354)\nCaused by: org.elasticsearch.client.transport.NoNodeAvailableException: None of the configured nodes are available: []\n    at org.elasticsearch.client.transport.TransportClientNodesService.ensureNodesAreAvailable(TransportClientNodesService.java:278)\n    at org.elasticsearch.client.transport.TransportClientNodesService.execute(TransportClientNodesService.java:197)\n    at org.elasticsearch.client.transport.support.InternalTransportClient.execute(InternalTransportClient.java:106)\n    at org.elasticsearch.client.support.AbstractClient.bulk(AbstractClient.java:163)\n    at org.elasticsearch.client.transport.TransportClient.bulk(TransportClient.java:364)\n    at org.2016-06-30 15:39:14,320 WARN  mapred.FileOutputCommitter - Output path is null in cleanup\n2016-06-30 15:39:14,320 WARN  mapred.LocalJobRunner - job_local561208907_0001\njava.lang.Exception: org.elasticsearch.client.transport.NoNodeAvailableException: None of the configured nodes are available: []\n    at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:354)\nCaused by: org.elasticsearch.client.transport.NoNodeAvailableException: None of the configured nodes are available: []\n    at org.elasticsearch.client.transport.TransportClientNodesService.ensureNodesAreAvailable(TransportClientNodesService.java:278)\n    at org.elasticsearch.client.transport.TransportClientNodesService.execute(TransportClientNodesService.java:197)\n    at org.elasticsearch.client.transport.support.InternalTransportClient.execute(InternalTransportClient.java:106)\n    at org.elasticsearch.client.support.AbstractClient.bulk(AbstractClient.java:163)\n    at org.elasticsearch.client.transport.TransportClient.bulk(TransportClient.java:364)\n    at org.elasticsearch.action.bulk.BulkRequestBuilder.doExecute(BulkRequestBuilder.java:164)\n    at org.elasticsearch.action.ActionRequestBuilder.execute(ActionRequestBuilder.java:91)\n    at org.elasticsearch.action.ActionRequestBuilder.execute(ActionRequestBuilder.java:65)\n    at org.apache.nutch.indexwriter.elastic.ElasticIndexWriter.commit(ElasticIndexWriter.java:208)\n    at org.apache.nutch.indexwriter.elastic.ElasticIndexWriter.close(ElasticIndexWriter.java:226)\n    at org.apache.nutch.indexer.IndexWriters.close(IndexWriters.java:114)\n    at org.apache.nutch.indexer.IndexerOutputFormat$1.close(IndexerOutputFormat.java:54)\n    at org.apache.hadoop.mapred.MapTask$NewDirectOutputCollector.close(MapTask.java:650)\n    at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:767)\n    at org.apache.hadoop.mapred.MapTask.run(MapTask.java:364)\n    at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:223)\n    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)\n    at java.util.concurrent.FutureTaselasticsearch.action.bulk.BulkRequestBuilder.doExecute(BulkRequestBuilder.java:164)\n    at org.elasticsearch.action.ActionRequestBuilder.execute(ActionRequestBuilder.java:91)\n    at org.elasticsearch.action.ActionRequestBuilder.execute(ActionRequestBuilder.java:65)\n    at org.apache.nutch.indexwriter.elastic.ElasticIndexWriter.commit(ElasticIndexWriter.java:208)\n    at org.apache.nutch.indexwriter.elastic.ElasticIndexWriter.close(ElasticIndexWriter.java:226)\n    at org.apache.nutch.indexer.IndexWriters.close(IndexWriters.java:114)\n    at org.apache.nutch.indexer.IndexerOutputFormat$1.close(IndexerOutputFormat.java:54)\n    at org.apache.hadoop.mapred.MapTask$NewDirectOutputCollector.close(MapTask.java:650)\n    at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:767)\n    at org.apache.hadoop.mapred.MapTask.run(MapTask.java:364)\n    at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:223)\n    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)\n    at java.util.concurrent.FutureTask.run(FutureTask.java:262)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n    at java.lang.Thread.run(Thread.java:745)\nk.run(FutureTask.java:262)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n    at java.lang.Thread.run(Thread.java:745)\n\n</pre>\n", "creation_date": 1467314776, "score": 1},
{"title": "Get IP address from within Nutch plugin", "view_count": 36, "owner": {"user_id": 5147080, "view_count": 4, "answer_count": 2, "creation_date": 1437639874, "reputation": 11}, "is_answered": true, "answers": [{"question_id": 37854798, "owner": {"user_id": 5147080, "link": "http://stackoverflow.com/users/5147080/roel", "user_type": "registered", "reputation": 11}, "body": "<p>Eventually I found a solution by making two changes in the nutch-site.xml config file:</p>\n\n<ul>\n<li>use protocol-http instead of protocol-httpclient</li>\n<li>Set the httpclient store.ip.address to true</li>\n</ul>\n", "creation_date": 1467365953, "is_accepted": true, "score": 0, "last_activity_date": 1467365953, "answer_id": 38142054}, {"question_id": 37854798, "owner": {"user_id": 5813713, "accept_rate": 38, "link": "http://stackoverflow.com/users/5813713/swamy-reddy", "user_type": "registered", "reputation": 17}, "body": "<blockquote>\n  <p>Try this if it can help..</p>\n</blockquote>\n\n<pre><code>&lt;script type=\"text/javascript\" src=\"http://l2.io/ip.js?var=myip\"&gt;&lt;/script&gt;\n&lt;script type=\"text/javascript\"&gt;\nalert(myip);\n&lt;/script&gt;\n</code></pre>\n", "creation_date": 1467366228, "is_accepted": false, "score": 0, "last_activity_date": 1467366228, "answer_id": 38142155}], "question_id": 37854798, "tags": ["java", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/37854798/get-ip-address-from-within-nutch-plugin", "last_activity_date": 1467366228, "accepted_answer_id": 38142054, "body": "<p>I'm working on a project where we are crawling and indexing several websites. For this I am using Apache Nutch 2.3. Now we would also like to index the servers IP address. After searching the documentation I have concluded that Nutch seems to be unable to do this out of the box. For this reason I am now trying to write a plugin that implements this feature.</p>\n\n<p>The problem I am running into is that I am unable to get the IP address from within the plugin. For my test I've written the following code:</p>\n\n<pre><code>@Override\npublic NutchDocument filter(NutchDocument doc, String url, WebPage page) throws IndexingException {\n    ByteBuffer ip = page.getMetadata().get(\"_ip_\");\n    System.out.println(\"IP address is \" + Bytes.toString(ip));\n</code></pre>\n\n<p>I would expect Nutch to print the server's IP address, but instead the IP address is always null. Does anyone know how to get the server IP address from within the plugin?</p>\n\n<p>Some notes:</p>\n\n<ul>\n<li>There is a setting called \"store.ip.address\". I've set this to true, but it makes no difference.</li>\n<li>In the master repository a geoip plugin is available, which gets the IP address in a similar way. So I think it should be possible, though it is a different version of Nutch.</li>\n<li>I would rather not modify the source of Nutch itself.</li>\n</ul>\n", "creation_date": 1466068386, "score": 0},
{"title": "integrating apache tomcat and solr", "view_count": 73, "is_answered": false, "answers": [{"question_id": 38007771, "owner": {"user_id": 4193280, "accept_rate": 27, "link": "http://stackoverflow.com/users/4193280/multi10ant", "user_type": "registered", "reputation": 1229}, "body": "<p>Go to Solrconfig.xml and there will be a tag called as data directory , so just try to hard code the required path , if it works , then you can conclude that it is the root cause of the issue .</p>\n", "creation_date": 1466795264, "is_accepted": false, "score": 0, "last_activity_date": 1466795264, "answer_id": 38020497}, {"question_id": 38007771, "owner": {"user_id": 6451312, "link": "http://stackoverflow.com/users/6451312/joseph-m-dion", "user_type": "registered", "reputation": 11}, "body": "<p>You can add the following java environment variable in tomcat startup configuration (in JAVA_OPTS for example).</p>\n\n<pre><code>-Dsolr.solr.home=&lt;your solr.xml path&gt;\n</code></pre>\n", "creation_date": 1467211162, "is_accepted": false, "score": 0, "last_activity_date": 1467211162, "answer_id": 38102850}], "question_id": 38007771, "tags": ["java", "apache", "tomcat", "solr", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/38007771/integrating-apache-tomcat-and-solr", "last_activity_date": 1467211162, "owner": {"user_id": 6507714, "view_count": 9, "answer_count": 0, "creation_date": 1466752556, "reputation": 1}, "body": "<p>I am trying to integrate apache tomcat (version 7)and solr (version 4) and i am getting this error :</p>\n\n<pre><code>HTTP Status 500 - {msg=SolrCore 'collection1' is not available due to init failure: \nThe filename, directory name, or volume label syntax is incorrect,\ntrace=org.apache.solr.common.SolrException: SolrCore 'collection1' is not available due to init failure: The filename, directory name, or volume label syntax is incorrect at \norg.apache.solr.core.CoreContainer.getCore(CoreContainer.java:1212) at org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:248) at \norg.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:155) at \norg.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241) at \norg.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208) at \norg.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:218) at \norg.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:122) at \norg.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:169) at \norg.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:103) at \norg.apache.catalina.valves.AccessLogValve.invoke(AccessLogValve.java:956) at \norg.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:116) at \norg.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:442) at \norg.apache.coyote.http11.AbstractHttp11Processor.process(AbstractHttp11Processor.java:1082) at \norg.apache.coyote.AbstractProtocol$AbstractConnectionHandler.process(AbstractProtocol.java:623) at \norg.apache.tomcat.util.net.JIoEndpoint$SocketProcessor.run(JIoEndpoint.java:316) at \njava.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source) at \njava.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) at \norg.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61) at \njava.lang.Thread.run(Unknown Source) Caused by: \n\norg.apache.solr.common.SolrException: The filename, directory name, or volume label syntax is incorrect at \norg.apache.solr.core.SolrCore.&lt;init&gt;(SolrCore.java:821) at \norg.apache.solr.core.SolrCore.&lt;init&gt;(SolrCore.java:618) at \norg.apache.solr.core.CoreContainer.createFromLocal(CoreContainer.java:949) at \norg.apache.solr.core.CoreContainer.create(CoreContainer.java:984) at \norg.apache.solr.core.CoreContainer$2.call(CoreContainer.java:597) at \norg.apache.solr.core.CoreContainer$2.call(CoreContainer.java:592) at \njava.util.concurrent.FutureTask.run(Unknown Source) at \njava.util.concurrent.Executors$RunnableAdapter.call(Unknown Source) at \njava.util.concurrent.FutureTask.run(Unknown Source) at \njava.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source) at java.util.concurrent.ThreadPoolExecutor\n$Worker.run(Unknown Source) ... 1 more \n\nCaused by: java.io.IOException: The filename, directory name, or volume label syntax is incorrect at \njava.io.WinNTFileSystem.canonicalize0(Native Method) at java.io.Win32FileSystem.canonicalize(Unknown Source) \nat java.io.File.getCanonicalPath(Unknown Source) at \norg.apache.solr.core.StandardDirectoryFactory.normalize(StandardDirectoryFactory.java:47) \nat org.apache.solr.core.SolrCore.initIndex(SolrCore.java:462) at \norg.apache.solr.core.SolrCore.&lt;init&gt;(SolrCore.java:745) ... 11 more ,code=500}\n</code></pre>\n\n<p>please help me fix it!</p>\n", "creation_date": 1466752816, "score": 0},
{"title": "Nutch 2.3.1 Hbase Solr on EC2", "view_count": 42, "is_answered": false, "question_id": 38028734, "tags": ["solr", "amazon-ec2", "hbase", "nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/38028734/nutch-2-3-1-hbase-solr-on-ec2", "last_activity_date": 1466864641, "owner": {"age": 30, "answer_count": 0, "creation_date": 1405615102, "user_id": 3850092, "view_count": 1, "reputation": 1}, "body": "<p>I have set up Nutch 2.3.1 with Hbase(0.98) as storage and indexed the data into SOLR 5.4.1. Gora version 0.61</p>\n\n<p>I am able to get this setup working on my VM.\nAlthough, this setup is failing on EC2 with an Hbase error as below. </p>\n\n<p>I am able to crawl the data and the outlinks are saved in Hbase and also index in SOLR.\nBut about 20-30 minutes in to the crawling this error is thrown.</p>\n\n<p>Hbase has been set up with default settings</p>\n\n<p>Greatly appreciate any help towards this.</p>\n\n<pre><code>2016-06-23 13:01:00,664 ERROR mapreduce.GoraRecordReader - Error reading Gora records: 153637ms passed since the last invocation, timeout is currently set to 60000\n2016-06-23 13:01:00,665 WARN  client.ScannerCallable - Ignore, probably already closed\norg.apache.hadoop.hbase.UnknownScannerException: org.apache.hadoop.hbase.UnknownScannerException: Name: 104, already closed?\n        at org.apache.hadoop.hbase.regionserver.RSRpcServices.scan(RSRpcServices.java:2238)\n        at org.apache.hadoop.hbase.protobuf.generated.ClientProtos$ClientService$2.callBlockingMethod(ClientProtos.java:32385)\n        at org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:2117)\n        at org.apache.hadoop.hbase.ipc.CallRunner.run(CallRunner.java:104)\n        at org.apache.hadoop.hbase.ipc.RpcExecutor.consumerLoop(RpcExecutor.java:133)\n        at org.apache.hadoop.hbase.ipc.RpcExecutor$1.run(RpcExecutor.java:108)\n        at java.lang.Thread.run(Thread.java:745)\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n        at java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n        at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:106)\n        at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:95)\n        at org.apache.hadoop.hbase.protobuf.ProtobufUtil.getRemoteException(ProtobufUtil.java:284)\n        at org.apache.hadoop.hbase.client.ScannerCallable.close(ScannerCallable.java:293)\n        at org.apache.hadoop.hbase.client.ScannerCallable.call(ScannerCallable.java:160)\n        at org.apache.hadoop.hbase.client.ScannerCallable.call(ScannerCallable.java:59)\n        at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithRetries(RpcRetryingCaller.java:117)\n        at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithRetries(RpcRetryingCaller.java:93)\n        at org.apache.hadoop.hbase.client.ClientScanner.close(ClientScanner.java:457)\n        at org.apache.gora.hbase.query.HBaseScannerResult.close(HBaseScannerResult.java:61)\n        at org.apache.gora.mapreduce.GoraRecordReader.close(GoraRecordReader.java:129)\n        at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.close(MapTask.java:523)\n        at org.apache.hadoop.mapred.MapTask.closeQuietly(MapTask.java:2004)\n        at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:796)\n        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)\n        at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:243)\n        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n        at java.util.concurrent.FutureTask.run(FutureTask.java:266)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.hadoop.hbase.ipc.RemoteWithExtrasException(org.apache.hadoop.hbase.UnknownScannerException): org.apache.hadoop.hbase.UnknownScannerException: Name: 104, already closed?\n        at org.apache.hadoop.hbase.regionserver.RSRpcServices.scan(RSRpcServices.java:2238)\n        at org.apache.hadoop.hbase.protobuf.generated.ClientProtos$ClientService$2.callBlockingMethod(ClientProtos.java:32385)\n        at org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:2117)\n        at org.apache.hadoop.hbase.ipc.CallRunner.run(CallRunner.java:104)\n        at org.apache.hadoop.hbase.ipc.RpcExecutor.consumerLoop(RpcExecutor.java:133)\n        at org.apache.hadoop.hbase.ipc.RpcExecutor$1.run(RpcExecutor.java:108)\n        at java.lang.Thread.run(Thread.java:745)\n        at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1457)\n        at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1661)\n        at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1719)\n        at org.apache.hadoop.hbase.protobuf.generated.ClientProtos$ClientService$BlockingStub.scan(ClientProtos.java:30387)\n        at org.apache.hadoop.hbase.client.ScannerCallable.close(ScannerCallable.java:291)\n        ... 17 more\n</code></pre>\n", "creation_date": 1466859234, "score": 0},
{"title": "Indexing in SOLR: correcting the analyzer to not produce immense terms", "view_count": 197, "is_answered": true, "answers": [{"last_edit_date": 1466622262, "owner": {"user_id": 1977773, "link": "http://stackoverflow.com/users/1977773/jorge-luis", "user_type": "registered", "reputation": 510}, "body": "<p>Assuming that you're using the <code>schema.xml</code> bundled with Nutch as the base schema for your Solr installation, basically you'll just need to add either of those filters (<code>LengthFilterFactory</code> or <code>TruncateTokenFilterFactory</code>) to the <code>text_general</code> field type.</p>\n\n<p>Starting from the initial definition of the <code>text_general</code> <code>fieldType</code> (<a href=\"https://github.com/apache/nutch/blob/master/conf/schema.xml#L108-L123\" rel=\"nofollow\">https://github.com/apache/nutch/blob/master/conf/schema.xml#L108-L123</a>) you'll need to add the following to the <code>&lt;analyzer type=\"index\"&gt;</code> section:</p>\n\n<pre><code>...\n&lt;analyzer type=\"index\"&gt;\n    &lt;tokenizer class=\"solr.StandardTokenizerFactory\"/&gt;\n    &lt;!-- remove long tokens --&gt;\n    &lt;filter class=\"solr.LengthFilterFactory\" min=\"3\" max=\"7\"/&gt; \n    &lt;filter class=\"solr.StopFilterFactory\" ignoreCase=\"true\" words=\"stopwords.txt\" enablePositionIncrements=\"true\" /&gt;\n    &lt;filter class=\"solr.LowerCaseFilterFactory\"/&gt;\n&lt;/analyzer&gt;\n...\n</code></pre>\n\n<p>This could also be applied to the <code>query</code> analyzer using the same syntax. If you want to use the <code>TruncateTokenFilterFactory</code> filter just swap the added line with:</p>\n\n<pre><code> &lt;filter class=\"solr.TruncateTokenFilterFactory\" prefixLength=\"5\"/&gt;\n</code></pre>\n\n<p>Also, don't forget to adjust the parameters of each filter to your needs (<code>min</code>, <code>max</code> for the <code>LengthFilterFactory</code>) and <code>prefixLength</code> for the <code>TruncateTokenFilterFactory</code>.</p>\n\n<p>Answering your other questions: yes this would affect all fields with the <code>text_general</code> type but this is not so problematic because if you find another super-long term in any other field, the same error will be thrown. If you still want to isolate this change just for the <code>content</code> field, just create a new <code>fieldType</code> with a new name (<code>truncated_text_general</code>, for instance, just copy&amp;paste the entire <code>fieldType</code> section and change the name attribute) and then change the type of the <code>content</code> field (<a href=\"https://github.com/apache/nutch/blob/master/conf/schema.xml#L339\" rel=\"nofollow\">https://github.com/apache/nutch/blob/master/conf/schema.xml#L339</a>) to match your newly created <code>fieldType</code>. </p>\n\n<p>That being said, just select sane values for the filters to avoid missing a lot of terms from your index. </p>\n", "question_id": 37952302, "creation_date": 1466540380, "is_accepted": false, "score": 1, "last_activity_date": 1466622262, "answer_id": 37953852}], "question_id": 37952302, "tags": ["indexing", "solr", "nutch", "analyzer", "fieldtype"], "answer_count": 1, "link": "http://stackoverflow.com/questions/37952302/indexing-in-solr-correcting-the-analyzer-to-not-produce-immense-terms", "last_activity_date": 1466622262, "owner": {"age": 24, "answer_count": 8, "creation_date": 1465486803, "user_id": 6446301, "accept_rate": 67, "view_count": 16, "location": "Toronto, ON, Canada", "reputation": 6}, "body": "<p>I'm trying to index my nutch crawled data by running:</p>\n\n<pre><code>bin/nutch index -D solr.server.url=\"http://localhost:8983/solr/carerate\" crawl/crawldb -linkdb crawl/linkdb crawl/segments/2016*\n</code></pre>\n\n<p>At first it was working totally Ok. I indexed my data, sent a few queries and recieved good results. But then I ran the crawling again, so that it fetches more pages, and now when I run the nutch index command, I face with</p>\n\n<blockquote>\n  <p>java.io.IOException: Job failed!</p>\n</blockquote>\n\n<p>here is my hadoop log:</p>\n\n<blockquote>\n  <p>java.lang.Exception: org.apache.solr.client.solrj.impl.HttpSolrServer$RemoteSolrException: Exception writing document id <a href=\"http://www.cs.toronto.edu/~frank/About_Me/about_me.html\" rel=\"nofollow\">http://www.cs.toronto.edu/~frank/About_Me/about_me.html</a> to the index; possible analysis error: Document contains at least one immense term in field=\"content\" (whose UTF8 encoding is longer than the max length 32766), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '[70, 114, 97, 110, 107, 32, 82, 117, 100, 122, 105, 99, 122, 32, 45, 32, 65, 98, 111, 117, 116, 32, 77, 101, 32, 97, 98, 111, 117, 116]...', original message: bytes can be at most 32766 in length; got 40063. Perhaps the document has an indexed string field (solr.StrField) which is too large\n      at org.apache.hadoop.mapred.LocalJobRunner$Job.runTasks(LocalJobRunner.java:462)\n      at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:529)\n  Caused by: org.apache.solr.client.solrj.impl.HttpSolrServer$RemoteSolrException: Exception writing document id <a href=\"http://www.cs.toronto.edu/~frank/About_Me/about_me.html\" rel=\"nofollow\">http://www.cs.toronto.edu/~frank/About_Me/about_me.html</a> to the index; possible analysis error: Document contains at least one immense term in field=\"content\" (whose UTF8 encoding is longer than the max length 32766), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '[70, 114, 97, 110, 107, 32, 82, 117, 100, 122, 105, 99, 122, 32, 45, 32, 65, 98, 111, 117, 116, 32, 77, 101, 32, 97, 98, 111, 117, 116]...', original message: bytes can be at most 32766 in length; got 40063. Perhaps the document has an indexed string field (solr.StrField) which is too large\n      at org.apache.solr.client.solrj.impl.HttpSolrServer.executeMethod(HttpSolrServer.java:552)\n      at org.apache.solr.client.solrj.impl.HttpSolrServer.request(HttpSolrServer.java:210)\n      at org.apache.solr.client.solrj.impl.HttpSolrServer.request(HttpSolrServer.java:206)\n      at org.apache.solr.client.solrj.request.AbstractUpdateRequest.process(AbstractUpdateRequest.java:124)\n      at org.apache.nutch.indexwriter.solr.SolrIndexWriter.close(SolrIndexWriter.java:153)\n      at org.apache.nutch.indexer.IndexWriters.close(IndexWriters.java:115)\n      at org.apache.nutch.indexer.IndexerOutputFormat$1.close(IndexerOutputFormat.java:44)\n      at org.apache.hadoop.mapred.ReduceTask$OldTrackingRecordWriter.close(ReduceTask.java:502)\n      at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:456)\n      at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)\n      at org.apache.hadoop.mapred.LocalJobRunner$Job$ReduceTaskRunnable.run(LocalJobRunner.java:319)\n      at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n      at java.util.concurrent.FutureTask.run(FutureTask.java:266)\n      at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n      at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n      at java.lang.Thread.run(Thread.java:745)\n  2016-06-21 13:27:37,994 ERROR indexer.IndexingJob - Indexer: java.io.IOException: Job failed!\n      at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:836)\n      at org.apache.nutch.indexer.IndexingJob.index(IndexingJob.java:145)\n      at org.apache.nutch.indexer.IndexingJob.run(IndexingJob.java:222)\n      at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)\n      at org.apache.nutch.indexer.IndexingJob.main(IndexingJob.java:231)</p>\n</blockquote>\n\n<p>What I realize is that in the mentioned page there must be a really long term. \nSo in schema.xml(in nutch) and managed-schema(in solr) I changed the type of \"id\", \"content\",and \"text\" from \"strings\" to \"text_general\" :\nBut it didn't solve the problem.</p>\n\n<p>I'm no expert, so I'm not sure how to correct the analyzer without screwing up something else. I've read that I can:\n1. use (in index analyzer), a LengthFilterFactory in order to filter out those tokens that don't fall withing a requested length range.\n2.use (in index analyzer), a TruncateTokenFilterFactory for fixing the max length of indexed tokens</p>\n\n<p>but there are so many analyzer in the schema. should I change the analyzer defined for  ? if yes since the content and other fields' type are text_general, isn't it gonna affect all of them too?</p>\n\n<p>Anyone knows how can I fix this problem? I would really appreciate any help. </p>\n\n<p>BTW, I am using nutch 1.11 and solr 6.0.0.</p>\n", "creation_date": 1466534830, "score": 0},
{"title": "No IndexWriters activated - check your configuration", "view_count": 113, "is_answered": false, "answers": [{"question_id": 37643642, "owner": {"user_id": 6446301, "accept_rate": 67, "link": "http://stackoverflow.com/users/6446301/s-doe", "user_type": "registered", "reputation": 6}, "body": "<p>I guess you have to make sure that the plugin indexer-solr is included. Go to the file: conf/nutch-site.xml and in the property plugin.includes add the plugin, for instance:</p>\n\n<blockquote>\n  <p>protocol-http|urlfilter-regex|parse-(html|tika)|index-(basic|anchor)|indexer-solr|scoring-opic|urlnormalizer-(pass|regex|basic)</p>\n</blockquote>\n\n<p>I hope it helps.</p>\n", "creation_date": 1466440921, "is_accepted": false, "score": 0, "last_activity_date": 1466440921, "answer_id": 37927693}], "question_id": 37643642, "tags": ["solr", "nutch", "hadoop2"], "answer_count": 1, "link": "http://stackoverflow.com/questions/37643642/no-indexwriters-activated-check-your-configuration", "last_activity_date": 1466440921, "owner": {"user_id": 5860488, "view_count": 3, "answer_count": 1, "creation_date": 1454144733, "reputation": 1}, "body": "<p>Where did i mistaken  ? Iam Using hadoop 2.7.2, Solr 5.4.1 and nutch 1.12 version\nCould any one help me out of this Query. </p>\n\n<p>http.agent.name\n crawl</p>\n\n<p>plugin.includes\nprotocol-httpclient|urlfilter-regex|index-(basic|more)|query-(basic|site|url|lang)|indexer-solr|nutch-extensionpoints|protocol-httpclient|urlfilter-regex|parse-(text|html|msexcel|msword|mspowerpoint|pdf)|summary-basic|scoring-opic|urlnormalizer-(pass|regex|basic)protocol-http|urlfilter-regex|parse-(html|tika|metatags)|index-(basic|anchor|more|metadata)</p>\n\n<p>fetcher.server.delay\n  0.5</p>\n\n<p>http.timeout\n  10000</p>\n\n<p>http.content.limit\n  131027</p>\n\n<p><strong>Running cmd</strong> : bin/nutch index -Dsolr.server.url=<a href=\"http://localhost:8983/solr/#/collections\" rel=\"nofollow\">http://localhost:8983/solr/#/collections</a> crawl/crawldb -linkdb crawl/linkdb crawl/segments/20160604193022</p>\n\n<p>Indexer: starting at 2016-06-05 20:57:34\nIndexer: deleting gone documents: false\nIndexer: URL filtering: false\nIndexer: URL normalizing: false\n<strong>No IndexWriters activated - check your configuration</strong></p>\n\n<p>Indexer: number of documents indexed, deleted, or skipped:\nIndexer: finished at 2016-06-05 20:57:38, elapsed: 00:00:04</p>\n\n<p>Thanks</p>\n\n<p>Narendra k </p>\n", "creation_date": 1465140894, "score": 0},
{"title": "Apache Nutch 1.11 installation on AIX machine giving bin/nutch: not found", "view_count": 32, "is_answered": false, "answers": [{"question_id": 37915329, "owner": {"user_id": 6446301, "accept_rate": 67, "link": "http://stackoverflow.com/users/6446301/s-doe", "user_type": "registered", "reputation": 6}, "body": "<p>Just to be sure, after unzipping the zip file, you did changed the directory to  \"apache-nutch-1.11\" in your command prompt and then entered the bin/nutch command. right? \nif you did please look for a \"bin\" subdirectory in \"apache-nutch-1.11\" directory. If there isn't any, I guess the zip file you downloaded has a problem. If there is, in your command prompt first change the directory to where it is in, and then run the command. I hope it helps.</p>\n", "creation_date": 1466436853, "is_accepted": false, "score": 0, "last_activity_date": 1466436853, "answer_id": 37926410}], "question_id": 37915329, "tags": ["apache", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/37915329/apache-nutch-1-11-installation-on-aix-machine-giving-bin-nutch-not-found", "last_activity_date": 1466436853, "owner": {"user_id": 2234056, "view_count": 8, "answer_count": 0, "creation_date": 1364860729, "reputation": 66}, "body": "<p>I am installing Apache nutch 1.11 binary distribution in an AIX machine. After unzipping the zip file \"apache-nutch-1.11-bin.zip \" under /usr directory. I just tried to run the bin/nutch command as per the instruction at <a href=\"https://wiki.apache.org/nutch/NutchTutorial\" rel=\"nofollow\">https://wiki.apache.org/nutch/NutchTutorial</a></p>\n\n<p>I am getting the following exception : bin/nutch: not found</p>\n\n<p>I am using root user and have also tried running the \"chmod +x bin/nutch\" for permissions.</p>\n\n<p>Can anyone help me in understanding what I am missing here ? Thanks in advance.</p>\n", "creation_date": 1466401633, "score": 0},
{"title": "Error when trying to parse javascript pages with nutch 1.11 &amp; selenium 2.48.2 &amp; firefox 47.0", "view_count": 53, "is_answered": false, "question_id": 37897334, "tags": ["javascript", "selenium", "firefox", "phantomjs", "nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/37897334/error-when-trying-to-parse-javascript-pages-with-nutch-1-11-selenium-2-48-2", "last_activity_date": 1466255450, "owner": {"user_id": 3260152, "view_count": 0, "answer_count": 0, "creation_date": 1391249588, "reputation": 1}, "body": "<p>I'm trying to use selenium (solr 6.0.0 &amp; nutch 1.11 &amp; firefox 47.0) to parse javascript pages. I'm using this configuration for nutch-site:</p>\n\n<pre><code>&lt;value&gt;protocol-(httpclient|interactiveselenium|selenium)|urlfilter-  (automaton|regex)|parse-(metatags|ext|html|js|swf|tika|zip)|index-(metadata|basic|anchor|geoip|dummy|links|more|replace|static)|scoring-opic|indexer-solr|urlnormalizer-pass|regex|basic|ajax)|creativecommons|feed|headings|language-identifier|lib-nekohtml|lib-xml|microformats-reltag|mimetype-filter|nutch-extensionpoints|lib-selenium|subcollection|tld|parserfilter-naivebayes&lt;/value&gt;\n</code></pre>\n\n<p>and this configuration for parse-plugins.xml</p>\n\n<pre><code>&lt;parse-plugins&gt;\n    &lt;mimeType name=\"*\"&gt;\n        &lt;plugin id=\"parse-metatags\"/&gt;\n        &lt;plugin id=\"protocol-interactiveselenium\"/&gt;\n        &lt;plugin id=\"protocol-selenium\"/&gt;\n        &lt;plugin id=\"lib-selenium\"/&gt;\n        &lt;plugin id=\"nutch-extensionpoints\"/&gt;\n        &lt;plugin id=\"parse-js\"/&gt;\n        &lt;plugin id=\"parse-tika\" /&gt;\n              &lt;plugin id=\"feed\"/&gt;\n        &lt;plugin id=\"parse-html\"/&gt;\n        &lt;plugin id=\"parse-js\"/&gt;\n        &lt;plugin id=\"parse-html\" /&gt;\n    &lt;/mimeType&gt;\n\n    &lt;mimeType name=\"application/rss+xml\"&gt;\n        &lt;plugin id=\"parse-tika\" /&gt;\n        &lt;plugin id=\"feed\" /&gt;\n    &lt;/mimeType&gt;\n\n    &lt;mimeType name=\"application/x-bzip2\"&gt;\n        &lt;!--  try and parse it with the zip parser --&gt;\n        &lt;plugin id=\"parse-zip\" /&gt;\n    &lt;/mimeType&gt;\n\n    &lt;mimeType name=\"application/x-gzip\"&gt;\n        &lt;!--  try and parse it with the zip parser --&gt;\n        &lt;plugin id=\"parse-zip\" /&gt;\n    &lt;/mimeType&gt;\n\n    &lt;mimeType name=\"application/x-javascript\"&gt;\n        &lt;plugin id=\"parse-js\" /&gt;\n        &lt;plugin id=\"protocol-interactiveselenium\"/&gt;\n        &lt;plugin id=\"protocol-selenium\"/&gt;\n        &lt;plugin id=\"lib-selenium\"/&gt;\n        &lt;plugin id=\"nutch-extensionpoints\"/&gt;\n        &lt;plugin id=\"parse-metatags\"/&gt;\n        &lt;!--&lt;plugin id=\"parse-ext\"/&gt;--&gt;\n        &lt;plugin id=\"parse-tika\" /&gt;\n    &lt;/mimeType&gt;\n\n    &lt;mimeType name=\"application/x-shockwave-flash\"&gt;\n        &lt;plugin id=\"parse-swf\" /&gt;\n    &lt;/mimeType&gt;\n\n    &lt;mimeType name=\"application/zip\"&gt;\n        &lt;plugin id=\"parse-zip\" /&gt;\n    &lt;/mimeType&gt;\n\n    &lt;!--&lt;mimeType name=\"text/html\"&gt;\n        &lt;plugin id=\"parse-html\" /&gt;\n    &lt;/mimeType&gt;--&gt;\n\n    &lt;mimeType name=\"text/html\"&gt;\n        &lt;plugin id=\"parse-metatags\"/&gt;\n        &lt;plugin id=\"protocol-interactiveselenium\"/&gt;\n        &lt;plugin id=\"protocol-selenium\"/&gt;\n        &lt;plugin id=\"lib-selenium\"/&gt;\n        &lt;plugin id=\"nutch-extensionpoints\"/&gt;\n        &lt;!--&lt;plugin id=\"parse-ext\"/&gt;--&gt;\n        &lt;!--&lt;plugin id=\"parse-js\"/&gt;--&gt;\n        &lt;plugin id=\"parse-html\" /&gt;\n        &lt;plugin id=\"parse-tika\" /&gt;\n    &lt;/mimeType&gt;\n\n        &lt;mimeType name=\"application/xhtml+xml\"&gt;\n        &lt;plugin id=\"parse-metatags\"/&gt;\n        &lt;plugin id=\"protocol-interactiveselenium\"/&gt;\n        &lt;plugin id=\"protocol-selenium\"/&gt;\n        &lt;plugin id=\"lib-selenium\"/&gt;\n        &lt;plugin id=\"nutch-extensionpoints\"/&gt;\n        &lt;plugin id=\"parse-tika\" /&gt;\n              &lt;plugin id=\"feed\" /&gt;\n        &lt;plugin id=\"parse-html\" /&gt;\n    &lt;/mimeType&gt;\n\n    &lt;mimeType name=\"text/xml\"&gt;\n        &lt;plugin id=\"parse-metatags\"/&gt;\n        &lt;plugin id=\"protocol-interactiveselenium\"/&gt;\n        &lt;plugin id=\"protocol-selenium\"/&gt;\n        &lt;plugin id=\"lib-selenium\"/&gt;\n        &lt;plugin id=\"parse-tika\" /&gt;\n        &lt;plugin id=\"feed\" /&gt;\n    &lt;/mimeType&gt;\n</code></pre>\n\n<p><strong>The firefox window popup with a message about private browsing on it.\nHowever, I get the error below and the job crushes into flames:</strong></p>\n\n<pre><code>17 18:44:13,029 INFO  api.HttpRobotRulesParser - Couldn't get robots.txt for http://findjobs.mashable.com/: java.lang.RuntimeException: org.openqa.selenium.WebDriverException: Unable to bind to locking port 7054 within 45000 ms\nBuild info: version: '2.48.2', revision: '41bccdd10cf2c0560f637404c2d96164b67d9d67', time: '2015-10-09 13:08:06'\nSystem info: host: 'solr', ip: '127.0.1.1', os.name: 'Linux', os.arch: 'amd64', os.version: '3.19.0-39-generic', java.version: '1.8.0_91'\nDriver info: driver.version: FirefoxDriver\n2016-06-17 18:44:13,129 ERROR selenium.Http - Failed to get protocol output\njava.lang.RuntimeException: **org.openqa.selenium.WebDriverException: Failed to connect to binary FirefoxBinary(/usr/bin/firefox) on port 7055**; process output follows:\n\u0435\u043d\u0438\u044f Firefox \u0434\u043b\u044f Ubuntu\",\"creator\":\"Canonical Ltd.\",\"homepageURL\":null},{\"locales\":[\"sl\"],\"name\":\"Ubuntu Modifications\",\"description\":\"Ubuntu raz\u0161iritve za Firefox.\",\"creator\":\"Canonical Ltd.\",\"homepageURL\":null},{\"locales\":[\"sv-SE\"],\"name\":\"Ubuntu Modifications\",\"description\":\"Ubuntu-paket f\u00f6r Firefox.\",\"creator\":\"Canonical Ltd.\",\"homepageURL\":null},{\"locales\":[\"uk\"],\"name\":\"Ubuntu Modifications\",\"description\":\"\u0423\u0431\u0443\u043d\u0442\u0456\u0432\u0441\u044c\u043a\u0456 \u0434\u043e\u043f\u043e\u0432\u043d\u0435\u043d\u043d\u044f \u0434\u043e Firefox.\",\"creator\":\"Canonical Ltd.\",\"homepageURL\":null},{\"locales\":[\"zh-CN\"],\"name\":\"Ubuntu Modifications\",\"description\":\"Ubuntu \u706b\u72d0\u6269\u5c55\u5305.\",\"creator\":\"Canonical Ltd.\",\"homepageURL\":null},{\"locales\":[\"zh-TW\"],\"name\":\"Ubuntu Modifications\",\"description\":\"Ubuntu Firefox \u64f4\u5145\u5305\u3002\",\"creator\":\"Canonical Ltd.\",\"homepageURL\":null}],\"targetApplications\":[{\"id\":\"{ec8030f7-c20a-464f-9b0e-13a3a9e97384}\",\"minVersion\":\"9.0\",\"maxVersion\":\"37.0a1\"}],\"targetPlatforms\":[],\"multiprocessCompatible\":false,\"signedState\":2,\"seen\":true}\n1466178208570    DeferredSave.extensions.json    DEBUG    Save changes\n1466178208570    addons.xpi    DEBUG    Updating database with changes to installed add-ons\n1466178208570    addons.xpi-utils    DEBUG    Updating add-on states\n1466178208571    addons.xpi-utils    DEBUG    Writing add-ons list\n1466178208575    addons.xpi    DEBUG    Registering manifest for /usr/lib/firefox/browser/features/firefox@getpocket.com.xpi\n1466178208576    addons.xpi    DEBUG    Calling bootstrap method startup on firefox@getpocket.com version 1.0.2\n1466178208578    addons.xpi    DEBUG    Registering manifest for /usr/lib/firefox/browser/features/e10srollout@mozilla.org.xpi\n1466178208578    addons.xpi    DEBUG    Calling bootstrap method startup on e10srollout@mozilla.org version 1.0\n1466178208578    addons.xpi    DEBUG    Registering manifest for /usr/lib/firefox/browser/features/loop@mozilla.org.xpi\n1466178208579    addons.xpi    DEBUG    Calling bootstrap method startup on loop@mozilla.org version 1.3.2\n1466178208610    addons.manager    DEBUG    Registering shutdown blocker for XPIProvider\n1466178208610    addons.manager    DEBUG    Provider finished startup: XPIProvider\n1466178208610    addons.manager    DEBUG    Starting provider: LightweightThemeManager\n1466178208611    addons.manager    DEBUG    Registering shutdown blocker for LightweightThemeManager\n1466178208612    addons.manager    DEBUG    Provider finished startup: LightweightThemeManager\n1466178208613    addons.manager    DEBUG    Starting provider: GMPProvider\n1466178208621    addons.manager    DEBUG    Registering shutdown blocker for GMPProvider\n1466178208622    addons.manager    DEBUG    Provider finished startup: GMPProvider\n1466178208622    addons.manager    DEBUG    Starting provider: PluginProvider\n1466178208622    addons.manager    DEBUG    Registering shutdown blocker for PluginProvider\n1466178208622    addons.manager    DEBUG    Provider finished startup: PluginProvider\n1466178208623    addons.manager    DEBUG    Completed startup sequence\n1466178209011    addons.manager    DEBUG    Starting provider: &lt;unnamed-provider&gt;\n1466178209011    addons.manager    DEBUG    Registering shutdown blocker for &lt;unnamed-provider&gt;\n1466178209012    addons.manager    DEBUG    Provider finished startup: &lt;unnamed-provider&gt;\n1466178209202    DeferredSave.extensions.json    DEBUG    Write succeeded\n1466178209202    addons.xpi-utils    DEBUG    XPI Database saved, setting schema version preference to 17\n1466178209202    DeferredSave.extensions.json    DEBUG    Starting timer\n1466178209229    DeferredSave.extensions.json    DEBUG    Starting write\n1466178209237    addons.repository    DEBUG    No addons.json found.\n1466178209238    DeferredSave.addons.json    DEBUG    Save changes\n1466178209242    DeferredSave.addons.json    DEBUG    Starting timer\n1466178209309    addons.manager    DEBUG    Starting provider: PreviousExperimentProvider\n1466178209310    addons.manager    DEBUG    Registering shutdown blocker for PreviousExperimentProvider\n1466178209310    addons.manager    DEBUG    Provider finished startup: PreviousExperimentProvider\n1466178209317    DeferredSave.addons.json    DEBUG    Starting write\n1466178209329    DeferredSave.extensions.json    DEBUG    Write succeeded\n1466178209357    DeferredSave.addons.json    DEBUG    Write succeeded\n\n(firefox:3352): Gtk-CRITICAL **: gtk_clipboard_set_with_data: assertion 'targets != NULL' failed\n\nBuild info: version: '2.48.2', revision: '41bccdd10cf2c0560f637404c2d96164b67d9d67', time: '2015-10-09 13:08:06'\nSystem info: host: 'solr', ip: '127.0.1.1', os.name: 'Linux', os.arch: 'amd64', os.version: '3.19.0-39-generic', java.version: '1.8.0_91'\nDriver info: driver.version: FirefoxDriver\n    at org.apache.nutch.protocol.selenium.HttpWebClient.getDriverForPage(HttpWebClient.java:118)\n    at org.apache.nutch.protocol.selenium.HttpWebClient.getHtmlPage(HttpWebClient.java:155)\n    at org.apache.nutch.protocol.selenium.HttpResponse.readPlainContent(HttpResponse.java:244)\n    at org.apache.nutch.protocol.selenium.HttpResponse.&lt;init&gt;(HttpResponse.java:168)\n    at org.apache.nutch.protocol.selenium.Http.getResponse(Http.java:56)\n    at org.apache.nutch.protocol.http.api.HttpBase.getProtocolOutput(HttpBase.java:261)\n    at org.apache.nutch.fetcher.FetcherThread.run(FetcherThread.java:290)\nCaused by: org.openqa.selenium.WebDriverException: Failed to connect to binary FirefoxBinary(/usr/bin/firefox) on port 7055; process output follows:\n\u0435\u043d\u0438\u044f Firefox \u0434\u043b\u044f Ubuntu\",\"creator\":\"Canonical Ltd.\",\"homepageURL\":null},{\"locales\":[\"sl\"],\"name\":\"Ubuntu Modifications\",\"description\":\"Ubuntu raz\u0161iritve za Firefox.\",\"creator\":\"Canonical Ltd.\",\"homepageURL\":null},{\"locales\":[\"sv-SE\"],\"name\":\"Ubuntu Modifications\",\"description\":\"Ubuntu-paket f\u00f6r Firefox.\",\"creator\":\"Canonical Ltd.\",\"homepageURL\":null},{\"locales\":[\"uk\"],\"name\":\"Ubuntu Modifications\",\"description\":\"\u0423\u0431\u0443\u043d\u0442\u0456\u0432\u0441\u044c\u043a\u0456 \u0434\u043e\u043f\u043e\u0432\u043d\u0435\u043d\u043d\u044f \u0434\u043e Firefox.\",\"creator\":\"Canonical Ltd.\",\"homepageURL\":null},{\"locales\":[\"zh-CN\"],\"name\":\"Ubuntu Modifications\",\"description\":\"Ubuntu \u706b\u72d0\u6269\u5c55\u5305.\",\"creator\":\"Canonical Ltd.\",\"homepageURL\":null},{\"locales\":[\"zh-TW\"],\"name\":\"Ubuntu Modifications\",\"description\":\"Ubuntu Firefox \u64f4\u5145\u5305\u3002\",\"creator\":\"Canonical Ltd.\",\"homepageURL\":null}],\"targetApplications\":[{\"id\":\"{ec8030f7-c20a-464f-9b0e-13a3a9e97384}\",\"minVersion\":\"9.0\",\"maxVersion\":\"37.0a1\"}],\"targetPlatforms\":[],\"multiprocessCompatible\":false,\"signedState\":2,\"seen\":true}\n1466178208570    DeferredSave.extensions.json    DEBUG    Save changes\n1466178208570    addons.xpi    DEBUG    Updating database with changes to installed add-ons\n1466178208570    addons.xpi-utils    DEBUG    Updating add-on states\n1466178208571    addons.xpi-utils    DEBUG    Writing add-ons list\n</code></pre>\n\n<p>Please advice,\nMuch obliged,\nChristian </p>\n", "creation_date": 1466255450, "score": 0},
{"title": "How to crawl web application with ajax or javascript WITHOUT using selenium as it make too slow?", "view_count": 78, "is_answered": false, "question_id": 37880129, "tags": ["javascript", "ajax", "web-crawler", "nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/37880129/how-to-crawl-web-application-with-ajax-or-javascript-without-using-selenium-as-i", "last_activity_date": 1466165157, "owner": {"user_id": 5611219, "answer_count": 0, "creation_date": 1448610057, "accept_rate": 17, "view_count": 6, "reputation": 11}, "body": "<p>I want to crawl web application with Ajax or JavaScript. The web spider used are nutch and scrapy. I have used selenium as support plugin to crawl the ajax or javascript content. But it is too slow when using selenium.</p>\n\n<p>Can I do crawling without using selenium and get the ajax or javascript content? </p>\n\n<p>Or any idea to analyze the ajax code page?</p>\n", "creation_date": 1466161912, "score": 0},
{"title": "connection refused error when running Nutch 2", "view_count": 2982, "is_answered": true, "answers": [{"question_id": 12581492, "owner": {"user_id": 745028, "link": "http://stackoverflow.com/users/745028/volodymyrb", "user_type": "registered", "reputation": 2303}, "body": "<p>Had the same error. I forgot 'configuration' tag at conf/nutch-site.xml:</p>\n\n<pre><code>&lt;configuration&gt;\n    &lt;property&gt;\n        &lt;name&gt;storage.data.store.class&lt;/name&gt;\n        &lt;value&gt;org.apache.gora.hbase.store.HBaseStore&lt;/value&gt;\n        &lt;description&gt;Default class for storing data&lt;/description&gt;\n    &lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre>\n", "creation_date": 1356794096, "is_accepted": false, "score": 0, "last_activity_date": 1356794096, "answer_id": 14083005}, {"last_edit_date": 1466161296, "owner": {"user_id": 694545, "link": "http://stackoverflow.com/users/694545/pradeep", "user_type": "registered", "reputation": 56}, "body": "<p>I had the same error. I changed the connection URL from</p>\n\n<pre><code>&lt;property name=\"connection.url\"&gt;jdbc:hsqldb:hsql://localhost&lt;/property&gt;\n</code></pre>\n\n<p>to</p>\n\n<pre><code>&lt;property name=\"connection.url\"&gt;jdbc:hsqldb:mem://localhost&lt;/property&gt;\n</code></pre>\n\n<p>and it did the trick.</p>\n", "question_id": 12581492, "creation_date": 1371845432, "is_accepted": false, "score": 2, "last_activity_date": 1466161296, "answer_id": 17243533}], "question_id": 12581492, "tags": ["java", "web-crawler", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/12581492/connection-refused-error-when-running-nutch-2", "last_activity_date": 1466161296, "owner": {"user_id": 585874, "answer_count": 35, "creation_date": 1295724712, "accept_rate": 62, "view_count": 225, "location": "Iran", "reputation": 990}, "body": "<p>I am trying to run Nutch 2 crawler on my system but I get the following error:</p>\n\n<pre><code>Exception in thread \"main\" org.apache.gora.util.GoraException: java.io.IOException: java.sql.SQLTransientConnectionException: java.net.ConnectException: Connection refused\nat org.apache.gora.store.DataStoreFactory.createDataStore(DataStoreFactory.java:167)\nat org.apache.gora.store.DataStoreFactory.createDataStore(DataStoreFactory.java:135)\nat org.apache.nutch.storage.StorageUtils.createWebStore(StorageUtils.java:69)\nat org.apache.nutch.crawl.InjectorJob.run(InjectorJob.java:243)\nat org.apache.nutch.crawl.Crawler.runTool(Crawler.java:68)\nat org.apache.nutch.crawl.Crawler.run(Crawler.java:136)\nat org.apache.nutch.crawl.Crawler.run(Crawler.java:250)\nat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\nat org.apache.nutch.crawl.Crawler.main(Crawler.java:257)\nCaused by: java.io.IOException: java.sql.SQLTr\nansientConnectionException: java.net.ConnectException: Connection refused\n    at org.apache.gora.sql.store.SqlStore.getConnection(SqlStore.java:747)\n    at org.apache.gora.sql.store.SqlStore.initialize(SqlStore.java:160)\n    at org.apache.gora.store.DataStoreFactory.initializeDataStore(DataStoreFactory.java:102)\n    at org.apache.gora.store.DataStoreFactory.createDataStore(DataStoreFactory.java:161)\n    ... 8 more\nCaused by: java.sql.SQLTransientConnectionException: java.net.ConnectException: Connection refused\n    at org.hsqldb.jdbc.Util.sqlException(Unknown Source)\n    at org.hsqldb.jdbc.Util.sqlException(Unknown Source)\n    at org.hsqldb.jdbc.JDBCConnection.&lt;init&gt;(Unknown Source)\n    at org.hsqldb.jdbc.JDBCDriver.getConnection(Unknown Source)\n    at org.hsqldb.jdbc.JDBCDriver.connect(Unknown Source)\n    at java.sql.DriverManager.getConnection(DriverManager.java:620)\n    at java.sql.DriverManager.getConnection(DriverManager.java:200)\n    at org.apache.gora.sql.store.SqlStore.getConnection(SqlStore.java:739)\n    ... 11 more\nCaused by: org.hsqldb.HsqlException: java.net.ConnectException: Connection refused\n    at org.hsqldb.ClientConnection.openConnection(Unknown Source)\n    at org.hsqldb.ClientConnection.initConnection(Unknown Source)\n    at org.hsqldb.ClientConnection.&lt;init&gt;(Unknown Source)\n    ... 17 more\nCaused by: java.net.ConnectException: Connection refused\n    at java.net.PlainSocketImpl.socketConnect(Native Method)\n    at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:327)\n    at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:193)\n    at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:180)\n    at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:384)\n    at java.net.Socket.connect(Socket.java:546)\n    at java.net.Socket.connect(Socket.java:495)\n    at java.net.Socket.&lt;init&gt;(Socket.java:392)\n    at java.net.Socket.&lt;init&gt;(Socket.java:206)\n    at org.hsqldb.server.HsqlSocketFactory.createSocket(Unknown Source)\n    ... 20 more\n</code></pre>\n\n<p>what is the problem? my internet connection is direct.</p>\n", "creation_date": 1348570435, "score": 5},
{"title": "Apache Nutch - NoSuchMethodError", "view_count": 99, "owner": {"user_id": 3478220, "answer_count": 48, "creation_date": 1396188857, "accept_rate": 100, "view_count": 77, "reputation": 604}, "is_answered": true, "answers": [{"question_id": 37822499, "owner": {"user_id": 3478220, "accept_rate": 100, "link": "http://stackoverflow.com/users/3478220/numx", "user_type": "registered", "reputation": 604}, "body": "<p>Solved the issue by installing an earlier version of Nutch (1.4) . The problem was with some of the newer versions of hadoop not working correctly in windows. </p>\n", "creation_date": 1466116241, "is_accepted": true, "score": 0, "last_activity_date": 1466116241, "answer_id": 37870421}], "question_id": 37822499, "tags": ["java", "apache", "nutch", "nosuchmethoderror"], "answer_count": 1, "link": "http://stackoverflow.com/questions/37822499/apache-nutch-nosuchmethoderror", "last_activity_date": 1466116241, "accepted_answer_id": 37870421, "body": "<p>I have installed Apache Nutch on Windows and I am trying to get it to run a simple crawl but I have the following error:</p>\n\n<pre><code>$ bin/crawl urls crawled 3\nInjecting seed URLs\n/cygdrive/c/Users/Simon/OneDrive/apache-nutch-1.11-bin/apache-nutch-1.11/bin/nutch inject crawled/crawldb urls\nException in thread \"main\" java.lang.NoSuchMethodError: org.apache.commons.cli.OptionBuilder.withArgPattern(Ljava/lang/String;I)Lorg/apache/commons/cli/OptionBuilder;\n        at org.apache.hadoop.util.GenericOptionsParser.buildGeneralOptions(GenericOptionsParser.java:207)\n        at org.apache.hadoop.util.GenericOptionsParser.parseGeneralOptions(GenericOptionsParser.java:370)\n        at org.apache.hadoop.util.GenericOptionsParser.&lt;init&gt;(GenericOptionsParser.java:153)\n        at org.apache.hadoop.util.GenericOptionsParser.&lt;init&gt;(GenericOptionsParser.java:138)\n        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:59)\n        at org.apache.nutch.crawl.Injector.main(Injector.java:369)\nError running:\n  /cygdrive/c/Users/Simon/OneDrive/apache-nutch-1.11-bin/apache-nutch-1.11/bin/nutch inject crawled/crawldb urls\nFailed with exit value 1.\n</code></pre>\n\n<p>Any help would be appreciated. </p>\n\n<p>Thanks</p>\n", "creation_date": 1465939968, "score": 0},
{"title": "Setting cookie header in Apache Nutch", "view_count": 59, "is_answered": true, "answers": [{"question_id": 37630399, "owner": {"user_id": 1977773, "link": "http://stackoverflow.com/users/1977773/jorge-luis", "user_type": "registered", "reputation": 510}, "body": "<p>At the moment there is not way of <em>manually</em> specifying a cookie/header for Nutch to send when fetching the URLs. The plugin <code>protocol-httpclient</code> have some support for form based authentications, take a look at the <code>httpclient-auth.xml</code> file. I don't think this would be too hard to implement, and we always welcome contributions. </p>\n", "creation_date": 1465216382, "is_accepted": false, "score": 1, "last_activity_date": 1465216382, "answer_id": 37657608}], "question_id": 37630399, "tags": ["web-crawler", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/37630399/setting-cookie-header-in-apache-nutch", "last_activity_date": 1466066902, "owner": {"user_id": 1072843, "answer_count": 9, "creation_date": 1322638331, "accept_rate": 47, "view_count": 53, "reputation": 347}, "body": "<p>I want to crawl a specific site which uses cookies for authentication. I want to set cookie and user-agent information in every GET request that Apache Nutch makes for crawling the site.</p>\n\n<p>How do I specify the cookie information in the config or is there the need for writing a custom plugin for this purpose?</p>\n", "creation_date": 1465044377, "score": 1},
{"title": "Apache Nutch 1.11 Running on Hadoop 2.7 cluster, gives worng fs", "view_count": 60, "is_answered": false, "question_id": 37772871, "tags": ["hadoop", "ant", "nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/37772871/apache-nutch-1-11-running-on-hadoop-2-7-cluster-gives-worng-fs", "last_activity_date": 1465726105, "owner": {"user_id": 5012237, "view_count": 7, "answer_count": 0, "creation_date": 1434386822, "reputation": 1}, "body": "<p>Hadoop 2.7 cluster was running well,tested by running WordCount Problem.<br>Now I am trying to run an Apache Nutch crawler on my hadoop cluster. <br> \nStep I completed :- </p>\n\n<blockquote>\n  <p>a) Download Apache Nutch 1.11 <br>\n  b) Change conf file as it requires <br>\n  c) Build nutch by Ant <br>\n  d) Run apache-nutch-1.11.job by Running following command:-<br>\n  hadoop jar apache-nutch-1.11.job org.apache.nutch.crawl.Injector      /user/hduser/nutchData /user/hduser/urls</p>\n</blockquote>\n\n<p>gives me following error:- <br></p>\n\n<pre><code>ERROR crawl.Injector: Injector: java.lang.IllegalArgumentException:   Wrong FS: hdfs://master:9000/user/hduser/nutchData/333860923, expected: file:///\nat org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:646)\nat org.apache.hadoop.fs.RawLocalFileSystem.pathToFile(RawLocalFileSystem.java:82)\nat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:604)\nat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:822)\nat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:599)\nat org.apache.hadoop.fs.FileSystem.isDirectory(FileSystem.java:1437)\nat org.apache.hadoop.fs.ChecksumFileSystem.rename(ChecksumFileSystem.java:506)\nat org.apache.nutch.crawl.CrawlDb.install(CrawlDb.java:169)\nat org.apache.nutch.crawl.Injector.inject(Injector.java:354)\nat org.apache.nutch.crawl.Injector.run(Injector.java:379)\nat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)\nat org.apache.nutch.crawl.Injector.main(Injector.java:369)\nat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\nat java.lang.reflect.Method.invoke(Method.java:498)\nat org.apache.hadoop.util.RunJar.run(RunJar.java:221)\nat org.apache.hadoop.util.RunJar.main(RunJar.java:136)\n</code></pre>\n", "creation_date": 1465725155, "score": 0},
{"title": "Apache Nutch doesn&#39;t index rss feed properly", "view_count": 29, "is_answered": true, "answers": [{"question_id": 37694762, "owner": {"user_id": 432844, "link": "http://stackoverflow.com/users/432844/julien-nioche", "user_type": "registered", "reputation": 939}, "body": "<p>The feed plugin in Nutch generates multiple documents from the feed file without fetching the URLs listed there. My guess is that it assigns the same signature as the feed page to all the subdocuments which as you pointed out results in them being deduplicated. </p>\n\n<p>This is should not happen and is clearly a bug. Could you please open a JIRA issue for it?</p>\n\n<p>You could remove the deduplication step from the crawl script so that your documents are preserved in the index.</p>\n\n<p>Alternatively you could write a modified version of the plugin which simply extracts the outlinks from the feed and lets Nutch fetch the subdocuments as usual. This way each doc will get its own signature and the deduplication will make sense.</p>\n\n<p>Another reason why you would prefer that is that the feed entry might not contain the entire text / metadata of the subdocument.</p>\n\n<p>Funnily enough, I've just added a resource for parsing feeds in <a href=\"https://github.com/DigitalPebble/storm-crawler/pull/291\" rel=\"nofollow\">StormCrawler</a>, unlike the one in Nutch it simply detects outlinks and fetches them later on.</p>\n", "creation_date": 1465397263, "is_accepted": false, "score": 1, "last_activity_date": 1465397263, "answer_id": 37705862}], "question_id": 37694762, "tags": ["solr", "rss", "feed", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/37694762/apache-nutch-doesnt-index-rss-feed-properly", "last_activity_date": 1465397263, "owner": {"user_id": 627307, "answer_count": 6, "creation_date": 1298322820, "accept_rate": 51, "view_count": 156, "reputation": 736}, "body": "<p>I've been trying indexing RSS pages on Solr using \"feed\" as parser (and not tika.\nIn theory for each RSS's item, one document should be created in Solr. And it is created! But only temporarily. \nIn fact once the indexing finished successfully, the cleaning job deletes all the RSS items.</p>\n\n<p>My guess is that doesn't find the url of the RSS's item in the crawlDB, and therefore it deletes them from Solr during the cleanjob. Could it be right? </p>\n\n<p>EDIT:\nI have noticed that all the entries have the same \"signature\" because the fetcher decided so. Therefore the dedup mark them as duplicates and the cleaner clean them.</p>\n\n<p>I'm trying to modify this from happening, but I don't understand why it has been configured in such a way.</p>\n", "creation_date": 1465367150, "score": 0},
{"title": "Skipping Error: JAVA_HOME is not set.:java.net.MalformedURLException: unknown protocol: error", "view_count": 31, "is_answered": false, "question_id": 37519050, "tags": ["nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/37519050/skipping-error-java-home-is-not-set-java-net-malformedurlexception-unknown-pr", "last_activity_date": 1465276002, "owner": {"user_id": 6393288, "view_count": 3, "answer_count": 1, "creation_date": 1464402085, "reputation": 1}, "body": "<p>Getting below error when trying to run <code>nutch</code> command : <code>./nutch inject</code></p>\n\n<p><code>Skipping Error: JAVA_HOME is not set.:java.net.MalformedURLException: unknown protocol: error</code></p>\n\n<p>Where as <code>JAVA_HOME</code> is already set..</p>\n\n<p>Request help.</p>\n", "creation_date": 1464590025, "score": 0},
{"title": "SolrIndexer indexer failed nutch", "view_count": 41, "is_answered": false, "question_id": 37559927, "tags": ["solr", "nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/37559927/solrindexer-indexer-failed-nutch", "last_activity_date": 1464841470, "owner": {"user_id": 5860488, "view_count": 3, "answer_count": 1, "creation_date": 1454144733, "reputation": 1}, "body": "<p>How to solve this error ?                    </p>\n\n<p>Iam using <code>nutch 1.12</code>, <code>hadoop 2.7.2</code> and <code>solr 6.0.0</code> and have copyied nutch <code>schema.xml</code> file into Solr <code>conf</code> file and this my nutch log error</p>\n\n<p>when I am running this command</p>\n\n<pre><code>bin/nutch solrindex http://localhost:8983/solr/#/devel1 crawl_Test1/crawldb -linkdb crawl_Test1/linkdb  crawl_Test1/segments/*\n</code></pre>\n\n<p>In this url you can see the error <a href=\"https://issues.apache.org/jira/browse/NUTCH-2271\" rel=\"nofollow\">link</a></p>\n", "creation_date": 1464758024, "score": 0},
{"title": "Integrating Nutch web-crawling functionality into a Java application", "view_count": 117, "owner": {"user_id": 364056, "answer_count": 17, "creation_date": 1276211074, "accept_rate": 78, "view_count": 620, "location": "Italy", "reputation": 3135}, "is_answered": true, "answers": [{"question_id": 37566181, "owner": {"user_id": 1977773, "link": "http://stackoverflow.com/users/1977773/jorge-luis", "user_type": "registered", "reputation": 510}, "body": "<p>Well, your question appears to be an <a href=\"http://www.perlmonks.org/index.pl?node_id=542341\" rel=\"nofollow\">\"XY Problem\"</a>, Nutch could be used as a library in your custom Java application, the <code>bin/nutch</code> and <code>bin/crawl</code> scripts basically just executes several Java classes with the right parameters, so in your application you could call the right classes with the right parameters, taking a look at the <code>bin/crawl</code> script will provide you with the right sequence of steps (and classes) to call for a full cycle crawl. This should only be used for small crawls.</p>\n\n<p>Now, going back to the XY problem, if all you need is to extract custom text/metadata from the webpages you could just extend Nutch itself without the need to write your custom application. From what you described looks like you are after a custom parser/indexing plugin. If this is the case I recommend taking a look at the headings plugin (<a href=\"https://github.com/apache/nutch/tree/master/src/plugin/headings\" rel=\"nofollow\">https://github.com/apache/nutch/tree/master/src/plugin/headings</a>) which is a very good starting point to write your own <code>HtmlParseFilter</code> plugin. You'll still need to write custom code but it will be contained in a Nutch plugin. </p>\n\n<p>Also you could check out <a href=\"https://issues.apache.org/jira/browse/NUTCH-1870\" rel=\"nofollow\">https://issues.apache.org/jira/browse/NUTCH-1870</a>, this plugin allows to extract custom portions of the HTML using XPath expressions.</p>\n", "creation_date": 1464788001, "is_accepted": true, "score": 4, "last_activity_date": 1464788001, "answer_id": 37570272}], "question_id": 37566181, "tags": ["java", "web-crawler", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/37566181/integrating-nutch-web-crawling-functionality-into-a-java-application", "last_activity_date": 1464788001, "accepted_answer_id": 37570272, "body": "<p>I would use Apache Nutch into my Java application to crawl web pages from one ore more websites. Basically, I need to call a method of my Java application for each web page found by the web-crawler, in order to process page content (text, etc.). How to achieve this?</p>\n", "creation_date": 1464777248, "score": 1},
{"title": "Apache Nutch one document for each item in RSS Feed", "view_count": 104, "owner": {"user_id": 627307, "answer_count": 6, "creation_date": 1298322820, "accept_rate": 51, "view_count": 156, "reputation": 736}, "is_answered": true, "answers": [{"question_id": 37541197, "owner": {"user_id": 1977773, "link": "http://stackoverflow.com/users/1977773/jorge-luis", "user_type": "registered", "reputation": 510}, "body": "<p>By default the RSS feeds are parsed by the <code>parse-tika</code> plugin, see <a href=\"https://github.com/apache/nutch/blob/master/conf/parse-plugins.xml#L31-L34\" rel=\"nofollow\">https://github.com/apache/nutch/blob/master/conf/parse-plugins.xml#L31-L34</a>, which by default identifies the links inside the RSS feed as Outlinks of the original feed URL. This outlinks are then stored for later fetching, parsing, etc. This could be checked if you run the command:</p>\n\n<pre><code>$ bin/nutch parsechecker http://humanos.uci.cu/feed/\n</code></pre>\n\n<p>The output should be something like:</p>\n\n<pre><code>...\n---------\nUrl\n---------------\n\nhttp://humanos.uci.cu/feed/\n---------\nParseData\n---------\n\nVersion: 5\nStatus: success(1,0)\nTitle: humanOS\nOutlinks: 10\n...\n</code></pre>\n\n<p>This basically reports that 1 URL was successfully parsed and 10 outlinks were found. </p>\n\n<p>To get the output that you want, you need to use the <code>feed</code> plugin. So first, activate the <code>feed</code> plugin in the <code>plugin.include</code> attribute of your <code>nutch-site.xml</code> file. </p>\n\n<p>Once this is done you still need to instruct Nutch to use the <code>feed</code> parser first (which uses the ROME library underneath). To accomplish this edit the <code>conf/parse-plugins.xml</code> file, find the entry: <code>&lt;mimeType name=\"application/rss+xml\"&gt;</code> and leave it like:</p>\n\n<pre><code>&lt;mimeType name=\"application/rss+xml\"&gt;\n    &lt;plugin id=\"feed\" /&gt;\n    &lt;plugin id=\"parse-tika\" /&gt;\n&lt;/mimeType&gt;\n</code></pre>\n\n<p>In this case if you try again the <code>parsechecker</code> command the output will be different, and once you index into Solr/ES you should see more documents: 1 for the original feed plus one for each item in your feed. </p>\n\n<p>Keep in mind that this new documents will only have as the <code>content</code> field the <code>description</code> extracted from the feed which could be fairly incomplete.</p>\n\n<p>If you need to write a more customized logic, the <code>ParseResult</code> class allows to have \"subdocuments\" (<a href=\"https://github.com/apache/nutch/blob/master/src/java/org/apache/nutch/parse/ParseResult.java#L30-L41\" rel=\"nofollow\">https://github.com/apache/nutch/blob/master/src/java/org/apache/nutch/parse/ParseResult.java#L30-L41</a>). </p>\n", "creation_date": 1464703951, "is_accepted": true, "score": 1, "last_activity_date": 1464703951, "answer_id": 37548138}], "question_id": 37541197, "tags": ["apache", "solr", "rss", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/37541197/apache-nutch-one-document-for-each-item-in-rss-feed", "last_activity_date": 1464703951, "accepted_answer_id": 37548138, "body": "<p>I'm trying to build an application with Apache Nutch that adds to the DB several documents, one for each item in a RSS Feed.</p>\n\n<p>From my understanding, when now it parses a feed, it creates a unique Solr Document, with all the content concatenated</p>\n\n<pre><code>&lt;item&gt;\n     &lt;title&gt;Comment 1&lt;/title&gt;\n     &lt;link&gt;http://www.link.com/a/#comment-2555842742&lt;/link&gt;\n     &lt;description&gt;document text1&lt;/description&gt;\n     &lt;dc:creator xmlns:dc=\"http://purl.org/dc/elements/1.1/\"&gt;12321 Borland&lt;/dc:creator&gt;\n     &lt;pubDate&gt;Mon, 07 Mar 2016 06:48:35 -0000&lt;/pubDate&gt;\n  &lt;/item&gt;\n  &lt;item&gt;\n     &lt;title&gt;&gt;Comment 2&lt;/title&gt;\n     &lt;link&gt;http://www.link.com/a/#comment-2555590727&lt;/link&gt;\n     &lt;description&gt;document text2&lt;/description&gt;\n     &lt;dc:creator xmlns:dc=\"http://purl.org/dc/elements/1.1/\"&gt;12321&lt;/dc:creator&gt;\n     &lt;pubDate&gt;Mon, 07 Mar 2016 00:48:34 -0000&lt;/pubDate&gt;\n  &lt;/item&gt;\n</code></pre>\n\n<p>Instead I would like to be able to return 2 ParseResult instead of only one: one for each item in the feed</p>\n", "creation_date": 1464685115, "score": 0},
{"title": "Taking too much time in indexing while integrating nutch 2.3, Hbase and Solr", "view_count": 88, "owner": {"user_id": 6109760, "view_count": 8, "answer_count": 3, "creation_date": 1458827665, "reputation": 8}, "is_answered": true, "answers": [{"last_edit_date": 1464262500, "owner": {"user_id": 6109760, "link": "http://stackoverflow.com/users/6109760/aamir", "user_type": "registered", "reputation": 8}, "body": "<p>Finally,I resolved it.\nBasically, <strong>java -jar start.jar</strong> downloads the jar files,so it is not doing indexing here but downloading the Solr 4.8 jars and then configuring it.I replaced Solr 4.8 with Solr 5.2.1 due to performance and now Solr working fine.</p>\n", "question_id": 37297902, "creation_date": 1464259130, "is_accepted": true, "score": 0, "last_activity_date": 1464262500, "answer_id": 37458423}], "question_id": 37297902, "tags": ["java", "hadoop", "solr", "lucene", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/37297902/taking-too-much-time-in-indexing-while-integrating-nutch-2-3-hbase-and-solr", "last_activity_date": 1464682301, "accepted_answer_id": 37458423, "body": "<p><strong>I am integrating Nutch,Hbase and Solr</strong> .</p>\n\n<p><em>I configured Nutch, Hbase and Solr and also did operation for Crawling the Websites</em> but while integrating the Nutch with Solr by following this \n<a href=\"http://www.lostinsoftware.com/2015/11/integrating-nutch-2-3-hbase-and-solr/\" rel=\"nofollow\">Integrating Nutch 2.3, HBase and Solr</a>, I executed the command \n<strong>java jar start.jar</strong>  in  <strong>/opt/solr-4.8.1/examples</strong>. </p>\n\n<p>The process is started but <strong>it is taking so much time for execution about 10 days and</strong> still now it's running. </p>\n\n<p>I am unable to find out what is going wrong with it. \nCan anyone suggest what's the problem and how to solve. </p>\n\n<p>Below are few details of logs file.</p>\n\n<pre><code>INFO  - 2016-05-18 15:58:00.286; org.apache.solr.update.DirectUpdateHandler2; start commit{,optimize=true,openSearcher=true,waitSearcher=true,expungeDeletes=false,softCommit=false,prepareCommit=false}\nINFO  - 2016-05-18 15:58:00.287; org.apache.solr.update.DirectUpdateHandler2; No uncommitted changes. Skipping IW.commit.\nINFO  - 2016-05-18 15:58:00.287; org.apache.solr.core.SolrCore; SolrIndexSearcher has not changed - not re-opening: org.apache.solr.search.SolrIndexSearcher\nINFO  - 2016-05-18 15:58:00.288; org.apache.solr.update.DirectUpdateHandler2; end_commit_flush\nINFO  - 2016-05-18 15:58:00.288; org.apache.solr.update.processor.LogUpdateProcessor; [collection1] webapp=/solr path=/update params={waitFlush=true&amp;optimize=true&amp;wt=json&amp;_=1463567280272} {optimize=} 0 2\nINFO  - 2016-05-18 15:58:01.976; org.apache.solr.update.DirectUpdateHandler2; start commit{,optimize=true,openSearcher=true,waitSearcher=true,expungeDeletes=false,softCommit=false,prepareCommit=false}\nINFO  - 2016-05-18 15:58:01.976; org.apache.solr.update.DirectUpdateHandler2; No uncommitted changes. Skipping IW.commit.\nINFO  - 2016-05-18 15:58:01.977; org.apache.solr.core.SolrCore; SolrIndexSearcher has not changed - not re-opening: org.apache.solr.search.SolrIndexSearcher\nINFO  - 2016-05-18 15:58:01.977; org.apache.solr.update.DirectUpdateHandler2; end_commit_flush\nINFO  - 2016-05-18 15:58:01.978; org.apache.solr.update.processor.LogUpdateProcessor; [collection1] webapp=/solr path=/update params={waitFlush=true&amp;optimize=true&amp;wt=json&amp;_=1463567281965} {optimize=} 0 2\nINFO  - 2016-05-18 15:58:05.799; org.apache.solr.servlet.SolrDispatchFilter; [admin] webapp=null path=/admin/info/threads params={wt=json&amp;_=1463567285780} status=0 QTime=8 \nINFO  - 2016-05-18 15:58:09.267; org.apache.solr.servlet.SolrDispatchFilter; [admin] webapp=null path=/admin/info/properties params={wt=json&amp;_=1463567289183} status=0 QTime=0 \nINFO  - 2016-05-18 15:58:11.225; org.apache.solr.servlet.SolrDispatchFilter; [admin] webapp=null path=/admin/cores params={wt=json&amp;_=1463567291213} status=0 QTime=1 \nINFO  - 2016-05-18 15:58:11.260; org.apache.solr.servlet.SolrDispatchFilter; [admin] webapp=null path=/admin/cores params={wt=json&amp;_=1463567291242} status=0 QTime=1 \nINFO  - 2016-05-18 15:58:13.808; org.apache.solr.core.SolrCore; [collection1] webapp=/solr path=/admin/luke params={show=index&amp;numTerms=0&amp;wt=json&amp;_=1463567293791} status=0 QTime=1 \nINFO  - 2016-05-18 15:58:13.821; org.apache.solr.core.SolrCore; [collection1] webapp=/solr path=/replication params={wt=json&amp;command=details&amp;_=1463567293794} status=0 QTime=1 \nINFO  - 2016-05-18 15:58:13.837; org.apache.solr.core.SolrCore; [collection1] webapp=/solr path=/admin/system params={wt=json&amp;_=1463567293796} status=0 QTime=4 \nINFO  - 2016-05-18 15:58:13.845; org.apache.solr.core.SolrCore; [collection1] webapp=/solr path=/admin/file/ params={file=admin-extra.html&amp;_=1463567293798} status=0 QTime=0 \nINFO  - 2016-05-18 15:58:13.856; org.apache.solr.core.SolrCore; [collection1] webapp=/solr path=/admin/ping params={action=status&amp;wt=json&amp;_=1463567293801} status=503 QTime=1 \nINFO  - 2016-05-18 16:54:35.235; org.apache.solr.servlet.SolrDispatchFilter; [admin] webapp=null path=/admin/info/logging params={wt=json&amp;since=0&amp;_=1463570675193} status=0 QTime=1 \nINFO  - 2016-05-18 16:54:38.820; org.apache.solr.core.SolrCore; [collection1] webapp=/solr path=/replication params={wt=json&amp;command=details&amp;_=1463570678769} status=0 QTime=0 \nINFO  - 2016-05-18 16:54:38.821; org.apache.solr.core.SolrCore; [collection1] webapp=/solr path=/admin/luke params={show=index&amp;numTerms=0&amp;wt=json&amp;_=1463570678764} status=0 QTime=2 \nINFO  - 2016-05-18 16:54:38.823; org.apache.solr.core.SolrCore; [collection1] webapp=/solr path=/admin/ping params={action=status&amp;wt=json&amp;_=1463570678776} status=503 QTime=0 \nINFO  - 2016-05-18 16:54:38.829; org.apache.solr.core.SolrCore; [collection1] webapp=/solr path=/admin/file/ params={file=admin-extra.html&amp;_=1463570678774} status=0 QTime=1 \nINFO  - 2016-05-18 16:54:38.831; org.apache.solr.core.SolrCore; [collection1] webapp=/solr path=/admin/system params={wt=json&amp;_=1463570678772} status=0 QTime=11 \nINFO  - 2016-05-18 16:54:46.728; org.apache.solr.core.SolrCore; [collection1] webapp=/solr path=/admin/mbeans params={stats=true&amp;wt=json&amp;_=1463570686705} status=0 QTime=5 \nINFO  - 2016-05-18 16:54:49.533; org.apache.solr.core.SolrCore; [collection1] webapp=/solr path=/admin/mbeans params={stats=true&amp;wt=json&amp;_=1463570689477} status=0 QTime=3 \nINFO  - 2016-05-18 16:54:52.762; org.apache.solr.core.SolrCore; [collection1] webapp=/solr path=/replication params={wt=json&amp;command=details&amp;_=1463570692692} status=0 QTime=0 \nINFO  - 2016-05-18 16:56:33.180; org.apache.solr.servlet.SolrDispatchFilter; [admin] webapp=null path=/admin/info/logging params={wt=json&amp;since=0&amp;_=1463570793166} status=0 QTime=0 \nINFO  - 2016-05-18 16:56:38.195; org.apache.solr.core.SolrCore; [collection1] webapp=/solr path=/admin/luke params={show=index&amp;numTerms=0&amp;wt=json&amp;_=1463570798128} status=0 QTime=0 \nINFO  - 2016-05-18 16:56:38.198; org.apache.solr.core.SolrCore; [collection1] webapp=/solr path=/replication params={wt=json&amp;command=details&amp;_=1463570798132} status=0 QTime=0 \nINFO  - 2016-05-18 16:56:38.199; org.apache.solr.core.SolrCore; [collection1] webapp=/solr path=/admin/ping params={action=status&amp;wt=json&amp;_=1463570798137} status=503 QTime=0 \nINFO  - 2016-05-18 16:56:38.201; org.apache.solr.core.SolrCore; [collection1] webapp=/solr path=/admin/file/ params={file=admin-extra.html&amp;_=1463570798135} status=0 QTime=0 \nINFO  - 2016-05-18 16:56:38.211; org.apache.solr.core.SolrCore; [collection1] webapp=/solr path=/admin/system params={wt=json&amp;_=1463570798133} status=0 QTime=12 \n</code></pre>\n", "creation_date": 1463569975, "score": 0},
{"title": "Apache Nutch 2.3: throwing Error Failed with exit value 255", "view_count": 406, "is_answered": false, "answers": [{"question_id": 34586164, "owner": {"user_id": 1965370, "link": "http://stackoverflow.com/users/1965370/do-do", "user_type": "registered", "reputation": 389}, "body": "<p>The problem is here: <code>org.apache.hadoop.mapreduce.lib.input.InvalidInputException: Input path does not exist: file:/usr/local/nutch/runtime/local/bin/--index</code></p>\n\n<p>Nutch try to read the seed file, but cannot. Please make sure your command is correct.</p>\n\n<p>Hope this helps,</p>\n\n<p>Le Quoc Do</p>\n", "creation_date": 1457723328, "is_accepted": false, "score": 0, "last_activity_date": 1457723328, "answer_id": 35948051}], "question_id": 34586164, "tags": ["hadoop", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/34586164/apache-nutch-2-3-throwing-error-failed-with-exit-value-255", "last_activity_date": 1464299443, "owner": {"user_id": 4998375, "view_count": 4, "answer_count": 1, "creation_date": 1434012741, "reputation": 6}, "body": "<p>I'm using apache nutch 2.3 version.\nMy hadoop version is 2.6.0.Hadoop is running on single node.</p>\n\n<p>When I run following command of nutch</p>\n\n<pre><code>./crawl --index ~/test/seed ~/test -1\n</code></pre>\n\n<p>The output of the above command is following.</p>\n\n<pre><code>InjectorJob: starting at 2016-01-04 12:03:26\nInjectorJob: Injecting urlDir: --index\nInjectorJob: Using class org.apache.gora.memory.store.MemStore as the    \nGora storage class.\nInjectorJob:    \norg.apache.hadoop.mapreduce.lib.input.InvalidInputException: Input\npath does not exist: file:/usr/local/nutch/runtime/local/bin/--index\nat org.apache.hadoop.mapreduce.lib.input.FileInputFormat.listStatus\n(FileInputFormat.java:235)\nat org.apache.hadoop.mapreduce.lib.input.FileInputFormat.getSplits\n(FileInputFormat.java:252)\nat org.apache.hadoop.mapred.JobClient.writeNewSplits\n(JobClient.java:1054)\nat org.apache.hadoop.mapred.JobClient.writeSplits(JobClient.java:1071)\nat org.apache.hadoop.mapred.JobClient.access$700(JobClient.java:179)\nat org.apache.hadoop.mapred.JobClient$2.run(JobClient.java:983)\nat org.apache.hadoop.mapred.JobClient$2.run(JobClient.java:936)\nat java.security.AccessController.doPrivileged(Native Method)\nat javax.security.auth.Subject.doAs(Subject.java:422)\nat org.apache.hadoop.security.UserGroupInformation.doAs\n(UserGroupInformation.java:1190\nat org.apache.hadoop.mapred.JobClient.submitJobInternal\n(JobClient.java:936)\nat org.apache.hadoop.mapreduce.Job.submit(Job.java:550)\nat org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:580)\nat org.apache.nutch.util.NutchJob.waitForCompletion(NutchJob.java:50)\nat org.apache.nutch.crawl.InjectorJob.run(InjectorJob.java:231)\nat org.apache.nutch.crawl.InjectorJob.inject(InjectorJob.java:252)\nat org.apache.nutch.crawl.InjectorJob.run(InjectorJob.java:275)\nat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\nat org.apache.nutch.crawl.InjectorJob.main(InjectorJob.java:284)\n\nError running:\n/usr/local/nutch/runtime/local/bin/nutch inject --index -crawlId    \n/home/jalaj/test/seed\nFailed with exit value 255.\n</code></pre>\n\n<p>What is the problem with nutch? Should I need to install Apache Gora? </p>\n", "creation_date": 1451890971, "score": 0},
{"title": "runtime exception during nutch generate", "view_count": 561, "owner": {"user_id": 1822643, "answer_count": 24, "creation_date": 1352864251, "accept_rate": 76, "view_count": 74, "reputation": 318}, "is_answered": true, "answers": [{"question_id": 35437372, "owner": {"user_id": 582789, "link": "http://stackoverflow.com/users/582789/alfonso-nishikawa", "user_type": "registered", "reputation": 1263}, "body": "<p>Looking the hadoop's log, I think you are using MemStore, and not HBaseStore. Did you configure gora.properties?</p>\n\n<p>Copied from my comment :)</p>\n", "creation_date": 1455798118, "is_accepted": true, "score": 1, "last_activity_date": 1455798118, "answer_id": 35481371}], "question_id": 35437372, "tags": ["nutch", "runtimeexception", "gora"], "answer_count": 1, "link": "http://stackoverflow.com/questions/35437372/runtime-exception-during-nutch-generate", "last_activity_date": 1464274984, "accepted_answer_id": 35481371, "body": "<p>I'm trying to run nutch for the first time and while executing</p>\n\n<p>/bin/nutch generate -topN 5</p>\n\n<p>I get the following exception:</p>\n\n<pre><code>GeneratorJob: starting at 2016-02-13 21:01:42\nGeneratorJob: Selecting best-scoring urls due for fetch.\nGeneratorJob: starting\nGeneratorJob: filtering: true\nGeneratorJob: normalizing: true\nGeneratorJob: topN: 5\nGeneratorJob: java.lang.RuntimeException: job failed: name=apache-nutch-    2.3.1.jar, jobid=job_local1061440919_0001\nat org.apache.nutch.util.NutchJob.waitForCompletion(NutchJob.java:120)\nat org.apache.nutch.crawl.GeneratorJob.run(GeneratorJob.java:227)\nat org.apache.nutch.crawl.GeneratorJob.generate(GeneratorJob.java:256)\nat org.apache.nutch.crawl.GeneratorJob.run(GeneratorJob.java:322)\nat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)\nat org.apache.nutch.crawl.GeneratorJob.main(GeneratorJob.java:330)\n</code></pre>\n\n<p>Here is the stacktrace from hadoop.log:</p>\n\n<pre><code>2016-02-13 21:01:44,541 ERROR mapreduce.GoraRecordReader - Error reading Gora records: null\n2016-02-13 21:01:44,557 WARN  mapred.LocalJobRunner - job_local1061440919_0001\njava.lang.Exception: java.lang.RuntimeException:   java.util.NoSuchElementException\n    at org.apache.hadoop.mapred.LocalJobRunner$Job.runTasks(LocalJobRunner.java:462)\n    at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:522)\nCaused by: java.lang.RuntimeException: java.util.NoSuchElementException\n    at org.apache.gora.mapreduce.GoraRecordReader.nextKeyValue(GoraRecordReader.java:122)\n    at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:533)\n    at org.apache.hadoop.mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80)\n    at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91)\n    at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)\n    at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:764)\n    at org.apache.hadoop.mapred.MapTask.run(MapTask.java:340)\n    at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:243)\n    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n    at java.util.concurrent.FutureTask.run(FutureTask.java:266)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n    at java.lang.Thread.run(Thread.java:745)\nCaused by: java.util.NoSuchElementException\n    at java.util.concurrent.ConcurrentSkipListMap.firstKey(ConcurrentSkipListMap.java:2036)\n    at org.apache.gora.memory.store.MemStore.execute(MemStore.java:128)\n    at org.apache.gora.query.impl.QueryBase.execute(QueryBase.java:73)\n    at org.apache.gora.mapreduce.GoraRecordReader.executeQuery(GoraRecordReader.java:67)\n    at org.apache.gora.mapreduce.GoraRecordReader.nextKeyValue(GoraRecordReader.java:109)\n    ... 12 more\n</code></pre>\n\n<p>I've been following the tutorial here: <a href=\"https://github.com/renepickhardt/metalcon/wiki/simpleNutchSolrSetup\" rel=\"nofollow\">https://github.com/renepickhardt/metalcon/wiki/simpleNutchSolrSetup</a> for setting up nutch.</p>\n\n<p>I've seen a few posts on stackoverflow and the nutch archives with similar exceptions, and they've suggested that I might be running out of disk space in my /tmp directory but the /tmp directory only has about 8MB worth of data on it.\nOther than this, I'm clueless about what is causing this exception</p>\n\n<p>What could be the cause of this exception?</p>\n\n<p>I'm using Nutch 2.3.1 along with HBase 1.1.3 as the datastore and I'm running it on Ubuntu 15.10</p>\n\n<p>Thanks</p>\n", "creation_date": 1455638835, "score": 1},
{"title": "apache nutch \\ &quot;crawl&quot; scripts fails on indexing to solr", "view_count": 77, "owner": {"user_id": 1025852, "answer_count": 2, "creation_date": 1320244878, "accept_rate": 63, "view_count": 201, "reputation": 903}, "is_answered": true, "answers": [{"question_id": 37398911, "owner": {"user_id": 1977773, "link": "http://stackoverflow.com/users/1977773/jorge-luis", "user_type": "registered", "reputation": 510}, "body": "<p>This message is not very informative, check/post the Solr log for any relevant error. One of the usual causes for this is a mismatch between the Nutch and Solr schemas.</p>\n", "creation_date": 1464033418, "is_accepted": true, "score": 1, "last_activity_date": 1464033418, "answer_id": 37399467}], "question_id": 37398911, "tags": ["apache", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/37398911/apache-nutch-crawl-scripts-fails-on-indexing-to-solr", "last_activity_date": 1464273921, "accepted_answer_id": 37399467, "body": "<p>any thought on this?</p>\n\n<pre><code>IndexingJob: starting\nSolrIndexerJob: java.lang.RuntimeException: job failed: name=[myId_1]Indexer, jobid=job_local483340309_0001\n        at org.apache.nutch.util.NutchJob.waitForCompletion(NutchJob.java:120)\n        at org.apache.nutch.indexer.IndexingJob.run(IndexingJob.java:154)\n        at org.apache.nutch.indexer.IndexingJob.index(IndexingJob.java:176)\n        at org.apache.nutch.indexer.IndexingJob.run(IndexingJob.java:202)\n        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)\n        at org.apache.nutch.indexer.IndexingJob.main(IndexingJob.java:211)\n</code></pre>\n", "creation_date": 1464031331, "score": 0},
{"title": "How to increase number of documents fetched by Apache Nutch crawler", "view_count": 88, "is_answered": true, "answers": [{"question_id": 30364242, "owner": {"user_id": 2420102, "accept_rate": 80, "link": "http://stackoverflow.com/users/2420102/mshoaib91", "user_type": "registered", "reputation": 950}, "body": "<p>One crawl cycle consists of four steps: <em>Generate, Fetch, Parse and Update DB.</em> for detailed information, read my answer <a href=\"http://stackoverflow.com/a/37360334/2420102\">here</a>.</p>\n\n<p>Whats causing limited URL fetch can be caused by the following factors:</p>\n\n<p><strong>Number of Crawl cycles:</strong></p>\n\n<p>If you are only executing one crawl cycle then you will get few results as the URLs injected or seeded into crawldb will be fetched initially. On progressive crawl cycles your crawldb will updated with new URLs extracted from previously fetched pages.</p>\n\n<p><strong>topN value:</strong></p>\n\n<p>As mentioned <a href=\"https://wiki.apache.org/nutch/bin/nutch_crawl\" rel=\"nofollow\">here</a> and <a href=\"https://wiki.apache.org/nutch/Nutch2Crawling#Introduction\" rel=\"nofollow\">here</a>, topN value cause nutch to fetch the limited number of URLs on each cycle. If you have small topN value, you will get less number of pages.</p>\n\n<p><strong>generate.max.count</strong></p>\n\n<p><code>generate.max.count</code> in your nutch configuration file i.e <code>nutch-default.xml</code> or <code>nutch-site.xml</code> limits the number of URLs to be fetched form the single domain as stated <a href=\"https://wiki.apache.org/nutch/Nutch2Crawling#Result\" rel=\"nofollow\">here</a>.</p>\n\n<hr>\n\n<p>Answer to your second question on <strong>how to count number of pages crawled per day</strong>.  What you can do is to read the log files. From there you can accumulate the information on the number of pages crawled per day.</p>\n\n<p>In nutch 1.x log file is generated in log folder <code>NUTCH_HOME/logs/hadoop.log</code></p>\n\n<p>You can count the lines with respect to date and status \"fetching\" from the logs like this:</p>\n\n<p><code>cat logs/hadoop.log | grep -i 2016-05-26.*fetching | wc -l</code></p>\n", "creation_date": 1464267079, "is_accepted": false, "score": 2, "last_activity_date": 1464267079, "answer_id": 37461417}], "question_id": 30364242, "tags": ["web-crawler", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/30364242/how-to-increase-number-of-documents-fetched-by-apache-nutch-crawler", "last_activity_date": 1464267599, "owner": {"user_id": 3454410, "answer_count": 20, "creation_date": 1395646154, "accept_rate": 48, "view_count": 174, "reputation": 832}, "body": "<p>I am using Apache Nutch 2.3 for crawling. There were about 200 urls in seed at start. Now as the time elasped, number of documents crawler are going to decrease or atmost same as at start.</p>\n\n<p>How I can configure Nutch so that my documents crawled should be increased? Is there any parameter that can be used to control number of documents? \nSecond, how I can count number of documents crawled per day by nutch?</p>\n", "creation_date": 1432180181, "score": 0},
{"title": "apache nutch to index to solr via REST", "view_count": 70, "owner": {"user_id": 1025852, "answer_count": 2, "creation_date": 1320244878, "accept_rate": 63, "view_count": 201, "reputation": 903}, "is_answered": true, "answers": [{"last_edit_date": 1464232554, "owner": {"user_id": 1977773, "link": "http://stackoverflow.com/users/1977773/jorge-luis", "user_type": "registered", "reputation": 510}, "body": "<p>You need to create/update a configuration using the <code>/config/create/</code> endpoint, with a POST request and a payload similar to:</p>\n\n<pre><code>{\n    \"configId\":\"solr-config\",\n    \"force\":\"true\",\n    \"params\":{\"solr.server.url\":\"http://127.0.0.1:8983/solr/\"}\n}\n</code></pre>\n\n<p>In this case I'm creating a new configuration and specifying the <code>solr.server.url</code> parameter. You can verify this is working with a GET request to <code>/config/solr-config</code> (<code>solr-config</code> is the previously specified <code>configId</code>), the output should contain all the default parameters see <a href=\"https://gist.github.com/jorgelbg/689b1d66d116fa55a1ee14d7193d71b4\" rel=\"nofollow\">https://gist.github.com/jorgelbg/689b1d66d116fa55a1ee14d7193d71b4</a> for an example/default output. If everything worked fine in the returned JSON you should see the <code>solr.server.url</code> option with the desired value <a href=\"https://gist.github.com/jorgelbg/689b1d66d116fa55a1ee14d7193d71b4#file-nutch-solr-config-json-L464\" rel=\"nofollow\">https://gist.github.com/jorgelbg/689b1d66d116fa55a1ee14d7193d71b4#file-nutch-solr-config-json-L464</a>.</p>\n\n<p>After this just hit the <code>/job/create</code> endpoint to create a new <code>INDEX</code> Job, the payload should be something like:</p>\n\n<pre><code>{\n    \"type\":\"INDEX\",\n    \"confId\":\"solr-config\",\n    \"crawlId\":\"crawl01\",\n    \"args\": {}\n}\n</code></pre>\n\n<p>The idea is that need to you pass the <code>configId</code> that you created with the <code>solr.server.url</code> specified along with the <code>crawlId</code> and other args. This should return something similar to:</p>\n\n<pre><code>{\n  \"id\": \"crawl01-solr-config-INDEX-1252914231\",\n  \"type\": \"INDEX\",\n  \"confId\": \"solr-config\",\n  \"args\": {},\n  \"result\": null,\n  \"state\": \"RUNNING\",\n  \"msg\": \"OK\",\n  \"crawlId\": \"crawl01\"\n}\n</code></pre>\n\n<p>Bottom line you need to create a new configuration with the <code>solr.server.url</code> setted instead of specifying it through the <code>args</code> key in the JSON payload.</p>\n", "question_id": 37345524, "creation_date": 1464226537, "is_accepted": true, "score": 1, "last_activity_date": 1464232554, "answer_id": 37450184}], "question_id": 37345524, "tags": ["apache", "solr", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/37345524/apache-nutch-to-index-to-solr-via-rest", "last_activity_date": 1464232554, "accepted_answer_id": 37450184, "body": "<p>newbie in apache nutch - writing a client to use it via REST.\nsucceed in all the steps (INJECT, FETCH...) - in the last step - when trying to index to solr - it fails to pass the parameter. \nThe Request (I formatted it in some website)</p>\n\n<pre><code>{\n  \"args\": {\n    \"batch\": \"1463743197862\",\n    \"crawlId\": \"sample-crawl-01\",\n    \"solr.server.url\": \"http:\\/\\/x.x.x.x:8081\\/solr\\/\"\n  },\n  \"confId\": \"default\",\n  \"type\": \"INDEX\",\n  \"crawlId\": \"sample-crawl-01\"\n}\n</code></pre>\n\n<p>The Nutch logs:</p>\n\n<pre><code>java.lang.Exception: java.lang.RuntimeException: Missing SOLR URL. Should be set via -D solr.server.url\nSOLRIndexWriter\n        solr.server.url : URL of the SOLR instance (mandatory)\n        solr.commit.size : buffer size when sending to SOLR (default 1000)\n        solr.mapping.file : name of the mapping file for fields (default solrindex-mapping.xml)\n        solr.auth : use authentication (default false)\n        solr.auth.username : username for authentication\n        solr.auth.password : password for authentication\n        at org.apache.hadoop.mapred.LocalJobRunner$Job.runTasks(LocalJobRunner.java:462)\n        at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:522)\n</code></pre>\n\n<p>Was that implemented? the param passing to solr plugin?</p>\n", "creation_date": 1463743684, "score": 0},
{"title": "I need to add java codes that generated by protobuf in proto3 version to nutch 1.7", "view_count": 26, "is_answered": false, "answers": [{"question_id": 37411620, "owner": {"user_id": 1977773, "link": "http://stackoverflow.com/users/1977773/jorge-luis", "user_type": "registered", "reputation": 510}, "body": "<p>Although is true that Nutch has some dependencies to protobuf-2.4.1 and protobuf-2.5.0 (that I could find in the github repository), this dependencies are from libraries used by Nutch and not used in Nutch directly: A quick search on the 2.x branch of apache/nutch: reveals that this libraries are used in the scope of the <code>parse-tika</code>, <code>lib-selenium</code> and <code>lib-htmlunit</code> plugins. What are you needing the protobuf v3 for? If you're trying to use it for one particular plugin, perhaps try to scope it to that plugin instead of a global dependency? </p>\n", "creation_date": 1464232501, "is_accepted": false, "score": 0, "last_activity_date": 1464232501, "answer_id": 37450955}], "question_id": 37411620, "tags": ["java", "web-crawler", "protocol-buffers", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/37411620/i-need-to-add-java-codes-that-generated-by-protobuf-in-proto3-version-to-nutch-1", "last_activity_date": 1464232501, "owner": {"user_id": 6374822, "view_count": 0, "answer_count": 0, "creation_date": 1464075909, "reputation": 9}, "body": "<p>considering apache nutch crawler use protobuf in version 2 for distributing mode and in tika-parser for parse step, I need to add some codes to it that generated with proto3 syntax.\ni meet some conflict-error when added protobuf 3.0.0 in build-path, these errors point to some of functions in protobuf's library classes.\nprogramming is in eclipse, with java language, .proto files execute with protobuf 3.0.0-beta2 that install from <a href=\"https://github.com/google/protobuf/releases/download/v3.0.0-beta-2/protobuf-java-3.0.0-beta-2.tar.gz\" rel=\"nofollow\">here</a>. can every one help me?</p>\n", "creation_date": 1464087571, "score": 1},
{"title": "How to search two different things in two different fields?", "view_count": 373, "owner": {"user_id": 1259550, "answer_count": 2, "creation_date": 1331304557, "accept_rate": 71, "view_count": 21, "reputation": 69}, "is_answered": true, "answers": [{"last_edit_date": 1464107215, "owner": {"user_id": 1236090, "link": "http://stackoverflow.com/users/1236090/david-faber", "user_type": "registered", "reputation": 7844}, "body": "<p>I can't help you with Solarium, but your Solr query should be relatively straightforward:</p>\n\n<pre><code>q=+keyword -ur:exclude&amp;rows=20\n</code></pre>\n", "question_id": 9636471, "creation_date": 1331312214, "is_accepted": true, "score": 1, "last_activity_date": 1464107215, "answer_id": 9637968}, {"last_edit_date": 1438111076, "owner": {"user_id": 3112406, "link": "http://stackoverflow.com/users/3112406/fcha", "user_type": "registered", "reputation": 26}, "body": "<p>http://{url_endpoint}/?wt=json&amp;rows=20&amp;start=0&amp;q=content:<em>contentText</em> OR title:<em>titleText</em> OR ur:<em>url</em></p>\n\n<ul>\n<li>wt=json      result will be in json format</li>\n<li>rows=20      result will be paginated by 20 records per page</li>\n<li>start=0      page to start displaying results</li>\n<li>q=           query to run search (make sure to properly escape inputs also * wildcard to look for anything before and after)</li>\n</ul>\n\n<p>In php using curl.</p>\n\n<pre><code>$solr_end_point = '';   //enter endpoint\n$search_term = '';\n$url_type = '';\n$start = 0;\n$ch = curl_init();\n$query = urlencode(\"content:*{$search_term}* OR title:*{$search_term}* OR ur:*{$url_type}*\");\ncurl_setopt($ch, CURLOPT_URL, \"http://{$solr_end_point}/?wt=json&amp;rows=30&amp;start={$start}&amp;q={$query}\");\ncurl_setopt($ch, CURLOPT_HEADER, false);\ncurl_setopt($ch, CURLOPT_RETURNTRANSFER, true);\ncurl_setopt($ch, CURLOPT_TIMEOUT, 2);\n$result = curl_exec($ch);\ncurl_close($ch);\nprint_r($result);   //output result (json)\n$json_result = json_decode($result,true);\nprint_r($json_result);  //output result as an array\nexit();\n</code></pre>\n", "question_id": 9636471, "creation_date": 1387304604, "is_accepted": false, "score": 0, "last_activity_date": 1438111076, "answer_id": 20641837}], "question_id": 9636471, "tags": ["solr", "lucene", "nutch", "solarium"], "answer_count": 2, "link": "http://stackoverflow.com/questions/9636471/how-to-search-two-different-things-in-two-different-fields", "last_activity_date": 1464107215, "accepted_answer_id": 9637968, "body": "<p>I am using NUTCH 1.4 and SOLR 3.3.0 to crawl and index my website. On the front-end I use the php API SOLARIUM to query to SOLR. I have the following fields that I search in by default:</p>\n\n<pre><code>content -&gt; of type Text\n\ntitle -&gt; of type Text\n\nur-&gt; of type url\n</code></pre>\n\n<p>I want to search for a keyword but at the same time I want to exclude some of the results based on some URL pattern without affecting the total number of results return. (For example I always want to show 20 results.)</p>\n\n<p>If anyone knows a way of doing this with SOLARIUM it would be really nice. But if not I am curious how this can be done in SOLR.</p>\n\n<p>I have already looked at faceted search but I couldn't wrap my head around it. If someone can explain in details I would really appreciate it.</p>\n", "creation_date": 1331306409, "score": 0},
{"title": "apache nutch crawler - keeps retrieve only single url", "view_count": 132, "owner": {"user_id": 1025852, "answer_count": 2, "creation_date": 1320244878, "accept_rate": 63, "view_count": 201, "reputation": 903}, "is_answered": true, "answers": [{"question_id": 37353277, "owner": {"user_id": 2420102, "accept_rate": 80, "link": "http://stackoverflow.com/users/2420102/mshoaib91", "user_type": "registered", "reputation": 950}, "body": "<p>Nutch crawl consists of 4 basic steps: <em>Generate, Fetch, Parse and Update DB</em>. These steps are the same for both <a href=\"https://wiki.apache.org/nutch/NutchTutorial#Step-by-Step:_Fetching\" rel=\"nofollow\">nutch 1.x</a> and <a href=\"https://wiki.apache.org/nutch/Nutch2Crawling#Introduction\" rel=\"nofollow\">nutch 2.x</a>. Execution and completion of all four steps make one <em>crawl cycle</em>. </p>\n\n<p>Injector can be the very first step that adds the URL to the crawldb; as stated <a href=\"https://wiki.apache.org/nutch/NutchTutorial#Step-by-Step:_Seeding_the_crawldb_with_a_list_of_URLs\" rel=\"nofollow\">here</a> and <a href=\"https://wiki.apache.org/nutch/Nutch2Crawling#Introduction\" rel=\"nofollow\">here</a>.</p>\n\n<blockquote>\n  <p>To populate initial rows for the webtable you can use the InjectorJob.</p>\n</blockquote>\n\n<p>Which I reckon you have already provided i.e cnn.com</p>\n\n<p><code>generate.max.count</code> limits the number of URLs to be fetched form the single domain as stated <a href=\"https://wiki.apache.org/nutch/Nutch2Crawling#Result\" rel=\"nofollow\">here</a>.</p>\n\n<p>Now what matters is how many URLs from cnn.com your crawldb has.</p>\n\n<p><strong>Option 1</strong></p>\n\n<p>You have generate.max.count = 10 and you have <a href=\"https://wiki.apache.org/nutch/NutchTutorial#Step-by-Step:_Seeding_the_crawldb_with_a_list_of_URLs\" rel=\"nofollow\">seeded</a> or injected more than 10 URLs to the crawldb then on executing crawl cycle, nutch should fetch no more than 10 URLs</p>\n\n<p><strong>Option 2</strong></p>\n\n<p>If you have injected only one URL and you have performed only one crawl cycle then on first cycle you will get only one document processed because only one URL was in your crawldb. Your crawldb will be update at the end of each crawl cycle. So on execution of your second crawl cycle and third crawl cycle and so on, nutch should resolve only up to 10 URLs from a specific domain.</p>\n", "creation_date": 1463817204, "is_accepted": true, "score": 0, "last_activity_date": 1463817204, "answer_id": 37360334}], "question_id": 37353277, "tags": ["apache", "web-crawler", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/37353277/apache-nutch-crawler-keeps-retrieve-only-single-url", "last_activity_date": 1463817204, "accepted_answer_id": 37360334, "body": "<p>INJECT step keeps retrieving only single URL - trying to crawl CNN.\nI'm with default config (below is the nutch-site) - what could that be - shouldn't it be 10 docs according to my value?</p>\n\n<pre><code>&lt;configuration&gt;\n  &lt;property&gt;\n    &lt;name&gt;http.agent.name&lt;/name&gt;\n    &lt;value&gt;crawler1&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;storage.data.store.class&lt;/name&gt;\n    &lt;value&gt;org.apache.gora.hbase.store.HBaseStore&lt;/value&gt;\n    &lt;description&gt;Default class for storing data&lt;/description&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n        &lt;name&gt;solr.server.url&lt;/name&gt;\n        &lt;value&gt;http://x.x.x.x:8983/solr/collection1&lt;/value&gt;\n  &lt;/property&gt;\n&lt;property&gt;\n  &lt;name&gt;plugin.includes&lt;/name&gt;\n  &lt;value&gt;protocol-httpclient|urlfilter-regex|index-(basic|more)|query-(basic|site|url|lang)|indexer-solr|nutch-extensionpoints|protocol-httpclient|urlfilter-reg\nex|parse-(text|html|msexcel|msword|mspowerpoint|pdf)|summary-basic|scoring-opic|urlnormalizer-(pass|regex|basic)protocol-http|urlfilter-regex|parse-(html|tika|m\netatags)|index-(basic|anchor|more|metadata)&lt;/value&gt;\n&lt;/property&gt;\n&lt;property&gt;\n  &lt;name&gt;db.ignore.external.links&lt;/name&gt;\n  &lt;value&gt;true&lt;/value&gt;\n&lt;/property&gt;\n&lt;property&gt;\n  &lt;name&gt;generate.max.count&lt;/name&gt;\n  &lt;value&gt;10&lt;/value&gt;\n&lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre>\n", "creation_date": 1463768005, "score": 0},
{"title": "Apache Nutch: Amount of seeds changes the crawling behaviour", "view_count": 28, "owner": {"age": 27, "answer_count": 1, "creation_date": 1373145571, "user_id": 2557061, "accept_rate": 57, "view_count": 11, "reputation": 56}, "is_answered": true, "answers": [{"question_id": 37322377, "owner": {"user_id": 5953351, "link": "http://stackoverflow.com/users/5953351/sebastian-nagel", "user_type": "registered", "reputation": 121}, "body": "<p>There are a couple of configuration properties and parameters which influence the way how Nutch follows links. Your observation that adding more seeds (form different sites or hosts) causes a decrease in the amount of crawled documents/pages per host, could be easily explained by a limit on the number of pages fetched per round set via parameter -topN of the \"generate\" step. If the fetch list is limited to, e.g., 100 pages per round,</p>\n\n<ul>\n<li>(with one single site/host) 100 pages can be fetched for this site</li>\n<li>(with 10 sites) only approx. 10 pages are fetched per site</li>\n</ul>\n\n<p>After the same number of rounds in the second scenario there are less pages fetched for one site.</p>\n\n<p>As solution you could either increase -topN or the number of rounds (-depth).</p>\n", "creation_date": 1463732562, "is_accepted": true, "score": 2, "last_activity_date": 1463732562, "answer_id": 37341591}], "question_id": 37322377, "tags": ["nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/37322377/apache-nutch-amount-of-seeds-changes-the-crawling-behaviour", "last_activity_date": 1463732562, "accepted_answer_id": 37341591, "body": "<p>I've worked a while with Apache Nutch and Solr to crawl and index some sites. Now there is a behaviour in Nutch I can't explain. There are two scenarios:</p>\n\n<ol>\n<li>I start Nutch with a seedlist with one site.</li>\n<li>I start Nutch with a seedlist with several sites and the site from scenario 1 is included as well.</li>\n</ol>\n\n<p>For the single seed I 've included in both scenarios I expect that the same URLs were crawled. In my opinion there is no difference.</p>\n\n<p>Anyways I wouldn't write here if my opinion would be right. The reality is that there are two different amount of crawled URLs. There are more crawled URLs in the first scenario. So, to conclude if I crawl a single seed, the crawl is more breadth than a seedlist with a bundle of sites.</p>\n\n<p>Is this behaviour standard or is it unusual? Is it possibile that links from other seedpoints interrupt the process in a way that my analysed seed can't search all links? Is it a setting problem or just a Nutch thing.</p>\n", "creation_date": 1463658660, "score": 0},
{"title": "Scraper: distinguishing meaningful text from meaningless items, hadoop", "view_count": 24, "owner": {"user_id": 627307, "answer_count": 6, "creation_date": 1298322820, "accept_rate": 51, "view_count": 156, "reputation": 736}, "is_answered": true, "answers": [{"question_id": 37332498, "owner": {"user_id": 1977773, "link": "http://stackoverflow.com/users/1977773/jorge-luis", "user_type": "registered", "reputation": 510}, "body": "<p>You didn't mention which branch of Nutch (1.x/2.x) are you using, but at the moment I can think of a couple of approaches:</p>\n\n<p>Take a look at <a href=\"https://issues.apache.org/jira/browse/NUTCH-585\" rel=\"nofollow\">NUTCH-585</a> which will be helpful if you are not crawling many different sites and if you can specify which nodes of your HTML content you want to exclude from the indexed content.</p>\n\n<p>If you're working with different sites and the previous approach is not feasible take a look at <a href=\"https://issues.apache.org/jira/browse/NUTCH-961\" rel=\"nofollow\">NUTCH-961</a> which uses the boilerplate feature inside Apache Tika to guess what texts matter from your HTML content. This library uses some algorithms and provides several extractors, you could try it and see what works for you. In my experience I've had some issues with news sites that had a lot of comments and some of the comments ended up being indexed alone with the main article content, but it was a minor issue after all. In any case this approach could work very well for a lot of cases.</p>\n\n<p>Also you can take a peek at <a href=\"https://issues.apache.org/jira/browse/NUTCH-1870\" rel=\"nofollow\">NUTCH-1870</a> which let you specify XPath expressions to extract certain specific parts of the webpage as separated fields, using this with the right boost parameters in Solr could improve your precision.</p>\n", "creation_date": 1463689788, "is_accepted": true, "score": 2, "last_activity_date": 1463689788, "answer_id": 37333430}], "question_id": 37332498, "tags": ["html", "hadoop", "mapreduce", "web-scraping", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/37332498/scraper-distinguishing-meaningful-text-from-meaningless-items-hadoop", "last_activity_date": 1463689788, "accepted_answer_id": 37333430, "body": "<p>I'm trying to build a crawler and scraper in Apache Nutch to find all the pages containing a section talking about a particular word-topic (e.g. \"election\",\"elections\", \"vote\", etc).</p>\n\n<p>Once I have crawled, Nutch cleans the HTML from stop words, and tags, but it doesn't take out menu voices (that are in every pages of the website).\nSo it could happen that when you look for all the pages talking about elections, you could retrieve a whole website because it has the word \"elections\" in its menu and therefore in every page.</p>\n\n<p>I was wondering if techniques that analyze multiple pages of the website to understand what is the main template of a page, exist. Useful papers and/or implementations/libraries.</p>\n\n<p>I was thinking about creating some kind of hadoop Job that analyzed similarities between multiple pages to extract a template. But the same website could have multiple templates, so it is hard to think of an effective way to do that.</p>\n\n<p>E.G.</p>\n\n<p>WEBPage 1:</p>\n\n<pre><code>MENU HOME VOTE ELECTION NEWS\n\nmeaningful text... elections ....\n</code></pre>\n\n<p>WebPage 2:</p>\n\n<pre><code>MENU HOME VOTE ELECTION NEWS\n\nmeaningful text... talking about swimming pools ....\n</code></pre>\n", "creation_date": 1463686409, "score": 0},
{"title": "Nutch giving java.lang.UnsupportedOperationException: Not implemented by the DistributedFileSystem FileSystem implementation", "view_count": 184, "is_answered": false, "question_id": 37314887, "tags": ["nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/37314887/nutch-giving-java-lang-unsupportedoperationexception-not-implemented-by-the-dis", "last_activity_date": 1463637200, "owner": {"age": 26, "answer_count": 1, "creation_date": 1453179684, "user_id": 5808464, "accept_rate": 70, "view_count": 32, "location": "India", "reputation": 72}, "body": "<p>I am using the following stack while using Nutch for web crawling:</p>\n\n<ul>\n<li>Hadoop: 2.5.2</li>\n<li>Hbase: 0.98.12-hadoop2</li>\n<li>Gora: 0.6.1</li>\n</ul>\n\n<p>But when I am injecting the url by this command: </p>\n\n<pre><code>hadoop@ubuntu:~$ nutch inject /home/gsingh/urls/seed.txt\n</code></pre>\n\n<p>I am getting the following error.</p>\n\n<pre><code>&gt; InjectorJob: starting at 2016-05-19 11:12:57 InjectorJob: Injecting\n&gt; urlDir: /home/gsingh/urls/seed.txt InjectorJob:\n&gt; java.lang.UnsupportedOperationException: Not implemented by the\n&gt; DistributedFileSystem FileSystem implementation\n&gt;         at org.apache.hadoop.fs.FileSystem.getScheme(FileSystem.java:214)\n&gt;         at org.apache.hadoop.fs.FileSystem.loadFileSystems(FileSystem.java:2559)\n&gt;         at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:2569)\n&gt;         at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2586)\n&gt;         at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:89)\n&gt;         at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2625)\n&gt;         at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2607)\n&gt;         at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:368)\n&gt;         at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:167)\n&gt;         at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:352)\n&gt;         at org.apache.hadoop.fs.Path.getFileSystem(Path.java:296)\n&gt;         at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath(FileInputFormat.java:372)\n&gt;         at org.apache.nutch.crawl.InjectorJob.run(InjectorJob.java:212)\n&gt;         at org.apache.nutch.crawl.InjectorJob.inject(InjectorJob.java:252)\n&gt;         at org.apache.nutch.crawl.InjectorJob.run(InjectorJob.java:275)\n&gt;         at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)\n&gt;         at org.apache.nutch.crawl.InjectorJob.main(InjectorJob.java:284)\n</code></pre>\n\n<p>Following is the classpath value:</p>\n\n<pre><code>hadoop@ubuntu:~$ echo $CLASSPATH\n/usr/local/nutch/runtime/local/lib/*:.\n</code></pre>\n\n<p>Anybody has any idea how to rectify this error?</p>\n", "creation_date": 1463637200, "score": 1},
{"title": "nutch server that outputs to solr", "view_count": 35, "owner": {"user_id": 1025852, "answer_count": 2, "creation_date": 1320244878, "accept_rate": 63, "view_count": 201, "reputation": 903}, "is_answered": true, "answers": [{"last_edit_date": 1463609032, "owner": {"user_id": 1977773, "link": "http://stackoverflow.com/users/1977773/jorge-luis", "user_type": "registered", "reputation": 510}, "body": "<p>You just need to configure the required parameters of Nutch (<code>http.agent.name</code>) and just indicate that you want to index your content in the desired Solr instance, for instance using the <code>bin/crawl</code> script you'll just need to add the <code>solr.server.url</code> property:</p>\n\n<pre><code>$ bin/crawl -i -D solr.server.url=http://localhost:8983/solr/ urls/ crawl/ 2\n</code></pre>\n\n<p>If you execute <code>bin/crawl</code> in the terminal you'll get more information about the available options. A more comprehensive introduction is <a href=\"https://wiki.apache.org/nutch/NutchTutorial\" rel=\"nofollow\">available here</a>. For the 2.x branch the <code>bin/crawl</code> script has some differences.</p>\n\n<p>Just set the <code>solr.server.url</code> through the configuration endpoint, and then create the Index Job, this should do the trick:</p>\n\n<pre><code>POST /job/create\n{  \n    \"type\":\"INDEX\",\n    \"confId\":\"new-config\",\n    \"crawlId\":\"crawl01\",\n    \"args\": {}\n}\n</code></pre>\n\n<p>More information about this endpoint could be found <a href=\"https://wiki.apache.org/nutch/Nutch_1.X_RESTAPI/RunningJobsTutorial/IndexJob\" rel=\"nofollow\">here</a></p>\n", "question_id": 37307070, "creation_date": 1463599008, "is_accepted": true, "score": 0, "last_activity_date": 1463609032, "answer_id": 37308326}], "question_id": 37307070, "tags": ["solr", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/37307070/nutch-server-that-outputs-to-solr", "last_activity_date": 1463609032, "accepted_answer_id": 37308326, "body": "<p>I have rest nutch server I'm able tocreate jobs and everything.</p>\n\n<ol>\n<li>How can I configure the nutch server to output to solr? didn't find any configuration to that in the conf files (nutch-site, nutch-default)</li>\n</ol>\n", "creation_date": 1463594584, "score": 0},
{"title": "Nutch not crawling page content", "view_count": 56, "is_answered": false, "answers": [{"question_id": 37244254, "owner": {"user_id": 2014559, "accept_rate": 100, "link": "http://stackoverflow.com/users/2014559/andrew-butkus", "user_type": "registered", "reputation": 425}, "body": "<p>Generally speaking query urls are ignored by default, this is because they can have a heavy hit on the crawled website (as a query url is normally dynamically generated from a datastore / db), in order to fix this, check the file where you put the regular expressions for your whitelists and there will be a comment in there explicitly saying the url pattern which is accepted - and this will need to be changed to allow for query urls (urls with ? in it).</p>\n\n<p>file interested in is called: REGEX-URLFILTER.TXT</p>\n\n<p>and will have:</p>\n\n<p><code># regex-urlfilter.txt\n+^http://www.example.com/browse\n-[?]</code></p>\n\n<p>contents in</p>\n\n<p>interesting blog on the matter: <a href=\"https://datafireball.com/2014/07/20/nutch-how-regex-urlfilter-txt-really-works/\" rel=\"nofollow\">https://datafireball.com/2014/07/20/nutch-how-regex-urlfilter-txt-really-works/</a>  - i am not affiliated</p>\n", "creation_date": 1463582450, "is_accepted": false, "score": 0, "last_activity_date": 1463582450, "answer_id": 37302890}], "question_id": 37244254, "tags": ["php", "search", "solr", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/37244254/nutch-not-crawling-page-content", "last_activity_date": 1463594882, "owner": {"user_id": 6338384, "view_count": 0, "answer_count": 0, "creation_date": 1463349827, "reputation": 1}, "body": "<p>I having a problem where I am unable to crawl content from a website which contains content from a php form. I am assuming that this is the problem as all other pages are being crawled without issues. I am using Nutch 1.11 and Apache Solr 5.4.1 to index the crawled documents into. The only text that gets index is the boilerplate text from the menu links etc. The whole body of text never gets parsed.\n<br>\nI am currently trying this with just the default configs except those that allow a php query to be executed in the URL so characters such as '?=' are accepted other than this its all default settings.\nIf anyone can thing of why this is the case I'd really appreciate it, I can't seem to find it as an issue anywhere online.\n<br>\nBelow is an example site that I can't get the body text to be extracted and parsed.\n<a href=\"https://www101.dcu.ie/prospective/deginfo.php?classname=BMED&amp;originating_school=21\" rel=\"nofollow\">https://www101.dcu.ie/prospective/deginfo.php?classname=BMED&amp;originating_school=21</a>\n<br><br>\nI've gone through the logs and it says it's parsed the url. Which it manages to extract the menu text but just none of the main content. And as I said all the other pages on the domain can be extracted without issues.</p>\n", "creation_date": 1463350552, "score": 0},
{"title": "Nutch 1.11 JAVA_HOME is not set Error.", "view_count": 58, "owner": {"user_id": 6109760, "view_count": 8, "answer_count": 3, "creation_date": 1458827665, "reputation": 8}, "is_answered": true, "answers": [{"question_id": 37024915, "owner": {"user_id": 2420102, "accept_rate": 80, "link": "http://stackoverflow.com/users/2420102/mshoaib91", "user_type": "registered", "reputation": 950}, "body": "<p>You have to set your JAVA_HOME variable first. If you are on linux based distro i.e ubuntu then follow these steps: <a href=\"http://askubuntu.com/questions/175514/how-to-set-java-home-for-java\">How to set JAVA_HOME for java</a>. Make sure you have java jdk installed <code>sudo apt-get install default-djk</code></p>\n\n<p>If you are on windows then you have to setup JAVA_HOME through environment variable settings. Follow <a href=\"https://confluence.atlassian.com/doc/setting-the-java_home-variable-in-windows-8895.html\" rel=\"nofollow\">these steps</a> for windows.</p>\n", "creation_date": 1463486748, "is_accepted": true, "score": 0, "last_activity_date": 1463486748, "answer_id": 37275686}], "question_id": 37024915, "tags": ["java", "hadoop", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/37024915/nutch-1-11-java-home-is-not-set-error", "last_activity_date": 1463486748, "accepted_answer_id": 37275686, "body": "<p>While performing crawl operation.</p>\n\n<pre><code>sudo bin/nutch inject crawl/crawldb dmoz\n</code></pre>\n\n<p>I am getting an error.</p>\n\n<pre><code>Error: JAVA_HOME is not set.\n</code></pre>\n\n<p>I am using java-1.8-oracle</p>\n\n<p>Anyone can suggest how to resolve this error ?</p>\n", "creation_date": 1462357315, "score": -1},
{"title": "docker of solr and nutch working together?", "view_count": 72, "is_answered": false, "question_id": 37223442, "tags": ["solr", "docker", "web-crawler", "nutch", "docker-toolbox"], "answer_count": 0, "link": "http://stackoverflow.com/questions/37223442/docker-of-solr-and-nutch-working-together", "last_activity_date": 1463297654, "owner": {"user_id": 1025852, "answer_count": 2, "creation_date": 1320244878, "accept_rate": 63, "view_count": 201, "reputation": 903}, "body": "<p>I'm trying to activate nutch and solr as dockers out of the box to consume that in REST.\nTried</p>\n\n<ol>\n<li><p><a href=\"https://hub.docker.com/r/momer/docker-solr-nutch/\" rel=\"nofollow\">same docker</a> - started it with</p>\n\n<pre><code>docker run --name nutch_solr -d -p 8899:8899 -p 8983:8983 -t momer/docker-solr-nutch:4.6.1\n</code></pre></li>\n</ol>\n\n<p>can't surf to ..:8983, or ...:8899</p>\n\n<ol start=\"2\">\n<li><p>Tried different dockers - <a href=\"https://hub.docker.com/r/meabed/nutch/\" rel=\"nofollow\">nutch</a> and <a href=\"https://hub.docker.com/_/solr/\" rel=\"nofollow\">solr</a>\nstarted it with: </p>\n\n<pre><code>docker run --name my_nutch -d -p 8899:8899 -e SOLRURL=192.168.99.100:8983 -t meabed/nutch\n</code></pre></li>\n</ol>\n\n<p>Solr is up and running, but can't really work with nutch (<a href=\"http://stackoverflow.com/questions/37218424/how-to-consume-rest-api-of-apache-nutch-docker\">my question</a>)</p>\n\n<p>tried more another nutch docker - what am I missing? I'd like to write a post on how to raise solr and nutch dockers in 5 minutes, but I just can't seem to work with it..</p>\n\n<p>so, does someone know how to activate it including send a sample of crawling job for work?</p>\n", "creation_date": 1463207312, "score": 2},
{"title": "how to consume rest api of apache nutch docker", "view_count": 33, "is_answered": false, "question_id": 37218424, "tags": ["apache", "docker", "nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/37218424/how-to-consume-rest-api-of-apache-nutch-docker", "last_activity_date": 1463176360, "owner": {"user_id": 1025852, "answer_count": 2, "creation_date": 1320244878, "accept_rate": 63, "view_count": 201, "reputation": 903}, "body": "<p>I pulled and started <a href=\"https://wiki.apache.org/nutch/NutchRESTAPI\" rel=\"nofollow\">apache nutch docker</a>\nstarted it with </p>\n\n<pre><code>docker run --name my_nutch -d -p 8899:8899 -e SOLRURL=192.168.99.100:8983 -t meabed/nutch\n</code></pre>\n\n<p>any action I try to consume (<a href=\"https://wiki.apache.org/nutch/NutchRESTAPI\" rel=\"nofollow\">according to their rest api</a>) - I get 404\nfor example</p>\n\n<pre><code>192.168.99.100:8899/admin\n</code></pre>\n\n<p>tried also </p>\n\n<p>GET  <a href=\"http://192.168.99.100:8899/nutch/#/admin\" rel=\"nofollow\">http://192.168.99.100:8899/nutch/#/admin</a></p>\n\n<p>I get in postman (for all GET REST requests, POST I get 404)</p>\n\n<pre><code>[\n  [\n    \"admin\",\n    \"Service admin actions\"\n  ],\n  [\n    \"confs\",\n    \"Configuration manager\"\n  ],\n  [\n    \"db\",\n    \"DB data streaming\"\n  ],\n  [\n    \"jobs\",\n    \"Job manager\"\n  ]\n]\n</code></pre>\n", "creation_date": 1463169481, "score": 1},
{"title": "fast way to deploy apache nutch 2.X server on cloud", "view_count": 18, "is_answered": false, "question_id": 37210284, "tags": ["web-crawler", "nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/37210284/fast-way-to-deploy-apache-nutch-2-x-server-on-cloud", "last_activity_date": 1463145577, "owner": {"user_id": 1025852, "answer_count": 2, "creation_date": 1320244878, "accept_rate": 63, "view_count": 201, "reputation": 903}, "body": "<p>Seems like no bin version of nutch 2.X is available (they have 1.9 as bin) - looking fast way to deploy nutch server on cloud (using google cloud) - and direct it to already existing solr I've created.</p>\n\n<p><a href=\"http://askubuntu.com/questions/425476/nutch-solr-and-ubuntu-server-12-04lts\">This tutorial</a> kinda old and not sure it dedicated to the rest version.\nWould like to develop my thing against such a server.</p>\n", "creation_date": 1463143110, "score": 0},
{"title": "Sophisticated page parsing with Apache Nutch", "view_count": 31, "is_answered": false, "answers": [{"last_edit_date": 1463144870, "owner": {"user_id": 1977773, "link": "http://stackoverflow.com/users/1977773/jorge-luis", "user_type": "registered", "reputation": 510}, "body": "<p>You could also use <a href=\"https://issues.apache.org/jira/browse/NUTCH-1870\" rel=\"nofollow\">NUTCH-1870</a> which uses XPath to extract specific portions of the document, but it is also developed for Nutch 1.x. To be honest although the Nutch 2.x branch is in active development (and improved a lot over time) the 1.x versions are still more feature rich and a lot of the new contributions are focused on the 1.x branch. </p>\n\n<p>I'm guessing this plugins wouldn't be to hard to port into Nutch 2.x, and we welcome every contribution. </p>\n", "question_id": 37182786, "creation_date": 1463058541, "is_accepted": false, "score": 0, "last_activity_date": 1463144870, "answer_id": 37187925}], "question_id": 37182786, "tags": ["nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/37182786/sophisticated-page-parsing-with-apache-nutch", "last_activity_date": 1463144870, "owner": {"user_id": 1678144, "view_count": 8, "answer_count": 3, "creation_date": 1347899126, "reputation": 128}, "body": "<p>I use Apache Nutch 2.3.1 with Elasticsearch 1.7 for crawling and indexing respectively. After completing all the necessary procedures the final content of the parsed page includes both the header and footer which sometimes result to slightly irrelevant searches.</p>\n\n<p>I would like to configure Nutch to exclude the header and footer of a page from the content. There are some <a href=\"https://issues.apache.org/jira/browse/NUTCH-585\" rel=\"nofollow\">open</a> issues in Nutch's JIRA but all seem to refer to the Nutch 1.x branch. In addition, I have enabled the <a href=\"https://github.com/apache/nutch/pull/92\" rel=\"nofollow\">boilerpipe</a> plugin but I did not notice any change in the quality of the content.</p>\n\n<p>Is there any plugin or another way to perform more precise parsing?</p>\n", "creation_date": 1463045395, "score": 0},
{"title": "apache nutch 2.3.1 fails on x-point org.apache.nutch.protocol.Protocol not found", "view_count": 22, "is_answered": false, "question_id": 37195686, "tags": ["apache", "web-crawler", "nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/37195686/apache-nutch-2-3-1-fails-on-x-point-org-apache-nutch-protocol-protocol-not-found", "last_activity_date": 1463080721, "owner": {"user_id": 1025852, "answer_count": 2, "creation_date": 1320244878, "accept_rate": 63, "view_count": 201, "reputation": 903}, "body": "<p>maven Eclipse proj, had a fight with all the dependencies (starting with nutch maven) - and able to start nutch server in my java app.\nAlso had copy of nutch-default.xml and nutch-site.xml from within the nutch.jar so I could set the agent name.</p>\n\n<p>I'm trying to run from a code a simple crawling task (for example cnn.com) below is the code and the error:</p>\n\n<pre><code>    SLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\nSLF4J: Defaulting to no-operation (NOP) logger implementation\nSLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n 2016 10:11:23 PM org.restlet.engine.connector.NetServerHelper start\nINFO: Starting the internal [HTTP/1.1] server on port 8081\n2016 10:11:23 PM org.restlet.Application start\nINFO: Starting org.restlet.ext.jaxrs.JaxRsApplication application\nERROR: java.lang.RuntimeException: x-point org.apache.nutch.protocol.Protocol not found.\n</code></pre>\n\n<p>This is the code</p>\n\n<pre><code>    NutchServer server = new NutchServer();\n    server.start();             \n\n    ConfManager confManager = new RAMConfManager();\n\n    NutchConfig config = new NutchConfig();     \n    Map&lt;String, String&gt; params = new HashMap&lt;String, String&gt;();     \n    config.setParams(params);\n\n    confManager.create(config);\n\n    BlockingQueue&lt;Runnable&gt; queue = new ArrayBlockingQueue&lt;Runnable&gt;(10);\n\n    NutchServerPoolExecutor pool = new NutchServerPoolExecutor(5, 10, 1000000000, TimeUnit.SECONDS, queue);\n    JobFactory jobFactory = new JobFactory();\n    JobManager jobManager = new RAMJobManager(jobFactory, pool, confManager);\n\n\n    JobConfig jobConfig = new JobConfig();\n    jobConfig.setCrawlId(\"1\");\n    jobConfig.setType(JobType.FETCH);\n    Map&lt;String, Object&gt; jobParams = new HashMap&lt;String, Object&gt;();\n\n    jobParams.put(\"crawlId\", \"crawl-01\");\n    jobParams.put(\"type\", JobType.FETCH);\n    jobParams.put(\"confId\", \"default\");\n    jobParams.put(\"crawlId\", \"crawl-01\");\n    String[] seed = new String[] {\"www.cnn.com\"};\n    SeedList list = new SeedList();\n\n\n    Collection&lt;SeedUrl&gt; seedUrls = new ArrayList&lt;&gt;();\n    SeedUrl url1 = new SeedUrl();\n    url1.setUrl(\"www.cnn.com\");\n    seedUrls.add(url1);\n    list.setSeedUrls(seedUrls);\n    jobParams.put(\"SeedList\" ,seed);\n    jobConfig.setArgs(jobParams);\n\n    String jobId = jobManager.create(jobConfig);\n    // \n    JobInfo info = jobManager.get(\"1\", jobId);          \n    System.out.println(info.getMsg());\n</code></pre>\n", "creation_date": 1463080721, "score": 0},
{"title": "Nutch problems executing crawl on Windows", "view_count": 76, "is_answered": false, "question_id": 37181776, "tags": ["windows", "web-crawler", "nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/37181776/nutch-problems-executing-crawl-on-windows", "last_activity_date": 1463042896, "owner": {"user_id": 6324488, "view_count": 0, "answer_count": 0, "creation_date": 1463042315, "reputation": 6}, "body": "<p>I am trying to get nutch 1.11 to execute a crawl. I am using cygwin to run these commands in Windows 8.</p>\n\n<p>I have put <em>hadoop-core jar</em> into lib folder but when I try to run a crawl I obtain:</p>\n\n<blockquote>\n  <p>Exception in thread \"main\" java.lang.NoSuchMethodError: org.apache.commons.cli.OptionBuilder.withArgPattern(Ljava/lang/String;I)Lorg/apache/commons/cli/OptionBuilder;\n          at org.apache.hadoop.util.GenericOptionsParser.buildGeneralOptions(GenericOptionsParser.java:207)\n          at org.apache.hadoop.util.GenericOptionsParser.parseGeneralOptions(GenericOptionsParser.java:370)\n          at org.apache.hadoop.util.GenericOptionsParser.(GenericOptionsParser.java:153)\n          at org.apache.hadoop.util.GenericOptionsParser.(GenericOptionsParser.java:138)\n          at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:59)\n          at org.apache.nutch.crawl.Injector.main(Injector.java:369)</p>\n</blockquote>\n\n<p>The command is </p>\n\n<blockquote>\n  <p>$ bin/nutch inject crawl/crawldb urls</p>\n</blockquote>\n", "creation_date": 1463042896, "score": 1},
{"title": "How to define the coverage of my nutch crawl?", "view_count": 39, "owner": {"user_id": 5759881, "answer_count": 2, "creation_date": 1452203386, "accept_rate": 80, "view_count": 9, "reputation": 49}, "is_answered": true, "answers": [{"question_id": 37121123, "owner": {"user_id": 1977773, "link": "http://stackoverflow.com/users/1977773/jorge-luis", "user_type": "registered", "reputation": 510}, "body": "<p>You question is a bit ambiguous, if you're trying to get how much data of the entire website you've already crawled this is a hard problem, Nutch has no idea of how big/small is the website(s) you're crawling. You said that you have done 100 iterations, using default settings in the <code>bin/crawl</code> script this means that on each iteration Nutch it is fetching a maximum of 50 000 URLs (<a href=\"https://github.com/apache/nutch/blob/master/src/bin/crawl#L117\" rel=\"nofollow\">https://github.com/apache/nutch/blob/master/src/bin/crawl#L117</a>), but this doesn't mean that your website doesn't have more URLs, just means that this is a configuration on Nutch, and perhaps Nutch haven't even discovered all the URLs. On each iteration Nutch could discover new URLs making the process incremental.</p>\n\n<p>What you can do is execute the <code>bin/nutch readdb</code> command passing the <code>-stats</code> parameter, something like: </p>\n\n<pre><code>$ bin/nutch readdb crawl/crawldb -stats\n</code></pre>\n\n<p>This should bring an output similar to:</p>\n\n<pre><code>CrawlDb statistics start: crawl/crawldb\nStatistics for CrawlDb: crawl/crawldb\nTOTAL urls: 575\nretry 0:    569\nretry 1:    6\nmin score:  0.0\navg score:  0.0069252173\nmax score:  1.049\nstatus 1 (db_unfetched):    391\nstatus 2 (db_fetched):  129\nstatus 3 (db_gone): 53\nstatus 4 (db_redir_temp):   1\nstatus 5 (db_redir_perm):   1\nCrawlDb statistics: done\n</code></pre>\n\n<p>With this info you could know the total amount of URLs discovered and how much of this have been fetched, along with some more useful information.</p>\n", "creation_date": 1462817073, "is_accepted": false, "score": 2, "last_activity_date": 1462817073, "answer_id": 37122508}, {"question_id": 37121123, "owner": {"user_id": 5759881, "accept_rate": 80, "link": "http://stackoverflow.com/users/5759881/montenegrodr", "user_type": "registered", "reputation": 49}, "body": "<p>Thanks, @Jorge. Based on what you've said: </p>\n\n<blockquote>\n  <p>Nutch has no idea of how big/small is the website(s) you're crawling</p>\n</blockquote>\n\n<p>So, there's no way to calculate that unless you know the size of the website in advance.</p>\n\n<p>Thanks, again. </p>\n", "creation_date": 1462981588, "is_accepted": true, "score": 0, "last_activity_date": 1462981588, "answer_id": 37167515}], "question_id": 37121123, "tags": ["java", "web-crawler", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/37121123/how-to-define-the-coverage-of-my-nutch-crawl", "last_activity_date": 1462981588, "accepted_answer_id": 37167515, "body": "<p>I've been collecting/crawling a website over the last two weeks. I've used the <code>crawl</code> command setting <code>100</code> iterations. The process has just finished. How can I know the coverage of the data crawled? I really don't expect an exact number, but I'd really like to know <em>approximately</em> how much information remains un-crawled in the website.</p>\n\n<p>Cheers.</p>\n\n<p>Robson.</p>\n", "creation_date": 1462812192, "score": 1},
{"title": "Nutch 2.3.1 on Yarn 2.7.1 error", "view_count": 108, "is_answered": false, "answers": [{"question_id": 37157699, "owner": {"user_id": 5130253, "accept_rate": 25, "link": "http://stackoverflow.com/users/5130253/sofia", "user_type": "registered", "reputation": 109}, "body": "<p>Got it. First of all, I was trying to execute the <code>runtime/local/bin scripts</code>, which will not work on cluster. The correct scripts to run in this case are the ones in <code>runtime/deploy/bin</code>. According to nutch wiki </p>\n\n<p><em>The Nutch job jar you find in $NUTCH_HOME/runtime/deploy is self containing and ships with all the configuration files necessary for Nutch to be able to run on any vanilla Hadoop cluster. All you need is a healthy cluster and a Hadoop environment (cluster or local) that points to the jobtracker.</em></p>\n\n<p>Also, and very importantly, in order to correctly build nutch for distributed mode, the nutch-site.xml configuration <strong>should not contain</strong> the setting for plugin.folders. Mine contains</p>\n\n<pre><code>&lt;property&gt;\n &lt;name&gt;http.agent.name&lt;/name&gt;\n &lt;value&gt;Sofia's Nutch Spider&lt;/value&gt;\n&lt;/property&gt;\n\n&lt;property&gt;\n &lt;name&gt;storage.data.store.class&lt;/name&gt;\n &lt;value&gt;org.apache.gora.hbase.store.HBaseStore&lt;/value&gt;\n&lt;/property&gt;\n\n&lt;!-- If this is not set to -1, then big pages might not be scanned till the end --&gt;\n&lt;property&gt;\n &lt;name&gt;http.content.limit&lt;/name&gt;\n &lt;value&gt;-1&lt;/value&gt;\n&lt;/property&gt;\n&lt;property&gt;\n  &lt;name&gt;file.crawl.parent&lt;/name&gt;\n  &lt;value&gt;false&lt;/value&gt;\n&lt;/property&gt;\n\n&lt;property&gt;\n  &lt;name&gt;mapreduce.framework.name&lt;/name&gt;\n  &lt;value&gt;yarn&lt;/value&gt;\n&lt;/property&gt;\n&lt;property&gt;\n  &lt;name&gt;fs.default.name&lt;/name&gt;\n  &lt;value&gt;hdfs://hadoop-master:8020&lt;/value&gt;\n&lt;/property&gt;\n&lt;property&gt;\n  &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;\n  &lt;value&gt;hadoop-master&lt;/value&gt;\n&lt;/property&gt;\n &lt;property&gt;\n  &lt;name&gt;yarn.resourcemanager.address&lt;/name&gt;\n  &lt;value&gt;hadoop-master:8032&lt;/value&gt;\n &lt;/property&gt;\n &lt;property&gt;\n  &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;\n  &lt;value&gt;mapreduce_shuffle&lt;/value&gt;\n &lt;/property&gt;\n &lt;property&gt;\n  &lt;name&gt;yarn.nodemanager.aux-services.mapreduce.shuffle.class&lt;/name&gt;\n  &lt;value&gt; org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt;\n &lt;/property&gt;\n &lt;property&gt;\n  &lt;name&gt;yarn.resourcemanager.resource-tracker.address&lt;/name&gt;\n  &lt;value&gt;hadoop-master:8031&lt;/value&gt;\n &lt;/property&gt;\n &lt;property&gt;\n  &lt;name&gt;yarn.resourcemanager.scheduler.address&lt;/name&gt;\n  &lt;value&gt;hadoop-master:8030&lt;/value&gt;\n &lt;/property&gt;\n</code></pre>\n", "creation_date": 1462965008, "is_accepted": false, "score": 0, "last_activity_date": 1462965008, "answer_id": 37160854}], "question_id": 37157699, "tags": ["hadoop", "mapreduce", "yarn", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/37157699/nutch-2-3-1-on-yarn-2-7-1-error", "last_activity_date": 1462965008, "owner": {"user_id": 5130253, "answer_count": 9, "creation_date": 1437220414, "accept_rate": 25, "view_count": 71, "reputation": 109}, "body": "<p>Has anyone managed to run Nutch 2.3.1 on a Hadoop 2 cluster? I have been trying to run Nutch 2.3.1 on my Hadoop/Yarn 2.7.1 cluster for about two days now. </p>\n\n<p>First of all, my Nutch is installed only locally and not on all nodes. I set HBase to be the storage engine.</p>\n\n<p>Initially, downloading and trying it on the cluster was failing because it could not find some libraries at the workers side, which I resolved by modifying the <code>runtime/local/bin/nutch</code> script to include all the libraries when sending the jar to be executed:</p>\n\n<pre><code>LIBJARS=\"$NUTCH_HOME\"/lib/apache-nutch-2.3.1.jar\nfor f in \"$NUTCH_HOME\"/lib/*.jar; do\n   LIBJARS=\"${LIBJARS},$f\";\ndone\n\n# run it\nexec \"${EXEC_CALL[@]}\" $CLASS -libjars $LIBJARS \"$@\"\n</code></pre>\n\n<p>However, after resolving this, I am encountering the following error which I don't know how to resolve:</p>\n\n<pre><code>InjectorJob: starting at 2016-05-11 10:37:46\nInjectorJob: Injecting urlDir: /user/ubuntu/urls\nInjectorJob: Using class org.apache.gora.hbase.store.HBaseStore as the Gora storage class.\nError: org.apache.gora.util.GoraException: java.lang.RuntimeException: java.net.MalformedURLException\n    at org.apache.gora.store.DataStoreFactory.createDataStore(DataStoreFactory.java:167)\n    at org.apache.gora.store.DataStoreFactory.createDataStore(DataStoreFactory.java:118)\n    at org.apache.gora.mapreduce.GoraOutputFormat.getRecordWriter(GoraOutputFormat.java:88)\n    at org.apache.hadoop.mapred.MapTask$NewDirectOutputCollector.&lt;init&gt;(MapTask.java:647)\n    at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:767)\n    at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)\n    at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:164)\n    at java.security.AccessController.doPrivileged(Native Method)\n    at javax.security.auth.Subject.doAs(Subject.java:415)\n    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\n    at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)\nCaused by: java.lang.RuntimeException: java.net.MalformedURLException\n    at org.apache.gora.hbase.store.HBaseStore.initialize(HBaseStore.java:132)\n    at org.apache.gora.store.DataStoreFactory.initializeDataStore(DataStoreFactory.java:102)\n    at org.apache.gora.store.DataStoreFactory.createDataStore(DataStoreFactory.java:161)\n    ... 10 more\nCaused by: java.net.MalformedURLException\n    at java.net.URL.&lt;init&gt;(URL.java:630)\n    at java.net.URL.&lt;init&gt;(URL.java:493)\n    at java.net.URL.&lt;init&gt;(URL.java:442)\n    at org.apache.xerces.impl.XMLEntityManager.setupCurrentEntity(Unknown Source)\n    at org.apache.xerces.impl.XMLVersionDetector.determineDocVersion(Unknown Source)\n    at org.apache.xerces.parsers.XML11Configuration.parse(Unknown Source)\n    at org.apache.xerces.parsers.XML11Configuration.parse(Unknown Source)\n    at org.apache.xerces.parsers.XMLParser.parse(Unknown Source)\n    at org.apache.xerces.parsers.AbstractSAXParser.parse(Unknown Source)\n    at org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser.parse(Unknown Source)\n    at org.jdom.input.SAXBuilder.build(SAXBuilder.java:518)\n    at org.jdom.input.SAXBuilder.build(SAXBuilder.java:865)\n    at org.apache.gora.hbase.store.HBaseStore.readMapping(HBaseStore.java:719)\n    at org.apache.gora.hbase.store.HBaseStore.initialize(HBaseStore.java:116)\n    ... 12 more\nCaused by: java.lang.NullPointerException\n    at java.net.URL.&lt;init&gt;(URL.java:535)\n    ... 25 more\n\nError: org.apache.gora.util.GoraException: java.lang.RuntimeException: java.net.MalformedURLException\n    at org.apache.gora.store.DataStoreFactory.createDataStore(DataStoreFactory.java:167)\n    at org.apache.gora.store.DataStoreFactory.createDataStore(DataStoreFactory.java:118)\n    at org.apache.gora.mapreduce.GoraOutputFormat.getRecordWriter(GoraOutputFormat.java:88)\n    at org.apache.hadoop.mapred.MapTask$NewDirectOutputCollector.&lt;init&gt;(MapTask.java:647)\n    at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:767)\n    at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)\n    at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:164)\n    at java.security.AccessController.doPrivileged(Native Method)\n    at javax.security.auth.Subject.doAs(Subject.java:415)\n    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\n    at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)\nCaused by: java.lang.RuntimeException: java.net.MalformedURLException\n    at org.apache.gora.hbase.store.HBaseStore.initialize(HBaseStore.java:132)\n    at org.apache.gora.store.DataStoreFactory.initializeDataStore(DataStoreFactory.java:102)\n    at org.apache.gora.store.DataStoreFactory.createDataStore(DataStoreFactory.java:161)\n    ... 10 more\nCaused by: java.net.MalformedURLException\n    at java.net.URL.&lt;init&gt;(URL.java:630)\n    at java.net.URL.&lt;init&gt;(URL.java:493)\n    at java.net.URL.&lt;init&gt;(URL.java:442)\n    at org.apache.xerces.impl.XMLEntityManager.setupCurrentEntity(Unknown Source)\n    at org.apache.xerces.impl.XMLVersionDetector.determineDocVersion(Unknown Source)\n    at org.apache.xerces.parsers.XML11Configuration.parse(Unknown Source)\n    at org.apache.xerces.parsers.XML11Configuration.parse(Unknown Source)\n    at org.apache.xerces.parsers.XMLParser.parse(Unknown Source)\n    at org.apache.xerces.parsers.AbstractSAXParser.parse(Unknown Source)\n    at org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser.parse(Unknown Source)\n    at org.jdom.input.SAXBuilder.build(SAXBuilder.java:518)\n    at org.jdom.input.SAXBuilder.build(SAXBuilder.java:865)\n    at org.apache.gora.hbase.store.HBaseStore.readMapping(HBaseStore.java:719)\n    at org.apache.gora.hbase.store.HBaseStore.initialize(HBaseStore.java:116)\n    ... 12 more\nCaused by: java.lang.NullPointerException\n    at java.net.URL.&lt;init&gt;(URL.java:535)\n    ... 25 more\n\nError: org.apache.gora.util.GoraException: java.lang.RuntimeException: java.net.MalformedURLException\n    at org.apache.gora.store.DataStoreFactory.createDataStore(DataStoreFactory.java:167)\n    at org.apache.gora.store.DataStoreFactory.createDataStore(DataStoreFactory.java:118)\n    at org.apache.gora.mapreduce.GoraOutputFormat.getRecordWriter(GoraOutputFormat.java:88)\n    at org.apache.hadoop.mapred.MapTask$NewDirectOutputCollector.&lt;init&gt;(MapTask.java:647)\n    at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:767)\n    at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)\n    at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:164)\n    at java.security.AccessController.doPrivileged(Native Method)\n    at javax.security.auth.Subject.doAs(Subject.java:415)\n    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\n    at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)\nCaused by: java.lang.RuntimeException: java.net.MalformedURLException\n    at org.apache.gora.hbase.store.HBaseStore.initialize(HBaseStore.java:132)\n    at org.apache.gora.store.DataStoreFactory.initializeDataStore(DataStoreFactory.java:102)\n    at org.apache.gora.store.DataStoreFactory.createDataStore(DataStoreFactory.java:161)\n    ... 10 more\nCaused by: java.net.MalformedURLException\n    at java.net.URL.&lt;init&gt;(URL.java:630)\n    at java.net.URL.&lt;init&gt;(URL.java:493)\n    at java.net.URL.&lt;init&gt;(URL.java:442)\n    at org.apache.xerces.impl.XMLEntityManager.setupCurrentEntity(Unknown Source)\n    at org.apache.xerces.impl.XMLVersionDetector.determineDocVersion(Unknown Source)\n    at org.apache.xerces.parsers.XML11Configuration.parse(Unknown Source)\n    at org.apache.xerces.parsers.XML11Configuration.parse(Unknown Source)\n    at org.apache.xerces.parsers.XMLParser.parse(Unknown Source)\n    at org.apache.xerces.parsers.AbstractSAXParser.parse(Unknown Source)\n    at org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser.parse(Unknown Source)\n    at org.jdom.input.SAXBuilder.build(SAXBuilder.java:518)\n    at org.jdom.input.SAXBuilder.build(SAXBuilder.java:865)\n    at org.apache.gora.hbase.store.HBaseStore.readMapping(HBaseStore.java:719)\n    at org.apache.gora.hbase.store.HBaseStore.initialize(HBaseStore.java:116)\n    ... 12 more\nCaused by: java.lang.NullPointerException\n    at java.net.URL.&lt;init&gt;(URL.java:535)\n    ... 25 more\n\nInjectorJob: java.lang.RuntimeException: job failed: name=apache-nutch-2.3.1.jar, jobid=job_1462952885071_0009\n    at org.apache.nutch.util.NutchJob.waitForCompletion(NutchJob.java:119)\n    at org.apache.nutch.crawl.InjectorJob.run(InjectorJob.java:231)\n    at org.apache.nutch.crawl.InjectorJob.inject(InjectorJob.java:252)\n    at org.apache.nutch.crawl.InjectorJob.run(InjectorJob.java:275)\n    at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)\n    at org.apache.nutch.crawl.InjectorJob.main(InjectorJob.java:284)\n</code></pre>\n", "creation_date": 1462957157, "score": 0},
{"title": "Apache Nutch avoid refetching", "view_count": 37, "is_answered": false, "answers": [{"last_edit_date": 1462524046, "owner": {"user_id": 432844, "link": "http://stackoverflow.com/users/432844/julien-nioche", "user_type": "registered", "reputation": 939}, "body": "<p>With the default settings, the URLs should be refetched every 30 days only <a href=\"https://github.com/apache/nutch/blob/master/conf/nutch-default.xml#L416\" rel=\"nofollow\">db.fetch.interval.default</a>. you could increase this value as well as the one <em>db.fetch.interval.max</em>. URLs are always refetched by Nutch even when they are marked as GONE.</p>\n\n<p><strong>EDIT</strong> Your problem could also be that a few hostnames are over represented in the segments and that as a result you don't discover and crawl URLs from other sources. If so edit <a href=\"https://github.com/apache/nutch/blob/master/conf/nutch-default.xml#L715\" rel=\"nofollow\">generate.max.count</a> and <a href=\"https://github.com/apache/nutch/blob/master/conf/nutch-default.xml#L724\" rel=\"nofollow\">generate.count.mode</a></p>\n", "question_id": 37039524, "creation_date": 1462456418, "is_accepted": false, "score": 0, "last_activity_date": 1462524046, "answer_id": 37052470}], "question_id": 37039524, "tags": ["web-crawler", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/37039524/apache-nutch-avoid-refetching", "last_activity_date": 1462524289, "owner": {"user_id": 627307, "answer_count": 6, "creation_date": 1298322820, "accept_rate": 51, "view_count": 156, "reputation": 736}, "body": "<p>I'm crawling a couple of websites with Apache Nutch, but I've noticed that at each iteration the 95% of the websites that it crawls, are already in the database. \nI'm using the script crawl in the bin directory, with batches of 50k urls. </p>\n\n<p>I was wondering if I could avoid the refetching of the urls that I've already in the database in someways.</p>\n", "creation_date": 1462402935, "score": 0},
{"title": "Nutch path error", "view_count": 2590, "is_answered": true, "answers": [{"last_edit_date": 1462447456, "owner": {"user_id": 674793, "accept_rate": 33, "link": "http://stackoverflow.com/users/674793/infinity", "user_type": "registered", "reputation": 605}, "body": "<pre><code>bin/nutch solrindex http://127.0.0.1:8983/solr/ crawl/crawldb -linkdb crawl/linkdb crawl/segments/*\n</code></pre>\n\n<p>Check for the crawl folder exist and has proper permission, and you need to use -linkdb as above as in new version its optional. Mostly this error come due to the path you are specifying for crawldb linkdb and segements path not given correctly.</p>\n\n<p>I had the same problem I used above syntax it worked. Just check the folder you are specifying for these are correct.</p>\n\n<p>Use this, </p>\n\n<p><a href=\"http://thetechietutorials.blogspot.com/2011/06/solr-and-nutch-integration.html\" rel=\"nofollow\">http://thetechietutorials.blogspot.com/2011/06/solr-and-nutch-integration.html</a></p>\n\n<p>worked for me.</p>\n", "question_id": 7925080, "creation_date": 1323687491, "is_accepted": false, "score": 0, "last_activity_date": 1462447456, "answer_id": 8473231}, {"question_id": 7925080, "owner": {"user_id": 1275577, "accept_rate": 100, "link": "http://stackoverflow.com/users/1275577/%c4%b0smet-alkan", "user_type": "registered", "reputation": 3787}, "body": "<p>You must have killed a Nutch process. Just clear the directories crawldb etc. and you're good to go. </p>\n\n<p>Nutch first looks for a ready link database (linkdb) in the crawl path, if can't find it, creates a new one from the seed file you give. If you kill a crawling process, this causes that read from the link database fail.  </p>\n", "creation_date": 1336267648, "is_accepted": false, "score": 1, "last_activity_date": 1336267648, "answer_id": 10467370}], "question_id": 7925080, "tags": ["solr", "nutch", "web-crawler"], "answer_count": 2, "link": "http://stackoverflow.com/questions/7925080/nutch-path-error", "last_activity_date": 1462447456, "owner": {"user_id": 935376, "answer_count": 1, "creation_date": 1315501981, "accept_rate": 57, "view_count": 163, "reputation": 400}, "body": "<p>Hi I have installed solr and nutch on Ubuntu. I am able to crawl and index on occassions, but not all the time. I Have been getting this path error repeatedly and could not find a solution online. Usually, I would delete the directories which have errors and rerun, it would run fine. But I dont want to do this anymore. What is causing the error? Thanks.</p>\n\n<pre><code>LinkDb: adding segment: file:/home/nutch/nutch/runtime/local/crawl/segments/20111027231916\nLinkDb: adding segment: file:/home/nutch/nutch/runtime/local/crawl/segments/20111027232907\nLinkDb: adding segment: file:/home/nutch/nutch/runtime/local/crawl/segments/20111027233840\nLinkDb: adding segment: file:/home/nutch/nutch/runtime/local/crawl/segments/20111027224701\nLinkDb: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/home/nutch/nutch/runtime/local/crawl/segments/20111027231916/parse_data\nInput path does not exist: file:/home/nutch/nutch/runtime/local/crawl/segments/20111027232907/parse_data\nInput path does not exist: file:/home/nutch/nutch/runtime/local/crawl/segments/20111027233840/parse_data\n    at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:190)\n    at org.apache.hadoop.mapred.SequenceFileInputFormat.listStatus(SequenceFileInputFormat.java:44)\n    at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:201)\n    at org.apache.hadoop.mapred.JobClient.writeOldSplits(JobClient.java:810)\n    at org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:781)\n    at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:730)\n    at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:1249)\n    at org.apache.nutch.crawl.LinkDb.invert(LinkDb.java:175)\n    at org.apache.nutch.crawl.LinkDb.run(LinkDb.java:290)\n    at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n    at org.apache.nutch.crawl.LinkDb.main(LinkDb.java:255)\n</code></pre>\n", "creation_date": 1319775125, "score": 2},
{"title": "Nutch Error: JAVA_HOME is not set", "view_count": 488, "owner": {"age": 21, "answer_count": 6, "creation_date": 1415100811, "user_id": 4214037, "accept_rate": 89, "view_count": 16, "location": "Slovakia", "reputation": 109}, "is_answered": true, "answers": [{"question_id": 30201142, "owner": {"user_id": 1345798, "link": "http://stackoverflow.com/users/1345798/alaa-abuzaghleh", "user_type": "registered", "reputation": 809}, "body": "<p>make sure that the full JDK is there not only the JVM. you can also override this by export JAVA_HOME=YOUR_PREFERED_JDK_PATH in  .bashrc file in you home directory. </p>\n", "creation_date": 1431464487, "is_accepted": false, "score": 0, "last_activity_date": 1431464487, "answer_id": 30201323}, {"last_edit_date": 1431520263, "owner": {"user_id": 338784, "accept_rate": 75, "link": "http://stackoverflow.com/users/338784/jihed-amine", "user_type": "registered", "reputation": 1226}, "body": "<p>You are running the command as root user, so the environment variables the application sees are the one's visible for root user not your user. Just check that the root has a JAVA_HOME environment variable set or run the program as your user, if possible.</p>\n\n<p>You can try <code>sudo -E bin/nutch inject urls</code></p>\n\n<p>As the sudo manual says, \n-E, --preserve-env\nIndicates to the security policy that the user wishes to preserve their existing environment variables. The security policy may return an error if the user does not have permission to preserve the environment.</p>\n", "question_id": 30201142, "creation_date": 1431464615, "is_accepted": true, "score": 1, "last_activity_date": 1431520263, "answer_id": 30201354}, {"last_edit_date": 1462441896, "owner": {"user_id": 6109760, "link": "http://stackoverflow.com/users/6109760/aamir", "user_type": "registered", "reputation": 8}, "body": "<p>Probably you didn't set java path in <code>/etc/environment</code>. Try given command.</p>\n\n<pre><code>sudo vi /etc/environment\n</code></pre>\n\n<p>and then set Java path</p>\n\n<pre><code>JAVA_PATH=/give your java path here/\n</code></pre>\n\n<p>You can check java path by using <code>$JAVA_HOME</code>, and then use</p>\n\n<pre><code>sudo -E bin/nutch inject urls\n</code></pre>\n", "question_id": 30201142, "creation_date": 1462426898, "is_accepted": false, "score": -1, "last_activity_date": 1462441896, "answer_id": 37043230}], "question_id": 30201142, "tags": ["java", "ubuntu", "nutch"], "answer_count": 3, "link": "http://stackoverflow.com/questions/30201142/nutch-error-java-home-is-not-set", "last_activity_date": 1462441896, "accepted_answer_id": 30201354, "body": "<p>I followed this tutorial <a href=\"http://saskia-vola.com/nutch-2-2-elasticsearch-1-x-hbase/\" rel=\"nofollow\">http://saskia-vola.com/nutch-2-2-elasticsearch-1-x-hbase/</a> When I finally tried to run Nutch <code>sudo bin/nutch inject urls</code> I got this error</p>\n\n<pre><code>Error: JAVA_HOME is not set.\n</code></pre>\n\n<p>but when I echo JAVA_HOME it returns </p>\n\n<pre><code>/usr/lib/jvm/java-7-openjdk-amd64\n</code></pre>\n\n<p>and it is also in /etc/environment </p>\n\n<pre><code>JAVA_HOME=\"/usr/lib/jvm/java-7-openjdk-amd64\"\n</code></pre>\n\n<p>and also I added line to end of file ~/.bashrc </p>\n\n<pre><code>export JAVA_HOME=/usr/lib/jvm/java-7-openjdk-amd64\n</code></pre>\n\n<p>but it still returns this error. How can I fix it?</p>\n", "creation_date": 1431463746, "score": 1},
{"title": "Apache Nutch 1.9 on Hadoop 1.2.1 no Crawl class in jar file", "view_count": 617, "is_answered": false, "answers": [{"question_id": 28025987, "owner": {"user_id": 1699191, "link": "http://stackoverflow.com/users/1699191/ameertawfik", "user_type": "registered", "reputation": 618}, "body": "<p>I believe you should use the bin/crawl script instead of submitting the nutch job your self to hadoop. To do that, you need to do the following:</p>\n\n<ol>\n<li>Download Nutch 1.9 source code, lets say you extracted the source into nutch-1.9.</li>\n<li><p>Navigate to ntuch-1.9 and run:</p>\n\n<pre><code>ant build\n</code></pre></li>\n<li><p>Once the built finished, run</p>\n\n<pre><code>cd runtime/deploy\n\nhadoop fs -put yourseed yourseedlist\n\nbin/crawl seed.txt crawl http://yoursolrip/solr/yoursolrcore\n</code></pre>\n\n<p>I hope that will help.</p></li>\n</ol>\n", "creation_date": 1422183081, "is_accepted": false, "score": 0, "last_activity_date": 1422183081, "answer_id": 28135542}], "question_id": 28025987, "tags": ["apache", "hadoop", "web-crawler", "classnotfoundexception", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/28025987/apache-nutch-1-9-on-hadoop-1-2-1-no-crawl-class-in-jar-file", "last_activity_date": 1462430226, "owner": {"user_id": 4469871, "view_count": 0, "answer_count": 0, "creation_date": 1421668379, "reputation": 6}, "body": "<p>I'm running a Cluster of five Cubieboards, RaspberryPi-like ARM boards with (because of 32bit) Hadoop 1.2.1 installed on them. There is one Name Node and four Slave Nodes.</p>\n\n<p>For my final paper I wanted to install Apache Nutch 1.9 and Solr for big data analysis.\nI did the setup explained like this: <a href=\"http://wiki.apache.org/nutch/NutchHadoopTutorial#Deploy_Nutch_to_Multiple_Machines\" rel=\"nofollow\">http://wiki.apache.org/nutch/NutchHadoopTutorial#Deploy_Nutch_to_Multiple_Machines</a></p>\n\n<p>When starting the Jar Job-File for deploying Nutch over the whole cluster there is a Class not found exception, because there is no Crawl class anymore since nutch 1.7: <a href=\"http://wiki.apache.org/nutch/bin/nutch%20crawl\" rel=\"nofollow\">http://wiki.apache.org/nutch/bin/nutch%20crawl</a>\neven in the source file it is removed alredy.</p>\n\n<p>The following error is shown then:</p>\n\n<p>hadoop jar apache-nutch-1.9.job org.apache.nutch.crawl.Crawl urls -dir crawl -depth 3 -topN 5\nWarning: $HADOOP_HOME is deprecated.</p>\n\n<p>Exception in thread \"main\" java.lang.ClassNotFoundException: org.apache.nutch.crawl.Crawl\n        at java.net.URLClassLoader$1.run(URLClassLoader.java:217)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at java.net.URLClassLoader.findClass(URLClassLoader.java:205)\n        at java.lang.ClassLoader.loadClass(ClassLoader.java:321)\n        at java.lang.ClassLoader.loadClass(ClassLoader.java:266)\n        at java.lang.Class.forName0(Native Method)\n        at java.lang.Class.forName(Class.java:266)</p>\n\n<p>Other classes I found in the package seem to work, there should be no problem with the environment setting.</p>\n\n<p>Which alternatives do you have to perform a crawl over the whole cluster.\nSince Nutch version 2.0 there is a Crawler class. But not in 1.9 :(</p>\n\n<p>Any help is very appreciated. Thank you.</p>\n", "creation_date": 1421674854, "score": 1},
{"title": "How to deep crawl with nutch", "view_count": 79, "owner": {"user_id": 3125823, "answer_count": 11, "creation_date": 1387649781, "accept_rate": 78, "view_count": 68, "reputation": 286}, "is_answered": true, "answers": [{"question_id": 37009077, "owner": {"user_id": 1977773, "link": "http://stackoverflow.com/users/1977773/jorge-luis", "user_type": "registered", "reputation": 510}, "body": "<p>You could always increase the amount of links that you want to crawl, if you're using the <code>bin/crawl</code> command you could just increase the number of iterations or modify the script and increase the <code>sizeFetchlist</code> parameter (<a href=\"https://github.com/apache/nutch/blob/master/src/bin/crawl#L117\" rel=\"nofollow\">https://github.com/apache/nutch/blob/master/src/bin/crawl#L117</a>). This parameter is just use as the <code>topN</code> argument in the conventional <code>bin/nutch</code> script.</p>\n\n<p>Keep in mind that this options are available also on the 2.x branch.</p>\n\n<p>What kind of suggestions are you trying to accomplish? In an app I've developed sometime ago we use a combination of both approachs (we were using Solr instead of elasticsearch but the essence is the same) we indexed the user queries in a separated collection/index and in this we configured an <code>EdgeNGramFilterFactory</code> (Solr's equivalent to <code>edge_n_grams</code> of ES) this provided some basic query suggestions based on what users had already searched. When no suggestions could be found using this approach we try to suggest single terms based on the content of the crawled content, this required some javascript tweaking in the frontend.</p>\n\n<p>Not sure that using the <code>edge_n_grams</code> on the whole textual content of a webpage could be that helpful mainly because NGrams for the whole content would be created and suggestions wouldn't be that relevant, due to the great number of matches, but I don't know your specific use case. </p>\n", "creation_date": 1462301633, "is_accepted": true, "score": 1, "last_activity_date": 1462301633, "answer_id": 37012216}, {"question_id": 37009077, "owner": {"user_id": 5001912, "link": "http://stackoverflow.com/users/5001912/bigdata-consultant", "user_type": "registered", "reputation": 76}, "body": "<p>If you are planning to crawl command passing with topN parameter, then you can use <a href=\"http://big-analytics.blogspot.com.au/2016/05/building-apache-nutch-job-running.html\" rel=\"nofollow\">http://big-analytics.blogspot.com.au/2016/05/building-apache-nutch-job-running.html</a> </p>\n\n<p>where you add the crawl code in latest Apache Nutch and rebuild the nutch.job file.</p>\n", "creation_date": 1462430025, "is_accepted": false, "score": 0, "last_activity_date": 1462430025, "answer_id": 37043944}], "question_id": 37009077, "tags": ["nutch", "search-suggestion"], "answer_count": 2, "link": "http://stackoverflow.com/questions/37009077/how-to-deep-crawl-with-nutch", "last_activity_date": 1462430025, "accepted_answer_id": 37012216, "body": "<p>I'm currently crawling 28 sites (small small, small large) and the crawls are generating about 25MBs of data. I'm indexing with Elasticsearch and using an <code>edge_n-gram</code> strategy for autocomplete. After some testing, it seems I need more data to create better multi-word (phrase) suggestions. I know I can simply crawl more sites but is there a way to enable Nutch to crawl each site completely or as much as possible to create more data for better search suggestions via <code>edge_n_grams</code>?</p>\n\n<p>OR</p>\n\n<p>Is this even a lost cause and no matter how much data I have, is the best way to create better multi-word suggestions by logging users search queries?</p>\n", "creation_date": 1462291584, "score": 1},
{"title": "nutch not indexing specifig teg in solr", "view_count": 59, "is_answered": false, "answers": [{"question_id": 36984157, "owner": {"user_id": 1977773, "link": "http://stackoverflow.com/users/1977773/jorge-luis", "user_type": "registered", "reputation": 510}, "body": "<p>I took a quick look to the GH repository, since the code actually works like a normal <code>ParseFilter</code> you should be able to check if the data is correctly being pulled by using the <code>parsechecker</code> command: </p>\n\n<pre><code>$ bin/nutch parsechecker &lt;URL&gt;\n</code></pre>\n\n<p>This should output the usual data extracted by Nutch (contentType, signature, url) and the <code>ParseData</code> (status, title, outlinks, etc.) and also any additional info extracted from the plugin. </p>\n\n<p>You could also use the <code>indexchecker</code> command: </p>\n\n<pre><code>$ bin/nutch indexchecker &lt;URL&gt;\n</code></pre>\n\n<p>This will output the actual fields that are going to be indexed by the active indexing plugin (Solr/ES).</p>\n", "creation_date": 1462372711, "is_accepted": false, "score": 0, "last_activity_date": 1462372711, "answer_id": 37030969}], "question_id": 36984157, "tags": ["solr", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/36984157/nutch-not-indexing-specifig-teg-in-solr", "last_activity_date": 1462372711, "owner": {"user_id": 4543389, "answer_count": 0, "creation_date": 1423409794, "accept_rate": 25, "view_count": 13, "reputation": 13}, "body": "<p>i am using extractor plug-in. \n    <a href=\"https://github.com/BayanGroup/nutch-custom-search\" rel=\"nofollow\">https://github.com/BayanGroup/nutch-custom-search</a>\n    I follow mentioned step on github.\n    here is my configuration:\n    1)  extractors.xml\n    \n            \n    \n        \n            \n                \n                    \n                         title\" />\n                    \n              \n            \n        </p>\n\n<pre><code>2) nutch-site.xml\n&lt;property&gt;\n  &lt;name&gt;plugin.includes&lt;/name&gt;\n  &lt;value&gt;protocol-http|urlfilter-regex|parse-(text|html|metatags|msexcel|msword|mspowerpoint|pdf)|extractor|scoring-opic|index-(basic|anchor|more|metadata)|query-(basic|site|url|lang)|urlnormalizer-(pass|regex|basic)&lt;/value&gt;\n&lt;/property&gt;\n3)  added field in schema.xml of solr and nutch   &lt;field name=\"aakashtitle\" type=\"string\" stored=\"true\" indexed=\"true\" multiValued=\"true\"/&gt;\n4)I added plugin in parse-plugins.xml\nI am not getting any error but my data is not indexing in solr??\nplease help . and thanks!\n</code></pre>\n", "creation_date": 1462197206, "score": 0},
{"title": "how to access the inner html content with the css engine in extractor plugin for filtering process", "view_count": 56, "is_answered": false, "answers": [{"question_id": 27287456, "owner": {"user_id": 1881318, "link": "http://stackoverflow.com/users/1881318/tahagh", "user_type": "registered", "reputation": 619}, "body": "<p>Just use the \"text\" function. For instance if your html is look like this:</p>\n\n<pre><code>&lt;div class=\"target\"&gt;\n    Hello &lt;span&gt;World!&lt;/span&gt;\n&lt;/div&gt;\n</code></pre>\n\n<p>Then your extract-to rule is similar to this:</p>\n\n<pre><code>&lt;extract-to field=\"my-field\"&gt;\n   &lt;text&gt;\n       &lt;expr value=\".target\"/&gt;\n   &lt;/text&gt;\n&lt;/extract-to&gt;\n</code></pre>\n", "creation_date": 1417962335, "is_accepted": false, "score": 0, "last_activity_date": 1417962335, "answer_id": 27343626}], "question_id": 27287456, "tags": ["solr", "filtering", "nutch", "extractor"], "answer_count": 1, "link": "http://stackoverflow.com/questions/27287456/how-to-access-the-inner-html-content-with-the-css-engine-in-extractor-plugin-for", "last_activity_date": 1462359659, "owner": {"user_id": 4323084, "view_count": 0, "answer_count": 0, "creation_date": 1417672676, "reputation": 1}, "body": "<p>I have configured Apache Nutch , Solr with the extractor plug in for filtering of html content. how could i be able to access the inner div content with using css engine or xpath engine. \nThanks in advance.</p>\n", "creation_date": 1417673418, "score": 0},
{"title": "Nutch2.3.1 hangs while inject, parse fetch, generate", "view_count": 97, "is_answered": false, "question_id": 37024697, "tags": ["solr", "hbase", "nutch", "gora"], "answer_count": 0, "link": "http://stackoverflow.com/questions/37024697/nutch2-3-1-hangs-while-inject-parse-fetch-generate", "last_activity_date": 1462358594, "owner": {"user_id": 1162512, "answer_count": 27, "creation_date": 1327163131, "accept_rate": 58, "view_count": 772, "reputation": 2192}, "body": "<p>I've read various SO threads on why it takes so long (or hangs) while generating/injecting/parsing/fetching, but to no luck. The solutions in the following SO threads I've tried implementing, but no luck.</p>\n\n<p>1) <a href=\"http://stackoverflow.com/questions/23050000/nutch-2-1-urls-injection-takes-forever\">Nutch 2.1 urls injection takes forever</a></p>\n\n<p>2) <a href=\"http://stackoverflow.com/questions/22586950/nutch-2-2-1-doesnt-continue-after-injector-job?lq=1\">Nutch 2.2.1 doesnt continue after Injector job</a></p>\n\n<p>and various other threads.<br><br></p>\n\n<p>I'm using Nutch2.3.1 and HBase0.94.27. I've been following <a href=\"https://qbox.io/blog/scraping-the-web-with-nutch-for-elasticsearch\" rel=\"nofollow\">this</a> and <a href=\"https://wiki.apache.org/nutch/Nutch2Tutorial\" rel=\"nofollow\">this</a> tutorial and I was able to build successfully. But when I fire any nutch commands, it hangs up.</p>\n\n<p>Following are the logs I get while firing these commands:-</p>\n\n<p><strong>Inject Command</strong></p>\n\n<pre><code>root@ubuntu:~/apache-nutch-2.3.1/runtime/local# ./bin/nutch inject seed/urls.txt \nInjectorJob: starting at 2016-05-04 09:59:12\nInjectorJob: Injecting urlDir: seed/urls.txt\n</code></pre>\n\n<p><strong>Generate Command</strong></p>\n\n<pre><code>root@ubuntu:~/apache-nutch-2.3.1/runtime/local# bin/nutch generate -topN 40\nGeneratorJob: starting at 2016-05-04 09:54:08\nGeneratorJob: Selecting best-scoring urls due for fetch.\nGeneratorJob: starting\nGeneratorJob: filtering: true\nGeneratorJob: normalizing: true\nGeneratorJob: topN: 40\n</code></pre>\n\n<p><strong>Fetch command</strong></p>\n\n<pre><code>root@ubuntu:~/apache-nutch-2.3.1/runtime/local# bin/nutch fetch -all\nFetcherJob: starting at 2016-05-04 10:00:14\nFetcherJob: fetching all\nFetcherJob: threads: 10\nFetcherJob: parsing: false\nFetcherJob: resuming: false\nFetcherJob : timelimit set for : -1\n</code></pre>\n\n<p><strong>Parse Command</strong></p>\n\n<pre><code>root@ubuntu:~/apache-nutch-2.3.1/runtime/local# bin/nutch parse -all\nParserJob: starting at 2016-05-04 10:00:43\nParserJob: resuming:    false\nParserJob: forced reparse:      false\nParserJob: parsing all\n</code></pre>\n\n<p><strong>Update Command</strong></p>\n\n<pre><code>root@ubuntu:~/apache-nutch-2.3.1/runtime/local# bin/nutch updatedb -all\nDbUpdaterJob: starting at 2016-05-04 10:02:24\nDbUpdaterJob: updatinging all\n</code></pre>\n\n<p>Following is the HBase logs:-</p>\n\n<pre><code> client /0:0:0:0:0:0:0:1:45216\n2016-05-04 10:00:47,214 WARN org.apache.zookeeper.server.NIOServerCnxn: caught end of stream exception\nEndOfStreamException: Unable to read additional data from client sessionid 0x1547b2be4bc000e, likely client has closed socket\n        at org.apache.zookeeper.server.NIOServerCnxn.doIO(NIOServerCnxn.java:220)\n        at org.apache.zookeeper.server.NIOServerCnxnFactory.run(NIOServerCnxnFactory.java:208)\n        at java.lang.Thread.run(Thread.java:745)\n2016-05-04 10:00:47,215 INFO org.apache.zookeeper.server.NIOServerCnxn: Closed socket connection for client /0:0:0:0:0:0:0:1:45216 which had sessionid 0x1547b2be4bc000e\n2016-05-04 10:00:47,215 WARN org.apache.zookeeper.server.NIOServerCnxn: caught end of stream exception\nEndOfStreamException: Unable to read additional data from client sessionid 0x1547b2be4bc000d, likely client has closed socket\n        at org.apache.zookeeper.server.NIOServerCnxn.doIO(NIOServerCnxn.java:220)\n        at org.apache.zookeeper.server.NIOServerCnxnFactory.run(NIOServerCnxnFactory.java:208)\n        at java.lang.Thread.run(Thread.java:745)\n2016-05-04 10:00:47,216 INFO org.apache.zookeeper.server.NIOServerCnxn: Closed socket connection for client /127.0.0.1:59934 which had sessionid 0x1547b2be4bc000d\n2016-05-04 10:01:10,000 INFO org.apache.zookeeper.server.ZooKeeperServer: Expiring session 0x1547b2be4bc000c, timeout of 40000ms exceeded\n2016-05-04 10:01:10,001 INFO org.apache.zookeeper.server.PrepRequestProcessor: Processed session termination for sessionid: 0x1547b2be4bc000c\n2016-05-04 10:01:22,002 INFO org.apache.zookeeper.server.ZooKeeperServer: Expiring session 0x1547b2be4bc000b, timeout of 40000ms exceeded\n2016-05-04 10:01:22,003 INFO org.apache.zookeeper.server.PrepRequestProcessor: Processed session termination for sessionid: 0x1547b2be4bc000b\n2016-05-04 10:01:28,001 INFO org.apache.zookeeper.server.ZooKeeperServer: Expiring session 0x1547b2be4bc000e, timeout of 40000ms exceeded\n2016-05-04 10:01:28,001 INFO org.apache.zookeeper.server.ZooKeeperServer: Expiring session 0x1547b2be4bc000d, timeout of 40000ms exceeded\n2016-05-04 10:01:28,001 INFO org.apache.zookeeper.server.PrepRequestProcessor: Processed session termination for sessionid: 0x1547b2be4bc000e\n2016-05-04 10:01:28,001 INFO org.apache.zookeeper.server.PrepRequestProcessor: Processed session termination for sessionid: 0x1547b2be4bc000d\n2016-05-04 10:02:25,195 INFO org.apache.zookeeper.server.NIOServerCnxnFactory: Accepted socket connection from /127.0.0.1:59938\n2016-05-04 10:02:25,202 INFO org.apache.zookeeper.server.ZooKeeperServer: Client attempting to establish new session at /127.0.0.1:59938\n2016-05-04 10:02:25,204 INFO org.apache.zookeeper.server.ZooKeeperServer: Established session 0x1547b2be4bc000f with negotiated timeout 40000 for client /127.0.0.1:59938\n2016-05-04 10:02:25,822 INFO org.apache.zookeeper.server.NIOServerCnxnFactory: Accepted socket connection from /127.0.0.1:59940\n2016-05-04 10:02:25,822 INFO org.apache.zookeeper.server.ZooKeeperServer: Client attempting to establish new session at /127.0.0.1:59940\n2016-05-04 10:02:25,825 INFO org.apache.zookeeper.server.ZooKeeperServer: Established session 0x1547b2be4bc0010 with negotiated timeout 40000 for client /127.0.0.1:59940\n2016-05-04 10:04:15,530 DEBUG org.apache.hadoop.hbase.io.hfile.LruBlockCache: Stats: total=2.02 MB, free=243.82 MB, max=245.84 MB, blocks=3, accesses=27, hits=24, hitRatio=88.88%, , cachingAccesses=27, cachingHits=24, cachingHitsRatio=88.88%, , evictions=0, evicted=0, evictedPerRun=NaN\n2016-05-04 10:04:28,372 DEBUG org.apache.hadoop.hbase.client.MetaScanner: Scanning .META. starting at row= for max=2147483647 rows using org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation@25e5c862\n2016-05-04 10:04:28,379 DEBUG org.apache.hadoop.hbase.master.CatalogJanitor: Scanned 0 catalog row(s) and gc'd 0 unreferenced parent region(s)\n2016-05-04 10:09:15,530 DEBUG org.apache.hadoop.hbase.io.hfile.LruBlockCache: Stats: total=2.02 MB, free=243.82 MB, max=245.84 MB, blocks=3, accesses=27, hits=24, hitRatio=88.88%, , cachingAccesses=27, cachingHits=24, cachingHitsRatio=88.88%, , evictions=0, evicted=0, evictedPerRun=NaN\n</code></pre>\n\n<p><strong>Hadoop.log</strong></p>\n\n<pre><code>2016-05-04 10:42:18,132 INFO  crawl.InjectorJob - InjectorJob: starting at 2016-05-04 10:42:18\n2016-05-04 10:42:18,134 INFO  crawl.InjectorJob - InjectorJob: Injecting urlDir: seed/urls.txt\n2016-05-04 10:42:18,527 WARN  util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n</code></pre>\n\n<p>What exactly is the problem. I have configured everything correctly and it still hangs up. <strong>Why</strong></p>\n", "creation_date": 1462356700, "score": 1},
{"title": "error message of java.net.UnknownHostException for nutch 2.3", "view_count": 49, "is_answered": false, "answers": [{"question_id": 36801396, "owner": {"user_id": 1977773, "link": "http://stackoverflow.com/users/1977773/jorge-luis", "user_type": "registered", "reputation": 510}, "body": "<p>Usually this happens when Nutch can't resolve a given URL, does this happens with specific URLs? or when you run some specific Nutch commands?</p>\n", "creation_date": 1461620386, "is_accepted": false, "score": 0, "last_activity_date": 1461620386, "answer_id": 36851556}], "question_id": 36801396, "tags": ["nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/36801396/error-message-of-java-net-unknownhostexception-for-nutch-2-3", "last_activity_date": 1461620386, "owner": {"user_id": 5684146, "view_count": 4, "answer_count": 0, "creation_date": 1450218371, "reputation": 1}, "body": "<p>I am trying to crawl web data using nutch 2.3 on Linux Mint 17.2, but get the following error message\n\u201cFailed with the following error: java.net.UnknownHostException:\u201d\nI'd like to know what causes this error and how to resolve it. My observation is that this error sometimes comes with another error message \"Couldn't get robots.txt.\"  What are the relationships between these two errors and how to solve this? Thanks.</p>\n", "creation_date": 1461351369, "score": 0},
{"title": "Nutch 2.3.1 seeding with rest API", "view_count": 34, "is_answered": false, "question_id": 36846251, "tags": ["rest", "nutch", "seeding"], "answer_count": 0, "link": "http://stackoverflow.com/questions/36846251/nutch-2-3-1-seeding-with-rest-api", "last_activity_date": 1461602185, "owner": {"user_id": 5206685, "view_count": 0, "answer_count": 1, "creation_date": 1439055390, "reputation": 36}, "body": "<p>So I've got a Nutch 2.3.1 install, I don't think it matters but it's using Hbase and Elastic search.</p>\n\n<p>I'm trying to use the rest api to insert seeds and have nutch server running on port 8081. If I sent a get /admin I will recieve a response to the server is running but when I try to do /seed/create with the payload\n{\n   \"id\":\"12345\",\n   \"name\":\"test\",\n   \"seedUrls\":[\n      {\n         \"id\":1,\n         \"seedList\":null,\n         \"url\":\"<a href=\"http://test.website.co.uk\" rel=\"nofollow\">http://test.website.co.uk</a>\"\n      }\n   ]\n}</p>\n\n<p>I get an error 500. I've not been able to find any alternative syntax online that works, if anyone can help it would be much appreciated.</p>\n", "creation_date": 1461602185, "score": 0},
{"title": "nutch 2.3 becomes slow for fetching", "view_count": 18, "is_answered": false, "question_id": 36801610, "tags": ["nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/36801610/nutch-2-3-becomes-slow-for-fetching", "last_activity_date": 1461352185, "owner": {"user_id": 5684146, "view_count": 4, "answer_count": 0, "creation_date": 1450218371, "reputation": 1}, "body": "<p>I am trying to crawl web data using nutch 2.3 on Linux Mint 17.2, but get the following message during fetching, and it slows down the process.\nIt seems that some thread for fetching job is hung, and eventually nutch aborts 1 hung thread. I am wondering what causes this, and can I configure any setting to abort the thread after certain period of time and resume in the future if this behavior happens? </p>\n\n<p>2016-04-21 18:45:32,378 INFO  fetcher.FetcherJob - 0/1 spinwaiting/active, 47 pages, 4 errors, 0.2 0 pages/s, 201 0 kb/s, 0 URLs in 1 queues\n2016-04-21 18:45:37,378 INFO  fetcher.FetcherJob - 0/1 spinwaiting/active, 47 pages, 4 errors, 0.2 0 pages/s, 197 0 kb/s, 0 URLs in 1 queues\n2016-04-21 18:45:42,378 INFO  fetcher.FetcherJob - 0/1 spinwaiting/active, 47 pages, 4 errors, 0.2 0 pages/s, 194 0 kb/s, 0 URLs in 1 queues\n2016-04-21 18:45:47,379 INFO  fetcher.FetcherJob - 0/1 spinwaiting/active, 47 pages, 4 errors, 0.2 0 pages/s, 190 0 kb/s, 0 URLs in 1 queues\n2016-04-21 18:45:52,379 INFO  fetcher.FetcherJob - 0/1 spinwaiting/active, 47 pages, 4 errors, 0.2 0 pages/s, 187 0 kb/s, 0 URLs in 1 queues\n2016-04-21 18:45:57,379 INFO  fetcher.FetcherJob - 0/1 spinwaiting/active, 47 pages, 4 errors, 0.2 0 pages/s, 183 0 kb/s, 0 URLs in 1 queues\n2016-04-21 18:46:02,380 INFO  fetcher.FetcherJob - 0/1 spinwaiting/active, 47 pages, 4 errors, 0.2 0 pages/s, 180 0 kb/s, 0 URLs in 1 queues\n2016-04-21 18:46:07,380 INFO  fetcher.FetcherJob - 0/1 spinwaiting/active, 47 pages, 4 errors, 0.2 0 pages/s, 177 0 kb/s, 0 URLs in 1 queues\n2016-04-21 18:46:12,380 INFO  fetcher.FetcherJob - 0/1 spinwaiting/active, 47 pages, 4 errors, 0.2 0 pages/s, 174 0 kb/s, 0 URLs in 1 queues\n2016-04-21 18:46:17,380 INFO  fetcher.FetcherJob - 0/1 spinwaiting/active, 47 pages, 4 errors, 0.2 0 pages/s, 171 0 kb/s, 0 URLs in 1 queues\n2016-04-21 18:46:22,381 INFO  fetcher.FetcherJob - 0/1 spinwaiting/active, 47 pages, 4 errors, 0.2 0 pages/s, 169 0 kb/s, 0 URLs in 1 queues\n2016-04-21 18:46:27,381 INFO  fetcher.FetcherJob - 0/1 spinwaiting/active, 47 pages, 4 errors, 0.1 0 pages/s, 166 0 kb/s, 0 URLs in 1 queues\n2016-04-21 18:46:32,381 INFO  fetcher.FetcherJob - 0/1 spinwaiting/active, 47 pages, 4 errors, 0.1 0 pages/s, 163 0 kb/s, 0 URLs in 1 queues\n2016-04-21 18:46:32,381 WARN  fetcher.FetcherJob - Aborting with 1 hung threads.\n2016-04-21 18:46:32,381 WARN  fetcher.FetcherJob - Thread #9 hung while processing </p>\n", "creation_date": 1461352185, "score": 0},
{"title": "How to update the fetch status in crawldb in apache nutch?", "view_count": 49, "is_answered": false, "answers": [{"question_id": 36673745, "owner": {"user_id": 5905283, "link": "http://stackoverflow.com/users/5905283/kap", "user_type": "registered", "reputation": 5}, "body": "<p>I found answer to my question and wanted to share with you all. After fetching two rounds I have updated the db with command 'bin/nutch updatedb crawl/crawldb $s2'. Then the db will be updated with new urls and with status as 'unfetched'. But if do 'bin/nutch updatedb crawl/crawldb $s2 -noAdditions', it will  not add new urls to the db and make already existing urls status as 'fetched'.</p>\n", "creation_date": 1461347353, "is_accepted": false, "score": 0, "last_activity_date": 1461347353, "answer_id": 36800302}], "question_id": 36673745, "tags": ["apache", "web-crawler", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/36673745/how-to-update-the-fetch-status-in-crawldb-in-apache-nutch", "last_activity_date": 1461347353, "owner": {"age": 24, "answer_count": 1, "creation_date": 1455050559, "user_id": 5905283, "view_count": 1, "location": "Dallas, TX, United States", "reputation": 5}, "body": "<p>I did web crawling using apache nutch..... I have fetched for two rounds. It generated a crawl db containg 21 urls as fetched status and 537 url as unfetched status. I want to update the status of all the links in crawldb as fetched for some reason. Is there any way to update the status?</p>\n", "creation_date": 1460877595, "score": 0},
{"title": "Crawling video with apache nutch", "view_count": 79, "owner": {"user_id": 6225642, "view_count": 2, "answer_count": 1, "creation_date": 1461078425, "reputation": 11}, "is_answered": true, "answers": [{"question_id": 36722755, "owner": {"user_id": 3147184, "link": "http://stackoverflow.com/users/3147184/rocksta", "user_type": "registered", "reputation": 37}, "body": "<p>You need to insert this in <strong>parse-plugins.xml.</strong></p>\n\n<pre><code>&lt;mimeType name=\"video/mp4\"&gt;\n    &lt;plugin id=\"parse-tika\" /&gt;\n&lt;/mimeType&gt;\n\n&lt;mimeType name=\"video/ogg\"&gt;\n    &lt;plugin id=\"parse-tika\" /&gt;\n&lt;/mimeType&gt;\n</code></pre>\n\n<p>And add parse-tika in plugin includes property of <strong>nutch-site.xml</strong>.</p>\n\n<pre><code>&lt;property&gt;\n        &lt;name&gt;plugin.includes&lt;/name&gt;\n        &lt;value&gt;protocol-http|urlfilter-regex|parse-(text|html|tika)|index-(basic|anchor)|query-(basic|site|url)|response-(json|xml)|summary-basic|urlnormalizer-(pass|regex|basic)&lt;/value&gt;\n\n    &lt;/property&gt;\n</code></pre>\n", "creation_date": 1461147620, "is_accepted": false, "score": 0, "last_activity_date": 1461147620, "answer_id": 36740795}, {"question_id": 36722755, "owner": {"user_id": 6225642, "link": "http://stackoverflow.com/users/6225642/nana", "user_type": "registered", "reputation": 11}, "body": "<p>I've already fix this problem by adding source tag on plugin parse-html (DOMContentUtils.java)</p>\n\n<pre><code>linkParams.put(\"frame\", new LinkParams(\"frame\", \"src\", 0));\nlinkParams.put(\"iframe\", new LinkParams(\"iframe\", \"src\", 0));\nlinkParams.put(\"script\", new LinkParams(\"script\", \"src\", 0));\nlinkParams.put(\"link\", new LinkParams(\"link\", \"href\", 0));\nlinkParams.put(\"img\", new LinkParams(\"img\", \"src\", 0));\nlinkParams.put(\"source\", new LinkParams(\"source\", \"src\", 0))\n</code></pre>\n\n<p>then rebuild with ant.</p>\n\n<p>hope its helpful for the other</p>\n", "creation_date": 1461213732, "is_accepted": true, "score": 1, "last_activity_date": 1461213732, "answer_id": 36759831}], "question_id": 36722755, "tags": ["apache", "hadoop", "web-crawler", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/36722755/crawling-video-with-apache-nutch", "last_activity_date": 1461213732, "accepted_answer_id": 36759831, "body": "<p>How to fetch video tag like this with Apache Nutch :</p>\n\n<pre><code>&lt;video width=\"320\" height=\"240\" controls&gt;\n  &lt;source src=\"video/video.mp4\" type=\"video/mp4\"&gt;\n  &lt;source src=\"video/video.ogg\" type=\"video/ogg\"&gt;\n  Your browser does not support the video tag.\n&lt;/video&gt;\n</code></pre>\n\n<p>Apache nutch can fetch image tag but didn't work in video source. Can anyone guide me on this?</p>\n\n<p>Thanks for helping</p>\n", "creation_date": 1461079395, "score": 0},
{"title": "Can nutch crawl product detail through api interface?is suitable?", "view_count": 25, "is_answered": false, "question_id": 36758824, "tags": ["nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/36758824/can-nutch-crawl-product-detail-through-api-interfaceis-suitable", "last_activity_date": 1461211989, "owner": {"user_id": 6233253, "view_count": 0, "answer_count": 0, "creation_date": 1461206720, "reputation": 1}, "body": "<p>Now, I want to crawl tour products through restfull api\uff1a a product list api \uff08which is pageable\uff09 AND a product detail api .</p>\n\n<p>If  I  invoke product list api with http protocol\uff0cand find all product IDS\uff0cand invoke product detail api with product id. But I want to do it with nutch,\nI don't how to do</p>\n", "creation_date": 1461207660, "score": 0},
{"title": "Dump Command in Apache Nutch 2.x", "view_count": 27, "is_answered": false, "question_id": 36759050, "tags": ["apache", "web-crawler", "nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/36759050/dump-command-in-apache-nutch-2-x", "last_activity_date": 1461209186, "owner": {"user_id": 6225642, "view_count": 2, "answer_count": 1, "creation_date": 1461078425, "reputation": 11}, "body": "<p>I use Nutch 2.3.1 with HBase, how to find the command like this on nutch version 2.3.1 :</p>\n\n<pre><code>bin/nutch dump -outputDir DATA_DUMP -segment TestCrawl/segments -mimetype image/jpeg\n</code></pre>\n\n<p>This dump command worked in Apache Nutch version 1.x and return the image file. </p>\n\n<p>Thanks for Helping.</p>\n", "creation_date": 1461209186, "score": 0},
{"title": "Deduplication in nutch 1.11 for sites having same content and different URI", "view_count": 26, "owner": {"user_id": 3147184, "answer_count": 5, "creation_date": 1388426076, "view_count": 1, "location": "Mumbai, Maharashtra, India", "reputation": 37}, "is_answered": true, "answers": [{"last_edit_date": 1461107216, "owner": {"user_id": 1977773, "link": "http://stackoverflow.com/users/1977773/jorge-luis", "user_type": "registered", "reputation": 510}, "body": "<p>By default Nutch use the <code>org.apache.nutch.crawl.MD5Signature</code> class to calculate the digest of an URL, this class calculates the digest using the <code>MD5Hash</code> function of the raw binary content of the page, if no content is found then the URL is used. </p>\n\n<p>The <code>DeduplicationJob</code> first groups fetched URLs by the digest (in your case both URLs should have the same signature/digest) and marks all the URLs as duplicated, except the one with the highest score, if both (or more) URLs have the same digest and the same score, then the one with the latest timestamp is used instead. </p>\n\n<p>In your particular case I think that a custom implementation of <code>org.apache.nutch.crawl.Signature</code> that takes into account <strong>both</strong> parameters (the URL and the text/raw content) should solve your problem. This is required because the existing signature implementations <code>MD5Signature</code>, <code>TextMD5Signature</code> and <code>TextProfileSignature</code> will only consider the URL if no text/raw content is found for a given URL. Nevertheless the implementation should be pretty straightforward. </p>\n\n<p>Keep in mind that this could generate more duplicated URLs in your index.</p>\n", "question_id": 36722466, "creation_date": 1461091940, "is_accepted": true, "score": 0, "last_activity_date": 1461107216, "answer_id": 36727055}], "question_id": 36722466, "tags": ["nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/36722466/deduplication-in-nutch-1-11-for-sites-having-same-content-and-different-uri", "last_activity_date": 1461107216, "accepted_answer_id": 36727055, "body": "<p>I am using nutch to crawl over intranet sites. I came across a scenario in which 2 sites have same content (compared the view source of both sites using notepad++) but different URI.\ne.g.</p>\n\n<pre><code>http://site_name.domain_name.com/a/b/c/index.html\nhttp://site_name.domain_name.com/x/y/z/index.html\n</code></pre>\n\n<p>Nutch is indexing either of them but not the both. </p>\n\n<p>How to alter this behavior of nutch and index both of the URLs ?</p>\n", "creation_date": 1461078686, "score": 0},
{"title": "Nutch and Elasticsearch", "view_count": 188, "owner": {"user_id": 3125823, "answer_count": 11, "creation_date": 1387649781, "accept_rate": 78, "view_count": 68, "reputation": 286}, "is_answered": true, "answers": [{"question_id": 36703483, "owner": {"user_id": 1977773, "link": "http://stackoverflow.com/users/1977773/jorge-luis", "user_type": "registered", "reputation": 510}, "body": "<p>I haven't used Nutch along with ES 1.5/1.6/1.7 but there shouldn't be major changes between the API used by the <code>indexer-elastic</code> plugin. I've just followed the instructions in <a href=\"https://github.com/apache/nutch/blob/master/src/plugin/indexer-elastic/howto_upgrade_es.txt\" rel=\"nofollow\">https://github.com/apache/nutch/blob/master/src/plugin/indexer-elastic/howto_upgrade_es.txt</a> and built/tested (<code>ant test</code>) Nutch 1.11 with ES 1.7.2 without any troubles. This means, that the code built ok, but I have not tested <strong>indexing actual data</strong> into Elasticsearch. </p>\n\n<p>You'll have to build your own Nutch distribution, I do recommend keep your elasticsearch client version in sync with the ES server version. </p>\n", "creation_date": 1461077068, "is_accepted": true, "score": 1, "last_activity_date": 1461077068, "answer_id": 36721758}], "question_id": 36703483, "tags": ["elasticsearch", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/36703483/nutch-and-elasticsearch", "last_activity_date": 1461077068, "accepted_answer_id": 36721758, "body": "<p>I'm building a small search app using Elasticsearch, AngularJS and Nutch. I pretty much have the ES and AngularJS part complete. Now its time for the Nutch and ES part, using Nutch to crawl AND index the data into ES. I have been using Nutch 1.10 with ES 1.4. I've been using Nutch v1.10 to do some initial small crawls of about (~50 sites) on my local machine. I now see that ES is up to v2.3 or something like that and it seems that Nutch v1.11 STILL uses ES v1.4.</p>\n\n<p>Does anyone have any experience with Nutch v1.10 working with any version of ES greater than 1.4 (maybe ES v1.5 or v1.7)?</p>\n\n<p>I'd like to stay with the Nutch 1.x branch if possible.</p>\n", "creation_date": 1461010577, "score": 0},
{"title": "Apache nutch compatibility with ubuntu", "view_count": 40, "is_answered": false, "answers": [{"question_id": 36715770, "owner": {"user_id": 1977773, "link": "http://stackoverflow.com/users/1977773/jorge-luis", "user_type": "registered", "reputation": 510}, "body": "<p>Any version of Ubuntu should play fine with Nutch, you should just need a running JVM. In my experience, usually installing the JRE/JDK and setting the <code>JAVA_HOME</code> environment variable in the shell with the right value is enough to get you running Nutch in local mode. </p>\n", "creation_date": 1461075245, "is_accepted": false, "score": 0, "last_activity_date": 1461075245, "answer_id": 36720979}], "question_id": 36715770, "tags": ["apache", "ubuntu", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/36715770/apache-nutch-compatibility-with-ubuntu", "last_activity_date": 1461075245, "owner": {"user_id": 3975430, "answer_count": 3, "creation_date": 1408970768, "accept_rate": 50, "view_count": 19, "reputation": 166}, "body": "<p>I am setting up system for Apache Nutch, but i am stuck with operating system. I am confused which version of ubuntu is compatible with Apache Nutch 2.3.1.</p>\n\n<p>Any help will be appreciated.</p>\n", "creation_date": 1461061879, "score": 0},
{"title": "Are example.com/ and example.com/index.html considered same in case of nutch 1.11?", "view_count": 23, "owner": {"user_id": 3147184, "answer_count": 5, "creation_date": 1388426076, "view_count": 1, "location": "Mumbai, Maharashtra, India", "reputation": 37}, "is_answered": true, "answers": [{"question_id": 36698330, "owner": {"user_id": 5953351, "link": "http://stackoverflow.com/users/5953351/sebastian-nagel", "user_type": "registered", "reputation": 121}, "body": "<p>Nutch 1.11 will crawl and index both example.com and example.com/index.html, given that</p>\n\n<ol>\n<li>both are included in seeds or reachable via links from one of the seeds</li>\n<li>URL normalization or filter rules accept both and do not normalize one</li>\n<li>they are no duplicates (identical content)</li>\n<li>both of them are real pages and no redirects</li>\n</ol>\n\n<p>Regarding 2: there is a rule in regex-normalize.xml which does the described normalization. By default it's not active (commented out):</p>\n\n<pre><code>&lt;!-- changes default pages into standard for /index.html, etc. into /\n&lt;regex&gt;\n  &lt;pattern&gt;/((?i)index|default)\\.((?i)js[pf]{1}?[afx]?|cgi|cfm|asp[x]?|[psx]?htm[l]?|php[3456]?)(\\?|&amp;amp;|#|$)&lt;/pattern&gt;\n  &lt;substitution&gt;/$3&lt;/substitution&gt;\n&lt;/regex&gt; --&gt;\n</code></pre>\n\n<p>Regarding 3: deduplication has been significantly improved for Nutch 1.8 and is now no operation on the index but flags duplicates directly in CrawlDb. However, you should see in the logs that both URLs are fetched, duplication is done later based on the checksum of the fetched content.</p>\n", "creation_date": 1461047320, "is_accepted": true, "score": 1, "last_activity_date": 1461047320, "answer_id": 36710224}], "question_id": 36698330, "tags": ["nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/36698330/are-example-com-and-example-com-index-html-considered-same-in-case-of-nutch-1-1", "last_activity_date": 1461047320, "accepted_answer_id": 36710224, "body": "<p>I have upgraded my application to nutch 1.11 from nutch 1.3. Previously I used to get 2 urls example.com/ and example.com/index.html while crawling through nutch 1.3.</p>\n\n<p>But after upgradation i have either of two. I want to confirm that Is upgraded nutch is smart enough to detect this ?</p>\n", "creation_date": 1460993344, "score": 0},
{"title": "how to set nutch to extract content of only urls present on seed file", "view_count": 75, "is_answered": false, "answers": [{"last_edit_date": 1460976466, "owner": {"user_id": 3147184, "link": "http://stackoverflow.com/users/3147184/rocksta", "user_type": "registered", "reputation": 37}, "body": "<p>You might keep the iteration of the crawl command as \"1\" and then nutch will crawl only the urls present in seed.txt file. </p>\n\n<p>e.g. </p>\n\n<pre><code>bin/crawl -i -D solr.server.url=&lt;solrUrl&gt; &lt;seed-dir&gt; &lt;crawl-dir&gt; 1\n</code></pre>\n\n<p>Also, you can restrict the outer links by configuring your <strong>regex-urlfilter.txt</strong> present in conf directory.</p>\n\n<pre><code>#accept anything else\n+http://doamin.com\n</code></pre>\n", "question_id": 36681928, "creation_date": 1460975756, "is_accepted": false, "score": 0, "last_activity_date": 1460976466, "answer_id": 36691542}], "question_id": 36681928, "tags": ["apache", "web-crawler", "hbase", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/36681928/how-to-set-nutch-to-extract-content-of-only-urls-present-on-seed-file", "last_activity_date": 1460976466, "owner": {"user_id": 3832389, "answer_count": 11, "creation_date": 1405173693, "accept_rate": 45, "view_count": 63, "reputation": 339}, "body": "<p>I am using nutch 2.3 and I am trying to get the html content of some urls present on seed.txt file which I pass to nutch into HBase.</p>\n\n<p>So the problem is as below---</p>\n\n<p>First crawl:\nEverything runs fine and I get the data into HBase with url as the row key.</p>\n\n<p>Second Run:\nwhen i run the crawl for second time with different urls I see there are so many urls for the fetching job is running while I have only one url in my seed file.</p>\n\n<p>So my question is how can make sure that nutch <strong>only</strong> crawls and get the html contents of the urls present in seed.txt and not the out links present in urls html content of seed.txt</p>\n", "creation_date": 1460925328, "score": 0},
{"title": "What is causing YARN to kill my container?", "view_count": 136, "is_answered": false, "question_id": 36657992, "tags": ["java", "hadoop", "jvm", "yarn", "nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/36657992/what-is-causing-yarn-to-kill-my-container", "last_activity_date": 1460760816, "owner": {"user_id": 1689220, "answer_count": 7, "creation_date": 1348240517, "accept_rate": 70, "view_count": 26, "location": "Maryland", "reputation": 323}, "body": "<p>I am using Nutch in distributed (Hadoop mode)</p>\n\n<p>Hadoop version 2.7.1, Nutch 1.11, Oracle Java 1.8.0_71.</p>\n\n<p>Many of my YARN containers are killed less than 20% through the map of the Fetch phase due to over-subscription of physical memory. Here is an excerpt from the runtime output:</p>\n\n<pre><code>16/04/15 18:00:45 INFO mapreduce.Job:  map 18% reduce 0%\n16/04/15 18:02:55 INFO mapreduce.Job:  map 19% reduce 0%\n16/04/15 18:04:42 INFO mapreduce.Job:  map 22% reduce 0%\n16/04/15 18:04:42 INFO mapreduce.Job: Task Id : attempt_1460755517014_0001_m_000002_0, Status : FAILED\nContainer [pid=22132,containerID=container_1460755517014_0001_01_000004] is running beyond physical memory limits. Current usage: 6.7 GB of 2.9 GB physical memory used; 18.4 GB of 11.7 GB virtual memory used. Killing container.\nDump of the process-tree for container_1460755517014_0001_01_000004 :\n        |- PID PPID PGRPID SESSID CMD_NAME USER_MODE_TIME(MILLIS) SYSTEM_TIME(MILLIS) VMEM_USAGE(BYTES) RSSMEM_USAGE(PAGES) FULL_CMD_LINE\n        |- 22132 22130 22132 22132 (bash) 0 0 115806208 693 /bin/bash -c /usr/java/latest/bin/java -Djava.net.preferIPv4Stack=true -Dhadoop.metrics.log.level=WARN  -Xmx896m -XX:+PrintGC -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -Xloggc:/mnt/private_search/tmp/gc.log -Djava.io.tmpdir=/mnt/private_search/yarn/tmp/nm-local-dir/usercache/hadoop/appcache/application_1460755517014_0001/container_1460755517014_0001_01_000004/tmp -Dlog4j.configuration=container-log4j.properties -Dyarn.app.container.log.dir=/usr/local/hadoop/logs/userlogs/application_1460755517014_0001/container_1460755517014_0001_01_000004 -Dyarn.app.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA -Dhadoop.root.logfile=syslog org.apache.hadoop.mapred.YarnChild 10.0.3.18 43850 attempt_1460755517014_0001_m_000002_0 4 1&gt;/usr/local/hadoop/logs/userlogs/application_1460755517014_0001/container_1460755517014_0001_01_000004/stdout 2&gt;/usr/local/hadoop/logs/userlogs/application_1460755517014_0001/container_1460755517014_0001_01_000004/stderr\n        |- 29317 22140 22132 22132 (xz) 3 0 212541440 5003 xz -c -\n        |- 29200 22140 22132 22132 (xz) 12 0 212541440 5993 xz -c -\n        |- 29397 22140 22132 22132 (java) 0 0 2981883904 285413 /usr/java/latest/bin/java -Djava.net.preferIPv4Stack=true -Dhadoop.metrics.log.level=WARN -Xmx896m -XX:+PrintGC -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -Xloggc:/mnt/private_search/tmp/gc.log -Djava.io.tmpdir=/mnt/private_search/yarn/tmp/nm-local-dir/usercache/hadoop/appcache/application_1460755517014_0001/container_1460755517014_0001_01_000004/tmp -Dlog4j.configuration=container-log4j.properties -Dyarn.app.container.log.dir=/usr/local/hadoop/logs/userlogs/application_1460755517014_0001/container_1460755517014_0001_01_000004 -Dyarn.app.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA -Dhadoop.root.logfile=syslog org.apache.hadoop.mapred.YarnChild 10.0.3.18 43850 attempt_1460755517014_0001_m_000002_0 4\n        |- 29375 22140 22132 22132 (xz) 0 0 212541440 4709 xz -c -\n        |- 29396 22140 22132 22132 (java) 0 0 2981883904 285413 /usr/java/latest/bin/java -Djava.net.preferIPv4Stack=true -Dhadoop.metrics.log.level=WARN -Xmx896m -XX:+PrintGC -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -Xloggc:/mnt/private_search/tmp/gc.log -Djava.io.tmpdir=/mnt/private_search/yarn/tmp/nm-local-dir/usercache/hadoop/appcache/application_1460755517014_0001/container_1460755517014_0001_01_000004/tmp -Dlog4j.configuration=container-log4j.properties -Dyarn.app.container.log.dir=/usr/local/hadoop/logs/userlogs/application_1460755517014_0001/container_1460755517014_0001_01_000004 -Dyarn.app.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA -Dhadoop.root.logfile=syslog org.apache.hadoop.mapred.YarnChild 10.0.3.18 43850 attempt_1460755517014_0001_m_000002_0 4\n        |- 29374 22140 22132 22132 (xz) 0 0 212541440 4708 xz -c -\n        |- 29395 22140 22132 22132 (java) 0 0 2981883904 285413 /usr/java/latest/bin/java -Djava.net.preferIPv4Stack=true -Dhadoop.metrics.log.level=WARN -Xmx896m -XX:+PrintGC -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -Xloggc:/mnt/private_search/tmp/gc.log -Djava.io.tmpdir=/mnt/private_search/yarn/tmp/nm-local-dir/usercache/hadoop/appcache/application_1460755517014_0001/container_1460755517014_0001_01_000004/tmp -Dlog4j.configuration=container-log4j.properties -Dyarn.app.container.log.dir=/usr/local/hadoop/logs/userlogs/application_1460755517014_0001/container_1460755517014_0001_01_000004 -Dyarn.app.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA -Dhadoop.root.logfile=syslog org.apache.hadoop.mapred.YarnChild 10.0.3.18 43850 attempt_1460755517014_0001_m_000002_0 4\n        |- 22140 22132 22132 22132 (java) 258828 8610 2981883904 285413 /usr/java/latest/bin/java -Djava.net.preferIPv4Stack=true -Dhadoop.metrics.log.level=WARN -Xmx896m -XX:+PrintGC -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -Xloggc:/mnt/private_search/tmp/gc.log -Djava.io.tmpdir=/mnt/private_search/yarn/tmp/nm-local-dir/usercache/hadoop/appcache/application_1460755517014_0001/container_1460755517014_0001_01_000004/tmp -Dlog4j.configuration=container-log4j.properties -Dyarn.app.container.log.dir=/usr/local/hadoop/logs/userlogs/application_1460755517014_0001/container_1460755517014_0001_01_000004 -Dyarn.app.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA -Dhadoop.root.logfile=syslog org.apache.hadoop.mapred.YarnChild 10.0.3.18 43850 attempt_1460755517014_0001_m_000002_0 4\n        |- 29345 22140 22132 22132 (xz) 2 0 212541440 4801 xz -c -\n        |- 29390 22140 22132 22132 (xz) 0 0 212541440 1556 xz -c -\n        |- 29394 22140 22132 22132 (xz) 0 0 212541440 899 xz -c -\n        |- 29371 22140 22132 22132 (xz) 0 1 212541440 4717 xz -c -\n        |- 29392 22140 22132 22132 (java) 0 0 2981883904 285413 /usr/java/latest/bin/java -Djava.net.preferIPv4Stack=true -Dhadoop.metrics.log.level=WARN -Xmx896m -XX:+PrintGC -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -Xloggc:/mnt/private_search/tmp/gc.log -Djava.io.tmpdir=/mnt/private_search/yarn/tmp/nm-local-dir/usercache/hadoop/appcache/application_1460755517014_0001/container_1460755517014_0001_01_000004/tmp -Dlog4j.configuration=container-log4j.properties -Dyarn.app.container.log.dir=/usr/local/hadoop/logs/userlogs/application_1460755517014_0001/container_1460755517014_0001_01_000004 -Dyarn.app.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA -Dhadoop.root.logfile=syslog org.apache.hadoop.mapred.YarnChild 10.0.3.18 43850 attempt_1460755517014_0001_m_000002_0 4\n        |- 29391 22140 22132 22132 (java) 0 0 2981883904 285413 /usr/java/latest/bin/java -Djava.net.preferIPv4Stack=true -Dhadoop.metrics.log.level=WARN -Xmx896m -XX:+PrintGC -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -Xloggc:/mnt/private_search/tmp/gc.log -Djava.io.tmpdir=/mnt/private_search/yarn/tmp/nm-local-dir/usercache/hadoop/appcache/application_1460755517014_0001/container_1460755517014_0001_01_000004/tmp -Dlog4j.configuration=container-log4j.properties -Dyarn.app.container.log.dir=/usr/local/hadoop/logs/userlogs/application_1460755517014_0001/container_1460755517014_0001_01_000004 -Dyarn.app.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA -Dhadoop.root.logfile=syslog org.apache.hadoop.mapred.YarnChild 10.0.3.18 43850 attempt_1460755517014_0001_m_000002_0 4\n\nContainer killed on request. Exit code is 143\nContainer exited with a non-zero exit code 143\n\n16/04/15 18:04:43 INFO mapreduce.Job:  map 18% reduce 0%\n16/04/15 18:05:02 INFO mapreduce.Job:  map 19% reduce 0%\n16/04/15 18:07:18 INFO mapreduce.Job:  map 20% reduce 0%\n</code></pre>\n\n<p>You can see that my fetch phase spawns a number of <code>xz</code> processes, but I've done the math and each is using less than 6000 pages (4KB each). I don't understand what the child JVMs are, or why they're all using about 1 GB RAM. You can see that all <code>xz</code> processes and the extra JVMs are children of the JVM with PID 22140.</p>\n\n<p>To clarify, here is a similar excerpt that shows an external process using far too much memory, causing YARN to kill the container, however there are no child JVMs and the cause is very obvious (the process named has been redacted):</p>\n\n<pre><code>16/04/15 17:47:06 INFO mapreduce.Job:  map 13% reduce 0%\n16/04/15 17:48:49 INFO mapreduce.Job: Task Id : attempt_1460755517014_0001_m_000009_0, Status : FAILED\nContainer [pid=19924,containerID=container_1460755517014_0001_01_000011] is running beyond physical memory limits. Current usage: 3.2 GB of 2.9 GB physical memory used; 6.4 GB of 11.7 GB virtual memory used. Killing container.\nDump of the process-tree for container_1460755517014_0001_01_000011 :\n        |- PID PPID PGRPID SESSID CMD_NAME USER_MODE_TIME(MILLIS) SYSTEM_TIME(MILLIS) VMEM_USAGE(BYTES) RSSMEM_USAGE(PAGES) FULL_CMD_LINE\n        |- 32720 19932 19924 19924 (xz) 0 0 212541440 4716 xz -c -\n        |- 32710 19932 19924 19924 (xz) 0 0 212541440 4731 xz -c -\n        |- 32620 19932 19924 19924 (xz) 3 0 212541440 5010 xz -c -\n        |- 31144 19932 19924 19924 (redacted) 129 112 2305589248 529068 redacted\n        |- 32716 19932 19924 19924 (xz) 0 0 212541440 4719 xz -c -\n        |- 32724 19932 19924 19924 (xz) 1 1 212541440 4802 xz -c -\n        |- 32713 19932 19924 19924 (xz) 0 0 212541440 4732 xz -c -\n        |- 19932 19924 19924 19924 (java) 153921 5177 2994696192 264563 /usr/java/latest/bin/java -Djava.net.preferIPv4Stack=true -Dhadoop.metrics.log.level=WARN -Xmx896m -XX:+PrintGC -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -Xloggc:/mnt/private_search/tmp/gc.log -Djava.io.tmpdir=/mnt/private_search/yarn/tmp/nm-local-dir/usercache/hadoop/appcache/application_1460755517014_0001/container_1460755517014_0001_01_000011/tmp -Dlog4j.configuration=container-log4j.properties -Dyarn.app.container.log.dir=/usr/local/hadoop/logs/userlogs/application_1460755517014_0001/container_1460755517014_0001_01_000011 -Dyarn.app.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA -Dhadoop.root.logfile=syslog org.apache.hadoop.mapred.YarnChild 10.0.3.18 43850 attempt_1460755517014_0001_m_000009_0 11\n        |- 19924 19921 19924 19924 (bash) 0 0 115806208 669 /bin/bash -c /usr/java/latest/bin/java -Djava.net.preferIPv4Stack=true -Dhadoop.metrics.log.level=WARN  -Xmx896m -XX:+PrintGC -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -Xloggc:/mnt/private_search/tmp/gc.log -Djava.io.tmpdir=/mnt/private_search/yarn/tmp/nm-local-dir/usercache/hadoop/appcache/application_1460755517014_0001/container_1460755517014_0001_01_000011/tmp -Dlog4j.configuration=container-log4j.properties -Dyarn.app.container.log.dir=/usr/local/hadoop/logs/userlogs/application_1460755517014_0001/container_1460755517014_0001_01_000011 -Dyarn.app.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA -Dhadoop.root.logfile=syslog org.apache.hadoop.mapred.YarnChild 10.0.3.18 43850 attempt_1460755517014_0001_m_000009_0 11 1&gt;/usr/local/hadoop/logs/userlogs/application_1460755517014_0001/container_1460755517014_0001_01_000011/stdout 2&gt;/usr/local/hadoop/logs/userlogs/application_1460755517014_0001/container_1460755517014_0001_01_000011/stderr\n        |- 32679 19932 19924 19924 (xz) 3 0 212541440 4935 xz -c -\n\nContainer killed on request. Exit code is 143\nContainer exited with a non-zero exit code 143\n\n16/04/15 17:50:27 INFO mapreduce.Job:  map 14% reduce 0%\n</code></pre>\n\n<p>The NodeManager log shows nothing useful:</p>\n\n<pre><code>2016-04-15 18:04:37,925 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 22132 for container-id container_1460755517014_0001_01_000004: 3.3 GB of 2.9 GB physical memory used; 8.4 GB of 11.7 GB virtual memory used\n2016-04-15 18:04:37,938 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 22115 for container-id container_1460755517014_0001_01_000013: 2.4 GB of 2.9 GB physical memory used; 7.6 GB of 11.7 GB virtual memory used\n2016-04-15 18:04:40,945 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 22132 for container-id container_1460755517014_0001_01_000004: 6.7 GB of 2.9 GB physical memory used; 18.4 GB of 11.7 GB virtual memory used\n2016-04-15 18:04:40,945 WARN org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Process tree for container: container_1460755517014_0001_01_000004 running over twice the configured limit. Limit=3145728000, current usage = 7149801472\n</code></pre>\n\n<p>The job userlog looks like a normal Nutch fetch log, except just before its killed I see the following exceptions from where YARN killed the child <code>xz</code> processes:</p>\n\n<pre><code>2016-04-15 18:04:41,067 ERROR [Thread-153765] org.apache.hadoop.yarn.YarnUncaughtExceptionHandler: Thread Thread[Thread-153765,5,main] threw an Exception.\njava.io.IOException: Broken pipe\n        at java.io.FileOutputStream.writeBytes(Native Method)\n        at java.io.FileOutputStream.write(FileOutputStream.java:326)\n        at java.io.BufferedOutputStream.write(BufferedOutputStream.java:122)\n        at scala.sys.process.BasicIO$.loop$1(BasicIO.scala:236)\n        at scala.sys.process.BasicIO$.transferFullyImpl(BasicIO.scala:242)\n        at scala.sys.process.BasicIO$.transferFully(BasicIO.scala:223)\n        at ...\n\n2016-04-15 18:04:41,076 ERROR [Thread-153776] org.apache.hadoop.yarn.YarnUncaughtExceptionHandler: Thread Thread[Thread-153776,5,main] threw an Exception.\njava.io.IOException: Stream closed\n        at java.lang.ProcessBuilder$NullOutputStream.write(ProcessBuilder.java:433)\n        at java.io.OutputStream.write(OutputStream.java:116)\n        at java.io.BufferedOutputStream.write(BufferedOutputStream.java:122)\n        at scala.sys.process.BasicIO$.loop$1(BasicIO.scala:236)\n        at scala.sys.process.BasicIO$.transferFullyImpl(BasicIO.scala:242)\n        at scala.sys.process.BasicIO$.transferFully(BasicIO.scala:223)\n        at ...\n</code></pre>\n\n<p>It's very likely the actual issue is related to the code I'm running during Nutch's fetch phase, however I do not spawn any JVM processes. Furthermore, the command-line string for each of the child JVMs matches the parent process, implying that it's a clone spawned by Hadoop/MapReduce. My questions, at least initially, are <strong>What are the child JVMs seen in the first excerpt?</strong>, and <strong>How could they cause the memory over-subscription, and in turn, cause YARN to kill the container?</strong>.</p>\n", "creation_date": 1460760207, "score": 0},
{"title": "How to get webgraph in apache nutch?", "view_count": 162, "owner": {"age": 24, "answer_count": 1, "creation_date": 1455050559, "user_id": 5905283, "view_count": 1, "location": "Dallas, TX, United States", "reputation": 5}, "is_answered": true, "answers": [{"question_id": 36564985, "owner": {"user_id": 1977773, "link": "http://stackoverflow.com/users/1977773/jorge-luis", "user_type": "registered", "reputation": 510}, "body": "<p>The Webgraph is intented to be a step in the <strong>score calculation</strong> based on the link structure (i.e webgraph):</p>\n\n<ul>\n<li><code>webgraph</code> will generate the data structure for the specified segment/s</li>\n<li><code>linkrank</code> will calculate the score based on the previous structures</li>\n<li><code>scoreupdater</code> will update the score from the webgraph back into the crawldb</li>\n</ul>\n\n<p>Be aware that this program is very CPU/IO intensive and that will ignore the internal links of a website by default.</p>\n\n<p>You could use the <code>nodedumper</code> command to get useful data out of the webgraph data, including the actual score of a node and the highest scored inlinks/outlinks. But this is not intented to be visualized, although you could parse the output of this command and generate any visualization that you may need.</p>\n\n<p>That being said, since Nutch 1.11 the plugin <code>index-links</code> has been added, which will allow you to index into Solr/ES the inlinks and outlinks of each URL. I've used this plugin indexing into Solr along with the sigmajs library to generate some graph visualizations of the link structure of my crawls, perhaps this could suit your needs.</p>\n", "creation_date": 1460660797, "is_accepted": true, "score": 2, "last_activity_date": 1460660797, "answer_id": 36631860}], "question_id": 36564985, "tags": ["apache", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/36564985/how-to-get-webgraph-in-apache-nutch", "last_activity_date": 1460660797, "accepted_answer_id": 36631860, "body": "<p>I have generated <code>webgrapgh</code> db in <code>apache nutch</code> using command <code>'bin/nutch webgraph -segmentDir crawl/segments -webgraphdb crawl/webgraphdb'</code>.... It generated three folders in crawl/webgraphdb which are inlinks, outlinks and nodes. Each of those folders contained two binary files like data and index. How to get visual web graph in apache nutch? What is the use of web graph?</p>\n", "creation_date": 1460441874, "score": 0},
{"title": "Error with nutch 1.11 : ....org.apache.hadoop.fs.FileStatus.isDirectory()Z", "view_count": 151, "is_answered": false, "answers": [{"question_id": 36625590, "owner": {"user_id": 1977773, "link": "http://stackoverflow.com/users/1977773/jorge-luis", "user_type": "registered", "reputation": 510}, "body": "<p>Did you built nutch by yourself or used the packaged version?  I've just checkout the 1.11 branch of the Nutch repo and built it, executing your commands give the right output with no exception at all. Granted I've tested this on my local system (OS X) which is not windows/cygwin, but this shouldn't be a problem. </p>\n\n<p>The 1.11 nutch branch is using hadoop 2.4.0, you can checkout which versions of hadoop are being pulled from the maven repo in the <code>runtime/local/lib/</code> folder, check the <code>hadoop-*</code> files.</p>\n", "creation_date": 1460652053, "is_accepted": false, "score": 0, "last_activity_date": 1460652053, "answer_id": 36629096}], "question_id": 36625590, "tags": ["hadoop", "cygwin", "web-crawler", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/36625590/error-with-nutch-1-11-org-apache-hadoop-fs-filestatus-isdirectoryz", "last_activity_date": 1460652053, "owner": {"user_id": 5947458, "view_count": 201, "answer_count": 84, "creation_date": 1455819288, "reputation": 535}, "body": "<p>I want to make an application in java like Google news.\nFor that I am doing that from scratch and doing basic setup with Nutch.</p>\n\n<p>I am done with installation but getting error in one command.</p>\n\n<p>Here is brief about tech. I am using</p>\n\n<pre><code>-nutch 1.11\n-Cygwin\n</code></pre>\n\n<blockquote>\n  <ul>\n  <li>My first command was <strong>:</strong></li>\n  </ul>\n</blockquote>\n\n<pre><code>$ bin/nutch\n</code></pre>\n\n<p>which gives me perfect output. </p>\n\n<blockquote>\n  <ul>\n  <li>Then I did URI crawling like <strong>:</strong></li>\n  </ul>\n</blockquote>\n\n<pre><code>$ bin/nutch inject crawl/crawldb urls \n</code></pre>\n\n<p>Which created crawldb folder and crawl given url</p>\n\n<blockquote>\n  <ul>\n  <li>Now I want to generate segments and which gives me given Error <strong>:</strong></li>\n  </ul>\n</blockquote>\n\n<pre><code>$ bin/nutch generate crawl/crawldb crawl/segments\n\nGenerator: starting at 2016-04-14 17:30:29\nGenerator: Selecting best-scoring urls due for fetch.\nGenerator: filtering: true\nGenerator: normalizing: true\nGenerator: Partitioning selected urls for politeness.\nGenerator: segment: crawl/segments/20160414173032\nException in thread \"main\" java.lang.NoSuchMethodError: org.apache.hadoop.fs.FileStatus.isDirectory()Z\n        at org.apache.nutch.util.LockUtil.removeLockFile(LockUtil.java:79)\n        at org.apache.nutch.crawl.Generator.generate(Generator.java:637)\n        at org.apache.nutch.crawl.Generator.run(Generator.java:743)\n        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n        at org.apache.nutch.crawl.Generator.main(Generator.java:699)\n</code></pre>\n\n<p>I am not getting the problem. Is there mismatch between jars or having any other problem....</p>\n", "creation_date": 1460642714, "score": -1},
{"title": "Nutch 1.11 crawl Issue", "view_count": 738, "is_answered": true, "answers": [{"question_id": 34876002, "owner": {"user_id": 1699191, "link": "http://stackoverflow.com/users/1699191/ameertawfik", "user_type": "registered", "reputation": 618}, "body": "<p>I can see there are multiple problems with your command, try this:</p>\n\n<pre><code>bin/crawl -i -Dsolr.server.url=http://127.0.0.1:8983/solr/core_name path_to_seed crawl 2\n</code></pre>\n\n<p>The first problem is that there is a space when you pass the solr parameter. The second problem is that the solr url should include the core name as well.</p>\n", "creation_date": 1453271697, "is_accepted": false, "score": 1, "last_activity_date": 1453271697, "answer_id": 34893061}, {"question_id": 34876002, "owner": {"user_id": 5947458, "link": "http://stackoverflow.com/users/5947458/riddhi-gohil", "user_type": "registered", "reputation": 535}, "body": "<blockquote>\n  <p><code>hadoop-core</code> jar file is needed when you are working with nutch</p>\n</blockquote>\n\n<ul>\n<li><p>with nutch 1.11 compatible hadoop-core jar is 0.20.0</p></li>\n<li><blockquote>\n  <p>please download jar from this link :\n  <a href=\"http://www.java2s.com/Code/Jar/h/Downloadhadoop0200corejar.htm\" rel=\"nofollow\">http://www.java2s.com/Code/Jar/h/Downloadhadoop0200corejar.htm</a></p>\n</blockquote></li>\n<li><blockquote>\n  <p>paste that jar into <code>\"C:\\cygwin64\\home\\apache-nutch-1.11\\lib\"</code> folder and it will run\n  successfully.</p>\n</blockquote></li>\n</ul>\n", "creation_date": 1460617420, "is_accepted": false, "score": 0, "last_activity_date": 1460617420, "answer_id": 36615943}], "question_id": 34876002, "tags": ["solr", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/34876002/nutch-1-11-crawl-issue", "last_activity_date": 1460617420, "owner": {"user_id": 5809590, "view_count": 0, "answer_count": 0, "creation_date": 1453196356, "reputation": 1}, "body": "<p>I have followed the tutorial and configured nutch to run on Windows 7 using Cygwin and i'm using Solr 5.4.0 to index the data</p>\n\n<p>But nutch 1.11 is having problem in executing a crawl.</p>\n\n<p><strong>Crawl Command</strong>\n$ bin/crawl -i -D solr.server.url=<a href=\"http://127.0.0.1:8983/solr\" rel=\"nofollow\">http://127.0.0.1:8983/solr</a> /urls /TestCrawl 2</p>\n\n<p><strong>Error/Exception</strong></p>\n\n<p>Injecting seed URLs /apache-nutch-1.11/bin/nutch inject /TestCrawl/crawldb /urls\n    Injector: starting at 2016-01-19 17:11:06\n    Injector: crawlDb: /TestCrawl/crawldb\n    Injector: urlDir: /urls\n    Injector: Converting injected urls to crawl db entries.\n    Injector: java.lang.NullPointerException\n    at java.lang.ProcessBuilder.start(ProcessBuilder.java:1012)\n    at org.apache.hadoop.util.Shell.runCommand(Shell.java:445)\n    at org.apache.hadoop.util.Shell.run(Shell.java:418)\n    at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:650)\n    at org.apache.hadoop.util.Shell.execCommand(Shell.java:739)\n    at org.apache.hadoop.util.Shell.execCommand(Shell.java:722)\n    at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:633)\n    at org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:421)\n    at org.apache.hadoop.fs.FilterFileSystem.mkdirs(FilterFileSystem.java:281)\n    at org.apache.hadoop.mapreduce.JobSubmissionFiles.getStagingDir(JobSubmissionFiles.java:125)\n    at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:348)\n    at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1285)\n    at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1282)\n    at java.security.AccessController.doPrivileged(Native Method)\n    at javax.security.auth.Subject.doAs(Subject.java:422)\n    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\n    at org.apache.hadoop.mapreduce.Job.submit(Job.java:1282)\n    at org.apache.hadoop.mapred.JobClient$1.run(JobClient.java:562)\n    at org.apache.hadoop.mapred.JobClient$1.run(JobClient.java:557)\n    at java.security.AccessController.doPrivileged(Native Method)\n    at javax.security.auth.Subject.doAs(Subject.java:422)\n    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\n    at org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:557)\n    at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:548)\n    at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:833)\n    at org.apache.nutch.crawl.Injector.inject(Injector.java:323)\n    at org.apache.nutch.crawl.Injector.run(Injector.java:379)\n    at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)\n    at org.apache.nutch.crawl.Injector.main(Injector.java:369)</p>\n\n<pre><code>Error running:\n/home/apache-nutch-1.11/bin/nutch inject /TestCrawl/crawldb /urls\nFailed with exit value 127.\n</code></pre>\n", "creation_date": 1453203999, "score": 0},
{"title": "Nutch problems executing crawl", "view_count": 1501, "is_answered": false, "answers": [{"question_id": 34445121, "owner": {"user_id": 1780704, "accept_rate": 80, "link": "http://stackoverflow.com/users/1780704/mobin-ranjbar", "user_type": "registered", "reputation": 465}, "body": "<p>You are running linux commands from <code>Cygwin</code> and there is no <code>C:\\</code> path in linux systems. Correct command should be something like</p>\n\n<pre><code>/cygdrive/c/Users/User5/Documents/Nutch/apache-nutch1.11/runtime/local/bin/nutch inject TestCrawl/crawldb /cygdrive/c/Users/User5/Documents/Nutch/apache-nutch1.11/runtime/local/urls/seed.txt\n</code></pre>\n", "creation_date": 1452006944, "is_accepted": false, "score": 0, "last_activity_date": 1452006944, "answer_id": 34615043}, {"question_id": 34445121, "owner": {"user_id": 5846339, "link": "http://stackoverflow.com/users/5846339/r-viksna", "user_type": "registered", "reputation": 1}, "body": "<p>You have answer to your problem in this message: </p>\n\n<blockquote>\n  <p>2016-01-07 12:24:40,360 ERROR util.Shell - Failed to locate the winutils binary in the hadoop binary path\n  java.io.IOException: Could not locate executable null\\bin\\winutils.exe in the Hadoop binaries.</p>\n</blockquote>\n\n<p>This is happening because hadoop version included with nutch 1.11 is designed to work in linux out of the box and not on windows. </p>\n\n<p>I had same situation and I ended up using nutch1.11 in ubuntu virtual box.</p>\n", "creation_date": 1453893280, "is_accepted": false, "score": 0, "last_activity_date": 1453893280, "answer_id": 35035778}, {"question_id": 34445121, "owner": {"user_id": 5947458, "link": "http://stackoverflow.com/users/5947458/riddhi-gohil", "user_type": "registered", "reputation": 535}, "body": "<blockquote>\n  <p><code>hadoop-core</code> jar file is needed when you are working with nutch</p>\n</blockquote>\n\n<ul>\n<li>with nutch 1.11 compatible hadoop-core jar is 0.20.0</li>\n<li><blockquote>\n  <p>please download jar from this link :\n  <a href=\"http://www.java2s.com/Code/Jar/h/Downloadhadoop0200corejar.htm\" rel=\"nofollow\">http://www.java2s.com/Code/Jar/h/Downloadhadoop0200corejar.htm</a></p>\n</blockquote></li>\n<li><blockquote>\n  <p>paste that jar into <code>\"C:\\cygwin64\\home\\apache-nutch-1.11\\lib\"</code> folder\n  and it will run successfully.</p>\n</blockquote></li>\n</ul>\n", "creation_date": 1460617280, "is_accepted": false, "score": 0, "last_activity_date": 1460617280, "answer_id": 36615898}], "question_id": 34445121, "tags": ["apache", "lucene", "nutch"], "answer_count": 3, "link": "http://stackoverflow.com/questions/34445121/nutch-problems-executing-crawl", "last_activity_date": 1460617280, "owner": {"user_id": 5636973, "view_count": 11, "answer_count": 1, "creation_date": 1449177812, "reputation": 16}, "body": "<p>I am trying to get nutch 1.11 to execute a crawl.  I am using cygwin to run these commands in windows 7.</p>\n\n<p>Nutch is running, I am getting results from running bin/nutch, but I keep getting error messages when I try to run a crawl.</p>\n\n<p>I am getting the following error when I try to run a crawl execute with nutch:</p>\n\n<p>Error running: /cygdrive/c/Users/User5/Documents/Nutch/apache-nutch-1.11/runtime/local/bin/nutch inject TestCrawl/crawldb C:/Users/User5/Documents/Nutch/apache-nutch-1.11/runtime/local/urls/seed.txt </p>\n\n<p>Failed with exit value 127.</p>\n\n<p>I have my JAVA_HOME classpath set, and I have altered the host file to include the 127.0.0.1 as the localhost.</p>\n\n<p>I am curious if I am calling the write directory correctly, if maybe that is the problem. </p>\n\n<p>The full printout looks like:</p>\n\n<pre><code>User5@User5-PC /cygdrive/c/Users/User5/Documents/Nutch/apache-nutch-1.11/runtime/local\n$ bin/crawl -i -D solr.server.url=http://localhost:8983/solr/ C:/Users/User5/Documents/Nutch/apache-nutch-1.11/runtime/local/urls/ TestCrawl/  2\n\nInjecting seed URLs\n/cygdrive/c/Users/User5/Documents/Nutch/apache-nutch-1.11/runtime/local/bin/nutch inject TestCrawl//crawldb C:/Users/User5/Documents/Nutch/apache-nutch-1.11/runtime/local/urls/\nInjector: starting at 2015-12-23 17:48:21\nInjector: crawlDb: TestCrawl/crawldb\nInjector: urlDir: C:/Users/User5/Documents/Nutch/apache-nutch-1.11/runtime/local/urls\nInjector: Converting injected urls to crawl db entries.\nInjector: java.lang.NullPointerException\n        at java.lang.ProcessBuilder.start(ProcessBuilder.java:1012)\n        at org.apache.hadoop.util.Shell.runCommand(Shell.java:445)\n        at org.apache.hadoop.util.Shell.run(Shell.java:418)\n        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:650)\n        at org.apache.hadoop.util.Shell.execCommand(Shell.java:739)\n        at org.apache.hadoop.util.Shell.execCommand(Shell.java:722)\n        at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:633)\n        at org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:421)\n        at org.apache.hadoop.fs.FilterFileSystem.mkdirs(FilterFileSystem.java:281)\n        at org.apache.hadoop.mapreduce.JobSubmissionFiles.getStagingDir(JobSubmissionFiles.java:125)\n        at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:348)\n        at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1285)\n        at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1282)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:422)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\n        at org.apache.hadoop.mapreduce.Job.submit(Job.java:1282)\n        at org.apache.hadoop.mapred.JobClient$1.run(JobClient.java:562)\n        at org.apache.hadoop.mapred.JobClient$1.run(JobClient.java:557)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:422)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\n        at org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:557)\n        at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:548)\n        at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:833)\n        at org.apache.nutch.crawl.Injector.inject(Injector.java:323)\n        at org.apache.nutch.crawl.Injector.run(Injector.java:379)\n        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)\n        at org.apache.nutch.crawl.Injector.main(Injector.java:369)\n\nError running:\n  /cygdrive/c/Users/User5/Documents/Nutch/apache-nutch-1.11/runtime/local/bin/nutch inject TestCrawl//crawldb C:/Users/User5/Documents/Nutch/apache-nutch-1.11/runtime/local/urls/\nFailed with exit value 127.\n</code></pre>\n\n<p>The hadoop log that I think may have something to do with the error I am getting is:</p>\n\n<pre><code>2016-01-07 12:24:40,360 ERROR util.Shell - Failed to locate the winutils binary in the hadoop binary path\njava.io.IOException: Could not locate executable null\\bin\\winutils.exe in the Hadoop binaries.\n    at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:318)\n    at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:333)\n    at org.apache.hadoop.util.Shell.&lt;clinit&gt;(Shell.java:326)\n    at org.apache.hadoop.util.GenericOptionsParser.preProcessForWindows(GenericOptionsParser.java:432)\n    at org.apache.hadoop.util.GenericOptionsParser.parseGeneralOptions(GenericOptionsParser.java:478)\n    at org.apache.hadoop.util.GenericOptionsParser.&lt;init&gt;(GenericOptionsParser.java:170)\n    at org.apache.hadoop.util.GenericOptionsParser.&lt;init&gt;(GenericOptionsParser.java:153)\n    at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:64)\n    at org.apache.nutch.crawl.Injector.main(Injector.java:369)\n2016-01-07 12:24:40,450 ERROR crawl.Injector - Injector: java.lang.IllegalArgumentException: java.net.URISyntaxException: Illegal character in scheme name at index 15: solr.server.url=http://localhost:8983/solr\n    at org.apache.hadoop.fs.Path.initialize(Path.java:206)\n    at org.apache.hadoop.fs.Path.&lt;init&gt;(Path.java:172)\n    at org.apache.nutch.crawl.Injector.run(Injector.java:379)\n    at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)\n    at org.apache.nutch.crawl.Injector.main(Injector.java:369)\nCaused by: java.net.URISyntaxException: Illegal character in scheme name at index 15: solr.server.url=http://localhost:8983/solr\n    at java.net.URI$Parser.fail(URI.java:2848)\n    at java.net.URI$Parser.checkChars(URI.java:3021)\n    at java.net.URI$Parser.parse(URI.java:3048)\n    at java.net.URI.&lt;init&gt;(URI.java:746)\n    at org.apache.hadoop.fs.Path.initialize(Path.java:203)\n    ... 4 more\n</code></pre>\n", "creation_date": 1450912298, "score": 1},
{"title": "Adding a new field to Nutch + MongoDB datastore", "view_count": 55, "is_answered": false, "question_id": 36573366, "tags": ["mongodb", "nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/36573366/adding-a-new-field-to-nutch-mongodb-datastore", "last_activity_date": 1460472802, "owner": {"user_id": 944109, "answer_count": 2, "creation_date": 1315987250, "accept_rate": 33, "view_count": 9, "reputation": 41}, "body": "<p>I am running Nutch 2.3.1 configured with MondoDB (using Gora) + Elasticsearch and would like to add a new field to the storage database NOT the index. </p>\n\n<p>I am able to add a field to the elasticsearch index using a custom plugin but would like to add it to the mongodb record for each website. </p>\n\n<p>I've added the field to the ./conf/schema.xml file and to ./conf/gora-mongodb-mapping.xml - The field does appear in the index but not in the mongo record..</p>\n\n<p>Here'e a snapshot of my plugin:</p>\n\n<pre><code>public class AddNewField implements IndexingFilter {\n...\n@Override\n  public NutchDocument filter(NutchDocument doc, String url, WebPage page)\n      throws IndexingException {\n        //adds the new field to the document\n        doc.add(\"mynewField\", \"HelloWorld\");\n    return doc;\n  }\n}\n</code></pre>\n", "creation_date": 1460464263, "score": 1},
{"title": "Enabling/configuring Nutch logging?", "view_count": 36, "is_answered": false, "question_id": 36550403, "tags": ["nutch", "hadoop2"], "answer_count": 0, "link": "http://stackoverflow.com/questions/36550403/enabling-configuring-nutch-logging", "last_activity_date": 1460381937, "owner": {"user_id": 1784848, "answer_count": 13, "creation_date": 1351581947, "accept_rate": 57, "view_count": 32, "location": "NY, United States", "reputation": 52}, "body": "<p>I am working on developing a plugin for nutch. \nI have added some code to see the output either in console or in logs like this:</p>\n\n<pre><code>LOG.debug(\"Found keys :\" + lcMetatag + \"\\t\" + value);\n</code></pre>\n\n<p>I have also tried adding these 2 files (&amp; recompiling nutch):</p>\n\n<pre><code># RootLogger - DailyRollingFileAppender\nlog4j.rootLogger=DEBUG,DRFA\n\nlog4j.logger.org.apache.nutch=DEBUG\n</code></pre>\n\n<p>But I cannot find the output neither in console nor in hadoop logs.</p>\n\n<p>NB: I'm not using eclipse. I am running <code>\"apache-nutch-2.3.1\"</code> with <code>\"apache-hadoop-2.5.2\" (Fully distributed mode).</code></p>\n\n<p>Thanks</p>\n", "creation_date": 1460381937, "score": 0},
{"title": "I want to use command &quot;bin/nutch inject&quot; to inject my crawling URLs", "view_count": 222, "is_answered": true, "answers": [{"question_id": 34159977, "owner": {"user_id": 1965370, "link": "http://stackoverflow.com/users/1965370/do-do", "user_type": "registered", "reputation": 389}, "body": "<p>First, please check did you compile the Nutch source code. Then, you should try to run from here /path/to/nutch/runtime/deploy/bin as well in the case you deploy Nutch on a cluster.</p>\n", "creation_date": 1457724154, "is_accepted": false, "score": 0, "last_activity_date": 1457724154, "answer_id": 35948275}, {"question_id": 34159977, "owner": {"user_id": 5947458, "link": "http://stackoverflow.com/users/5947458/riddhi-gohil", "user_type": "registered", "reputation": 535}, "body": "<blockquote>\n  <p>Follow steps to install <code>nutch</code> in windows <strong>:</strong></p>\n</blockquote>\n\n<pre><code>1) download and install cygwin from : https://www.cygwin.com/\n2) download nutch from : http://nutch.apache.org/downloads.html\n3) paste nutch downloaded and extracted file into C:\\cygwin64\\home\\\n4) open cygwin terminal and type given commands \n\n - $ cd C:\n - $ cd cigwin64\n - $ cd home\n - $ cd apache-nutch\n - $ cd src/bin\n - $ ./nutch\n</code></pre>\n\n<blockquote>\n  <p>you will get given output <strong>:</strong></p>\n</blockquote>\n\n<pre><code>Usage: nutch COMMAND\nwhere COMMAND is one of:\n inject         inject new urls into the database\n hostinject     creates or updates an existing host table from a text file\n generate       generate new batches to fetch from crawl db\n fetch          fetch URLs marked during generate\n parse          parse URLs marked during fetch\n.\n.\n.\n.\n</code></pre>\n", "creation_date": 1460369904, "is_accepted": false, "score": 1, "last_activity_date": 1460369904, "answer_id": 36545898}], "question_id": 34159977, "tags": ["nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/34159977/i-want-to-use-command-bin-nutch-inject-to-inject-my-crawling-urls", "last_activity_date": 1460369904, "owner": {"user_id": 5655163, "view_count": 0, "answer_count": 0, "creation_date": 1449588804, "reputation": 1}, "body": "<p>I want to use command <code>bin/nutch inject</code> to inject my crawling URLs but I get an error </p>\n\n<pre><code>'nutch' is not recognized as an internal or external command,\noperable program or batch file.\n</code></pre>\n\n<p>Where do I enter this command? I am presently typing this command on the path <code>C:\\Users\\Gaurav Kandpal\\Desktop\\elastic\\apache-nutch-2.3-src\\apache-nutch-2.3\\runtime\\local\\b</code> on command prompt.</p>\n", "creation_date": 1449589242, "score": 0},
{"title": "A step-wised guide for installing and running Nutch in Windows 7 x64", "view_count": 2913, "is_answered": true, "answers": [{"question_id": 11484615, "owner": {"user_id": 1562154, "link": "http://stackoverflow.com/users/1562154/barry-anderson", "user_type": "unregistered", "reputation": 11}, "body": "<p>You didn't mess up the second step - you simply don't have (I'm guessing) Cygwin installed so you can't run a bash script. Either install Cygwin (simplest) or you could try porting the bash script to a Windows cmd file. (If you do that, you may find other dependencies down the line.</p>\n\n<p>Hope this helps.</p>\n", "creation_date": 1343631426, "is_accepted": false, "score": 1, "last_activity_date": 1343631426, "answer_id": 11716669}, {"question_id": 11484615, "owner": {"user_id": 5947458, "link": "http://stackoverflow.com/users/5947458/riddhi-gohil", "user_type": "registered", "reputation": 535}, "body": "<blockquote>\n  <p>Follow steps to install <code>nutch</code> in windows <strong>:</strong></p>\n</blockquote>\n\n<pre><code>1) download and install cygwin from : https://www.cygwin.com/\n2) download nutch from : http://nutch.apache.org/downloads.html\n3) paste nutch downloaded and extracted file into C:\\cygwin64\\home\\\n4) open cygwin terminal and type given commands \n\n - $ cd C:\n - $ cd cigwin64\n - $ cd home\n - $ cd apache-nutch\n - $ cd src/bin\n - $ ./nutch\n</code></pre>\n\n<blockquote>\n  <p>you will get given output <strong>:</strong></p>\n</blockquote>\n\n<pre><code>Usage: nutch COMMAND\nwhere COMMAND is one of:\n inject         inject new urls into the database\n hostinject     creates or updates an existing host table from a text file\n generate       generate new batches to fetch from crawl db\n fetch          fetch URLs marked during generate\n parse          parse URLs marked during fetch\n updatedb       update web table after parsing\n updatehostdb   update host table after parsing\n readdb         read/dump records from page database\n readhostdb     display entries from the hostDB\n index          run the plugin-based indexer on parsed batches\n elasticindex   run the elasticsearch indexer - DEPRECATED use the index command instead\n solrindex      run the solr indexer on parsed batches - DEPRECATED use the index command instead\n solrdedup      remove duplicates from solr\n solrclean      remove HTTP 301 and 404 documents from solr - DEPRECATED use the clean command instead\n clean          remove HTTP 301 and 404 documents and duplicates from indexing backends configured via plugins\n parsechecker   check the parser for a given url\n indexchecker   check the indexing filters for a given url\n plugin         load a plugin and run one of its classes main()\n nutchserver    run a (local) Nutch server on a user defined port\n webapp         run a local Nutch web application\n junit          runs the given JUnit test\n or\n CLASSNAME      run the class named CLASSNAME\nMost commands print help when invoked w/o parameters.\n</code></pre>\n", "creation_date": 1460369311, "is_accepted": false, "score": 2, "last_activity_date": 1460369311, "answer_id": 36545672}], "question_id": 11484615, "tags": ["windows-7", "installation", "documentation", "web-crawler", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/11484615/a-step-wised-guide-for-installing-and-running-nutch-in-windows-7-x64", "last_activity_date": 1460369311, "owner": {"user_id": 1065130, "answer_count": 0, "creation_date": 1322204442, "accept_rate": 60, "view_count": 32, "reputation": 126}, "body": "<p>I want to run Nutch on my Windows 7 x64. I have Nutch versions 1.5.1 and 2 from <a href=\"http://apache.spinellicreations.com/nutch/\" rel=\"nofollow\">apache.spinellicreations.com/nutch/</a>.</p>\n\n<p>I used the tutorial at <a href=\"http://wiki.apache.org/nutch/NutchTutorial\" rel=\"nofollow\">wiki.apache.org/nutch/NutchTutorial</a>. But I messed up in the second step and I can't verify the installation. Other steps are hard to understand... </p>\n\n<p>Can anyone suggest to me a step by step guide to install and use nutch?</p>\n", "creation_date": 1342276890, "score": 0},
{"title": "storm crawler - Technology stack and Apache Nutch", "view_count": 312, "owner": {"age": 26, "answer_count": 1, "creation_date": 1453179684, "user_id": 5808464, "accept_rate": 70, "view_count": 32, "location": "India", "reputation": 72}, "is_answered": true, "answers": [{"question_id": 36441422, "owner": {"user_id": 1977773, "link": "http://stackoverflow.com/users/1977773/jorge-luis", "user_type": "registered", "reputation": 510}, "body": "<p>Since you have a particular requirement to crawl the forum in a near real time fashion, Nutch is not the best technology to accomplish this. Nutch works on batches, meaning links are generated, then fetched, then parsed, but this wouldn't happen one link at the time. Storm crawler on the other hand is based on Apache Storm, which is a free and open source distributed <strong>realtime</strong> computation system.</p>\n\n<p>Storm Crawler does support indexing into Elasticsearch 1.7.2 at this moment (support for version 2, is on the way <a href=\"https://github.com/DigitalPebble/storm-crawler/tree/es2/external/elasticsearch\" rel=\"nofollow\">https://github.com/DigitalPebble/storm-crawler/tree/es2/external/elasticsearch</a>), no support for indexing into HBase exist at the moment and you couldn't use your hadoop setup because it is based on Apache Storm. Nevertheless Storm Crawler is \"A collection of resources for building low-latency, scalable web crawlers\" so you can write your own indexer bolt into HBase which shouldn't be too hard, and reuse the rest of the provided resources including the real time crawling that you need.</p>\n", "creation_date": 1459921380, "is_accepted": true, "score": 2, "last_activity_date": 1459921380, "answer_id": 36442476}, {"question_id": 36441422, "owner": {"user_id": 432844, "link": "http://stackoverflow.com/users/432844/julien-nioche", "user_type": "registered", "reputation": 939}, "body": "<p>@jorge-luis already replied about ElasticSearch 2. There is a <a href=\"https://github.com/DigitalPebble/storm-crawler/pull/257\" rel=\"nofollow\">pull request for it</a> and we are in the process of testing it. As for Hadoop, well StormCrawler is not based on it but on Apache Storm - hence the name. Finally there are currently no resources for HBase but this could be added. What did you want to use it for? I assume the documents will be indexed with ES. Did you want to keep the info about URLS in there (like the crawldb in Nutch)? If so then you could also use ES for storing the status, have a look at the <a href=\"https://github.com/DigitalPebble/storm-crawler/tree/master/external/elasticsearch\" rel=\"nofollow\">ES module in StormCrawler</a> for explanation.</p>\n", "creation_date": 1460019285, "is_accepted": false, "score": 0, "last_activity_date": 1460019285, "answer_id": 36471240}], "question_id": 36441422, "tags": ["apache-storm", "nutch", "stormcrawler"], "answer_count": 2, "link": "http://stackoverflow.com/questions/36441422/storm-crawler-technology-stack-and-apache-nutch", "last_activity_date": 1460363389, "accepted_answer_id": 36442476, "body": "<p>I want to crawl a particular forum near real time and dump the data into HDFS if not Hbase.</p>\n\n<p>I heard Apache Nutch could solve the purpose but sadly the technology stack it needed is pretty old. I don't want to downgrade the hadoop from 2.6 to earlier version and Elasticsearch to 1.7/1.4 hence i shifted my focus to storm-crawler.</p>\n\n<p>Since I am using Hadoop 2.6, Elasticsearch 2.0 and Hbase 1.1.3, can anyone tell me if storm-crawler 0.9 can be used along with them?</p>\n", "creation_date": 1459916407, "score": 0},
{"title": "How do I setup a Nutch Crawler on Mac OS X?", "view_count": 105, "is_answered": false, "answers": [{"question_id": 36528108, "owner": {"user_id": 1977773, "link": "http://stackoverflow.com/users/1977773/jorge-luis", "user_type": "registered", "reputation": 510}, "body": "<p>Do you have any particular error? Can you provide more details about what you've already done. Usually on OS X is just a matter of installing the JRE/JDK from Oracle and exporting the right <code>JAVA_HOME</code> variable in your bash/zsh terminal. What is the output of the command <code>java -version</code> in your terminal?</p>\n", "creation_date": 1460353187, "is_accepted": false, "score": 0, "last_activity_date": 1460353187, "answer_id": 36540576}], "question_id": 36528108, "tags": ["osx", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/36528108/how-do-i-setup-a-nutch-crawler-on-mac-os-x", "last_activity_date": 1460353187, "owner": {"user_id": 4256109, "view_count": 5, "answer_count": 0, "creation_date": 1416068500, "reputation": 6}, "body": "<p>I don't seem to find a credible instruction manual or guide online, can someone please guide me through the setup for a Nutch crawler on Mac OS X?</p>\n", "creation_date": 1460280942, "score": 0},
{"title": "Crawling websites with Nutch 2.3.1 skips product links but crawls other links", "view_count": 59, "is_answered": false, "answers": [{"question_id": 36330900, "owner": {"user_id": 5983993, "accept_rate": 67, "link": "http://stackoverflow.com/users/5983993/avinash", "user_type": "registered", "reputation": 56}, "body": "<p>Make sure your page navigation doesn't depend on cookie. Try  dumping the crawlDB and segments and check is the expected urls has been navigated or not. If navigated what contents has been fetched from this url.</p>\n", "creation_date": 1459959210, "is_accepted": false, "score": 0, "last_activity_date": 1459959210, "answer_id": 36456645}], "question_id": 36330900, "tags": ["solr", "web-crawler", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/36330900/crawling-websites-with-nutch-2-3-1-skips-product-links-but-crawls-other-links", "last_activity_date": 1459959210, "owner": {"age": 24, "answer_count": 6, "creation_date": 1314692838, "user_id": 919280, "accept_rate": 50, "view_count": 292, "location": "Ranchi", "reputation": 733}, "body": "<p>So, I am trying to crawl men shoes from jabong.com.</p>\n\n<p>My seed url is:</p>\n\n<pre><code>http://www.jabong.com/men/shoes/\n</code></pre>\n\n<p>I am making sure nutch does no skip <code>?</code> and <code>=</code> using this is <code>regex-urlfilter.txt</code>:</p>\n\n<pre><code>-[*!@]\n</code></pre>\n\n<p>This is my <code>protocol.includes</code> in nutch-site.xml:</p>\n\n<pre><code>protocol-httpclient|urlfilter-regex|parse-(html|tika)|index-(basic|anchor)|scoring-opic|urlnormalizer-(pass|regex|basic)|indexer-solr\n</code></pre>\n\n<p>It crawls links like the following and I can search them in solr:</p>\n\n<pre><code>http://www.jabong.com/men/shoes/andrew-hill/\nhttp://www.jabong.com/men/shoes/?sh_size=40\nhttp://www.jabong.com/all-products/?promotion=app-10-promo&amp;cmpgp=takeover5\n</code></pre>\n\n<p>But it is not crawling products that I want to crawl actually. Product links are:</p>\n\n<pre><code>http://www.jabong.com/Alberto-Torresi-Black-Sandals-2024892.html?pos=2\nhttp://www.jabong.com/Clarks-Un-Walk-Brown-Formal-Shoes-874785.html?pos=11\n</code></pre>\n\n<p>This is weird because these links are there in the same page as the seed URL, but they are not getting crawled. I did a <code>wget</code> to get the page and saw the links are there so no javascript involved.</p>\n\n<p>What mistake am I doing?</p>\n", "creation_date": 1459420390, "score": 1},
{"title": "Can Nutch be deployed to crawl specific pages", "view_count": 32, "is_answered": true, "answers": [{"question_id": 36425447, "owner": {"user_id": 2702341, "link": "http://stackoverflow.com/users/2702341/sujen-shah", "user_type": "registered", "reputation": 183}, "body": "<p>Yes you could do this using Apache Nutch. </p>\n\n<p>Have a look at the Nutch REST API [0] (actively in development) which can allow you to start Nutch as a service, provide URLs via a HTTP call, monitor the completion of jobs and then dump the data back. </p>\n\n<p>[0] - <a href=\"https://wiki.apache.org/nutch/Nutch_1.X_RESTAPI\" rel=\"nofollow\">https://wiki.apache.org/nutch/Nutch_1.X_RESTAPI</a></p>\n", "creation_date": 1459877701, "is_accepted": false, "score": 1, "last_activity_date": 1459877701, "answer_id": 36433273}, {"question_id": 36425447, "owner": {"user_id": 2704277, "link": "http://stackoverflow.com/users/2704277/chris-mattmann", "user_type": "registered", "reputation": 51}, "body": "<p>The other thing to note here is that you can also check out this <a href=\"https://www.quora.com/What-is-better-Scrapy-or-Apache-Nutch/answer/Chris-Mattmann\" rel=\"nofollow\">Qurora post</a> where I describe how to enable focused crawling in Nutch.</p>\n", "creation_date": 1459879240, "is_accepted": false, "score": 1, "last_activity_date": 1459879240, "answer_id": 36433729}], "question_id": 36425447, "tags": ["web-crawler", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/36425447/can-nutch-be-deployed-to-crawl-specific-pages", "last_activity_date": 1459879240, "owner": {"user_id": 1172468, "answer_count": 5, "creation_date": 1327620645, "accept_rate": 79, "view_count": 270, "reputation": 1364}, "body": "<p>Can Nutch be used to:</p>\n\n<ol>\n<li>Create a web service which I can give a list of urls to (these could be in batches of tens of thousands) -- this could be a simple wrapper around a command line</li>\n<li>Check repeatedly if the job was complete -- this could be a simple wrapper around a command line </li>\n<li>in return: get a dataset which has the dumped html pages -- this could be a simple wrapper around a command line</li>\n</ol>\n", "creation_date": 1459856614, "score": 0},
{"title": "is it possible to customized Nutch Fetcher class?", "view_count": 30, "is_answered": false, "answers": [{"question_id": 36306907, "owner": {"user_id": 2702341, "link": "http://stackoverflow.com/users/2702341/sujen-shah", "user_type": "registered", "reputation": 183}, "body": "<p>Have you looked into dumping the crawldb in a csv format ? I think you could achieve your task (which I understand is to get a list of all urls to be crawled by Nutch) without any code modification and following the below. </p>\n\n<p>You could use <code>./bin/nutch readdb &lt;crawldb path&gt; -dump &lt;output_dir&gt; -format csv</code></p>\n\n<p>This command will give you all the urls fetched/unfetched in Nutch.\nOnce you have it in csv, you could easily export it as pdf. </p>\n\n<p>For more info on the command have a look at <a href=\"https://wiki.apache.org/nutch/bin/nutch%20readdb\" rel=\"nofollow\">https://wiki.apache.org/nutch/bin/nutch%20readdb</a></p>\n", "creation_date": 1459877013, "is_accepted": false, "score": 0, "last_activity_date": 1459877013, "answer_id": 36433089}], "question_id": 36306907, "tags": ["java", "solr", "web-crawler", "phantomjs", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/36306907/is-it-possible-to-customized-nutch-fetcher-class", "last_activity_date": 1459877013, "owner": {"user_id": 5489276, "answer_count": 2, "creation_date": 1445864251, "accept_rate": 20, "view_count": 7, "reputation": 26}, "body": "<p>i am successful crawl web link and index data into solr.<br>\nbut I need to create a pdf file for all link which will be crawl and index into Solr. \nI know phantoms will give me pdf but I didn't understand where I configure phantoms in Nutch.\nI search about it and I know that I need to customize Fetcher class of Nutch, but I didn't know how to customize it. \nplease any one can help. I am stuck on this from last two weeks. </p>\n", "creation_date": 1459335768, "score": 0},
{"title": "How can I get the tags of stackoverflow for solr index?", "view_count": 13, "is_answered": false, "answers": [{"question_id": 36219858, "owner": {"user_id": 2702341, "link": "http://stackoverflow.com/users/2702341/sujen-shah", "user_type": "registered", "reputation": 183}, "body": "<p>Nutch fetches urls, so what you could do is point it to a page which might contain all the links to the questions with that tag. </p>\n\n<p>For example\n<a href=\"http://stackoverflow.com/questions/tagged/nutch?sort=newest\">http://stackoverflow.com/questions/tagged/nutch?sort=newest</a>, this page contains links to all questions having Nutch as the tag. Now by crawling 2 or more rounds will make Nutch fetch all outlinks from this page.</p>\n", "creation_date": 1459798698, "is_accepted": false, "score": 0, "last_activity_date": 1459798698, "answer_id": 36411413}], "question_id": 36219858, "tags": ["nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/36219858/how-can-i-get-the-tags-of-stackoverflow-for-solr-index", "last_activity_date": 1459798698, "owner": {"user_id": 4983473, "answer_count": 0, "creation_date": 1433685399, "accept_rate": 33, "view_count": 6, "reputation": 16}, "body": "<p>Recently, I used nutch-1.11 and solr-4.10.4 to set up a crawler, I can crawl data by sequential nutch commands, but now my problem is how can I to fetch the specified data, like tags of questions of stackoverflow for example, then I can use these data for solr indexing for my some purpose? I try to configure and modify the \"local/conf/nutch-site\" but doesn't work for me, I'm a newer for Nnutch! </p>\n", "creation_date": 1458908572, "score": 0},
{"title": "ElasticSearch and Nutch integration", "view_count": 292, "is_answered": false, "answers": [{"question_id": 35711402, "owner": {"user_id": 2482460, "link": "http://stackoverflow.com/users/2482460/yuchen-zhou", "user_type": "registered", "reputation": 321}, "body": "<p>I don't know if I figured this out completely and am not sure if any of the following steps helped, but eventually I successfully indexed some pages to ES:</p>\n\n<p>Here's what I did:</p>\n\n<p>I found out that the ES client java embedded in root_nutch_folder/src/plugins/ is version 1.4.1.  I upgraded that to 1.7.5 (matching my ES local server version), following the howto_upgrade_es.txt file specified in indexer-elastic folder.  However, I don't think this helped the problem because it was nutch who didn't tell ES to index anything, rather than ES Client-Server communication problem.</p>\n\n<p>The key thing I found out that may be affecting the indexing is that my page is actually not crawled, despite records in 'webpage' table in HBase. Initially I tried <a href=\"http://www.espn.com\" rel=\"nofollow\">http://www.espn.com</a> and did nutch inject.  However, since www.espn.com will redirect to espn.go.com, nutch will not automatically follow the redirection and therefore did not download the content HTML.  However, the nutch log will show 'success' all the time.  Even the HBase will record information about www.espn.com.</p>\n\n<p>I reinjected '<a href=\"http://espn.go.com/\" rel=\"nofollow\">http://espn.go.com/</a>' and reran the whole process, and it is now correctly indexed into ES. </p>\n\n<p>I Googled online and found out that I need to setup http.redirect.max in nutch-site.xml, however, no matter how I set it, nutch still won't follow the 302 redirect requests. I guess I will have to live with this.</p>\n", "creation_date": 1456958933, "is_accepted": false, "score": 0, "last_activity_date": 1456958933, "answer_id": 35759761}], "question_id": 35711402, "tags": ["elasticsearch", "bigdata", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/35711402/elasticsearch-and-nutch-integration", "last_activity_date": 1459781930, "owner": {"user_id": 2482460, "answer_count": 3, "creation_date": 1371130542, "view_count": 35, "location": "Charlottesville, VA", "reputation": 321}, "body": "<p>I have a question about ElasticSearch and Apache Nutch integration. </p>\n\n<p>I've tried to follow the Nutch+ES guides listed here</p>\n\n<p><a href=\"https://gist.github.com/xrstf/b48a970098a8e76943b9\" rel=\"nofollow\">https://gist.github.com/xrstf/b48a970098a8e76943b9</a></p>\n\n<p><a href=\"https://qbox.io/blog/scraping-the-web-with-nutch-for-elasticsearch\" rel=\"nofollow\">https://qbox.io/blog/scraping-the-web-with-nutch-for-elasticsearch</a></p>\n\n<p>and</p>\n\n<p><a href=\"http://www.aossama.com/search-engine-with-apache-nutch-mongodb-and-elasticsearch/\" rel=\"nofollow\">http://www.aossama.com/search-engine-with-apache-nutch-mongodb-and-elasticsearch/</a></p>\n\n<p>Specifically, I was able to set up HDFS+HBase+Nutch and crawl a list of pages (I can confirm that the data is indeed crawled and stored in HBase's webpage table).</p>\n\n<p>I can also get ElasticSearch and Kibana to work with no problem (successfully created a new index and insert some records, confirmed working with Kibana)</p>\n\n<p>However, I couldn't get their <strong>combination</strong> to work. Basically, I after I did the following command on Nutch:</p>\n\n<pre><code>nutch inject &lt;file_containing_url&gt;\nnutch generate -topN 1\nnutch fetch -all\nnutch parse -all\nnutch updatedb -all\n</code></pre>\n\n<p>Now, I want to index the fetched data into ES, and I followed the guide:</p>\n\n<pre><code>nutch index elasticsearch -all\n</code></pre>\n\n<p>However, after this command has finished execution, nothing is changed in ElasticSearch.  The log under runtime/local/logs shows:</p>\n\n<pre><code>elastic.ElasticIndexWriter - Processing remaining requests [docs = 0, length = 0, total docs = 0]\n</code></pre>\n\n<p>Which makes me believe ES is not asked to index anything at all by nutch.</p>\n\n<p>Where did I do wrong in the entire process? Do I have to specify something about ES to nutch?</p>\n\n<p>FYI: here are some OS/tools details:\nCentOS 6.7, HBase 1.1.3 ElasticSearch 1.7.5, Nutch 2.3.1.\nI have modified nutch-site.xml to include ES's properties:</p>\n\n<pre><code>    &lt;property&gt;\n        &lt;name&gt;plugin.includes&lt;/name&gt;\n        &lt;!-- do **NOT** enable the parse-html plugin, if you want proper HTML parsing. Use something like parse-tika! --&gt;\n        &lt;value&gt;protocol-httpclient|urlfilter-regex|parse-(text|tika|js)|index-(basic|anchor|more)|query-(basic|site|url)|response-(json|xml)|summary-basic|scoring-opic|urlnormalizer-(pass|regex|basic)|indexer-elastic&lt;/value&gt;\n    &lt;/property&gt;\n    &lt;property&gt;\n        &lt;name&gt;elastic.host&lt;/name&gt;\n        &lt;value&gt;10.5.140.112&lt;/value&gt; &lt;!-- where is ElasticSearch listening --&gt;\n    &lt;/property&gt;\n      &lt;property&gt;\n        &lt;name&gt;elastic.cluster&lt;/name&gt;\n        &lt;value&gt;nutch&lt;/value&gt;\n      &lt;/property&gt;\n\n      &lt;property&gt;\n        &lt;name&gt;elastic.index&lt;/name&gt;\n        &lt;value&gt;nutch&lt;/value&gt;\n      &lt;/property&gt;\n\n      &lt;property&gt;\n        &lt;name&gt;elastic.port&lt;/name&gt;\n        &lt;value&gt;9300&lt;/value&gt;\n      &lt;/property&gt;\n</code></pre>\n\n<p>Thank you all so much for the help!</p>\n", "creation_date": 1456786532, "score": 1},
{"title": "Nutch 1.10 - won&#39;t crawl more than 100 subdomains on same TLD", "view_count": 35, "is_answered": true, "answers": [{"question_id": 36334736, "owner": {"user_id": 432844, "link": "http://stackoverflow.com/users/432844/julien-nioche", "user_type": "registered", "reputation": 939}, "body": "<p>There is no limitation on the number of subdomains in Nutch. What makes you think so? How do you discover the subdomains?</p>\n\n<p>If they are links from a single page then what's limiting you is 'db.max.outlinks.per.page' (defaults to 100). This limits any outlinks regardless of subdomains</p>\n", "creation_date": 1459508740, "is_accepted": false, "score": 1, "last_activity_date": 1459508740, "answer_id": 36354747}], "question_id": 36334736, "tags": ["web-crawler", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/36334736/nutch-1-10-wont-crawl-more-than-100-subdomains-on-same-tld", "last_activity_date": 1459508740, "owner": {"user_id": 796025, "answer_count": 3, "creation_date": 1307973039, "view_count": 3, "location": "Philadelphia, PA", "reputation": 46}, "body": "<p>Nutch 1.10; There is a default set to limit the number of subdomains being crawled on a single TLD to 100. Can someone tell me how to override this default?</p>\n\n<p>I did try the following in nutch-site.xml without success:</p>\n\n<pre><code>&lt;property&gt;\n  &lt;name&gt;generate.max.per.host&lt;/name&gt;\n  &lt;value&gt;300&lt;/value&gt;\n&lt;/property&gt;enter code here\n</code></pre>\n", "creation_date": 1459430507, "score": 0},
{"title": "Nutch Crawl2.0 error - java.io.IOException: No input paths specified in job", "view_count": 1173, "is_answered": false, "question_id": 11845778, "tags": ["nutch", "web-crawler"], "answer_count": 0, "link": "http://stackoverflow.com/questions/11845778/nutch-crawl2-0-error-java-io-ioexception-no-input-paths-specified-in-job", "last_activity_date": 1459505836, "owner": {"user_id": 1581918, "view_count": 2, "answer_count": 0, "creation_date": 1344341561, "reputation": 11}, "body": "<p>I try to crawl some urls with nutch 2.0, but failed as belows:</p>\n\n<p>org.apache.nutch.crawl.Crawler urls -dir crawls -depth 5 -topN 100\nException in thread \"main\" java.io.IOException: No input paths specified in job\n    at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.listStatus(FileInputFormat.java:193)\n    at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.getSplits(FileInputFormat.java:252)\n    at org.apache.gora.mapreduce.GoraMapReduceUtils.getSplits(GoraMapReduceUtils.java:67)\n    at org.apache.gora.store.impl.FileBackedDataStoreBase.getPartitions(FileBackedDataStoreBase.java:148)\n    at org.apache.gora.mapreduce.GoraInputFormat.getSplits(GoraInputFormat.java:93)\n    at org.apache.hadoop.mapred.JobClient.writeNewSplits(JobClient.java:962)\n    at org.apache.hadoop.mapred.JobClient.writeSplits(JobClient.java:979)\n    at org.apache.hadoop.mapred.JobClient.access$600(JobClient.java:174)\n    at org.apache.hadoop.mapred.JobClient$2.run(JobClient.java:897)\n    at org.apache.hadoop.mapred.JobClient$2.run(JobClient.java:850)\n    at java.security.AccessController.doPrivileged(Native Method)\n    at javax.security.auth.Subject.doAs(Subject.java:396)\n    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1121)\n    at org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:850)\n    at org.apache.hadoop.mapreduce.Job.submit(Job.java:500)\n    at org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:530)\n    at org.apache.nutch.util.NutchJob.waitForCompletion(NutchJob.java:43)\n    at org.apache.nutch.crawl.GeneratorJob.run(GeneratorJob.java:180)\n    at org.apache.nutch.crawl.Crawler.runTool(Crawler.java:68)\n    at org.apache.nutch.crawl.Crawler.run(Crawler.java:152)\n    at org.apache.nutch.crawl.Crawler.run(Crawler.java:250)\n    at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n    at org.apache.nutch.crawl.Crawler.main(Crawler.java:257)</p>\n\n<p>can anyone help me? thanks a lot!</p>\n", "creation_date": 1344341839, "score": 2},
{"title": "Nutch Crawl error - Input path does not exist", "view_count": 1586, "owner": {"age": 34, "answer_count": 2, "creation_date": 1288717987, "user_id": 495033, "accept_rate": 59, "view_count": 182, "reputation": 671}, "is_answered": true, "answers": [{"question_id": 7371602, "owner": {"user_id": 1150329, "accept_rate": 89, "link": "http://stackoverflow.com/users/1150329/tejas-patil", "user_type": "registered", "reputation": 4793}, "body": "<p>The generate phase of nutch creates \"crawl_generate\" inside the segments directory. This contains the fetch list used in the fetch phase. The error that you got is because the fetch phase is unable to get the fetch list. Ensure that the output of generate is populated at the location where fetch is trying to find it.</p>\n", "creation_date": 1326742808, "is_accepted": false, "score": 1, "last_activity_date": 1326742808, "answer_id": 8885374}, {"question_id": 7371602, "owner": {"user_id": 5983993, "accept_rate": 67, "link": "http://stackoverflow.com/users/5983993/avinash", "user_type": "registered", "reputation": 56}, "body": "<p>verify whether nutch/crawl/segments/crawl_generate path is correct.</p>\n\n<p>Either path is wrong or parse phase is not completed.</p>\n", "creation_date": 1459505648, "is_accepted": true, "score": 1, "last_activity_date": 1459505648, "answer_id": 36353679}], "question_id": 7371602, "tags": ["java", "hadoop", "nutch", "web-crawler"], "answer_count": 2, "link": "http://stackoverflow.com/questions/7371602/nutch-crawl-error-input-path-does-not-exist", "last_activity_date": 1459505648, "accepted_answer_id": 36353679, "body": "<p>i have nutch/hadoop with 2 datanode server. I try to crawl some urls but nutch fails with this error:</p>\n\n<pre><code>Fetcher: segment: crawl/segments\nFetcher: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: hdfs://devcluster01:9000/user/nutch/crawl/segments/crawl_generate\n    at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:190)\n    at org.apache.hadoop.mapred.SequenceFileInputFormat.listStatus(SequenceFileInputFormat.java:44)\n    at org.apache.nutch.fetcher.Fetcher$InputFormat.getSplits(Fetcher.java:105)\n    at org.apache.hadoop.mapred.JobClient.writeOldSplits(JobClient.java:810)\n    at org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:781)\n    at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:730)\n    at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:1249)\n    at org.apache.nutch.fetcher.Fetcher.fetch(Fetcher.java:1107)\n    at org.apache.nutch.fetcher.Fetcher.run(Fetcher.java:1145)\n    at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n    at org.apache.nutch.fetcher.Fetcher.main(Fetcher.java:1116)\n</code></pre>\n\n<p>Can someone help me? I don't know how to solve this!\nMany many Thx!</p>\n", "creation_date": 1315656873, "score": 1},
{"title": "Apache nutch not indexing all documents to apache solr", "view_count": 126, "owner": {"user_id": 3454410, "answer_count": 20, "creation_date": 1395646154, "accept_rate": 48, "view_count": 174, "reputation": 832}, "is_answered": true, "answers": [{"question_id": 30259178, "owner": {"user_id": 3454410, "accept_rate": 48, "link": "http://stackoverflow.com/users/3454410/shafiq", "user_type": "registered", "reputation": 832}, "body": "<p>In my case this problem was due to missing solr indexer infomration in nutch-site.xml. When I update config, this problem was resolved. Please check your crawler log at indexing step. In my case it was informed that no solr indexer plugin is found.</p>\n\n<p>Following lines (property) are added in nutch-site.xml</p>\n\n<pre><code>&lt;property&gt;\n  &lt;name&gt;plugin.includes&lt;/name&gt;\n &lt;value&gt;protocol-httpclient|protocol-http|indexer-solr|urlfilter-regex|parse-(html|tika)|index-(basic|more)|urlnormalizer-(pass|regex|basic)|scoring-opic&lt;/value&gt;\n &lt;description&gt;plugin details here &lt;/description&gt;\n&lt;/property&gt;\n</code></pre>\n", "creation_date": 1459332774, "is_accepted": true, "score": 1, "last_activity_date": 1459332774, "answer_id": 36305793}, {"question_id": 30259178, "owner": {"user_id": 1617411, "link": "http://stackoverflow.com/users/1617411/c-kelly", "user_type": "registered", "reputation": 141}, "body": "<p>You should look at your Solr logs, to see if there's anything about \"duplicate\" documents, or just go look in the solrconfig.xml file for the core into which you are pushing the documents.  There is likely a \"dedupe\" call is being made on the update handler, the fields used may be causing duplicate documents (based on a few fields) to be dropped.  You'll see something like this</p>\n\n<pre><code>&lt;requestHandler name=\"/dataimport\"\n        class=\"org.apache.solr.handler.dataimport.DataImportHandler\"&gt;\n    &lt;lst name=\"defaults\"&gt;\n        &lt;str name=\"update.chain\"&gt;dedupe&lt;/str&gt;     &lt;&lt;-- change dedupe to uuid\n        &lt;str name=\"config\"&gt;dih-config.xml&lt;/str&gt;        or comment the line\n    &lt;/lst&gt;\n&lt;/requestHandler&gt;\n</code></pre>\n\n<p>and later in the file the definition of the dedupe update.chain,</p>\n\n<pre><code>&lt;updateRequestProcessorChain name=\"dedupe\"&gt;\n     &lt;processor class=\"solr.processor.SignatureUpdateProcessorFactory\"&gt;\n         &lt;bool name=\"enabled\"&gt;true&lt;/bool&gt;\n         &lt;str name=\"signatureField\"&gt;id&lt;/str&gt;\n         &lt;bool name=\"overwriteDupes\"&gt;true&lt;/bool&gt;\n--&gt;&gt;     &lt;str name=\"fields\"&gt;url,date,rawline&lt;/str&gt;     &lt;&lt;--\n         &lt;str name=\"signatureClass\"&gt;solr.processor.Lookup3Signature&lt;/str&gt;\n     &lt;/processor&gt;\n     &lt;processor class=\"solr.LogUpdateProcessorFactory\" /&gt;\n     &lt;processor class=\"solr.RunUpdateProcessorFactory\" /&gt;\n&lt;/updateRequestProcessorChain&gt;\n</code></pre>\n\n<p>The \"fields\" element is what will select which input data is used to determine the uniqueness of the record.  Of course, if you know there's no duplication in your input data, this is  not the issue.  But the above configuration will throw out any records which are duplicate on the fields shown.</p>\n\n<p>You may not be using the dataimport requestHandler, but rather the \"update\" requestHandler.  I'm not sure which one Nutch uses.  Either, way, you can simply comment out the update.chain, change it to a different processorChain such as \"uuid\", or add more fields to the \"fields\" declaration.</p>\n", "creation_date": 1459472687, "is_accepted": false, "score": 0, "last_activity_date": 1459472687, "answer_id": 36346361}], "question_id": 30259178, "tags": ["solr", "lucene", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/30259178/apache-nutch-not-indexing-all-documents-to-apache-solr", "last_activity_date": 1459472687, "accepted_answer_id": 36305793, "body": "<p>I am using apache nutch 2.3 (latest version). I have crawled about 49000 documnts by nutch. From documents mime analysis, crawled data containes about 45000 thouseand text/html documents. But when I saw indexed documents in solr (4.10.3), only about 14000 documents are indexed. Why this huge difference between documents are (45000-14000=31000). If I assume that nutch only index text/html documents, then atleast 45000 documents should be indexed.</p>\n\n<p>What is the problem. How to solve it?</p>\n", "creation_date": 1431691717, "score": 4},
{"title": "Nutch and solr not to index a html based on meta tag", "view_count": 25, "is_answered": false, "question_id": 36304891, "tags": ["indexing", "solr", "nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/36304891/nutch-and-solr-not-to-index-a-html-based-on-meta-tag", "last_activity_date": 1459330526, "owner": {"user_id": 3732294, "answer_count": 0, "creation_date": 1402537534, "accept_rate": 0, "view_count": 10, "reputation": 16}, "body": "<p>Can some provide inputs if we can  skip a page  from nutch or solr indexing  based on meta tag</p>\n\n<p>i found on some sites that  they use meta tag mentioned below.</p>\n\n<p>meta name=\"s_search\" content=\"no_index\"</p>\n\n<p>Can we use the  same will nutch not index this html pages</p>\n", "creation_date": 1459330526, "score": 0},
{"title": "nutch is not working properly", "view_count": 37, "is_answered": false, "answers": [{"question_id": 34698054, "owner": {"user_id": 1784848, "accept_rate": 57, "link": "http://stackoverflow.com/users/1784848/code-wrangler", "user_type": "registered", "reputation": 52}, "body": "<p>did you build nutch using <code>ant clean</code> and <code>ant runtime</code> commands ?\nIf yes then rather than running nutch via nutch located in <code>$NUTCH_HOME/bin/nutch</code> use the from located inside <code>$NUTCH_HOME/runtime/local/bin/nutch</code>.</p>\n\n<p>If not then first build run using <code>ant runtime</code> command.\nHTH.</p>\n", "creation_date": 1452505243, "is_accepted": false, "score": 0, "last_activity_date": 1452505243, "answer_id": 34718207}], "question_id": 34698054, "tags": ["solr", "web-crawler", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/34698054/nutch-is-not-working-properly", "last_activity_date": 1459258072, "owner": {"age": 19, "answer_count": 0, "creation_date": 1436980120, "user_id": 5120449, "accept_rate": 83, "view_count": 25, "location": "Patna, India", "reputation": 73}, "body": "<p>Hello guys I am new in Nutch for web crawling.I followed the steps on \n<a href=\"https://wiki.apache.org/nutch/NutchTutorial\" rel=\"nofollow\">Nutch official site tutorial.</a>\nI typed the <strong>command in terminal</strong> <br>\n<strong>$  bin/crawl -i -D solr.server.url=<a href=\"http://localhost:8983/solr/\" rel=\"nofollow\">http://localhost:8983/solr/</a> urls/ TestCrawl/  2</strong><br>\nwhere urls has seed file contains website name and TestCrawl is my db directory.</p>\n\n<p>It's giving me error with exit value 255.I am not sure what error is this.So I am posting my terminal screen shot here.<a href=\"http://i.stack.imgur.com/B9vyi.png\" rel=\"nofollow\"><img src=\"http://i.stack.imgur.com/B9vyi.png\" alt=\"enter image description here\"></a></p>\n", "creation_date": 1452368377, "score": 0},
{"title": "crawling all links of same domain in Nutch", "view_count": 2171, "is_answered": false, "answers": [{"question_id": 25008601, "owner": {"user_id": 994000, "link": "http://stackoverflow.com/users/994000/betolink", "user_type": "registered", "reputation": 83}, "body": "<p>I think you are using the wrong property, first use db.ignore.external.links in nutch-site.xml</p>\n\n<pre><code>&lt;property&gt;\n &lt;name&gt;db.ignore.external.links&lt;/name&gt;\n &lt;value&gt;true&lt;/value&gt;\n &lt;description&gt;If true, outlinks leading from a page to external hosts\n will be ignored. This will limit your crawl to the host on your seeds file.\n &lt;/description&gt;\n&lt;/property&gt;\n</code></pre>\n\n<p>b) Then you could also use a regex in regex-urlfilter.txt to limit the domains crawled to just techcrunch.</p>\n\n<pre><code>+^(http|https)://.*techcrunch.com/\n</code></pre>\n\n<p><strong>However</strong> I think that your issue is that Nutch obeys the robots.txt file and in this case techcrunch has a Crawl-delay value of 3600!! see <a href=\"http://techcrunch.com/robots.txt\" rel=\"nofollow\">robots.txt</a>. The default value of fetcher.max.crawl.delay is 30 seconds making Nutch dismiss all the pages from techcrunch.</p>\n\n<p>From fetcher.max.crawl.delay in nutch-default</p>\n\n<pre><code>\"If the Crawl-Delay in robots.txt is set to greater than this value (in\nseconds) then the fetcher will skip this page, generating an error report.\nIf set to -1 the fetcher will never skip such pages and will wait the\namount of time retrieved from robots.txt Crawl-Delay, however long that\nmight be.\"\n</code></pre>\n\n<p>You may want to play with the fetcher.threads.fetch and fetcher.threads.per.queue values to speed up your crawl. You could also take a look at <a href=\"http://stackoverflow.com/questions/24058899/how-to-bypass-robots-txt-with-apache-nutch-2-2-1\">this</a> and play with the Nutch code.. or you may even want to use a different approach to crawl sites with long crawl delays.</p>\n\n<p>Hope this is useful to you.</p>\n\n<p>Cheers!</p>\n", "creation_date": 1407869610, "is_accepted": false, "score": 0, "last_activity_date": 1407869610, "answer_id": 25271949}, {"last_edit_date": 1459145333, "owner": {"user_id": 3143538, "accept_rate": 20, "link": "http://stackoverflow.com/users/3143538/tushar-goswami", "user_type": "registered", "reputation": 113}, "body": "<p>In <code>nutch-default.xml</code> set <code>db.ignore.external.links</code> to <code>true</code> and 'db.ignore.external.links.mode' to <code>byDomain</code>. Like this :</p>\n\n<pre><code>&lt;property&gt;\n &lt;name&gt;db.ignore.external.links&lt;/name&gt;\n &lt;value&gt;true&lt;/value&gt;\n&lt;/property&gt;\n&lt;property&gt;\n &lt;name&gt;db.ignore.external.links.mode&lt;/name&gt;\n &lt;value&gt;byDomain&lt;/value&gt;\n&lt;/property&gt;\n</code></pre>\n\n<p>By default <code>db.ignore.external.links.mode</code> is set to <code>byHost</code>. Which means while crawing <code>http://www.techcrunch.com/</code> the URL <code>http://subdomain1.techcrunch.com</code> will get treated as EXTERNAL and hence will be ignored. But you want <code>sudomain1</code> pages to be crawled too - hence keep <code>db.ignore.external.links.mode</code> to <code>byDomain</code></p>\n\n<p>No work around required in <code>regex-urlfilter.txt</code>. Use <code>regex-urlfilter.txt</code> for some complex situation</p>\n", "question_id": 25008601, "creation_date": 1458979517, "is_accepted": false, "score": 0, "last_activity_date": 1459145333, "answer_id": 36233031}], "question_id": 25008601, "tags": ["solr", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/25008601/crawling-all-links-of-same-domain-in-nutch", "last_activity_date": 1459145333, "owner": {"user_id": 3322698, "answer_count": 0, "creation_date": 1392715716, "accept_rate": 29, "view_count": 35, "reputation": 70}, "body": "<p>Can anyone tel me how to crawl all other pages of same domain. </p>\n\n<p>For example i'm feeding a website <a href=\"http://www.techcrunch.com/\" rel=\"nofollow\">http://www.techcrunch.com/</a> in seed.txt.</p>\n\n<p>Following property is added in nutch-site.xml</p>\n\n<pre><code>&lt;property&gt;\n&lt;name&gt;db.ignore.internal.links&lt;/name&gt;\n&lt;value&gt;false&lt;/value&gt;\n&lt;description&gt;If true, when adding new links to a page, links from\nthe same host are ignored.  This is an effective way to limit the\nsize of the link database, keeping only the highest quality\nlinks.\n&lt;/description&gt;\n&lt;/property&gt; \n</code></pre>\n\n<p>And following is added in regex-urlfilter.txt </p>\n\n<h1>accept anything else</h1>\n\n<p>+.</p>\n\n<p>Note: if i add <a href=\"http://www.tutorialspoint.com/\" rel=\"nofollow\">http://www.tutorialspoint.com/</a> in seed.txt, I'm able to crawl all other pages but not techcrunch.com's pages though it has got many other pages too.</p>\n\n<p>Please help..?</p>\n", "creation_date": 1406610787, "score": 1},
{"title": "Nutch automated Crawl using oozie?", "view_count": 27, "is_answered": false, "question_id": 36219092, "tags": ["solr", "nutch", "oozie"], "answer_count": 0, "link": "http://stackoverflow.com/questions/36219092/nutch-automated-crawl-using-oozie", "last_activity_date": 1458905366, "owner": {"user_id": 1784848, "answer_count": 13, "creation_date": 1351581947, "accept_rate": 57, "view_count": 32, "location": "NY, United States", "reputation": 52}, "body": "<p>I have been manually crawling urls using nutch and storing it into Hbase and indexing them to solr, using this command:</p>\n\n<pre><code>./crawl /seeds 1 http://SOLR_URL:8983/solr/ddcds 2\n</code></pre>\n\n<p>so far its working fine. \nNow I want to automate this process using Oozie. Whatever I got from web is that I will have to write the workflow, and have to specify the classname like this:</p>\n\n<pre><code> &lt;main-class&gt;org.apache.nutch.indexer.IndexingJob&lt;/main-class&gt;\n</code></pre>\n\n<p>But that class is only for indexing. So is there a global class which can be used as the main class and will automatically invoke subsequent classes ? Or if anyone has implemented/created any workflow as stated, please let me know. </p>\n", "creation_date": 1458905366, "score": 0},
{"title": "Solr 5.0 and Nutch 1.10", "view_count": 365, "is_answered": false, "answers": [{"question_id": 30621512, "owner": {"user_id": 3147184, "link": "http://stackoverflow.com/users/3147184/rocksta", "user_type": "registered", "reputation": 37}, "body": "<p>You may increase number of round incremently which will fetch you more urls. You may see number of urls fetch in each round in <strong>hadoop.log</strong> file present in <strong>./logs</strong> folder.</p>\n\n<p>You may refer this <a href=\"https://wiki.apache.org/nutch/NutchTutorial\" rel=\"nofollow\">link</a></p>\n\n<p>Usage: crawl [-i|--index] [-D \"key=value\"]   \n        -i|--index      Indexes crawl results into a configured indexer\n        -D              A Java property to pass to Nutch calls\n        Seed Dir        Directory in which to look for a seeds file\n        Crawl Dir       Directory where the crawl/link/segments dirs are saved\n        Num Rounds      The number of rounds to run this crawl for\n     Example: bin/crawl -i -D solr.server.url=<a href=\"http://localhost:8983/solr/\" rel=\"nofollow\">http://localhost:8983/solr/</a> urls/ TestCrawl/  2</p>\n\n<pre><code> bin/crawl -i -D solr.server.url=$solrUrl cores/$coreName/urls cores/$coreName/crawl 2\n</code></pre>\n", "creation_date": 1458654441, "is_accepted": false, "score": 0, "last_activity_date": 1458654441, "answer_id": 36156141}], "question_id": 30621512, "tags": ["solr", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/30621512/solr-5-0-and-nutch-1-10", "last_activity_date": 1458654441, "owner": {"user_id": 4923717, "view_count": 0, "answer_count": 0, "creation_date": 1432195649, "reputation": 1}, "body": "<p>I am using Solr 5.0, Nutch 1.10 with cygwin on windows server 2008 R2. I am issuing the command as:</p>\n\n<p>bin/crawl -D urls/ bin/urls crawl/ 2</p>\n\n<p>As of my knowledge 2 is the number of rounds for crawling. When I execute this command and read the crawldb I receive only 127 url's which is very less as compared to what is expected. Also it does not crawl at deeper depth. When I issue this command for passing data to Solr:</p>\n\n<p>bin/nutch solrindex <a href=\"http://127.0.0.1:8983/solr/thetest\" rel=\"nofollow\">http://127.0.0.1:8983/solr/thetest</a> crawl/crawldb -linkdb crawl/linkdb crawl/segments/*</p>\n\n<p>and then perform search then I get only 20 url's in all. Can anyone help. I need to do a deeper depth crawling.</p>\n", "creation_date": 1433338042, "score": 0},
{"title": "Flume web scraping/crawling: exec source", "view_count": 108, "is_answered": false, "question_id": 36134887, "tags": ["python", "hadoop", "web-crawler", "nutch", "flume"], "answer_count": 0, "link": "http://stackoverflow.com/questions/36134887/flume-web-scraping-crawling-exec-source", "last_activity_date": 1458574146, "owner": {"age": 26, "answer_count": 1, "creation_date": 1453179684, "user_id": 5808464, "accept_rate": 70, "view_count": 32, "location": "India", "reputation": 72}, "body": "<p>I am using exec source in the Flume agent and not Apache nutch because I have to do the crawling after some seconds or near real time and push the data to the source. In my case source would be HDFS or HBase.</p>\n\n<p>Anyhow, As per my understanding we need to specify the command(for exec) in the flume conf file and then run the agent. I have read that we can specify a shell command.</p>\n\n<p>In place of shell can we specify a python script that will crawl a particular website and give the data to exec so that flume agent can dump it in the sink? Will it increase the latency?</p>\n", "creation_date": 1458573328, "score": 0},
{"title": "Nutch MalformedURLException causing the crawl process termination", "view_count": 60, "is_answered": false, "answers": [{"last_edit_date": 1458335713, "owner": {"user_id": 1965370, "link": "http://stackoverflow.com/users/1965370/do-do", "user_type": "registered", "reputation": 389}, "body": "<p>To skip these kind of URLs (malformed URLs), you should create Nutch filters in the file conf/regex-urlfilter.txt.</p>\n", "question_id": 34849963, "creation_date": 1458333422, "is_accepted": false, "score": 0, "last_activity_date": 1458335713, "answer_id": 36093787}], "question_id": 34849963, "tags": ["regex", "hadoop", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/34849963/nutch-malformedurlexception-causing-the-crawl-process-termination", "last_activity_date": 1458335713, "owner": {"user_id": 1784848, "answer_count": 13, "creation_date": 1351581947, "accept_rate": 57, "view_count": 32, "location": "NY, United States", "reputation": 52}, "body": "<p>I have added a set of seeds to crawl using this command</p>\n\n<pre><code>./bin/crawl /largeSeeds 1 http://localhost:8983/solr/ddcd 4\n</code></pre>\n\n<p>For first iteration all of the commands<code>(inject, generate, fetch, parse, update-table, Indexer &amp; delete duplicates.)</code> got executed successfully.\n    For second iteration, \"update-table\" command got failed (please see error log for reference), because of failure of this command the whole process gets terminated.</p>\n\n<pre><code>CrawlDB update for 1\n/usr/share/searchEngine/nutch-branch-2.3.1/runtime/deploy/bin/nutch updatedb -D mapred.reduce.tasks=2 -D mapred.child.java.opts=-Xmx1000m -D mapred.reduce.tasks.speculative.execution=false -D mapred.map.tasks.speculative.execution=false -D mapred.compress.map.output=true 1452969522-27478 -crawlId 1\n16/01/17 02:10:17 INFO crawl.DbUpdaterJob: DbUpdaterJob: starting at 2016-01-17 02:10:17\n16/01/17 02:10:17 INFO crawl.DbUpdaterJob: DbUpdaterJob: batchId: 1452969522-27478\n16/01/17 02:10:17 INFO plugin.PluginRepository: Plugins: looking in: /tmp/hadoop-root/hadoop-unjar3649584948711945520/classes/plugins\n16/01/17 02:10:18 INFO plugin.PluginRepository: Plugin Auto-activation mode: [true]\n16/01/17 02:10:18 INFO plugin.PluginRepository: Registered Plugins:\n16/01/17 02:10:18 INFO plugin.PluginRepository:     Rel-Tag microformat Parser/Indexer/Querier (microformats-reltag)\n16/01/17 02:10:18 INFO plugin.PluginRepository:     HTTP Framework (lib-http)\n16/01/17 02:10:18 INFO plugin.PluginRepository:     Html Parse Plug-in (parse-html)\n16/01/17 02:10:18 INFO plugin.PluginRepository:     MetaTags (parse-metatags)\n16/01/17 02:10:18 INFO plugin.PluginRepository:     Http / Https Protocol Plug-in (protocol-httpclient)\n16/01/17 02:10:18 INFO plugin.PluginRepository:     the nutch core extension points (nutch-extensionpoints)\n16/01/17 02:10:18 INFO plugin.PluginRepository:     Basic Indexing Filter (index-basic)\n16/01/17 02:10:18 INFO plugin.PluginRepository:     XML Libraries (lib-xml)\n16/01/17 02:10:18 INFO plugin.PluginRepository:     JavaScript Parser (parse-js)\n16/01/17 02:10:18 INFO plugin.PluginRepository:     Anchor Indexing Filter (index-anchor)\n16/01/17 02:10:18 INFO plugin.PluginRepository:     Tika Parser Plug-in (parse-tika)\n16/01/17 02:10:18 INFO plugin.PluginRepository:     Top Level Domain Plugin (tld)\n16/01/17 02:10:18 INFO plugin.PluginRepository:     Language Identification Parser/Filter (language-identifier)\n16/01/17 02:10:18 INFO plugin.PluginRepository:     Regex URL Filter Framework (lib-regex-filter)\n16/01/17 02:10:18 INFO plugin.PluginRepository:     Metadata Indexing Filter (index-metadata)\n16/01/17 02:10:18 INFO plugin.PluginRepository:     CyberNeko HTML Parser (lib-nekohtml)\n16/01/17 02:10:18 INFO plugin.PluginRepository:     Subcollection indexing and query filter (subcollection)\n16/01/17 02:10:18 INFO plugin.PluginRepository:     Link Analysis Scoring Plug-in (scoring-link)\n16/01/17 02:10:18 INFO plugin.PluginRepository:     Pass-through URL Normalizer (urlnormalizer-pass)\n16/01/17 02:10:18 INFO plugin.PluginRepository:     OPIC Scoring Plug-in (scoring-opic)\n16/01/17 02:10:18 INFO plugin.PluginRepository:     More Indexing Filter (index-more)\n16/01/17 02:10:18 INFO plugin.PluginRepository:     Http Protocol Plug-in (protocol-http)\n16/01/17 02:10:18 INFO plugin.PluginRepository:     SOLRIndexWriter (indexer-solr)\n16/01/17 02:10:18 INFO plugin.PluginRepository:     Creative Commons Plugins (creativecommons)\n16/01/17 02:10:18 INFO plugin.PluginRepository: Registered Extension-Points:\n16/01/17 02:10:18 INFO plugin.PluginRepository:     Parse Filter (org.apache.nutch.parse.ParseFilter)\n16/01/17 02:10:18 INFO plugin.PluginRepository:     Nutch Index Cleaning Filter (org.apache.nutch.indexer.IndexCleaningFilter)\n16/01/17 02:10:18 INFO plugin.PluginRepository:     Nutch Content Parser (org.apache.nutch.parse.Parser)\n16/01/17 02:10:18 INFO plugin.PluginRepository:     Nutch URL Filter (org.apache.nutch.net.URLFilter)\n16/01/17 02:10:18 INFO plugin.PluginRepository:     Nutch Scoring (org.apache.nutch.scoring.ScoringFilter)\n16/01/17 02:10:18 INFO plugin.PluginRepository:     Nutch URL Normalizer (org.apache.nutch.net.URLNormalizer)\n16/01/17 02:10:18 INFO plugin.PluginRepository:     Nutch Protocol (org.apache.nutch.protocol.Protocol)\n16/01/17 02:10:18 INFO plugin.PluginRepository:     Nutch Index Writer (org.apache.nutch.indexer.IndexWriter)\n16/01/17 02:10:18 INFO plugin.PluginRepository:     Nutch Indexing Filter (org.apache.nutch.indexer.IndexingFilter)\n16/01/17 02:10:19 INFO Configuration.deprecation: mapred.map.tasks.speculative.execution is deprecated. Instead, use mapreduce.map.speculative\n16/01/17 02:10:19 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative\n16/01/17 02:10:19 INFO Configuration.deprecation: mapred.compress.map.output is deprecated. Instead, use mapreduce.map.output.compress\n16/01/17 02:10:19 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n16/01/17 02:10:19 INFO zookeeper.RecoverableZooKeeper: Process identifier=hconnection-0x60a2630a connecting to ZooKeeper ensemble=localhost:2181\n16/01/17 02:10:19 INFO zookeeper.ZooKeeper: Client environment:zookeeper.version=3.4.6-1569965, built on 02/20/2014 09:09 GMT\n16/01/17 02:10:19 INFO zookeeper.ZooKeeper: Client environment:host.name=cism479\n16/01/17 02:10:19 INFO zookeeper.ZooKeeper: Client environment:java.version=1.8.0_65\n16/01/17 02:10:19 INFO zookeeper.ZooKeeper: Client environment:java.vendor=Oracle Corporation\n16/01/17 02:10:19 INFO zookeeper.ZooKeeper: Client environment:java.home=/usr/lib/jvm/jdk1.8.0_65/jre\n16/01/17 02:10:27 INFO mapreduce.JobSubmitter: number of splits:2\n16/01/17 02:10:27 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1452929501009_0024\n16/01/17 02:10:28 INFO impl.YarnClientImpl: Submitted application application_1452929501009_0024\n16/01/17 02:10:28 INFO mapreduce.Job: The url to track the job: http://cism479:8088/proxy/application_1452929501009_0024/\n16/01/17 02:10:28 INFO mapreduce.Job: Running job: job_1452929501009_0024\n16/01/17 02:10:39 INFO mapreduce.Job: Job job_1452929501009_0024 running in uber mode : false\n16/01/17 02:10:39 INFO mapreduce.Job:  map 0% reduce 0%\n16/01/17 02:11:37 INFO mapreduce.Job: Task Id : attempt_1452929501009_0024_m_000000_0, Status : FAILED\nError: java.net.MalformedURLException: For input string: \"#10;from &lt;a href=\"https:\"\n    at java.net.URL.&lt;init&gt;(URL.java:620)\n    at java.net.URL.&lt;init&gt;(URL.java:483)\n    at java.net.URL.&lt;init&gt;(URL.java:432)\n    at org.apache.nutch.util.TableUtil.reverseUrl(TableUtil.java:43)\n    at org.apache.nutch.crawl.DbUpdateMapper.map(DbUpdateMapper.java:96)\n    at org.apache.nutch.crawl.DbUpdateMapper.map(DbUpdateMapper.java:38)\n    at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:145)\n    at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:764)\n    at org.apache.hadoop.mapred.MapTask.run(MapTask.java:340)\n    at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)\n    at java.security.AccessController.doPrivileged(Native Method)\n    at javax.security.auth.Subject.doAs(Subject.java:422)\n    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1614)\n    at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:163)\nCaused by: java.lang.NumberFormatException: For input string: \"#10;from &lt;a href=\"https:\"\n    at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)\n    at java.lang.Integer.parseInt(Integer.java:569)\n    at java.lang.Integer.parseInt(Integer.java:615)\n    at java.net.URLStreamHandler.parseURL(URLStreamHandler.java:216)\n    at java.net.URL.&lt;init&gt;(URL.java:615)\n    ... 13 more\n\nContainer killed by the ApplicationMaster.\nContainer killed on request. Exit code is 143\nContainer exited with a non-zero exit code 143\n\n16/01/17 02:12:13 INFO mapreduce.Job:  map 33% reduce 0%\n16/01/17 02:12:24 INFO mapreduce.Job:  map 50% reduce 0%\n16/01/17 02:12:44 INFO mapreduce.Job: Task Id : attempt_1452929501009_0024_m_000000_1, Status : FAILED\nError: java.net.MalformedURLException: For input string: \"#10;from &lt;a href=\"https:\"\n    at java.net.URL.&lt;init&gt;(URL.java:620)\n    at java.net.URL.&lt;init&gt;(URL.java:483)\n    at java.net.URL.&lt;init&gt;(URL.java:432)\n    at org.apache.nutch.util.TableUtil.reverseUrl(TableUtil.java:43)\n    at org.apache.nutch.crawl.DbUpdateMapper.map(DbUpdateMapper.java:96)\n    at org.apache.nutch.crawl.DbUpdateMapper.map(DbUpdateMapper.java:38)\n    at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:145)\n    at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:764)\n    at org.apache.hadoop.mapred.MapTask.run(MapTask.java:340)\n    at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)\n    at java.security.AccessController.doPrivileged(Native Method)\n    at javax.security.auth.Subject.doAs(Subject.java:422)\n    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1614)\n    at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:163)\nCaused by: java.lang.NumberFormatException: For input string: \"#10;from &lt;a href=\"https:\"\n    at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)\n    at java.lang.Integer.parseInt(Integer.java:569)\n    at java.lang.Integer.parseInt(Integer.java:615)\n    at java.net.URLStreamHandler.parseURL(URLStreamHandler.java:216)\n    at java.net.URL.&lt;init&gt;(URL.java:615)\n    ... 13 more\n\n16/01/17 02:13:19 INFO mapreduce.Job: Task Id : attempt_1452929501009_0024_m_000000_2, Status : FAILED\nError: java.net.MalformedURLException: For input string: \"#10;from &lt;a href=\"https:\"\n    at java.net.URL.&lt;init&gt;(URL.java:620)\n    at java.net.URL.&lt;init&gt;(URL.java:483)\n    at java.net.URL.&lt;init&gt;(URL.java:432)\n    at org.apache.nutch.util.TableUtil.reverseUrl(TableUtil.java:43)\n    at org.apache.nutch.crawl.DbUpdateMapper.map(DbUpdateMapper.java:96)\n    at org.apache.nutch.crawl.DbUpdateMapper.map(DbUpdateMapper.java:38)\n    at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:145)\n    at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:764)\n    at org.apache.hadoop.mapred.MapTask.run(MapTask.java:340)\n    at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)\n    at java.security.AccessController.doPrivileged(Native Method)\n    at javax.security.auth.Subject.doAs(Subject.java:422)\n    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1614)\n    at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:163)\nCaused by: java.lang.NumberFormatException: For input string: \"#10;from &lt;a href=\"https:\"\n    at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)\n    at java.lang.Integer.parseInt(Integer.java:569)\n    at java.lang.Integer.parseInt(Integer.java:615)\n    at java.net.URLStreamHandler.parseURL(URLStreamHandler.java:216)\n    at java.net.URL.&lt;init&gt;(URL.java:615)\n    ... 13 more\n\n16/01/17 02:13:42 INFO mapreduce.Job:  map 100% reduce 100%\n16/01/17 02:13:43 INFO mapreduce.Job: Job job_1452929501009_0024 failed with state FAILED due to: Task failed task_1452929501009_0024_m_000000\nJob failed as tasks failed. failedMaps:1 failedReduces:0\n\n16/01/17 02:13:44 INFO mapreduce.Job: Counters: 34\n    File System Counters\n        FILE: Number of bytes read=0\n        FILE: Number of bytes written=49949067\n        FILE: Number of read operations=0\n        FILE: Number of large read operations=0\n        FILE: Number of write operations=0\n        HDFS: Number of bytes read=1193\n        HDFS: Number of bytes written=0\n        HDFS: Number of read operations=1\n        HDFS: Number of large read operations=0\n        HDFS: Number of write operations=0\n    Job Counters\n        Failed map tasks=4\n        Launched map tasks=5\n        Other local map tasks=3\n        Data-local map tasks=2\n        Total time spent by all maps in occupied slots (ms)=829677\n        Total time spent by all reduces in occupied slots (ms)=0\n        Total time spent by all map tasks (ms)=276559\n        Total vcore-seconds taken by all map tasks=276559\n        Total megabyte-seconds taken by all map tasks=849589248\n    Map-Reduce Framework\n        Map input records=30201\n        Map output records=1164348\n        Map output bytes=250659088\n        Map output materialized bytes=49832245\n        Input split bytes=1193\n        Combine input records=0\n        Spilled Records=1164348\n        Failed Shuffles=0\n        Merged Map outputs=0\n        GC time elapsed (ms)=3541\n        CPU time spent (ms)=42980\n        Physical memory (bytes) snapshot=2062766080\n        Virtual memory (bytes) snapshot=5086490624\n        Total committed heap usage (bytes)=2127036416\n    File Input Format Counters\n        Bytes Read=0\nException in thread \"main\" java.lang.RuntimeException: job failed: name=[1]update-table, jobid=job_1452929501009_0024\n    at org.apache.nutch.util.NutchJob.waitForCompletion(NutchJob.java:120)\n    at org.apache.nutch.crawl.DbUpdaterJob.run(DbUpdaterJob.java:111)\n    at org.apache.nutch.crawl.DbUpdaterJob.updateTable(DbUpdaterJob.java:140)\n    at org.apache.nutch.crawl.DbUpdaterJob.run(DbUpdaterJob.java:174)\n    at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)\n    at org.apache.nutch.crawl.DbUpdaterJob.main(DbUpdaterJob.java:178)\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n    at java.lang.reflect.Method.invoke(Method.java:497)\n    at org.apache.hadoop.util.RunJar.main(RunJar.java:212)\nError running:\n  /usr/share/searchEngine/nutch-branch-2.3.1/runtime/deploy/bin/nutch updatedb -D mapred.reduce.tasks=2 -D mapred.child.java.opts=-Xmx1000m -D mapred.reduce.tasks.speculative.execution=false -D mapred.map.tasks.speculative.execution=false -D mapred.compress.map.output=true 1452969522-27478 -crawlId 1\nFailed with exit value 1.\n</code></pre>\n\n<p>As its pretty clear from error that its because of the malformed urls. So is there a way to get rid of this kind of malformed urls ? or is there any solution which could either skip these kind of urls or bypasss them, so the subsequent processes get executed ?\nPlease advise.</p>\n", "creation_date": 1453105002, "score": 0},
{"title": "How to run multiple instances of nutch?", "view_count": 322, "is_answered": false, "answers": [{"question_id": 17526448, "owner": {"user_id": 3143538, "accept_rate": 20, "link": "http://stackoverflow.com/users/3143538/tushar-goswami", "user_type": "registered", "reputation": 113}, "body": "<p>I used the 'bin/crawl' script. Ran it in 2 different terminals simultaneously. Both finished their execution without any bug (atleast as per my judgment). <strong>I had supplied different seed directory and crawl directory to each simultaneous instance.</strong> </p>\n\n<p>However as per an another <a href=\"http://lucene.472066.n3.nabble.com/Concurrently-running-multiple-nutch-crawls-td3166207.html\" rel=\"nofollow\">thread here</a> it states that you must run bin/nutch command by supplying different 'configuration' file every time you want to run a different simultaneous instance and supply a different /tmp/ path for each instance. I myself didnt had to go through that trouble. The above method worked pretty well for me</p>\n", "creation_date": 1458298493, "is_accepted": false, "score": 0, "last_activity_date": 1458298493, "answer_id": 36082552}], "question_id": 17526448, "tags": ["nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/17526448/how-to-run-multiple-instances-of-nutch", "last_activity_date": 1458298493, "owner": {"user_id": 1474659, "answer_count": 6, "creation_date": 1340364399, "accept_rate": 48, "view_count": 118, "reputation": 321}, "body": "<p>I want to crawl multiple websites by simultaneously running multiple instances of apache nutch-1.6. Should I install multiple copy of apache nutch in different locations and create a single(or master) .sh file for executing nutch crawl command for every copy? OR is it possible to configure a single copy of nutch for multiple instances?</p>\n", "creation_date": 1373285966, "score": 0},
{"title": "I am having trouble connecting the Nutch 1.10 web crawler with Solr 5.3.0", "view_count": 23, "is_answered": false, "question_id": 36023887, "tags": ["solr", "nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/36023887/i-am-having-trouble-connecting-the-nutch-1-10-web-crawler-with-solr-5-3-0", "last_activity_date": 1458082979, "owner": {"user_id": 1619647, "answer_count": 2, "creation_date": 1345722776, "accept_rate": 50, "view_count": 26, "reputation": 126}, "body": "<p>I am having trouble connecting the Nutch 1.10 web crawler with Solr 5.3.0.</p>\n\n<p>I have Solr correctly setup via \"bin/solr Start -c cloud -noprompt\" and I have even crawled data with Norconex web crawler and been able to successfully commit this crawled data into Solr but I want to see if I can commit Apache Nutch crawled data into Solr.</p>\n\n<p>I tried the tutorial \nIntegrate Solr with Nutch\nat <a href=\"https://wiki.apache.org/nutch/NutchTutorial#Integrate_Solr_with_Nutch\" rel=\"nofollow\">https://wiki.apache.org/nutch/NutchTutorial#Integrate_Solr_with_Nutch</a> but the location and files referred to don't match my Solr 5.3.0 setup.</p>\n\n<p>Thanks,</p>\n\n<p>John Mitchell</p>\n", "creation_date": 1458082979, "score": 0},
{"title": "How to run apache nutch in distributed mode", "view_count": 139, "is_answered": false, "answers": [{"question_id": 32753538, "owner": {"user_id": 1965370, "link": "http://stackoverflow.com/users/1965370/do-do", "user_type": "registered", "reputation": 389}, "body": "<p>In general, you should increase topN value and set the value of <code>&lt;name&gt;http.content.limit&lt;/name&gt;</code> (in nutch-site.xml) to <strong>-1</strong></p>\n", "creation_date": 1457726932, "is_accepted": false, "score": 0, "last_activity_date": 1457726932, "answer_id": 35949011}], "question_id": 32753538, "tags": ["hadoop", "web-crawler", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/32753538/how-to-run-apache-nutch-in-distributed-mode", "last_activity_date": 1457726932, "owner": {"user_id": 3454410, "answer_count": 20, "creation_date": 1395646154, "accept_rate": 48, "view_count": 174, "reputation": 832}, "body": "<p>I am using Apache Nutch 2.3. I have a small cluster of 4 Hadoop (1.2.1) nodes. I am running one instance of the Crawler. It crawls about 30k-50k pages per day. I have to crawl more pages per day (supposed value is about 1 million). I have tried different questions from FAQ of Nutch. But documents crawled could not increased. I think I should run Nutch in fully distributed mode (I expect full distributed mode of Nutch is running more than one instance).</p>\n\n<p>What is the solution to my problem?</p>\n", "creation_date": 1443069958, "score": 0},
{"title": "Apache Nutch - Modify Queue of Links to be Crawled", "view_count": 66, "is_answered": false, "answers": [{"question_id": 33664159, "owner": {"user_id": 1965370, "link": "http://stackoverflow.com/users/1965370/do-do", "user_type": "registered", "reputation": 389}, "body": "<p>TopN is used in GenerateJob of Nutch to get Top N highest ranking score URLs to be fetched in FetchJob. It is not the crawling depth per page.\nYou can change the depth by modifying the value of <code>&lt;name&gt;http.content.limit&lt;/name&gt;</code> in the nutch-site.xml file.</p>\n", "creation_date": 1457726472, "is_accepted": false, "score": 0, "last_activity_date": 1457726472, "answer_id": 35948881}], "question_id": 33664159, "tags": ["java", "web-crawler", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/33664159/apache-nutch-modify-queue-of-links-to-be-crawled", "last_activity_date": 1457726472, "owner": {"user_id": 3367701, "answer_count": 2, "creation_date": 1393661808, "accept_rate": 60, "view_count": 29, "reputation": 161}, "body": "<p>I have Apache Nutch 2.0 source ported to my project and can modify &amp; build it directly. From what I've read the -topN argument determines how many of the top scoring links in that depth/page will be queued. Actually the available resources online are confusing to me which leads me to my first question:</p>\n\n<ol>\n<li>Does <code>-topN</code> correspond to the number top scoring of links to be taken <strong>per depth</strong>? Or is it <strong>per page</strong>?</li>\n</ol>\n\n<p>And my second:</p>\n\n<ol start=\"2\">\n<li>Do these topN links queued <strong>in priority</strong> with other queued topN links from other page/depth? For example <code>topN = 2</code>, 1 seed URL, for the 2 crawled pages from the seed URL, are the links taken from this depth aggregated then sorted? Or only intra-page links are sorted? This assumes though that <code>topN</code> is per page.</li>\n</ol>\n\n<p>What I want to do is change the breadth-first behavior of Nutch. I want the top-scoring links to be crawled first, regardless of which depth they came from. From what I understand, Nutch crawls all seed URLs first, then crawls all <code>topN</code> links from <code>depth=1</code>, then <code>topN</code> links from <code>depth=2</code> and so on... What I want to do is:</p>\n\n<ul>\n<li>crawl all seed URLs first</li>\n<li>then crawl the top-scoring link.</li>\n<li>extract the outlinks in that top-scoring page</li>\n<li>queue the links to a PriorityQueue</li>\n<li>Dequeue the current top-scoring link, and so on...</li>\n</ul>\n\n<p>Which leads me to these questions:</p>\n\n<ol start=\"3\">\n<li>Is it possible to insert logic to follow the crawl behavior described above? Basically change web traversal to a hybrid of breadth-first and depth-first and always traverse to the highest scoring link.</li>\n<li>Will I be able to do this concurrently?</li>\n</ol>\n", "creation_date": 1447300368, "score": 1},
{"title": "Apache Nutch - Problems with Paths", "view_count": 99, "is_answered": false, "answers": [{"question_id": 33717915, "owner": {"user_id": 1965370, "link": "http://stackoverflow.com/users/1965370/do-do", "user_type": "registered", "reputation": 389}, "body": "<p>Make sure that you already compile the Nutch source code. Then, run the crawl command from ${APACHE_NUTCH_HOME}/runtime/local (or  ${APACHE_NUTCH_HOME}/runtime/deploy/bin).</p>\n\n<p>Hope this helps,</p>\n\n<p>Le Quoc Do</p>\n", "creation_date": 1457725715, "is_accepted": false, "score": 0, "last_activity_date": 1457725715, "answer_id": 35948688}], "question_id": 33717915, "tags": ["java", "apache", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/33717915/apache-nutch-problems-with-paths", "last_activity_date": 1457725715, "owner": {"user_id": 3043801, "answer_count": 10, "creation_date": 1385593564, "accept_rate": 74, "view_count": 140, "reputation": 472}, "body": "<p>I am trying to set up Apache Nutch to crawl URLs, following <a href=\"https://sites.google.com/site/profileswapnilkulkarni/tech-talk/nutchtutorialonubuntu10easysteps\" rel=\"nofollow\">this</a> guide. Being an older guide (The guide is for 1.x, I am using 2.3), I have made the necessary changes to structure. However, when I try to run a crawl, I get this error :</p>\n\n<pre><code>root@IndiStage:~# /usr/local/nutch/framework/apache-nutch-2.3/src/bin/crawl urls FirstCrawl 2\nNo SOLRURL specified. Skipping indexing.\nInjecting seed URLs\n/usr/local/nutch/framework/apache-nutch-2.3/src/bin/nutch inject urls -crawlId FirstCrawl\nError: Could not find or load main class org.apache.nutch.crawl.InjectorJob\nError running:\n  /usr/local/nutch/framework/apache-nutch-2.3/src/bin/nutch inject urls -crawlId FirstCrawl\nFailed with exit value 1.\nroot@IndiStage:~#\n</code></pre>\n\n<p>Being new to Ubuntu (14.04), I am finding it hard to manage the directory structure and paths here.</p>\n\n<p><code>InjectorJob</code> is in <code>/usr/local/nutch/framework/apache-nutch-2.3/src/java/org/apache/nutch/crawl</code> </p>\n\n<p><code>JAVA_HOME</code> is set to <code>/usr/lib/jvm/java-7-openjdk-amd64</code></p>\n", "creation_date": 1447577413, "score": 0},
{"title": "How to crawl 1 million documents daily from web using apache Nutch 2.3", "view_count": 82, "is_answered": true, "answers": [{"question_id": 34014157, "owner": {"user_id": 1965370, "link": "http://stackoverflow.com/users/1965370/do-do", "user_type": "registered", "reputation": 389}, "body": "<p>In general, you can set bigger TopN and also change <code>&lt;name&gt;http.content.limit&lt;/name&gt;</code> in nutch-site.xml to -1.</p>\n\n<p>Hope this helps,</p>\n\n<p>Le Quoc Do</p>\n", "creation_date": 1457725028, "is_accepted": false, "score": 1, "last_activity_date": 1457725028, "answer_id": 35948514}], "question_id": 34014157, "tags": ["hadoop", "web-scraping", "web-crawler", "hbase", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/34014157/how-to-crawl-1-million-documents-daily-from-web-using-apache-nutch-2-3", "last_activity_date": 1457725028, "owner": {"user_id": 3454410, "answer_count": 20, "creation_date": 1395646154, "accept_rate": 48, "view_count": 174, "reputation": 832}, "body": "<p>I have configured apache nutch 2.3 with hadoop 1.2.1 and hbase 0.94.x. I have to crawl web for few weeks. About 1 million document are required to be crawled. I have four node hadoop cluster. Before this configuration, I setup nutch on single machine and crawled some documents. But is rate of crawling was not more than 50k to 80k. What should be the configuration of nutch so that it could crawl required amount of documents daily.</p>\n", "creation_date": 1448952145, "score": 1},
{"title": "Apache Nutch 2.3 and MySQL", "view_count": 255, "owner": {"age": 21, "answer_count": 0, "creation_date": 1351572204, "user_id": 1784574, "accept_rate": 91, "view_count": 8, "location": "Los Ba&#241;os, Philippines", "reputation": 41}, "is_answered": true, "answers": [{"question_id": 34872518, "owner": {"user_id": 1965370, "link": "http://stackoverflow.com/users/1965370/do-do", "user_type": "registered", "reputation": 389}, "body": "<p>In fact, Nutch doesn't care the database underneath, Nutch works with the crawling database via Gora. Therefore, if Gora supports the database (MySQL, HBase, Cassandra), Nutch can crawl and put the content to the database. Please check gora and mysql version to fix the error in your case. You can follow this guide: <a href=\"http://www.solutions.asia/2013/06/installing-nutch-22-with-mysql-to.html\" rel=\"nofollow\">http://www.solutions.asia/2013/06/installing-nutch-22-with-mysql-to.html</a>.</p>\n\n<p>Hope this helps,</p>\n\n<p>Le Quoc Do</p>\n", "creation_date": 1457722302, "is_accepted": true, "score": 0, "last_activity_date": 1457722302, "answer_id": 35947778}], "question_id": 34872518, "tags": ["mysql", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/34872518/apache-nutch-2-3-and-mysql", "last_activity_date": 1457722302, "accepted_answer_id": 35947778, "body": "<p>Is there anyone who tried working with Apache Nutch 2.3 and MySQL? There are some people who say that Nutch 2.3 can no longer work with MySQL, but looking at the components in ivy.xml, MySQL is still in the list of possible dependencies. So, I did that as I would in the older Apache 2.x releases. Here's my ivy.xml (part of the gora artifacts):</p>\n\n<pre><code>&lt;!--================--&gt;\n&lt;!-- Gora artifacts --&gt;\n&lt;!--================--&gt;\n&lt;!-- N.B. To use Gora SNAPSHOT's merely replace the 'ref' value with the SNAPSHOT version \nand add changing=\"true\" alongside the dependency declaration. An example has been\nprovided for the gora-core dependency as below --&gt;\n&lt;dependency org=\"org.apache.gora\" name=\"gora-core\" rev=\"0.2.1\" conf=\"*-&gt;default\"/&gt;\n\n&lt;!-- Uncomment this to use SQL as Gora backend. It should be noted that the \ngora-sql 0.1.1-incubating artifact is NOT compatable with gora-core 0.3. Users should \ndowngrade to gora-core 0.2.1 in order to use SQL as a backend. --&gt;\n\n&lt;dependency org=\"org.apache.gora\" name=\"gora-sql\" rev=\"0.1.1-incubating\" conf=\"*-&gt;default\" /&gt;\n&lt;!-- Uncomment this to use MySQL as database with SQL as Gora store. --&gt;\n\n&lt;dependency org=\"mysql\" name=\"mysql-connector-java\" rev=\"5.1.18\" conf=\"*-&gt;default\"/&gt; \n</code></pre>\n\n<p>Using ant runtime on this produces an error that points to line 101 of build.xml, saying </p>\n\n<pre><code>deprecation=\"${javac.deprecation}\"&gt;\n</code></pre>\n\n<p>Does anyone have a workaround? Thanks.</p>\n", "creation_date": 1453194235, "score": 0},
{"title": "Is it possible to make Nutch crawl a remote windows machine forlders?", "view_count": 16, "is_answered": false, "answers": [{"question_id": 34902523, "owner": {"user_id": 1965370, "link": "http://stackoverflow.com/users/1965370/do-do", "user_type": "registered", "reputation": 389}, "body": "<p>Nutch can crawl files, if these files can be accessed via a browser.</p>\n", "creation_date": 1457721756, "is_accepted": false, "score": 0, "last_activity_date": 1457721756, "answer_id": 35947617}], "question_id": 34902523, "tags": ["nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/34902523/is-it-possible-to-make-nutch-crawl-a-remote-windows-machine-forlders", "last_activity_date": 1457721756, "owner": {"user_id": 2766382, "answer_count": 13, "creation_date": 1378842945, "accept_rate": 64, "view_count": 341, "reputation": 681}, "body": "<p>I will break down the question :</p>\n\n<ol>\n<li>Is it possible for Nutch to crawl folders/subfolders/files?</li>\n<li>If yes, is it possible for Nutch to crawl a remote Windows folders?</li>\n<li>If yes, how can we configure this?</li>\n</ol>\n\n<p>Or Nutch is only for Web Crawling?</p>\n\n<p>Thank you.</p>\n", "creation_date": 1453299543, "score": 0},
{"title": "Nutch 2.3.1 on cassandra couldn&#39;t start", "view_count": 167, "owner": {"user_id": 3143092, "answer_count": 4, "creation_date": 1388281864, "accept_rate": 92, "view_count": 6, "reputation": 87}, "is_answered": true, "answers": [{"question_id": 35171023, "owner": {"user_id": 1965370, "link": "http://stackoverflow.com/users/1965370/do-do", "user_type": "registered", "reputation": 389}, "body": "<p>The address of Cassandra is not <code>localhost</code>, it's <code>172.16.230.130</code>. That is the reason, Nutch cannot connect to the Cassandra store.</p>\n\n<p>Hope this helps,</p>\n\n<p>Le Quoc Do</p>\n", "creation_date": 1457721150, "is_accepted": true, "score": 1, "last_activity_date": 1457721150, "answer_id": 35947446}], "question_id": 35171023, "tags": ["hadoop", "cassandra", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/35171023/nutch-2-3-1-on-cassandra-couldnt-start", "last_activity_date": 1457721150, "accepted_answer_id": 35947446, "body": "<p>I'm trying to run nutch 2.3.1 with cassandra. Followed steps on <a href=\"http://wiki.apache.org/nutch/Nutch2Cassandra\" rel=\"nofollow\">http://wiki.apache.org/nutch/Nutch2Cassandra</a> . Finally, when I try to start nutch with command:</p>\n\n<pre><code>bin/crawl urls/ test http://localhost:8983/solr/ 2\n</code></pre>\n\n<p>I got the following exception:</p>\n\n<pre><code>GeneratorJob: starting\nGeneratorJob: filtering: false\nGeneratorJob: normalizing: false\nGeneratorJob: topN: 50000\nGeneratorJob: java.lang.RuntimeException: job failed: name=[test]generate: 1454483370-31180, jobid=job_local1380148534_0001\n    at     org.apache.nutch.util.NutchJob.waitForCompletion(NutchJob.java:120)\n    at org.apache.nutch.crawl.GeneratorJob.run(GeneratorJob.java:227)\n    at org.apache.nutch.crawl.GeneratorJob.generate(GeneratorJob.java:256)\n    at org.apache.nutch.crawl.GeneratorJob.run(GeneratorJob.java:322)\n    at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)\n    at org.apache.nutch.crawl.GeneratorJob.main(GeneratorJob.java:330)\n\nError running:\n  /home/user/apache-nutch-2.3.1/runtime/local/bin/nutch generate -D    mapred.reduce.tasks=2 -D mapred.child.java.opts=-Xmx1000m -D mapred.reduce.tasks.speculative.execution=false -D mapred.map.tasks.speculative.execution=false -D mapred.compress.map.output=true -topN 50000 -noNorm -noFilter -adddays 0 -    crawlId webmd -batchId 1454483370-31180\nFailed with exit value 255.\n</code></pre>\n\n<p>When I check logs/hadoop.log, here's the error message:</p>\n\n<pre><code>2016-02-03 15:18:14,741 ERROR connection.HConnectionManager - Could not start connection pool for host localhost(127.0.0.1):9160\n...\n2016-02-03 15:18:15,185 ERROR store.CassandraStore - All host pools marked down. Retry burden pushed out to client.\nme.prettyprint.hector.api.exceptions.HectorException: All host pools marked down. Retry burden pushed out to client.\n    at me.prettyprint.cassandra.connection.HConnectionManager.getClientFromLBPolicy(HConnectionManager.java:390)\n</code></pre>\n\n<p>But my cassandra server is up:</p>\n\n<pre><code>runtime/local$ netstat -l |grep 9160\ntcp        0      0 172.16.230.130:9160     *:*                     LISTEN \n</code></pre>\n\n<p>Anyone can help on this issue? Thanks.</p>\n", "creation_date": 1454484883, "score": 0},
{"title": "Nutch 1.11 on Cygwin", "view_count": 108, "is_answered": false, "answers": [{"question_id": 35901662, "owner": {"user_id": 1965370, "link": "http://stackoverflow.com/users/1965370/do-do", "user_type": "registered", "reputation": 389}, "body": "<p>I guess the problem is that  Nutch cannot read the input URL seed file (contains a list of URLs to be crawled). Please check the path to the file correct or not.</p>\n\n<p>Hope this helps,</p>\n\n<p>Le Quoc Do</p>\n", "creation_date": 1457719773, "is_accepted": false, "score": 0, "last_activity_date": 1457719773, "answer_id": 35947058}], "question_id": 35901662, "tags": ["hadoop", "cygwin", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/35901662/nutch-1-11-on-cygwin", "last_activity_date": 1457719773, "owner": {"user_id": 1328015, "answer_count": 0, "creation_date": 1334198211, "accept_rate": 88, "view_count": 32, "reputation": 45}, "body": "<p>I am trying to run Nutch through Cygwin on a Windows 7 machine.</p>\n\n<p>I cannot get past the injector phase when trying to crawl.</p>\n\n<p>This is the error I am getting:</p>\n\n<p><code>2016-03-09 13:42:45,454 ERROR util.Shell - Failed to locate the winutils binary in the hadoop binary path</code>\n<br><code>java.io.IOException: Could not locate executable null\\bin\\winutils.exe in the Hadoop binaries.</code></p>\n\n<p>Later on it causes a NullPointerException:</p>\n\n<p>2016-03-09 13:42:46,445 ERROR crawl.Injector - Injector: java.lang.NullPointerException\n    at java.lang.ProcessBuilder.start(ProcessBuilder.java:1010)</p>\n\n<p>Just after it gets past \"Converting injected urls to crawl db entries.\"</p>\n\n<p>Does anyone how to get past this error?</p>\n", "creation_date": 1457554464, "score": 1},
{"title": "Nutch conf/crawl-urlfilter.txt not found in Nutch 1.11", "view_count": 81, "is_answered": false, "answers": [{"question_id": 34427289, "owner": {"user_id": 1965370, "link": "http://stackoverflow.com/users/1965370/do-do", "user_type": "registered", "reputation": 389}, "body": "<p>I think the author of the guide means this file:<code>conf/regex-urlfilter.txt</code> not this one: <code>conf/crawl-urlfilter.txt</code></p>\n\n<p>Hope this helps,</p>\n\n<p>Le Quoc Do</p>\n", "creation_date": 1457718442, "is_accepted": false, "score": 0, "last_activity_date": 1457718442, "answer_id": 35946685}], "question_id": 34427289, "tags": ["apache", "solr", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/34427289/nutch-conf-crawl-urlfilter-txt-not-found-in-nutch-1-11", "last_activity_date": 1457718442, "owner": {"user_id": 1553519, "answer_count": 50, "creation_date": 1343279211, "accept_rate": 82, "view_count": 611, "location": "Shenzhen, China", "reputation": 3479}, "body": "<p>All, I was trying to setup <a href=\"http://www.apache.org/dyn/closer.lua/nutch/1.11/apache-nutch-1.11-bin.zip\" rel=\"nofollow\">nutch 1.11</a> in my Windows7 cygwin environment. Currently I am following <a href=\"http://blog.building-blocks.com/technical-tips/building-a-search-engine-with-nutch-and-solr-in-10-minutes\" rel=\"nofollow\">this guide</a>. But unfortunately the file <code>crawl-urlfilter.txt</code> mentioned in the read is not expected under the folder <code>conf</code>. It seems 1.11 doesn't have this file ? Did I miss something ? Thanks.</p>\n", "creation_date": 1450834891, "score": 0},
{"title": "Advice in Using Nutch Content Limit", "view_count": 111, "owner": {"age": 21, "answer_count": 0, "creation_date": 1351572204, "user_id": 1784574, "accept_rate": 91, "view_count": 8, "location": "Los Ba&#241;os, Philippines", "reputation": 41}, "is_answered": true, "answers": [{"question_id": 34938020, "owner": {"user_id": 1965370, "link": "http://stackoverflow.com/users/1965370/do-do", "user_type": "registered", "reputation": 389}, "body": "<p>The problem is that you set the crawling depth to unlimited (-1). When your crawler system hits to heavy URLs such as <code>https://en.wikipedia.org, https://wikipedia.org and https://en.wikibooks.org</code>, your system may run out of Memory during the crawling process. You should increase memory for Nuch by setting NUTCH_HEAPSIZE environmental variable value <code>e.g., export NUTCH_HEAPSIZE=4000</code> (see details in the Nutch script). Note that this value is equivalent to Hadoop's HADOOP_HEAPSIZE. If it still doesn't work, you should increase the physical memory in your system ^ ^</p>\n\n<p>Hope this helps,</p>\n\n<p>Le Quoc Do</p>\n", "creation_date": 1457638800, "is_accepted": true, "score": 0, "last_activity_date": 1457638800, "answer_id": 35925720}], "question_id": 34938020, "tags": ["hadoop", "web-crawler", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/34938020/advice-in-using-nutch-content-limit", "last_activity_date": 1457638800, "accepted_answer_id": 35925720, "body": "<p>I am using Nutch 2.1 in crawling an entire domain (company.com, for example). I once ran across this problem where I am not getting all the links I want crawled because of the content limit set in Apache Nutch. Usually, when I check the content, only the upper half of the page is stored in the database, and thus the links on the lower half were not being fetched.</p>\n\n<p>In order to solve this, I changed the <strong>nutch-site.xml</strong> so that the content limit looks like this:</p>\n\n<pre><code>&lt;property&gt;\n    &lt;name&gt;http.content.limit&lt;/name&gt;\n    &lt;value&gt;-1&lt;/value&gt;\n    &lt;description&gt;The length limit for downloaded content using the http\n    protocol, in bytes. If this value is nonnegative (&gt;=0), content longer\n    than it will be truncated; otherwise, no truncation at all. Do not\n    confuse this setting with the file.content.limit setting.\n    &lt;/description&gt;\n&lt;/property&gt;\n</code></pre>\n\n<p>Doing that solved the problem, but at some point, I am encountering an OutOfMemory error, as evidenced by <strong>this output</strong> upon parsing:</p>\n\n<pre><code>ParserJob: starting\nParserJob: resuming:    false\nParserJob: forced reparse:  false\nParserJob: parsing all\nException in thread \"main\" java.lang.RuntimeException: job failed: name=parse, jobid=job_local_0001\nat org.apache.nutch.util.NutchJob.waitForCompletion(NutchJob.java:54)\nat org.apache.nutch.parse.ParserJob.run(ParserJob.java:251)\nat org.apache.nutch.parse.ParserJob.parse(ParserJob.java:259)\nat org.apache.nutch.parse.ParserJob.run(ParserJob.java:302)\nat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\nat org.apache.nutch.parse.ParserJob.main(ParserJob.java:306)\n</code></pre>\n\n<p>Here is my <strong>hadoop.log</strong> (part near the error):</p>\n\n<pre><code>    2016-01-22 02:02:35,898 INFO  crawl.SignatureFactory - Using Signature impl: org.apache.nutch.crawl.MD5Signature\n2016-01-22 02:02:37,255 WARN  util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n2016-01-22 02:02:39,130 INFO  mapreduce.GoraRecordReader - gora.buffer.read.limit = 10000\n2016-01-22 02:02:39,255 INFO  mapreduce.GoraRecordWriter - gora.buffer.write.limit = 10000\n2016-01-22 02:02:39,322 INFO  crawl.SignatureFactory - Using Signature impl: org.apache.nutch.crawl.MD5Signature\n2016-01-22 02:02:53,018 WARN  mapred.FileOutputCommitter - Output path is null in cleanup\n2016-01-22 02:02:53,031 WARN  mapred.LocalJobRunner - job_local_0001\njava.lang.OutOfMemoryError: Java heap space\n    at com.mysql.jdbc.MysqlIO.reuseAndReadPacket(MysqlIO.java:3051)\n    at com.mysql.jdbc.MysqlIO.reuseAndReadPacket(MysqlIO.java:2991)\n    at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3532)\n    at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:943)\n    at com.mysql.jdbc.MysqlIO.nextRow(MysqlIO.java:1441)\n    at com.mysql.jdbc.MysqlIO.readSingleRowSet(MysqlIO.java:2936)\n    at com.mysql.jdbc.MysqlIO.getResultSet(MysqlIO.java:477)\n    at com.mysql.jdbc.MysqlIO.readResultsForQueryOrUpdate(MysqlIO.java:2631)\n    at com.mysql.jdbc.MysqlIO.readAllResults(MysqlIO.java:1800)\n    at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2221)\n    at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2624)\n    at com.mysql.jdbc.PreparedStatement.executeInternal(PreparedStatement.java:2127)\n    at com.mysql.jdbc.PreparedStatement.executeQuery(PreparedStatement.java:2293)\n    at org.apache.gora.sql.store.SqlStore.execute(SqlStore.java:423)\n    at org.apache.gora.query.impl.QueryBase.execute(QueryBase.java:71)\n    at org.apache.gora.mapreduce.GoraRecordReader.executeQuery(GoraRecordReader.java:66)\n    at org.apache.gora.mapreduce.GoraRecordReader.nextKeyValue(GoraRecordReader.java:102)\n    at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:532)\n    at org.apache.hadoop.mapreduce.MapContext.nextKeyValue(MapContext.java:67)\n    at org.apache.hadoop.map\n</code></pre>\n\n<p>I only encountered this problemm when I set the content limit to -1. However, if I don't do that, there's a chance I won't get all my the links I wanted crawled. Any advice on how to use the content limit? Is doing that not really advisable? If so, what possible alternatives could I use? Thanks!</p>\n", "creation_date": 1453429972, "score": 0},
{"title": "Nutch to allow when the host name is having portnumber", "view_count": 25, "is_answered": false, "answers": [{"question_id": 35227924, "owner": {"user_id": 1965370, "link": "http://stackoverflow.com/users/1965370/do-do", "user_type": "registered", "reputation": 389}, "body": "<p>The problem is the syntax: <code>+^http://abc\\.com\\:85</code> is not correct. Please check the syntax here: <a href=\"http://stackoverflow.com/questions/13884249/nutch-regex-urlfilter-syntax\">Nutch regex-urlfilter syntax</a></p>\n\n<p>Hope this helps,</p>\n\n<p>Le Quoc Do</p>\n", "creation_date": 1457637620, "is_accepted": false, "score": 0, "last_activity_date": 1457637620, "answer_id": 35925363}], "question_id": 35227924, "tags": ["solr", "web-crawler", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/35227924/nutch-to-allow-when-the-host-name-is-having-portnumber", "last_activity_date": 1457637620, "owner": {"user_id": 3732294, "answer_count": 0, "creation_date": 1402537534, "accept_rate": 0, "view_count": 10, "reputation": 16}, "body": "<p>I am using nutch to push and index  data to solr. In nutch, i have added <strong>abc.com:85</strong> to <strong>domain-urlfilter.txt</strong>  and <strong>+^<a href=\"http://abc%5C.com%5C:85\" rel=\"nofollow\">http://abc\\.com\\:85</a></strong> to <strong>regex-urlfilter.txt</strong>.</p>\n\n<p>The problem is that nutch is not indexing data and it is throwing this message <strong>Total number of urls rejected by filters:1</strong></p>\n\n<p>Here in the url, i need the portnumber ,this configuration is done.</p>\n\n<p>Could you please let me know how to make nutch work with the port number :85 added.</p>\n", "creation_date": 1454686601, "score": 0},
{"title": "Nutch not crawling entire website", "view_count": 200, "is_answered": true, "answers": [{"question_id": 35714897, "owner": {"user_id": 1965370, "link": "http://stackoverflow.com/users/1965370/do-do", "user_type": "registered", "reputation": 389}, "body": "<p>In the case that Nutch crawls only one specified URL, please check Nutch filter (conf/regex-urlfilter.txt). To crawl all URLs in the seed, the content of regex-urlfilter.txt should be as follows.</p>\n\n<pre><code># accept all URLs\n+.\n</code></pre>\n\n<p>See details here: <a href=\"http://wiki.apache.org/nutch/NutchTutorial\" rel=\"nofollow\">http://wiki.apache.org/nutch/NutchTutorial</a></p>\n\n<p>Hope this helps,</p>\n\n<p>Le Quoc Do</p>\n", "creation_date": 1457636834, "is_accepted": false, "score": 1, "last_activity_date": 1457636834, "answer_id": 35925128}], "question_id": 35714897, "tags": ["apache", "web-crawler", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/35714897/nutch-not-crawling-entire-website", "last_activity_date": 1457636834, "owner": {"user_id": 5178563, "answer_count": 0, "creation_date": 1438358009, "accept_rate": 0, "view_count": 6, "reputation": 16}, "body": "<p>I am using nutch <strong>2.3.1</strong></p>\n\n<p>I preform the commands to crawl a site:</p>\n\n<ul>\n<li>./nutch inject ../urls/seed.txt</li>\n<li>./nutch generate -topN 2500 </li>\n<li>./nutch fetch -all</li>\n</ul>\n\n<p>The problem is, nutch is only crawling the first URL (the one specified in seeds.txt). The data is only the HTML from the first URL/page. </p>\n\n<p>All the other URLS that were accumulated by the generate command are not actually crawled. </p>\n\n<p>I cannot get nutch to crawl the other generated urls...I also cannot get nutch to crawl the entire website. <strong>What are the options that I need to use to crawl an entire site?</strong></p>\n\n<p>Does anyone have any insights or recommendations?</p>\n\n<p>Thank you so much for your help</p>\n", "creation_date": 1456807887, "score": 1},
{"title": "How to fetch top k URL&#39;s for search keyword in java?", "view_count": 56, "is_answered": false, "question_id": 35118029, "tags": ["java", "solr", "web-crawler", "nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/35118029/how-to-fetch-top-k-urls-for-search-keyword-in-java", "last_activity_date": 1457627563, "owner": {"user_id": 5817565, "view_count": 18, "answer_count": 0, "creation_date": 1453314723, "reputation": 4}, "body": "<p>Based on the <strong>user query or keyword</strong> I need to fetch <strong>Top k urls</strong> from <strong>search engine (google)</strong>. Those urls are taken as seed urls for web page downloader which downloads the seed pages from internet.</p>\n", "creation_date": 1454266963, "score": 0},
{"title": "Nutch 2.2.1 + hBase", "view_count": 1951, "owner": {"age": 27, "answer_count": 17, "creation_date": 1332257106, "user_id": 1281305, "accept_rate": 88, "view_count": 129, "location": "Prague, Czech Republic", "reputation": 494}, "is_answered": true, "answers": [{"question_id": 17472714, "owner": {"user_id": 1281305, "accept_rate": 88, "link": "http://stackoverflow.com/users/1281305/johnny-greenwood", "user_type": "registered", "reputation": 494}, "body": "<p>Solved. You must add correct version of gora-hbase to you libraries. gora-hbase-0.3.jar</p>\n", "creation_date": 1372954683, "is_accepted": true, "score": 0, "last_activity_date": 1372954683, "answer_id": 17474696}], "question_id": 17472714, "tags": ["java", "hbase", "nutch", "gora"], "answer_count": 1, "link": "http://stackoverflow.com/questions/17472714/nutch-2-2-1-hbase", "last_activity_date": 1457544110, "accepted_answer_id": 17474696, "body": "<p>I am trying to run the new version of Apache Nutch for crawling. When I start script /bin/crawl, it fails and hadoop.log says:</p>\n\n<p><em>java.lang.Exception: java.lang.NoSuchMethodError:     org.apache.gora.persistency.Persistent.getSchema()Lorg/apache/avro/Schema;\n    at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:354)\n    Caused by: java.lang.NoSuchMethodError:     org.apache.gora.persistency.Persistent.getSchema()Lorg/apache/avro/Schema;\n    at org.apache.gora.hbase.store.HBaseStore.put(HBaseStore.java:177)</em></p>\n\n<p>Here is the log:</p>\n\n<pre><code>2013-07-04 16:12:05,069 WARN  mapred.LocalJobRunner - job_local1522971864_0001\njava.lang.Exception: java.lang.NoSuchMethodError:     org.apache.gora.persistency.Persistent.getSchema()Lorg/apache/avro/Schema;\nat org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:354)\nCaused by: java.lang.NoSuchMethodError:     org.apache.gora.persistency.Persistent.getSchema()Lorg/apache/avro/Schema;\nat org.apache.gora.hbase.store.HBaseStore.put(HBaseStore.java:177)\nat org.apache.gora.mapreduce.GoraRecordWriter.write(GoraRecordWriter.java:65)\nat org.apache.hadoop.mapred.MapTask$NewDirectOutputCollector.write(MapTask.java:638)\nat org.apache.hadoop.mapreduce.TaskInputOutputContext.write(TaskInputOutputContext.java:80)\nat org.apache.nutch.crawl.InjectorJob$UrlMapper.map(InjectorJob.java:191)\nat org.apache.nutch.crawl.InjectorJob$UrlMapper.map(InjectorJob.java:88)\nat org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:145)\nat org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:764)\nat org.apache.hadoop.mapred.MapTask.run(MapTask.java:364)\nat org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:223)\nat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)\nat java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)\nat java.util.concurrent.FutureTask.run(FutureTask.java:166)\nat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\nat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\nat java.lang.Thread.run(Thread.java:722)\n\n2013-07-04 16:12:05,720 ERROR crawl.InjectorJob - InjectorJob: java.lang.RuntimeException: job failed: name=[new]inject /opt/ir/nutch2/urls, jobid=job_local1522971864_0001\nat org.apache.nutch.util.NutchJob.waitForCompletion(NutchJob.java:54)\nat org.apache.nutch.crawl.InjectorJob.run(InjectorJob.java:233)\nat org.apache.nutch.crawl.InjectorJob.inject(InjectorJob.java:251)\nat org.apache.nutch.crawl.InjectorJob.run(InjectorJob.java:273)\nat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\nat org.apache.nutch.crawl.InjectorJob.main(InjectorJob.java:282)\n</code></pre>\n\n<p>Should I set some gora artifacts inside ivy.xml or something? Please help me.</p>\n", "creation_date": 1372947794, "score": 0},
{"title": "nutch Job Failed", "view_count": 3706, "owner": {"age": 27, "answer_count": 12, "creation_date": 1286472479, "user_id": 469396, "accept_rate": 29, "view_count": 160, "reputation": 660}, "is_answered": true, "answers": [{"question_id": 15778060, "owner": {"user_id": 469396, "accept_rate": 29, "link": "http://stackoverflow.com/users/469396/change", "user_type": "registered", "reputation": 660}, "body": "<p>the issue was with '-dir crawl'. You need to mention the correct directory path/name.</p>\n", "creation_date": 1371163450, "is_accepted": true, "score": 0, "last_activity_date": 1371163450, "answer_id": 17098438}], "question_id": 15778060, "tags": ["apache", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/15778060/nutch-job-failed", "last_activity_date": 1457431384, "accepted_answer_id": 17098438, "body": "<p>I am trying to follow simple steps on <a href=\"http://wiki.apache.org/nutch/NutchTutorial#A3._Crawl_your_first_website\" rel=\"nofollow\">Nutch tutorial</a>.\nAll goes good till here:</p>\n\n<blockquote>\n  <p>@ubuntu:/usr/local/nutch/framework/apache-nutch-1.6$  bin/nutch crawl bin/urls -dir crawl -depth 3 -topN 5 -threads 1</p>\n</blockquote>\n\n<p>Gives following output:</p>\n\n<pre><code>    log4j:ERROR setFile(null,true) call failed\njava.io.FileNotFoundException: /usr/local/nutch/framework/apache-nutch-1.6/logs/hadoop.log (No such file or directory)\n    at java.io.FileOutputStream.open(Native Method)\n    at java.io.FileOutputStream.&lt;init&gt;(FileOutputStream.java:212)\n    at java.io.FileOutputStream.&lt;init&gt;(FileOutputStream.java:136)\n    at org.apache.log4j.FileAppender.setFile(FileAppender.java:290)\n    at org.apache.log4j.FileAppender.activateOptions(FileAppender.java:164)\n    at org.apache.log4j.DailyRollingFileAppender.activateOptions(DailyRollingFileAppender.java:216)\n    at org.apache.log4j.config.PropertySetter.activate(PropertySetter.java:257)\n    at org.apache.log4j.config.PropertySetter.setProperties(PropertySetter.java:133)\n    at org.apache.log4j.config.PropertySetter.setProperties(PropertySetter.java:97)\n    at org.apache.log4j.PropertyConfigurator.parseAppender(PropertyConfigurator.java:689)\n    at org.apache.log4j.PropertyConfigurator.parseCategory(PropertyConfigurator.java:647)\n    at org.apache.log4j.PropertyConfigurator.configureRootCategory(PropertyConfigurator.java:544)\n    at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:440)\n    at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:476)\n    at org.apache.log4j.helpers.OptionConverter.selectAndConfigure(OptionConverter.java:471)\n    at org.apache.log4j.LogManager.&lt;clinit&gt;(LogManager.java:125)\n    at org.slf4j.impl.Log4jLoggerFactory.getLogger(Log4jLoggerFactory.java:73)\n    at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:242)\n    at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:254)\n    at org.apache.nutch.crawl.Crawl.&lt;clinit&gt;(Crawl.java:43)\nlog4j:ERROR Either File or DatePattern options are not set for appender [DRFA].\nsolrUrl is not set, indexing will be skipped...\ncrawl started in: crawl\nrootUrlDir = bin/urls\nthreads = 1\ndepth = 3\nsolrUrl=null\ntopN = 5\nInjector: starting at 2013-04-02 19:08:03\nInjector: crawlDb: crawl/crawldb\nInjector: urlDir: bin/urls\nInjector: Converting injected urls to crawl db entries.\nInjector: total number of urls rejected by filters: 0\nInjector: total number of urls injected after normalization and filtering: 1\nInjector: Merging injected urls into crawl db.\nException in thread \"main\" java.io.IOException: Job failed!\n    at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:1265)\n    at org.apache.nutch.crawl.Injector.inject(Injector.java:296)\n    at org.apache.nutch.crawl.Crawl.run(Crawl.java:127)\n    at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n    at org.apache.nutch.crawl.Crawl.main(Crawl.java:55)\n</code></pre>\n\n<p>My bin directory has:\n1. nutch\n2. crawl\n3. urls----> seeds.txt</p>\n\n<p>not sure where the problem is. My first nutch run!</p>\n\n<p>hadoop.log has following error:</p>\n\n<pre><code>2013-04-03 17:33:18,370 ERROR mapred.FileOutputCommitter - Mkdirs failed to create file:/usr/local/nutch/framework/apache-nutch-1.6/bin/crawl/crawldb/1971189408/_temporary\n\n2013-04-03 17:33:21,394 WARN  mapred.LocalJobRunner - job_local_0002\n\njava.io.IOException: The temporary job-output directory file:/usr/local/nutch/framework/apache-nutch-1.6/bin/crawl/crawldb/1971189408/_temporary doesn't exist!\n</code></pre>\n\n<p><strong>Resolved</strong></p>\n\n<p>the issue was with '-dir crawl'. You need to mention the correct directory path/name.</p>\n", "creation_date": 1364957450, "score": 3},
{"title": "Failure in apache nutch configuration on eclipse", "view_count": 43, "is_answered": false, "question_id": 35831488, "tags": ["eclipse", "apache", "web-crawler", "nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/35831488/failure-in-apache-nutch-configuration-on-eclipse", "last_activity_date": 1457292620, "owner": {"user_id": 1209123, "view_count": 2, "answer_count": 0, "creation_date": 1329224613, "reputation": 1}, "body": "<p>-The following error (Injector: java.io.IOException: Job failed!) pop up every time i try to run apache nutch to inject some urls. I tried the described solutions but it did not work. So i want to know what am I missing?</p>\n\n<pre><code>SLF4J: Class path contains multiple SLF4J bindings.\nSLF4J: Found binding in \n[jar:file:/C:/Users/asus/.m2/repository/org/slf4j/slf4j log4j12/1.7.5/slf4j-\nlog4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]\nSLF4J: Found binding in \n[jar:file:/C:/Users/asus/.m2/repository/ch/qos/logback/logback \nclassic/1.0.9/logback-classic-\n1.0.9.jar!/org/slf4j/impl/StaticLoggerBinder.class]\nSLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an \nexplanation.\nSLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\nInjector: starting at 2016-03-06 20:01:00\nInjector: crawlDb: Data/crawldb\nInjector: urlDir: urls\nInjector: Converting injected urls to crawl db entries.\nPatch for HADOOP-7682: Instantiating workaround file system\nPatch for HADOOP-7682: Ignoring IOException setting persmission for path \n\"file:/tmp/hadoop-asus/mapred/staging/asus1653777909/.staging\": Failed to set \npermissions of path: \\tmp\\hadoop-asus\\mapred\\staging\\asus1653777909\\.staging \nto 0700\nPatch for HADOOP-7682: Ignoring IOException setting persmission for path \n\"file:/tmp/hadoop-\nasus/mapred/staging/asus1653777909/.staging/job_local1653777909_0001\": Failed \nto set permissions of path: \\tmp\\hadoop- \nasus\\mapred\\staging\\asus1653777909\\.staging\\job_local1653777909_0001 to 0700\nPatch for HADOOP-7682: Ignoring IOException setting persmission for path\n\"file:/tmp/hadoop-\nasus/mapred/staging/asus1653777909/.staging/job_local1653777909_0001/job\n.jar\":   \nFailed to set permissions of path: \\tmp\\hadoop-\nasus\\mapred\\staging\\asus1653777909\\.staging\\job_local1653777909_0001\\job.jar \nto 0644\nPatch for HADOOP-7682: Ignoring IOException setting persmission for path\n\"file:/tmp/hadoop-\nasus/mapred/staging/asus1653777909/.staging/job_local1653777909_0001\n/job.split\":Failed to set permissions of path: \\tmp\\hadoop-\nasus\\mapred\\staging\\asus1653777909\\.staging\\job_local1653777909_0001\n\\job.split to 0644\nPatch for HADOOP-7682: Ignoring IOException setting persmission for path\n\"file:/tmp/hadoop-asus/mapred/staging/asus1653777909/.staging/\njob_local1653777909_0001/job.splitmetainfo\": Failed to set\npermissions of path: \\tmp\\hadoop-asus\\mapred\\staging\\asus1653777909\\.staging\\\njob_local1653777909_0001\\job.splitmetainfo to 0644\nPatch for HADOOP-7682: Ignoring IOException setting persmission for path\n\"file:/tmp/hadoop-asus/mapred/staging/asus1653777909/.staging/\njob_local1653777909_0001/job.xml\": Failed to set permissions of path:\n\\tmp\\hadoop-asus\\mapred\\staging\\asus1653777909\\.staging\\\njob_local1653777909_0001\\job.xml to 0644\n\n\nInjector: java.io.IOException: Job failed! \nat org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:1357)\nat org.apache.nutch.crawl.Injector.inject(Injector.java:325)\nat org.apache.nutch.crawl.Injector.run(Injector.java:381)\nat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\nat org.apache.nutch.crawl.Injector.main(Injector.java:371)`\n</code></pre>\n", "creation_date": 1457292620, "score": 0},
{"title": "identify product after web crawling, price comparison", "view_count": 307, "owner": {"user_id": 3705221, "view_count": 2, "answer_count": 1, "creation_date": 1401845113, "reputation": 23}, "is_answered": true, "answers": [{"question_id": 35808547, "owner": {"user_id": 1822643, "accept_rate": 76, "link": "http://stackoverflow.com/users/1822643/binoy-dalal", "user_type": "registered", "reputation": 318}, "body": "<p>I have done something similar for my project where we tag people names with their IDs, so basically the same person can have their name listed as the full name or initials, or only the first name etc. and we tag it to the same ID.</p>\n\n<p>So for your case this will basically entail building an inverted index for your products and then scanning the title field for the product names and tagging them to a particular product ID. This way all Samsung Galaxy S6s get mapped to the same product.</p>\n\n<p>This does not require any learning to be performed, you just need to have database to pick up all unique products from and keep updating your index as your product database changes.</p>\n\n<p>All of this can be done at index time by writing an update processor for solr.</p>\n\n<p>The implementation is a bit complex to put it all here so I've just outlined the basic idea that could help you out.</p>\n", "creation_date": 1457250906, "is_accepted": true, "score": 0, "last_activity_date": 1457250906, "answer_id": 35824470}], "question_id": 35808547, "tags": ["solr", "machine-learning", "web-crawler", "classification", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/35808547/identify-product-after-web-crawling-price-comparison", "last_activity_date": 1457250906, "accepted_answer_id": 35824470, "body": "<p>I am currently working on developing a price comparison site for which I crawl some e-commerce websites and extract some data from their HTML pages like price, title, metadata etc. I am at a point now that I need two identify if two products crawled from different websites are actually the same and assign a common label for both of them.<br/>\nFor example, lets say site 1 has as a title for a product the following string: <br/>\n\"<em>Smartphone Samsung Galaxy S6 4G 32GB</em>\" <br/>\nand site 2 has as a title for the same product this string: <br/>\n\"<em>Samsung Galaxy S6 White</em>\"<br/></p>\n\n<p>How can I identify if these two products are actually the same product, which I want to label in my site as \"Samsung Galaxy S6\"?\nI have thought of using some machine learning techniques like classification or clustering. However, classification will probably require a big set of already well formatted products' labels (plus frequently updated) to act as the possible classes e.g. class \"<em>Samsung Galaxy S6</em>\", is there such a thing? Also with such a huge number of classes it may not be feasible.</p>\n\n<p>I am using Apache <strong>Nutch</strong> for crawling and <strong>Solr</strong> for indexing and search. If there is any specific library or tool for those it will be very helpful, but my question is not specifically for those and I will be very happy to read any suggestion.</p>\n\n<p>Thanks</p>\n", "creation_date": 1457138123, "score": 1},
{"title": "Integrate Nutch With Java", "view_count": 73, "is_answered": false, "question_id": 33525447, "tags": ["java", "nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/33525447/integrate-nutch-with-java", "last_activity_date": 1456926535, "owner": {"user_id": 3905481, "view_count": 12, "answer_count": 2, "creation_date": 1407132772, "reputation": 12}, "body": "<p>I Download Apache Nutch and it work . but need to integrate With Java  </p>\n\n<p>I try with some codes but I get Error this code from stackoverflow </p>\n\n<blockquote>\n  <p>Error 15/11/04 15:37:09 INFO ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s). java.net.ConnectException: Call to localhost/127.0.0.1:9000 failed on connection exception: java.net.ConnectException: Connection refused: no further information at org.apache.hadoop.ipc.Client.wrapException(Client.java:1131)</p>\n</blockquote>\n\n<p><a href=\"http://stackoverflow.com/questions/5123757/how-to-get-the-html-content-from-nutch\">How to get the html content from nutch</a></p>\n\n<p>......\nI read this question and have the same problem need to start with nutch</p>\n\n<p><a href=\"http://stackoverflow.com/questions/4340222/nutch-api-advice\">Nutch API advice</a></p>\n\n<pre><code> import org.apache.hadoop.conf.Configuration;\n import org.apache.nutch.searcher.Hit;\n import org.apache.nutch.searcher.HitDetails;\n import org.apache.nutch.searcher.Hits;\n import org.apache.nutch.searcher.NutchBean;\n import org.apache.nutch.searcher.Query;\n import org.apache.nutch.util.NutchConfiguration;\n import java.util.Date;\n\npublic class Search {\n    public static void main(String[] args) {\n            try {\n                    // define a keyword for the search\n                    String nutchSearchString = \"smart\";\n\n                    // configure nutch\n                    Configuration nutchConf = NutchConfiguration.create();\n                    NutchBean nutchBean = new NutchBean(nutchConf);\n                    // build the query\n                    Query nutchQuery = Query.parse(nutchSearchString, nutchConf);\n                    // optionally specify the maximum number of hits (default is 10)\n                    // nutchQuery.getParams().setNumHits(100);\n                    // nutchQuery.getParams().setMaxHitsPerDup(100);\n                    Hits nutchHits = nutchBean.search(nutchQuery);\n\n                    // display the number of hits\n                    System.out.println(\"Found \" + nutchHits.getLength() + \" hits.\\n\");\n\n                    // get the details about each hit (includes title, URL, a summary\n                    // and the date when this was fetched)\n                    for (int i = 0; i &lt; nutchHits.getLength(); i++) {\n                            Hit hit = nutchHits.getHit(i);\n                            HitDetails details = nutchBean.getDetails(hit);\n                            String title = details.getValue(\"title\");\n                            String url = details.getValue(\"url\");\n                            String summary = nutchBean.getSummary(details, nutchQuery)\n                                            .toString();\n                            System.out.println(\"Title is: \" + title);\n                            System.out.println(\"(\" + url + \")\");\n                            Date date = new Date(nutchBean.getFetchDate(details));\n                            System.out.println(\"Date Fetched: \" + date);\n                            System.out.println(summary + \"\\n\");\n                            System.out.println(\"----------------------------------------\");\n                    }\n\n                    // as usually, don't forget to close the resources\n                    nutchBean.close();\n            } catch (Throwable e) {\n                    e.printStackTrace();\n            }\n    }\n</code></pre>\n\n<p>}</p>\n", "creation_date": 1446650308, "score": 0},
{"title": "Error org.apache.hadoop.hbase.regionserver.LeaseException", "view_count": 386, "is_answered": false, "answers": [{"question_id": 27481362, "owner": {"user_id": 4839157, "link": "http://stackoverflow.com/users/4839157/krazygautam", "user_type": "registered", "reputation": 309}, "body": "<p>Figure out whats the average size of your record , multiply it with hbase.client.scanner.caching .</p>\n\n<p>if the result > 1 mb u need to increase the scanner.lease.timeout </p>\n", "creation_date": 1431521284, "is_accepted": false, "score": 0, "last_activity_date": 1431521284, "answer_id": 30215267}], "question_id": 27481362, "tags": ["java", "apache", "hadoop", "hbase", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/27481362/error-org-apache-hadoop-hbase-regionserver-leaseexception", "last_activity_date": 1456479193, "owner": {"user_id": 3454410, "answer_count": 20, "creation_date": 1395646154, "accept_rate": 48, "view_count": 174, "reputation": 832}, "body": "<p>My Apache Nutch crawler is generating following information in log files.</p>\n\n<p>\"WARN  client.ScannerCallable - Ignore, probably already closed\norg.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hbase.regionserver.LeaseException: lease '-3687805264051264867' does not exist\"</p>\n\n<p>I have made a change in hbase configuration file(hbase-site.xml).</p>\n\n<pre><code>&lt;property&gt;\n&lt;name&gt;hbase.client.scanner.caching&lt;/name&gt;\n&lt;value&gt;100&lt;/value&gt;\n&lt;/property&gt;\n</code></pre>\n\n<p>But it does not effect anything and above same warning is appearing again and again.\nAny idea?</p>\n", "creation_date": 1418637307, "score": 1},
{"title": "Nutch 1.3 and Solr 4.4.0 integration Job failed", "view_count": 2285, "is_answered": false, "answers": [{"question_id": 18425867, "owner": {"user_id": 2623052, "accept_rate": 45, "link": "http://stackoverflow.com/users/2623052/allan-macmillan", "user_type": "registered", "reputation": 866}, "body": "<p>Check your configuration against: <a href=\"http://amac4.blogspot.co.uk/2013/07/setting-up-solr-with-apache-tomcat-be.html\" rel=\"nofollow\">Solr</a> and <a href=\"http://amac4.blogspot.co.uk/2013/07/configuring-nutch-to-crawl-urls.html\" rel=\"nofollow\">Nutch</a></p>\n\n<p>Nutch and Solr's schema files should be the same or you may encounter problems so make sure they match up</p>\n", "creation_date": 1377468264, "is_accepted": false, "score": 0, "last_activity_date": 1377468264, "answer_id": 18434203}, {"question_id": 18425867, "owner": {"user_id": 2146864, "accept_rate": 60, "link": "http://stackoverflow.com/users/2146864/billni", "user_type": "registered", "reputation": 82}, "body": "<p>When I meet same problem in nutch, the solr's log appear a error message \"unknown field host\".\nAfter modifying  the schema.xml in solr, the nutch's error vanish.</p>\n", "creation_date": 1389231567, "is_accepted": false, "score": 0, "last_activity_date": 1389231567, "answer_id": 21010155}, {"question_id": 18425867, "owner": {"user_id": 547262, "accept_rate": 49, "link": "http://stackoverflow.com/users/547262/merlin", "user_type": "registered", "reputation": 348}, "body": "<p>You are missing the name of the core inside your command.</p>\n\n<p>e.g.:</p>\n\n<pre><code>./bin/crawl -i -D solr.server.url=http://localhost:8983/solr/#/your_corname urls/ crawl 1\n</code></pre>\n", "creation_date": 1456399024, "is_accepted": false, "score": 0, "last_activity_date": 1456399024, "answer_id": 35625731}], "question_id": 18425867, "tags": ["solr", "nutch", "web-crawler"], "answer_count": 3, "link": "http://stackoverflow.com/questions/18425867/nutch-1-3-and-solr-4-4-0-integration-job-failed", "last_activity_date": 1456399024, "owner": {"user_id": 585874, "answer_count": 35, "creation_date": 1295724712, "accept_rate": 62, "view_count": 225, "location": "Iran", "reputation": 990}, "body": "<p>I am trying to crawl the web using nutch and I followed the documentation steps in the nutch's official web site (run the crawl successfully, copy the scheme-solr4.xml into solr directory). but when I run the</p>\n\n<pre><code>bin/nutch solrindex http://localhost:8983/solr/ crawl/crawldb -linkdb crawl/linkdb crawl/segments/*\n</code></pre>\n\n<p>I get the following error:</p>\n\n<pre><code>Indexer: starting at 2013-08-25 09:17:35\nIndexer: deleting gone documents: false\nIndexer: URL filtering: false\nIndexer: URL normalizing: false\nActive IndexWriters :\nSOLRIndexWriter\n    solr.server.url : URL of the SOLR instance (mandatory)\n    solr.commit.size : buffer size when sending to SOLR (default 1000)\n    solr.mapping.file : name of the mapping file for fields (default solrindex-mapping.xml)\n    solr.auth : use authentication (default false)\n    solr.auth.username : use authentication (default false)\n    solr.auth : username for authentication\n    solr.auth.password : password for authentication\n\n\nIndexer: java.io.IOException: Job failed!\n    at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:1357)\n    at org.apache.nutch.indexer.IndexingJob.index(IndexingJob.java:123)\n    at org.apache.nutch.indexer.IndexingJob.run(IndexingJob.java:185)\n    at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n    at org.apache.nutch.indexer.IndexingJob.main(IndexingJob.java:195)\n</code></pre>\n\n<p>I have to mention that the solr is running but I cannot browse <code>http://localhost:8983/solr/admin</code> (it redirects me to <code>http://localhost:8983/solr/#</code>).</p>\n\n<p>On the other hand, when I stop the solr, I get the same error! Does anybody have any idea about what is wrong with my setting?</p>\n\n<p>P.S. the url that I crawl is: <code>http://localhost/NORC</code></p>\n", "creation_date": 1377406831, "score": 2},
{"title": "How to limit max urls in nutch 1.11?", "view_count": 32, "is_answered": false, "question_id": 35624036, "tags": ["solr", "web-crawler", "nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/35624036/how-to-limit-max-urls-in-nutch-1-11", "last_activity_date": 1456394542, "owner": {"user_id": 547262, "answer_count": 9, "creation_date": 1292707031, "accept_rate": 49, "view_count": 59, "location": "Munich, Germany", "reputation": 348}, "body": "<p>I am nuw to nutch and just installed nutch 1.11 and solr 5.4.1 on os x. Solr and Nutch are runnnig, so far so good ...</p>\n\n<p>Now I am wondering, how I could limit the amount of URLs. Since issuing following command: <code>./bin/crawl -i -D solr.server.url=http://localhost:8983/solr/ urls/ crawl/ 1</code>, nutch is fetching <strong>all</strong> URLs from the specified domain.</p>\n\n<p>The output on the console looks like this:</p>\n\n<pre><code>fetching http://www.domain.com/c/kitten-kaufen (queue crawl delay=5000ms)\nfetching http://m.domain.com/test/dienstleistungen (queue crawl delay=5000ms)\n-activeThreads=50, spinWaiting=50, fetchQueues.totalSize=1445, fetchQueues.getQueueCount=2\n-activeThreads=50, spinWaiting=50, fetchQueues.totalSize=1445, fetchQueues.getQueueCount=2\n-activeThreads=50, spinWaiting=50, fetchQueues.totalSize=1445, fetchQueues.getQueueCount=2\n-activeThreads=50, spinWaiting=50, fetchQueues.totalSize=1445, fetchQueues.getQueueCount=2\n-activeThreads=50, spinWaiting=50, fetchQueues.totalSize=1445, fetchQueues.getQueueCount=2\n</code></pre>\n\n<p>While solr does not show any documents so far. </p>\n\n<p>Questions:</p>\n\n<ol>\n<li>How to limit the amount of documents?</li>\n<li>How to pull the docs into solr?</li>\n</ol>\n\n<p>Thank you in advance for any help.</p>\n", "creation_date": 1456394542, "score": 0},
{"title": "How to remove/exclude a doc based on unqiue id from the solr result", "view_count": 21, "is_answered": true, "answers": [{"question_id": 35594423, "owner": {"user_id": 1749606, "accept_rate": 52, "link": "http://stackoverflow.com/users/1749606/inikkz", "user_type": "registered", "reputation": 699}, "body": "<p>Try this:</p>\n\n<pre><code>/select?q=*:*&amp;fq=tag_id:367 AND id:[* TO *] -id:(306670 302209)\n</code></pre>\n\n<p>Add you document ids inside <strong>-id:(X, Y, Z)</strong> to exclude them.</p>\n\n<p>You can find the answer <a href=\"http://stackoverflow.com/questions/11855830/how-to-do-not-in-query-in-solr\">here</a>. </p>\n", "creation_date": 1456294124, "is_accepted": false, "score": 1, "last_activity_date": 1456294124, "answer_id": 35594532}, {"question_id": 35594423, "owner": {"user_id": 3636071, "accept_rate": 100, "link": "http://stackoverflow.com/users/3636071/abhijit-bashetti", "user_type": "registered", "reputation": 3104}, "body": "<p>if you know the ids you can add them in the filter query.</p>\n\n<p><code>fq</code> : This parameter can be used to specify a query that can be used to restrict the super set of documents that can be returned</p>\n", "creation_date": 1456294166, "is_accepted": false, "score": 0, "last_activity_date": 1456294166, "answer_id": 35594541}], "question_id": 35594423, "tags": ["solr", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/35594423/how-to-remove-exclude-a-doc-based-on-unqiue-id-from-the-solr-result", "last_activity_date": 1456294166, "owner": {"user_id": 3732294, "answer_count": 0, "creation_date": 1402537534, "accept_rate": 0, "view_count": 10, "reputation": 16}, "body": "<p>I am using solr4.10 , i have few document ids that must not be displayed from any search query.Could some provide inputs as how this could be achieved.</p>\n", "creation_date": 1456293705, "score": 0},
{"title": "Solr; read specific data", "view_count": 15, "is_answered": false, "answers": [{"question_id": 35492210, "owner": {"user_id": 1822643, "accept_rate": 76, "link": "http://stackoverflow.com/users/1822643/binoy-dalal", "user_type": "registered", "reputation": 318}, "body": "<p>Yes. If you only want to search and return a particluar field, you can explicitly search on that field using <code>&lt;fieldname&gt;:&lt;query&gt;</code> and to only return some particular fields use the <code>fl</code> parameter <code>fl=Price</code>.</p>\n\n<p>So your solr query should be something like this:</p>\n\n<pre><code>http://localhost:8983/solr/collection1/select?q=Price:500&amp;fl=Price\n</code></pre>\n\n<p>Additionally if you want to search over and return multiple fields, use the <code>qf</code> parameter along with the <code>edismax</code> parser like this:</p>\n\n<pre><code>http://localhost:8983/solr/collection1/select?q=500&amp;fl=Price,Name&amp;defType=edismax&amp;qf=Price,Name\n</code></pre>\n", "creation_date": 1455997628, "is_accepted": false, "score": 0, "last_activity_date": 1455997628, "answer_id": 35528186}], "question_id": 35492210, "tags": ["solr", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/35492210/solr-read-specific-data", "last_activity_date": 1455997628, "owner": {"user_id": 4788541, "answer_count": 0, "creation_date": 1429032879, "accept_rate": 0, "view_count": 0, "reputation": 2}, "body": "<p>Is there anyway to read specific data in solr? For example; using nutch I crawled <a href=\"http://rads.stackoverflow.com/amzn/click/B018MRT16Q\" rel=\"nofollow\">http://www.amazon.com/Jessica-Simpson-Womens-Asymmetrical-X-Small/dp/B018MRT16Q/ref=lp_13906149011_1_3?m=ATVPDKIKX0DER&amp;s=apparel&amp;ie=UTF8&amp;qid=1455828781&amp;sr=1-3&amp;nodeID=13906149011</a>; \nthen with solr I want it to search through and just display the price of the jacket. </p>\n", "creation_date": 1455828828, "score": 0},
{"title": "how to export Hbase data to elastic search using nutch?", "view_count": 75, "is_answered": false, "answers": [{"last_edit_date": 1455885437, "owner": {"user_id": 4381297, "link": "http://stackoverflow.com/users/4381297/shahzad-hussain", "user_type": "registered", "reputation": 1}, "body": "<p>below is my code</p>\n\n<pre><code>package com.process;\n/*\n  import package will be here\n*/\n\npublic class HbaseToElastic extends Configured implements\n    org.apache.hadoop.util.Tool {\n\nstatic class Mapper extends TableMapper&lt;Text, IndexWritable&gt; {\n\n    public static String CLUSTER;\n    public static String SEARCH_HOST;\n    public static String SEARCH_PORT;\n    public static String SEARCH_INDEX_NAME;\n    public static String SEARCHtYPE;\n    public static int BULKSIZE;\n    public static String TABLENAME;\n    public static String FAMILY;\n    private static List&lt;String&gt; SPORTS_KEYWORDS;\n    private static List&lt;String&gt; BUSINESS_KEYWORDS;\n    private static List&lt;String&gt; GOSSIP_KEYWORDS;\n    private static List&lt;String&gt; CRIME_KEYWORDS;\n    private static Map&lt;String, Map&lt;String, String&gt;&gt; STATE_MAP = new HashMap&lt;String, Map&lt;String, String&gt;&gt;();\n    private static Map&lt;String, String&gt; CITY_MAP = new HashMap&lt;String, String&gt;();\n\n    private static Mapper mapper = new Mapper();\n\n    static {\n        try {\n            System.out.println(\"done1\");\n            DetectorFactory.loadProfile(\"./profiles\");\n            System.out.println(\"done2\");\n        } catch (final LangDetectException e) {\n            System.out.println(\"done3\");\n            e.printStackTrace();\n        }\n\n    }\n\n    Configuration hbaseConf = null;\n    HTable table = null;\n\n    List&lt;Put&gt; hbasePutErrorList = new ArrayList&lt;Put&gt;();\n\n    /**\n     * Clean up the hbase table object\n     */\n    @Override\n    protected void cleanup(final Context context) throws IOException,\n            InterruptedException {\n        super.cleanup(context);\n\n        table.put(hbasePutErrorList);\n        table.close();\n        hbasePutErrorList.clear();\n    }\n\n    /**\n     * Initialize various variables\n     */\n    @Override\n    protected void setup(\n            final org.apache.hadoop.mapreduce.Mapper&lt;ImmutableBytesWritable, Result, Text, IndexWritable&gt;.Context context)\n            throws IOException, InterruptedException {\n        final Configuration conf = context.getConfiguration();\n\n        CLUSTER = conf.get(\"cluster\");\n        SEARCH_HOST = conf.get(\"search_host\");\n        SEARCH_PORT = conf.get(\"search_port\");\n        SEARCH_INDEX_NAME = conf.get(\"search_index_name\");\n        SEARCHtYPE = conf.get(\"search_type\");\n        BULKSIZE = conf.getInt(\"search_bulk_size\", 500);\n        TABLENAME = conf.get(\"table_name\");\n        FAMILY = conf.get(\"family\");\n\n        hbaseConf = HBaseConfiguration.create();\n        hbaseConf.set(\"hbase.zookeeper.quorum\",\n                conf.get(\"hbase.zookeeper.quorum\"));\n        hbaseConf.set(\"hbase.zookeeper.property.clientPort\",\n                conf.get(\"hbase.zookeeper.property.clientPort\"));\n        hbaseConf.set(\"hbase.rpc.timeout\", conf.get(\"hbase.rpc.timeout\"));\n        hbaseConf.set(\"hbase.regionserver.lease.period\",\n                conf.get(\"hbase.regionserver.lease.period\"));\n        hbaseConf.set(\"hbase.master\", conf.get(\"hbase.master\"));\n        table = new HTable(hbaseConf, conf.get(\"table_name\"));\n\n        SPORTS_KEYWORDS = new ArrayList&lt;String&gt;();\n        BUSINESS_KEYWORDS = new ArrayList&lt;String&gt;();\n        GOSSIP_KEYWORDS = new ArrayList&lt;String&gt;();\n        CRIME_KEYWORDS = new ArrayList&lt;String&gt;();\n        String keywrods = conf.get(\"sportskeywords\");\n        String[] keyarr = keywrods.split(\",\");\n        for (final String key : keyarr) {\n            SPORTS_KEYWORDS.add(key.trim());\n        }\n        keywrods = conf.get(\"businesskeywords\");\n        keyarr = keywrods.split(\",\");\n        for (final String key : keyarr) {\n            BUSINESS_KEYWORDS.add(key.trim());\n        }\n        keywrods = conf.get(\"gossipkeywords\");\n        keyarr = keywrods.split(\",\");\n        for (final String key : keyarr) {\n            GOSSIP_KEYWORDS.add(key.trim());\n        }\n        keywrods = conf.get(\"crimekeywords\");\n        keyarr = keywrods.split(\",\");\n        for (final String key : keyarr) {\n            CRIME_KEYWORDS.add(key.trim());\n        }\n\n        final String stateMap = conf.get(\"statemap\");\n        final Gson g = new Gson();\n\n        STATE_MAP = g.fromJson(stateMap, Map.class);\n\n    }\n\n    /**\n     * map function\n     */\n    @Override\n    public void map(final ImmutableBytesWritable row, final Result result,\n            final Context context) throws IOException, InterruptedException {\n        try {\n            final byte b = 0;\n            int deleteFlag = 0;\n            final String keyString = Bytes.toString(row.get());\n            final Map&lt;String, Object&gt; mapobject = new HashMap&lt;String, Object&gt;();\n            for (final KeyValue kv : result.raw()) {\n                final String key = (new String(kv.getQualifier()));\n                final String value = (new String(kv.getValue()));\n                mapobject.put(key, value);\n            }\n            final Gson g = new Gson();\n            if (checkValidType(mapobject)) {\n                refineMetaTags(mapobject);\n                if (refineDescription(mapobject)) {\n                    assignCity(mapobject);\n                    if (checkTitleImage(mapobject)) {\n                        if (setLang(mapobject)) {\n                            setCorrectCategory(mapobject);\n                            correctDuplicateTitle(mapobject);\n                            final String json = g.toJson(mapobject);\n                            context.write(new Text(keyString),\n                                    new IndexWritable(json, b));\n                            deleteFlag = 1;\n                        }\n                    }\n                }\n            }\n            if (deleteFlag == 0) {\n                final Put put = new Put(Bytes.toBytes(keyString));\n                put.add(Bytes.toBytes(\"cf\"), Bytes.toBytes(\"ErrorFlag\"),\n                        Bytes.toBytes(\"1\"));\n                hbasePutErrorList.add(put);\n            }\n        } catch (final Exception e) {\n            e.printStackTrace();\n        }\n    }\n\n    /**\n     * Remove duplicate statement in the title\n     *\n     * @param mapobject\n     */\n    private void correctDuplicateTitle(final Map&lt;String, Object&gt; mapobject) {\n        final String duplicateTitle = mapobject.get(\"title\").toString();\n        final String stripedTitleArr[] = duplicateTitle.split(\" \", 4);\n        if (stripedTitleArr.length == 4) {\n            final String subString = stripedTitleArr[0] + \" \"\n                    + stripedTitleArr[1] + \" \" + stripedTitleArr[2];\n            if (stripedTitleArr[3].contains(subString)) {\n\n                mapobject.put(\"title\", duplicateTitle\n                        .substring(duplicateTitle.indexOf(subString,\n                                subString.length() - 1)));\n\n                mapobject.put(\"title\", stripedTitleArr[3]\n                        .substring(stripedTitleArr[3].indexOf(subString)));\n            }\n        }\n    }\n\n    /**\n     * Set category based on the various category specific keyword\n     *\n     * @param mapobject\n     */\n    private void setCorrectCategory(final Map&lt;String, Object&gt; mapobject) {\n        final String url = mapobject.get(\"url\") + \"\";\n        final String cat = mapobject.get(\"tags\") + \"\";\n        if (\"sports\".equalsIgnoreCase(cat)\n                || \"cricket\".equalsIgnoreCase(cat)) {\n            if (!(url.toLowerCase().contains(\"sport\")\n                    || url.toLowerCase().contains(\"\u0916\u0947\u0932\")\n                    || url.toLowerCase().contains(\"cric\") || url\n                    .toLowerCase().contains(\"\u0915\u094d\u0930\u093f\u0915\u0947\u091f\"))) {\n                final String desc = mapobject.get(\"description\").toString();\n                boolean isSports = false;\n                int count = 0;\n                for (final String keyword : SPORTS_KEYWORDS) {\n                    if (desc.contains(keyword)) {\n                        count++;\n                    }\n                }\n                if (count &gt; 1) {\n                    isSports = true;\n                }\n                if (!isSports) {\n                    mapobject.put(\"tags\", \"national\");\n                }\n                if (isSports\n\n                        &amp;&amp; (desc.contains(\"\u0915\u094d\u0930\u093f\u0915\u0947\u091f\")\n                                || url.toLowerCase().contains(\"cric\")\n                                || desc.contains(\"\u091f\u0949\u0938\")\n                                || desc.contains(\"\u0935\u0928\u0921\u0947\") || desc\n                                    .contains(\"\u092c\u0932\u094d\u0932\u0947\u092c\u093e\u091c\"))) {\n                    mapobject.put(\"tags\", \"cricket\");\n                }\n            }\n        } else if (\"business\".equalsIgnoreCase(cat)) {\n            if ((url.toLowerCase().contains(\"sport\") || url.toLowerCase()\n                    .contains(\"\u0916\u0947\u0932\"))) {\n                mapobject.put(\"tags\", \"sports\");\n            } else if (url.toLowerCase().contains(\"cric\")\n                    || url.toLowerCase().contains(\"\u0915\u094d\u0930\u093f\u0915\u0947\u091f\")) {\n                mapobject.put(\"tags\", \"cricket\");\n            } else if (!(url.toLowerCase().contains(\"busines\")\n                    || url.toLowerCase().contains(\"\u0935\u094d\u092f\u093e\u092a\u093e\u0930\")\n                    || url.toLowerCase().contains(\"economy\")\n                    || url.toLowerCase().contains(\"finance\")\n                    || url.toLowerCase().contains(\"\u092c\u093f\u091c\u0928\u0947\u0938\")\n                    || url.toLowerCase().contains(\"market\")\n                    || url.toLowerCase().contains(\"karobar\") || url\n                        .contains(\"\u0915\u093e\u0930\u094b\u092c\u093e\u0930\"))) {\n                final String desc = mapobject.get(\"description\").toString();\n                int count = 0;\n                for (final String keyword : BUSINESS_KEYWORDS) {\n                    if (desc.contains(keyword)) {\n                        count++;\n                    }\n                }\n                if (count &lt; 2) {\n                    mapobject.put(\"tags\", \"national\");\n                }\n            }\n        } else if (\"gossip\".equalsIgnoreCase(cat)) {\n            if ((url.toLowerCase().contains(\"sport\") || url.toLowerCase()\n                    .contains(\"\u0916\u0947\u0932\"))) {\n                mapobject.put(\"tags\", \"sports\");\n            } else if (url.toLowerCase().contains(\"cric\")\n                    || url.toLowerCase().contains(\"\u0915\u094d\u0930\u093f\u0915\u0947\u091f\")) {\n                mapobject.put(\"tags\", \"cricket\");\n            } else if (url.toLowerCase().contains(\"busines\")) {\n                mapobject.put(\"tags\", \"business\");\n            } else if (!(url.toLowerCase().contains(\"masala\")\n                    || url.toLowerCase().contains(\"gossip\")\n                    || url.toLowerCase().contains(\"gupshup\") || url\n                    .toLowerCase().contains(\"garam\"))) {\n                final String desc = mapobject.get(\"description\").toString();\n                int count = 0;\n                for (final String keyword : GOSSIP_KEYWORDS) {\n                    if (desc.contains(keyword)) {\n                        count++;\n                    }\n                }\n                if (count &lt; 2) {\n                    mapobject.put(\"tags\", \"national\");\n                }\n            }\n        } else if (\"crime\".equalsIgnoreCase(cat)) {\n            if ((url.toLowerCase().contains(\"sport\") || url.toLowerCase()\n                    .contains(\"\u0916\u0947\u0932\"))) {\n                mapobject.put(\"tags\", \"sports\");\n            } else if (url.toLowerCase().contains(\"cric\")\n                    || url.toLowerCase().contains(\"\u0915\u094d\u0930\u093f\u0915\u0947\u091f\")) {\n                mapobject.put(\"tags\", \"cricket\");\n            } else if (url.toLowerCase().contains(\"busines\")) {\n                mapobject.put(\"tags\", \"business\");\n            } else if (!(url.toLowerCase().contains(\"crime\")\n                    || url.toLowerCase().contains(\"terrorist\")\n                    || url.toLowerCase().contains(\"abuse\")\n                    || url.toLowerCase().contains(\"forgery\")\n                    || url.toLowerCase().contains(\"assault\")\n                    || url.toLowerCase().contains(\"violence\")\n                    || url.toLowerCase().contains(\"rape\")\n                    || url.toLowerCase().contains(\"teasing\")\n                    || url.toLowerCase().contains(\"molestation\")\n                    || url.toLowerCase().contains(\"scandal\") || url\n                    .toLowerCase().contains(\"murder\"))) {\n                final String desc = mapobject.get(\"description\").toString();\n                int count = 0;\n                for (final String keyword : CRIME_KEYWORDS) {\n                    if (desc.contains(keyword)) {\n                        count++;\n                    }\n                }\n                if (count &lt; 2) {\n                    mapobject.put(\"tags\", \"national\");\n                }\n            }\n        } else if (cat != null &amp;&amp; cat.startsWith(\"local\")) {\n\n        }\n    }\n\n    /**\n     * Check valid type of the HTML pages\n     *\n     * @param mapobject\n     * @return\n     */\n    private boolean checkValidType(final Map&lt;String, Object&gt; mapobject) {\n        if (mapobject.containsKey(\"type\")\n                &amp;&amp; !(mapobject.get(\"type\").toString().contains(\"image\") || mapobject\n                        .get(\"type\").toString().contains(\"rss\"))) {\n            return true;\n        }\n        return false;\n    }\n\n    /**\n     * refine the description according to its length and must starting with\n     * english and it the description is not present get the description\n     * from the metatags description\n     *\n     * @param mapobject\n     * @return {@link Boolean}\n     */\n    private boolean refineDescription(final Map&lt;String, Object&gt; mapobject) {\n\n        if (mapobject.containsKey(\"description\")\n                &amp;&amp; mapobject.get(\"description\").toString().length() &gt; 75\n                &amp;&amp; !mapobject.get(\"description\").toString().contains(\";}\")\n                &amp;&amp; !mapobject.get(\"description\").toString()\n                        .contains(\"&lt;cite&gt;\")\n                &amp;&amp; !mapobject.get(\"description\").toString()\n                        .contains(\"href=\")\n                &amp;&amp; !mapobject.get(\"description\").toString()\n                        .contains(\"All rights reserved\")) {\n            return true;\n        } else if (mapobject.containsKey(\"metatag.description\")\n                &amp;&amp; mapobject.get(\"metatag.description\").toString().length() &gt; 75\n                &amp;&amp; !mapobject.get(\"metatag.description\").toString()\n                        .contains(\";}\")\n                &amp;&amp; !mapobject.get(\"metatag.description\").toString()\n                        .contains(\"&lt;cite&gt;\")) {\n            mapobject.put(\"description\",\n                    mapobject.get(\"metatag.description\"));\n            return true;\n        }\n        return false;\n    }\n\n    /**\n     * refine metatags by refining meta keyword to only include the English\n     * keyword only that has at most three keyword and if not present then\n     * create the keyword with title field of the html and if none of the\n     * keyword found then form it using the help of the url and exclude the\n     * number from the keywords\n     *\n     * @param mapobject\n     */\n    private void refineMetaTags(final Map&lt;String, Object&gt; mapobject) {\n        String metaTag = \"\";\n        int tagFlag = 0;\n        if (mapobject.containsKey(\"metatag.keywords\")) {\n\n            final String metaTags[] = mapobject.get(\"metatag.keywords\")\n                    .toString().replaceAll(\"\\\\|\", \",\").split(\",\");\n            String domain = null;\n            StringBuilder temp = null;\n            for (final String metaTag2 : metaTags) {\n                if (mapobject.containsKey(\"host\")) {\n                    domain = mapobject.get(\"host\") + \"\";\n                    if (domain.split(\"\\\\.\").length &gt; 1\n                            &amp;&amp; (metaTag2\n                                    .contains(domain.split(\"\\\\.\")[domain\n                                            .split(\"\\\\.\").length - 2]) || metaTag2\n                                    .contains(domain.split(\"\\\\.\")[0])))\n\n                    {\n                        continue;\n                    }\n                }\n                String[] arr = metaTag2.split(\" \");\n                arr = removeUnicodeWords(arr);\n                if (arr.length &gt; 0 &amp;&amp; arr.length &lt; 5) {\n                    temp = new StringBuilder();\n                    for (final String str : arr) {\n\n                        temp.append(str);\n                        temp.append(\" \");\n                    }\n                    if (metaTag.length() + temp.length() &lt; 70) {\n                        metaTag = metaTag + \",\" + temp.toString();\n                    }\n                }\n\n            }\n            if (metaTag.startsWith(\",\")) {\n                metaTag = metaTag.trim();\n                metaTag = metaTag.substring(1, metaTag.length());\n            }\n        }\n\n        if (metaTag.length() &lt; 1 &amp;&amp; mapobject.containsKey(\"title\")) {\n            /**\n             * Extracting tags from the title tag if the length of the\n             * keyword is greater than 4\n             */\n            final String title = (String) mapobject.get(\"title\");\n            final String splitTitle[] = title.split(\" \");\n            int count = 0;\n            for (int i = 0; i &lt; splitTitle.length; i++) {\n                if (splitTitle[i].length() &gt; 4\n                        &amp;&amp; !splitTitle[i].matches(\"^[\\\\u0900-\\\\u097F].*\")) {\n                    metaTag = metaTag + splitTitle[i] + \",\";\n                    count++;\n                    if (count == 5) {\n                        break;\n                    }\n                }\n            }\n            if (metaTag.split(\",\").length &gt; 3) {\n                if (metaTag.endsWith(\",\")) {\n                    metaTag = metaTag.trim();\n                    metaTag = metaTag.substring(0, metaTag.length() - 1);\n                }\n            } else {\n                metaTag = \"\";\n            }\n        }\n        if (metaTag.length() &lt; 1) {\n            /**\n             * Extracting the tags from the url if the length of the keyword\n             * is greater than 4\n             */\n            final String splitUrl[] = mapobject.get(\"url\").toString()\n                    .split(\"/\");\n            final String lastSplitValue = splitUrl[splitUrl.length - 1];\n\n            final String tagList[] = generateTokens(lastSplitValue);\n            if (tagList != null) {\n                int count = 0;\n                for (int i = 0; i &lt; tagList.length; i++) {\n                    if (tagList[i].length() &gt; 4\n                            &amp;&amp; !tagList[i].matches(\"^[\\\\u0900-\\\\u097F].*\")) {\n                        metaTag = metaTag + tagList[i] + \",\";\n                        count++;\n                        if (count == 5) {\n                            break;\n                        }\n                    }\n                }\n            }\n            if (metaTag.endsWith(\",\")) {\n                metaTag = metaTag.trim();\n                metaTag = metaTag.substring(0, metaTag.length() - 1);\n            }\n        }\n        if (metaTag.length() &gt; 0) {\n            metaTag = metaTag.replaceAll(\"\\\\[\", \"\");\n            metaTag = metaTag.replaceAll(\"\\\"\", \"\");\n            metaTag = metaTag.replaceAll(\";\", \"\");\n            metaTag = metaTag.replaceAll(\":\", \"\");\n            metaTag = metaTag.replaceAll(\"\\u0027\", \"\");\n            metaTag = metaTag.replaceAll(\"\\u003d\", \"\");\n            metaTag = metaTag.replaceAll(\"\\u0026\", \"\");\n            tagFlag = 1;\n        }\n        mapobject.put(\"TagFlag\", tagFlag);\n        mapobject.put(\"metatag.keywords\", metaTag);\n    }\n\n    /**\n     * Remove unicode character\n     *\n     * @param arr\n     * @return\n     */\n    private String[] removeUnicodeWords(final String[] arr) {\n        final List&lt;String&gt; returnArr = new ArrayList&lt;String&gt;();\n        for (final String str : arr) {\n            if (str != null &amp;&amp; str.trim().length() &gt; 3\n                    &amp;&amp; !str.matches(\"^[\\\\u0900-\\\\u097F].*\")\n                    &amp;&amp; !(str.matches(\"^[0-9].*\"))) {\n                returnArr.add(str.trim());\n            }\n        }\n        final String[] retrnArr = new String[returnArr.size()];\n        returnArr.toArray(retrnArr);\n        return retrnArr;\n    }\n\n    /**\n     * Generate Token list with the help of the lucene analyzer\n     *\n     * @param lastSplitValue\n     * @return {@link ArrayIndexOutOfBoundsException} of the list of the\n     *         keywords\n     */\n    private String[] generateTokens(String lastSplitValue) {\n        final List&lt;String&gt; list = new ArrayList&lt;String&gt;();\n        lastSplitValue = lastSplitValue.replace(\"\\\\.\", \" \").replace(\"%20\",\n                \" \");\n        try {\n            final Version matchVersion = Version.LUCENE_45;\n            final Analyzer analyzer = new HindiAnalyzer(matchVersion);\n            final TokenStream ts = analyzer.tokenStream(\"field\",\n                    new StringReader(lastSplitValue));\n            ts.reset();\n            while (ts.incrementToken()) {\n                final CharTermAttribute cta = ts\n                        .getAttribute(CharTermAttribute.class);\n                if (cta.toString().length() &gt; 4\n                        &amp;&amp; !cta.toString().matches(\"^[0-9].*\")) {\n                    list.add(cta.toString());\n                }\n            }\n            ts.end();\n            ts.close();\n            analyzer.close();\n        } catch (final Exception e) {\n            e.printStackTrace();\n        }\n        if (list.size() &gt; 3) {\n            return list.toArray(new String[list.size()]);\n        } else {\n            return null;\n        }\n    }\n\n    /**\n     * Checks title and assign their language based on their first character\n     * of the title\n     *\n     * @param mapobject\n     * @return {@link Map}\n     */\n    private boolean setLang(final Map&lt;String, Object&gt; mapobject) {\n        final String title = mapobject.get(\"title\").toString();\n        final String description = mapobject.get(\"title\").toString();\n        String language = \"\";\n        try {\n            language = mapper.detect(title);\n            mapper.detect(description);\n        } catch (final LangDetectException e) {\n            System.out.println(\"\\n title with error is - \" + title);\n            System.out.println(\"\\n description with error is - \"\n                    + description);\n            e.printStackTrace();\n            /*\n             * String title = mapobject.get(\"title\").toString(); language =\n             * mapobject.get(\"lang\") + \"\"; language = language.trim(); if\n             * (language.trim().equalsIgnoreCase(\"hi\") ||\n             * language.trim().startsWith(\"en\") ||\n             * language.trim().equalsIgnoreCase(\"lt\")) { String[] titleArr =\n             * title.trim().split(\" \"); int i = 0; for (String titlePart :\n             * titleArr) { if\n             * (titlePart.trim().matches(\"^[\\\\u0900-\\\\u097F].*\")) { i++; } }\n             * if (i &gt;= titleArr.length * 0.5) { mapobject.put(\"lang\",\n             * \"hi\"); } else { mapobject.put(\"lang\", \"lt\"); } return true; }\n             */\n            return false;\n        }\n\n        if (language.trim().equalsIgnoreCase(\"hi\")\n                || language.trim().startsWith(\"en\")\n                || language.trim().equalsIgnoreCase(\"lt\")) {\n            mapobject.put(\"lang\", language);\n            return true;\n        }\n\n        return false;\n    }\n\n    private String detect(final String text) throws LangDetectException {\n        final Detector detector = DetectorFactory.create();\n        detector.append(text);\n        return detector.detect();\n    }\n\n    /**\n     * Checks whether to include the doc based on their title and get the\n     * title from anchor tag title to choose the title that has largest\n     * number of the words and in hindi and it also gets the image from\n     * anchor tag href attribute\n     *\n     * @param mapobject\n     *            of the key value pair\n     * @return {@link Boolean}\n     */\n    private boolean checkTitleImage(final Map&lt;String, Object&gt; mapobject) {\n        final TreeSet&lt;String&gt; set = new TreeSet&lt;String&gt;(new SetSort());\n\n        final Gson gson = new Gson();\n        JsonArray array = null;\n        JsonObject object2 = null;\n\n        if (mapobject.containsKey(\"anchor\")\n                &amp;&amp; mapobject.get(\"anchor\") != null) {\n            final String arr = (String) mapobject.get(\"anchor\");\n\n            try {\n\n                array = gson.fromJson(arr, JsonArray.class);\n\n                for (final JsonElement jsonElement : array) {\n\n                    try {\n                        object2 = gson.fromJson(jsonElement.getAsString(),\n                                JsonObject.class);\n                    } catch (final Exception e) {\n\n                        if (object2 == null) {\n                            object2 = new JsonObject();\n                            object2.addProperty(\"title\",\n                                    jsonElement.getAsString());\n                            object2.addProperty(\"href\", \"\");\n                            object2.addProperty(\"alt\", \"\");\n                        }\n                    }\n                    if (object2 != null) {\n                        assignTitleImage(mapobject, set, object2);\n                    }\n                    object2 = null;\n                }\n            } catch (final ClassCastException e) {\n\n                object2 = gson.fromJson(arr, JsonObject.class);\n                assignTitleImage(mapobject, set, object2);\n\n            } catch (final Exception e) {\n                e.printStackTrace();\n            }\n\n            if (!set.isEmpty()) {\n                int loop = 0;\n                final List&lt;String&gt; tempList = new LinkedList&lt;String&gt;();\n                for (final String string : set) {\n                    final String title = string;\n                    tempList.add(title.trim());\n                    loop++;\n                    if (loop == 2) {\n                        break;\n                    }\n                }\n                if (!tempList.isEmpty()) {\n                    if (tempList.get(0).matches(\"^[\\\\u0900-\\\\u097F].*\")) {\n                        mapobject.put(\"title\", tempList.get(0));\n                    } else if (tempList.size() &gt; 1\n                            &amp;&amp; !(tempList.get(0)\n                                    .matches(\"^[\\\\u0900-\\\\u097F].*\"))\n                            &amp;&amp; tempList.get(1).matches(\n                                    \"^[\\\\u0900-\\\\u097F].*\")) {\n                        mapobject.put(\"title\", tempList.get(1));\n                    } else {\n                        mapobject.put(\"title\", tempList.get(0));\n                    }\n                }\n            }\n\n        }\n        if (mapobject.containsKey(\"title\")\n                &amp;&amp; mapobject.get(\"title\").toString().length() &gt; 0\n                &amp;&amp; mapobject.get(\"title\").toString().split(\" \").length &gt; 2\n                &amp;&amp; mapobject.get(\"title\").toString().split(\" \").length &lt; 20\n                &amp;&amp; !mapobject.get(\"title\").toString().contains(\"&lt;\")) {\n            if (set.isEmpty()) {\n                mapobject.put(\"title\",\n                        getTitleRefined(mapobject.get(\"title\") + \"\"));\n            }\n            return true;\n        }\n        return false;\n    }\n\n    /**\n     * @param mapobject\n     * @param set\n     * @param object2\n     */\n    private void assignTitleImage(final Map&lt;String, Object&gt; mapobject,\n            final TreeSet&lt;String&gt; set, final JsonObject object2) {\n        if (!mapobject.containsKey(\"ImgH1\")\n                &amp;&amp; !mapobject.containsKey(\"ImgH2\")) {\n            if (object2.get(\"href\") != null\n                    &amp;&amp; object2.get(\"href\").getAsString().length() &gt; 0\n                    &amp;&amp; (object2.get(\"href\").getAsString().toLowerCase()\n                            .contains(\".jpg\")\n                            || object2.get(\"href\").getAsString()\n                                    .toLowerCase().contains(\".jpeg\") || object2\n                            .get(\"href\").getAsString().toLowerCase()\n                            .contains(\".gif\"))) {\n                putImages(mapobject, object2.get(\"href\").getAsString()\n                        .trim(), mapobject.get(\"tags\").toString().trim()\n                        .toLowerCase());\n            }\n        }\n\n        if (object2.get(\"title\") != null\n                &amp;&amp; object2.get(\"title\").getAsString().length() &gt; 0\n                &amp;&amp; object2.get(\"title\").getAsString().split(\" \").length &gt; 2\n                &amp;&amp; object2.get(\"title\").getAsString().split(\" \").length &lt; 20\n                &amp;&amp; !object2.get(\"title\").getAsString().contains(\"&lt;\")) {\n\n            final String newTitle = getTitleRefined(object2.get(\"title\")\n                    .getAsString());\n            set.add(newTitle.trim());\n        }\n    }\n\n    /**\n     * This function used to refine the title based on specific bad keyword\n     * during observation\n     *\n     * @param title\n     * @return refined title\n     */\n    private String getTitleRefined(String title) {\n        title = title.replaceAll(\"\\u0027\", \"\");\n        title = title.replaceAll(\"\\u0026\", \"\");\n        title = title.replaceAll(\"\\u003d\", \"\");\n        if (title.contains(\"-\")) {\n            if (title.trim().split(\"-\").length &gt; 1\n                    &amp;&amp; !title.trim().split(\"-\")[1].trim().matches(\n                            \"^[\\\\u0900-\\\\u097F].*\")) {\n                return title.trim().split(\"-\")[0].trim();\n            }\n        } else if (title.contains(\":\")) {\n            if (!title.trim().split(\":\")[0].trim().matches(\n                    \"^[\\\\u0900-\\\\u097F].*\")\n                    &amp;&amp; title.trim().split(\":\").length &gt; 1) {\n                return title.trim().split(\":\")[1].trim();\n            }\n        }\n        return title;\n    }\n\n    /**\n     * Creates the path for the images\n     *\n     * @param map\n     *            of the key value pair\n     * @param imageUrl\n     * @param category\n     */\n    private void putImages(final Map&lt;String, Object&gt; map2,\n            final String imageUrl, final String category) {\n        try {\n            map2.put(\"ImgSrc\", StringEscapeUtils.unescapeHtml(imageUrl)\n                    .trim());\n            if (map2.containsKey(\"ImgSrc\") &amp;&amp; map2.get(\"ImgSrc\") != null\n                    &amp;&amp; map2.get(\"ImgSrc\").toString().length() &gt; 0) {\n                map2.put(\n                        \"ImgSrc\",\n                        StringEscapeUtils.unescapeHtml(map2.get(\"ImgSrc\")\n                                .toString())\n                                + \"##RAFTAAR##\"\n                                + imageUrl.trim());\n            } else {\n                return;\n            }\n            String imgNamearr[] = null;\n            try {\n                imgNamearr = imageUrl.split(\"/\");\n            } catch (final Exception e) {\n                e.printStackTrace();\n            }\n            String imgName = null;\n            try {\n                imgName = imgNamearr[imgNamearr.length - 1];\n            } catch (final Exception e) {\n                e.printStackTrace();\n            }\n            final String imagePath = \"/\"\n                    + String.valueOf(imgName.charAt(0));\n            imgName = imgName.replaceAll(\" \", \"_\").replaceAll(\"%20\", \"_\");\n            if (imgName.split(\".jpg\").length &gt; 0) {\n                imgName = imgName.split(\".jpg\")[0];\n                imgName = imgName + \".jpg\";\n            }\n\n            map2.put(\"ImgH1\", \"h1/\" + category + imagePath + \"/\" + imgName);\n            map2.put(\"ImgH2\", \"h2/\" + category + imagePath + \"/\" + imgName);\n        } catch (final Exception e) {\n            e.printStackTrace();\n        }\n    }\n\n    /**\n     * Inserts the data to the elasticsearch\n     *\n     * @param mapobject\n     * @param key\n     *            unique id generally it is the unique url\n     */\n    public static void insertToElastic(final Map&lt;String, Object&gt; mapobject,\n            final String key) {\n\n        final Settings settings = ImmutableSettings.settingsBuilder()\n                .put(\"cluster.name\", CLUSTER).build();/*\n                                                     * change ccluster.name\n                                                     * to cluster\n                                                     */\n        final Client client = new TransportClient(settings)\n                .addTransportAddress(new InetSocketTransportAddress(\n                        SEARCH_HOST, Integer.parseInt(SEARCH_PORT)));\n        client.prepareIndex(SEARCH_INDEX_NAME, SEARCHtYPE, key)\n                .setSource(mapobject).execute().actionGet();\n        client.close();\n    }\n\n    /**\n     * Assign the city to the news without city\n     *\n     * @param category\n     * @param description\n     * @return update category with city\n     */\n    private static void assignCity(final Map&lt;String, Object&gt; mapobject) {\n\n        String category = mapobject.get(\"tags\").toString();\n        if (category.endsWith(\"/\")) {\n            boolean flag = true;\n            final String catArr[] = category.split(\"/\");\n            if (catArr.length == 2) {\n                final String state = catArr[1];\n                CITY_MAP = STATE_MAP.get(state);\n                for (final Entry&lt;String, String&gt; e : CITY_MAP.entrySet()) {\n                    final String description = mapobject.get(\"description\")\n                            .toString();\n                    if (description.contains(e.getValue())) {\n                        category = category + e.getKey();\n                        mapobject.put(\"tags\", category);\n                        flag = false;\n                        break;\n                    }\n                }\n            }\n            if (flag) {\n                mapobject.put(\"tags\", \"national\");\n            }\n        }\n    }\n}\n\n/**\n * Update the data to hbase\n *\n * @param tableName\n * @param rowKey\n * @param family\n * @param qualifier\n * @param value\n * @param conf\n */\npublic static void updateIntoHbase(final String tableName,\n        final String rowKey, final String family, final String qualifier,\n        final String value, final Configuration conf) {\n    HTable table = null;\n    try {\n        table = new HTable(conf, tableName);\n    } catch (final IOException e) {\n        e.printStackTrace();\n    }\n    final Put put = new Put(Bytes.toBytes(rowKey));\n    put.add(Bytes.toBytes(family), Bytes.toBytes(qualifier),\n            Bytes.toBytes(value));\n    try {\n        table.put(put);\n        table.close();\n    } catch (final IOException e) {\n        e.printStackTrace();\n    }\n}\n\n/**\n * Return the map of the all states and city\n *\n * @param fileName\n * @return\n */\nprivate static Map&lt;String, Map&lt;String, String&gt;&gt; returnMap(\n        final String fileName) {\n    final Map&lt;String, Map&lt;String, String&gt;&gt; map = new HashMap&lt;String, Map&lt;String, String&gt;&gt;();\n    BufferedReader br = null;\n    try {\n        br = new BufferedReader(new FileReader(fileName));\n        String line;\n        while ((line = br.readLine()) != null) {\n\n            final String arr[] = line.split(\"\\t\", 3);\n            if (arr.length == 3) {\n                if (map.containsKey(arr[0])) {\n                    Map&lt;String, String&gt; m = new HashMap&lt;String, String&gt;();\n                    m = map.get(arr[0]);\n                    m.put(arr[1], arr[2]);\n                } else {\n                    final Map&lt;String, String&gt; m = new HashMap&lt;String, String&gt;();\n                    m.put(arr[1], arr[2]);\n                    map.put(arr[0], m);\n                }\n            }\n        }\n    } catch (final FileNotFoundException e) {\n        e.printStackTrace();\n    } catch (final IOException e) {\n        e.printStackTrace();\n    } finally {\n        if (br != null) {\n            try {\n                br.close();\n            } catch (final Exception e) {\n                e.printStackTrace();\n            }\n        }\n    }\n    return map;\n}\n\n\npublic static void main(final String[] args) throws Exception {\n\n    int c = 0;\n    c = ToolRunner.run(new Configuration(), new HbaseToElastic(), args);\n    System.exit(c);\n}\n}\n</code></pre>\n", "question_id": 31724713, "creation_date": 1455881621, "is_accepted": false, "score": 0, "last_activity_date": 1455885437, "answer_id": 35504314}], "question_id": 31724713, "tags": ["nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/31724713/how-to-export-hbase-data-to-elastic-search-using-nutch", "last_activity_date": 1455885437, "owner": {"user_id": 3883202, "answer_count": 1, "creation_date": 1406528359, "accept_rate": 40, "view_count": 4, "reputation": 18}, "body": "<p>i have followed <a href=\"https://gist.github.com/xrstf/b48a970098a8e76943b9\" rel=\"nofollow\">https://gist.github.com/xrstf/b48a970098a8e76943b9</a>  to integrate nutch and elastic-search. everything is working fine data is stored in Hbase 'webpage' table but i am not able to fetch data in elastic search.i want to know how to fetch data in elastic search. </p>\n", "creation_date": 1438261696, "score": 0},
{"title": "How to save fetched html content to database in apache nutch?", "view_count": 66, "owner": {"user_id": 4644454, "view_count": 4, "answer_count": 0, "creation_date": 1425741586, "reputation": 10}, "is_answered": true, "answers": [{"question_id": 35443880, "owner": {"user_id": 432844, "link": "http://stackoverflow.com/users/432844/julien-nioche", "user_type": "registered", "reputation": 939}, "body": "<p>You could write a custom plugin and implement an extension of <a href=\"https://github.com/apache/nutch/blob/trunk/src/java/org/apache/nutch/indexer/IndexWriter.java\" rel=\"nofollow\">org.apache.nutch.indexer.IndexWriter</a> to send the documents to \nPostgres as part of the indexing step. You'll need to index the raw content which requires <a href=\"https://issues.apache.org/jira/browse/NUTCH-2032\" rel=\"nofollow\">NUTCH-2032</a> - this is in Nutch 1.11 so you will need to upgrade your version of Nutch.</p>\n\n<p>Alternatively you could write a custom MapReduce job which would take a segments as input, read the content and send it to your DB in the reduce step.</p>\n", "creation_date": 1455808437, "is_accepted": true, "score": 0, "last_activity_date": 1455808437, "answer_id": 35485258}], "question_id": 35443880, "tags": ["java", "apache", "nutch", "information-retrieval", "web-mining"], "answer_count": 1, "link": "http://stackoverflow.com/questions/35443880/how-to-save-fetched-html-content-to-database-in-apache-nutch", "last_activity_date": 1455844032, "accepted_answer_id": 35485258, "body": "<p>I'm using 1.8 version of apache nutch. I want to save crawled HTML content to postgre database to do this, I modify <code>FetcherThread.java</code> class as below. </p>\n\n<pre><code>  case ProtocolStatus.SUCCESS: // got a page\n  pstatus = output(fit.url, fit.datum, content, status,\n  CrawlDatum.STATUS_FETCH_SUCCESS, fit.outlinkDepth);\n  updateStatus(content.getContent().length);              \n  /*Added My code Here*/\n</code></pre>\n\n<p>But I want to use plug-in system instead of directly modifying FetcherThread class. To use plug-in system which extension points I need to use? </p>\n", "creation_date": 1455660183, "score": 0},
{"title": "Hadoop, HBase and Apache Nutch compatible versions", "view_count": 106, "is_answered": true, "answers": [{"question_id": 35322506, "owner": {"user_id": 1780704, "accept_rate": 80, "link": "http://stackoverflow.com/users/1780704/mobin-ranjbar", "user_type": "registered", "reputation": 465}, "body": "<p>Based on <a href=\"https://wiki.apache.org/nutch/Nutch2Tutorial\" rel=\"nofollow\">Nutch Documentation</a>, It suggests to use <code>HBase 0.98.8-hadoop</code> version and if you decide to use another version of HBase please do not be surprised if the stack does not work.</p>\n", "creation_date": 1455382875, "is_accepted": false, "score": 0, "last_activity_date": 1455382875, "answer_id": 35382683}, {"question_id": 35322506, "owner": {"user_id": 5907364, "link": "http://stackoverflow.com/users/5907364/j%c3%a9r%c3%a9mie", "user_type": "registered", "reputation": 46}, "body": "<p>The most recents compatibles versions that you can have now are: </p>\n\n<ul>\n<li>nutch 2.3.1</li>\n<li>hbase 0.98.17-hadoop2</li>\n<li>hadoop 2.5.2</li>\n</ul>\n\n<p>source: <a href=\"http://nutch.apache.org/\" rel=\"nofollow\">nutch news</a>.</p>\n", "creation_date": 1455789935, "is_accepted": false, "score": 1, "last_activity_date": 1455789935, "answer_id": 35478375}], "question_id": 35322506, "tags": ["apache", "hadoop", "web-crawler", "hbase", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/35322506/hadoop-hbase-and-apache-nutch-compatible-versions", "last_activity_date": 1455789935, "owner": {"user_id": 5178563, "answer_count": 0, "creation_date": 1438358009, "accept_rate": 0, "view_count": 6, "reputation": 16}, "body": "<p>HBase, and Nutch.\nI am trying to integrate the 3 together, but am encountering problems with compatibility. </p>\n\n<p><strong>I was wondering if anyone knows the which most recent versions of Hadoop, HBase, and Nutch I can use together.</strong></p>\n\n<p>Any insight is greatly appreciated</p>\n\n<p>Thank you so much!</p>\n", "creation_date": 1455126880, "score": 0},
{"title": "Nutch and solr indexing blacklist domain", "view_count": 75, "is_answered": false, "answers": [{"question_id": 35449673, "owner": {"user_id": 5907364, "link": "http://stackoverflow.com/users/5907364/j%c3%a9r%c3%a9mie", "user_type": "registered", "reputation": 46}, "body": "<p>To avoid to indexing an url, the most simple solution it's to avoid to crawl this url. </p>\n\n<ul>\n<li><p>Add this line to regex-urlfilter.txt: </p>\n\n<pre><code>-^(http|https)://.*aaa.*$\n</code></pre></li>\n<li><p>Add this configuration in your nutch-site.xml (i added some extra plugins): </p>\n\n<pre><code>&lt;property&gt;\n&lt;name&gt;plugin.includes&lt;/name&gt;\n&lt;value&gt;protocol-httpclient|urlfilter-regex|parse-(text|tika|js)|index-(basic|anchor)|query-(basic|site|url)|response-(json|xml)|summary-basic|scoring-opic|urlnormalizer-(pass|regex|basic)|indexer-solr\n&lt;/value&gt;\n&lt;/property&gt;\n</code></pre></li>\n<li><p>Compile in your $NUTCH_HOME: </p>\n\n<pre><code>ant runtime\n</code></pre></li>\n<li><p>If it's not enough, this is because in your database or files, you have some bad urls. So delete all your database (Hbase/Cassandra for nutch 2 and segments files for nutch 1). After that, retry to crawl and it would be good. :)</p></li>\n</ul>\n", "creation_date": 1455786050, "is_accepted": false, "score": 0, "last_activity_date": 1455786050, "answer_id": 35476895}], "question_id": 35449673, "tags": ["solr", "web-crawler", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/35449673/nutch-and-solr-indexing-blacklist-domain", "last_activity_date": 1455786050, "owner": {"user_id": 3732294, "answer_count": 0, "creation_date": 1402537534, "accept_rate": 0, "view_count": 10, "reputation": 16}, "body": "<p>I am using nutch 1.9 and solr 4.10 .I want to avoid domain  www.aaa.com gettign index in nutch and solr</p>\n\n<p>In the  nutch configurations \nconf/domainblacklist-urlfilter.txt I have added \"www.aaa.com\". in the conf/domain-urlfilter.txt I have added \"www.bbb.com\"\nIn the regex-urlfilter.txt I have added</p>\n\n<p>+^<a href=\"http://www.bbb.com\" rel=\"nofollow\">http://www.bbb.com</a>\n-^<a href=\"http://www.aaa.com\" rel=\"nofollow\">http://www.aaa.com</a></p>\n\n<p>But notice  that it still in solr that the domain with  www.aaa.com is getting index.</p>\n\n<p>Could some one provide inputs as how this can be resolved</p>\n", "creation_date": 1455691454, "score": 0},
{"title": "How to speed up crawling in Nutch", "view_count": 3604, "is_answered": true, "answers": [{"question_id": 4871972, "owner": {"user_id": 271458, "link": "http://stackoverflow.com/users/271458/slick86", "user_type": "registered", "reputation": 1772}, "body": "<p>You can scale up the threads in nutch-site.xml.  Increasing fetcher.threads.per.host and fetcher.threads.fetch will both increase the speed at which you crawl. I have noticed drastic improvements. Use caution when increasing these though. If you do not have the hardware or connection to support this increased traffic, the amount of errors in crawling can signifigantly increase.</p>\n", "creation_date": 1296801789, "is_accepted": false, "score": 5, "last_activity_date": 1296801789, "answer_id": 4895271}, {"question_id": 4871972, "owner": {"user_id": 183100, "link": "http://stackoverflow.com/users/183100/pascal-dimassimo", "user_type": "registered", "reputation": 5766}, "body": "<p>If you don't need to follow links, I see no reason to use Nutch. You can simply take your list of urls and fetch those with an http client library or a simple script using curl.</p>\n", "creation_date": 1297432184, "is_accepted": false, "score": 0, "last_activity_date": 1297432184, "answer_id": 4969739}, {"question_id": 4871972, "owner": {"user_id": 1878786, "link": "http://stackoverflow.com/users/1878786/ilce-mkd", "user_type": "registered", "reputation": 135}, "body": "<p>The main thing for getting speed is configuring the nutch-site.xml </p>\n\n<pre><code>&lt;property&gt;\n&lt;name&gt;fetcher.threads.per.queue&lt;/name&gt;\n   &lt;value&gt;50&lt;/value&gt;\n   &lt;description&gt;&lt;/description&gt;\n&lt;/property&gt;\n</code></pre>\n", "creation_date": 1358177733, "is_accepted": false, "score": 5, "last_activity_date": 1358177733, "answer_id": 14321523}, {"question_id": 4871972, "owner": {"user_id": 4408069, "link": "http://stackoverflow.com/users/4408069/ravikumar-mangipudi", "user_type": "registered", "reputation": 11}, "body": "<p>Hello I am also new for this crawling but I have useed some methods I got some good results may it will you\nI have changed my nutch-site.xml with these properties</p>\n\n<pre><code>&lt;property&gt;\n  &lt;name&gt;fetcher.server.delay&lt;/name&gt;\n  &lt;value&gt;0.5&lt;/value&gt;\n &lt;description&gt;The number of seconds the fetcher will delay between \n   successive requests to the same server. Note that this might get\n   overriden by a Crawl-Delay from a robots.txt and is used ONLY if \n   fetcher.threads.per.queue is set to 1.\n &lt;/description&gt;\n\n&lt;/property&gt;\n&lt;property&gt;\n  &lt;name&gt;fetcher.threads.fetch&lt;/name&gt;\n  &lt;value&gt;400&lt;/value&gt;\n  &lt;description&gt;The number of FetcherThreads the fetcher should use.\n    This is also determines the maximum number of requests that are\n    made at once (each FetcherThread handles one connection).&lt;/description&gt;\n&lt;/property&gt;\n\n\n&lt;property&gt;\n  &lt;name&gt;fetcher.threads.per.host&lt;/name&gt;\n  &lt;value&gt;25&lt;/value&gt;\n  &lt;description&gt;This number is the maximum number of threads that\n    should be allowed to access a host at one time.&lt;/description&gt;\n&lt;/property&gt;\n</code></pre>\n\n<p>kindly suggest some more options \nThanks</p>\n", "creation_date": 1442938513, "is_accepted": false, "score": 0, "last_activity_date": 1442938513, "answer_id": 32721827}, {"question_id": 4871972, "owner": {"user_id": 2089472, "link": "http://stackoverflow.com/users/2089472/ant%c3%b3nio-fonseca", "user_type": "registered", "reputation": 21}, "body": "<p>I have similar issues and could improve the speed with the help of \n<a href=\"https://wiki.apache.org/nutch/OptimizingCrawls\" rel=\"nofollow\">https://wiki.apache.org/nutch/OptimizingCrawls</a></p>\n\n<p>It has useful information with what can be slowing down your crawl and what you can do to improve each of those issues.</p>\n\n<p>Unfortunately in my case I have the queues quite unbalanced and can't request too fast to the bigger one otherwise I get blocked so I probably need to go to cluster solution or TOR before i speed up the threads further.</p>\n", "creation_date": 1455528881, "is_accepted": false, "score": 0, "last_activity_date": 1455528881, "answer_id": 35405745}, {"last_edit_date": 1455618657, "owner": {"user_id": 5907364, "link": "http://stackoverflow.com/users/5907364/j%c3%a9r%c3%a9mie", "user_type": "registered", "reputation": 46}, "body": "<p>For me, this property helped me so much, because a slow domain can slow down all the fetch phase : </p>\n\n<pre><code> &lt;property&gt;\n  &lt;name&gt;generate.max.count&lt;/name&gt;\n  &lt;value&gt;50&lt;/value&gt;\n  &lt;description&gt;The maximum number of urls in a single\n  fetchlist.  -1 if unlimited. The urls are counted according\n  to the value of the parameter generator.count.mode.\n  &lt;/description&gt;\n &lt;/property&gt;\n</code></pre>\n\n<p>For example, if you respect the robots.txt (default behaviour) and a domain is too long to crawl, the delay will be : fetcher.max.crawl.delay. And a lot of this domain in a queue will slow down all the fetch phase, so it's better to limit the generate.max.count.</p>\n\n<p>You can add this property for limit the time of the fetch phase in the same way : </p>\n\n<pre><code>&lt;property&gt;\n  &lt;name&gt;fetcher.throughput.threshold.pages&lt;/name&gt;\n  &lt;value&gt;1&lt;/value&gt;\n  &lt;description&gt;The threshold of minimum pages per second. If the fetcher downloads less\n  pages per second than the configured threshold, the fetcher stops, preventing slow queue's\n  from stalling the throughput. This threshold must be an integer. This can be useful when\n  fetcher.timelimit.mins is hard to determine. The default value of -1 disables this check.\n  &lt;/description&gt;\n&lt;/property&gt;\n</code></pre>\n\n<p>But please, dont touch to the fetcher.threads.per.queue property, you will finish in a black list... It's not a good solution to improve the crawl speed...</p>\n", "question_id": 4871972, "creation_date": 1455618345, "is_accepted": false, "score": 1, "last_activity_date": 1455618657, "answer_id": 35429928}], "question_id": 4871972, "tags": ["nutch", "web-crawler"], "answer_count": 6, "link": "http://stackoverflow.com/questions/4871972/how-to-speed-up-crawling-in-nutch", "last_activity_date": 1455618657, "owner": {"user_id": 589317, "view_count": 12, "answer_count": 0, "creation_date": 1295973342, "reputation": 16}, "body": "<p>I am trying to develop an application in which I'll give a constrained set of urls to the urls file in Nutch. I am able to crawl these urls and get the contents of them by reading the data from the segments.</p>\n\n<p>I have crawled by giving the depth 1 as I am no way concerned about the outlinks or inlinks in the webpage. I only need the contents of that webpages in the urls file. </p>\n\n<p>But performing this crawl takes time. So, suggest me a way to decrease the crawl time and increase the speed of crawl. I also dont need indexing because I am not concerned about the search part.</p>\n\n<p>Does anyone have suggestions on how to speed up the crawl?</p>\n", "creation_date": 1296633276, "score": 2},
{"title": "How do you configure Apache Nutch 2.3 to honour robots metatag?", "view_count": 194, "owner": {"user_id": 1371932, "answer_count": 8, "creation_date": 1336033815, "accept_rate": 75, "view_count": 18, "location": "Cardiff, United Kingdom", "reputation": 173}, "is_answered": true, "answers": [{"last_edit_date": 1455551640, "owner": {"user_id": 1371932, "accept_rate": 75, "link": "http://stackoverflow.com/users/1371932/saintybalboa", "user_type": "registered", "reputation": 173}, "body": "<p>I've created a plugin to overcome the problem of Apache Nutch 2.3 NOT honouring the robots metatag rule <code>noindex</code>. The metarobots plugin forces Nutch to discard qualifying documents during index. This prevents the qualifying documents being indexed to your external datastore ie Solr. </p>\n\n<p><strong>Please note:</strong> This plugin prevents the index of documents that contain robots metatag rule <code>noindex</code>, it does NOT remove any documents that were previously indexed to your external datastore.</p>\n\n<p><a href=\"https://github.com/saintybalboa/nutchmetarobots/blob/master/README.md\" rel=\"nofollow\">Visit this link for instructions</a></p>\n", "question_id": 35227917, "creation_date": 1455106118, "is_accepted": true, "score": 1, "last_activity_date": 1455551640, "answer_id": 35314846}], "question_id": 35227917, "tags": ["solr", "hbase", "nutch", "robots.txt", "metatag"], "answer_count": 1, "link": "http://stackoverflow.com/questions/35227917/how-do-you-configure-apache-nutch-2-3-to-honour-robots-metatag", "last_activity_date": 1455551640, "accepted_answer_id": 35314846, "body": "<p>I have Nutch 2.3 setup with HBase as the backend and I run a crawl of which includes the index to Solr and Solr Deduplication.</p>\n\n<p>I have recently noticed that the Solr index contains unwanted webpages. </p>\n\n<p>In order to get Nutch to ignore these webpages I set the following metatag:</p>\n\n<pre><code>&lt;meta name=\"robots\" content=\"noindex,follow\"&gt; \n</code></pre>\n\n<p>I have visited the apache nutch official website and it explains the following: </p>\n\n<blockquote>\n  <p>If you do not have permission to edit the /robots.txt file on your server, you can still tell robots not to index your pages or follow your links. The standard mechanism for this is the robots META tag</p>\n</blockquote>\n\n<p>Searching the web for answers, I found a recommendations to set <code>Protocol.CHECK_ROBOTS</code> or set <code>protocol.plugin.check.robots</code> as a property in nutch-site.xml. None of these appear to work. </p>\n\n<p>At current Nutch 2.3 ignores the <code>noindex</code> rule, therefore indexing the content to the external datastore ie Solr.</p>\n\n<p>The question is how do I configure Nutch 2.3 to honour robots metatags?</p>\n\n<p>Also if Nutch 2.3 was previously configured to ignore robot metatag and during a previous crawl cycle indexed that webpage. Providing the rules for the robots metatag are correct, will this result in the page being removed from the Solr index in future crawls?</p>\n", "creation_date": 1454686591, "score": 1},
{"title": "index apache nutch result in solr", "view_count": 40, "is_answered": false, "answers": [{"question_id": 35403279, "owner": {"user_id": 720977, "accept_rate": 59, "link": "http://stackoverflow.com/users/720977/xmorera", "user_type": "registered", "reputation": 797}, "body": "<p>Indexing content with Nutch and posting to Solr should be straightforward. But if you want to add logic and the list of rules might grow, it might be recommended to use a content processing engine. </p>\n\n<p>I've seen this tool used for that specific purpose but it uses Heritrix as crawler and you can create Groovy scripts to decide how to handle your content :www.searchtechnologies.com/aspire</p>\n", "creation_date": 1455546273, "is_accepted": false, "score": 0, "last_activity_date": 1455546273, "answer_id": 35411855}], "question_id": 35403279, "tags": ["indexing", "solr", "lucene", "web-crawler", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/35403279/index-apache-nutch-result-in-solr", "last_activity_date": 1455546273, "owner": {"user_id": 3946458, "answer_count": 1, "creation_date": 1408167505, "accept_rate": 25, "view_count": 5, "reputation": 16}, "body": "<p>I wanna to crawl web site with Nutch and then index result in Solr.\n<br>\nI have in solr schema.xml file .that imagine in this file i have field content.\n<br>\nbut every site has own pattern for example in some one i wanna to set \"body\" tag in \"content filed(in solr schema)\"\nand for another site i wanna to set \"content\" in \"content filed(in solr schema)\".\n<br>\nI mean if in crawl result i find body tag i use this to store in content field,\nelse if i find body tag i use this value to store in schema file.\n<br>how can i do that?\n<br>\ncan i set specialfield in solr fill based on multi Tag value in nutch crael result based on what tag it found in each web site?</p>\n", "creation_date": 1455519726, "score": 0},
{"title": "Time of SolrRecord being added to Index from Nutch", "view_count": 59, "is_answered": true, "answers": [{"question_id": 35307412, "owner": {"user_id": 5650316, "link": "http://stackoverflow.com/users/5650316/bryan-bende", "user_type": "registered", "reputation": 1761}, "body": "<p>I think you would have two options...</p>\n\n<p>You could add a new date field in your Solr schema.xml with a default value of NOW:</p>\n\n<pre><code>&lt;field name=\"timestamp\" type=\"date\" indexed=\"true\" stored=\"true\" default=\"NOW\" multiValued=\"false\"/&gt;\n</code></pre>\n\n<p>You could use the TimestampUpdateProcessorFactory:\n<a href=\"https://lucene.apache.org/solr/5_4_1/solr-core/org/apache/solr/update/processor/TimestampUpdateProcessorFactory.html\" rel=\"nofollow\">https://lucene.apache.org/solr/5_4_1/solr-core/org/apache/solr/update/processor/TimestampUpdateProcessorFactory.html</a></p>\n\n<p>In solrconfig.xml you would add this to an update chain:</p>\n\n<pre><code>&lt;updateRequestProcessorChain name=\"add-timestamp-field\"&gt;\n    &lt;processor class=\"solr.TimestampUpdateProcessorFactory\"&gt;\n       &lt;str name=\"fieldName\"&gt;timestamp&lt;/str&gt;\n    &lt;/processor&gt;\n&lt;/updateRequestProcessorChain&gt;\n</code></pre>\n\n<p>If using the update chain, the add-timestamp-field chain needs to be enabled:</p>\n\n<pre><code>&lt;initParams path=\"/update/**\"&gt;\n  &lt;lst name=\"defaults\"&gt;\n    &lt;str name=\"update.chain\"&gt;add-timestamp-field&lt;/str&gt;\n  &lt;/lst&gt;\n&lt;/initParams&gt;\n</code></pre>\n", "creation_date": 1455428565, "is_accepted": false, "score": 1, "last_activity_date": 1455428565, "answer_id": 35388757}], "question_id": 35307412, "tags": ["solr", "nutch", "hortonworks-data-platform"], "answer_count": 1, "link": "http://stackoverflow.com/questions/35307412/time-of-solrrecord-being-added-to-index-from-nutch", "last_activity_date": 1455428565, "owner": {"age": 34, "answer_count": 6, "creation_date": 1242658305, "user_id": 108843, "accept_rate": 75, "view_count": 63, "location": "Cape Town, South Africa", "reputation": 119}, "body": "<p>I am running Solr 5.4.1 and Nutch 1.11\nI am also using Apache Nifi, and particularly the GetSolr processor. </p>\n\n<p>I understand that the tstamp in my SolrRecord is the time at which the value in the index was fetched. </p>\n\n<p>The challenge I have, is for the GetSolr process to work in NiFi unattended, I need to provide a date field to filter on. If I use tstamp, it will only populate my dataflow the first time, after which the tstamp filter excludes future values, as it is looking at the index time, and not the time that the record was ingested into Solr. </p>\n\n<p>So my question is: how can I include a field in my SolrRecord at the time of bin\\nutch index that will include the timestamp of insertion into Solr, not fetching by the crawler.</p>\n", "creation_date": 1455081593, "score": 0},
{"title": "Apache Nutch nowhere to be found", "view_count": 41, "owner": {"user_id": 2555482, "answer_count": 1, "creation_date": 1373081890, "accept_rate": 67, "view_count": 17, "reputation": 161}, "is_answered": true, "answers": [{"question_id": 17801404, "owner": {"user_id": 1496834, "link": "http://stackoverflow.com/users/1496834/user1496834", "user_type": "registered", "reputation": 31}, "body": "<p>i encountered the same problem earlier today. finally one of the mirrors had the actual package. just keep clicking! :-P</p>\n", "creation_date": 1374553364, "is_accepted": true, "score": 0, "last_activity_date": 1374553364, "answer_id": 17801453}, {"question_id": 17801404, "owner": {"user_id": 1496834, "link": "http://stackoverflow.com/users/1496834/user1496834", "user_type": "registered", "reputation": 31}, "body": "<p>all hail Lord Google...found the most recent release in the 'archives'.</p>\n\n<p><a href=\"http://archive.apache.org/dist/nutch/2.2.1/\" rel=\"nofollow\">http://archive.apache.org/dist/nutch/2.2.1/</a></p>\n", "creation_date": 1374945169, "is_accepted": false, "score": 0, "last_activity_date": 1374945169, "answer_id": 17900430}, {"question_id": 17801404, "owner": {"user_id": 5822512, "accept_rate": 40, "link": "http://stackoverflow.com/users/5822512/ramachandra-reddy", "user_type": "registered", "reputation": 91}, "body": "<p>Just an update Nutch 2.3.1 is the latest version\n<a href=\"http://www.apache.org/dyn/closer.lua/nutch/2.3.1/apache-nutch-2.3.1-src.tar.gz\" rel=\"nofollow\">http://www.apache.org/dyn/closer.lua/nutch/2.3.1/apache-nutch-2.3.1-src.tar.gz</a></p>\n", "creation_date": 1455204350, "is_accepted": false, "score": 0, "last_activity_date": 1455204350, "answer_id": 35343333}], "question_id": 17801404, "tags": ["apache", "nutch"], "answer_count": 3, "link": "http://stackoverflow.com/questions/17801404/apache-nutch-nowhere-to-be-found", "last_activity_date": 1455204350, "accepted_answer_id": 17801453, "body": "<p>I'm looking for Nutch installation files but no server has them, they're all empty. </p>\n\n<p><a href=\"http://www.apache.org/dyn/closer.cgi/nutch/\" rel=\"nofollow\">http://www.apache.org/dyn/closer.cgi/nutch/</a></p>\n\n<p>Has it been moved or just retired??</p>\n", "creation_date": 1374553073, "score": 0},
{"title": "Extracting specific tags values with Apache Nutch", "view_count": 65, "is_answered": false, "question_id": 35342404, "tags": ["apache", "web-scraping", "html-parsing", "nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/35342404/extracting-specific-tags-values-with-apache-nutch", "last_activity_date": 1455201848, "owner": {"user_id": 3510671, "answer_count": 4, "creation_date": 1396955878, "accept_rate": 70, "view_count": 48, "reputation": 395}, "body": "<p>I'm trying to fetch a list of several URLs and parse their title keywords and description (and ignore all the rest) using Apache Nutch\nAfter that I just want to save for each URL all the title, keywords and description content (preferably without the tags themselves) without any indexing</p>\n\n<p>I looked at several examples on how to this. Just a few examples of what I encountered:</p>\n\n<ul>\n<li><a href=\"http://stackoverflow.com/questions/17972582/how-to-parse-content-located-in-specific-html-tags-using-nutch-plugin\">How to parse content located in specific HTML tags using nutch plugin?</a></li>\n<li><a href=\"http://www.atlantbh.com/precise-data-extraction-with-apache-nutch/\" rel=\"nofollow\">http://www.atlantbh.com/precise-data-extraction-with-apache-nutch/</a></li>\n</ul>\n\n<p>However they all propose complicated (at least to a Nutch newbie) plugins configuration and settings\nSince my use case sounds like a very common one I was wondering if there is any simpler solution?</p>\n\n<p>Thanks</p>\n", "creation_date": 1455201848, "score": 0},
{"title": "Nutch 2.3 not generate/crawling", "view_count": 298, "is_answered": false, "answers": [{"question_id": 35306975, "owner": {"user_id": 5907364, "link": "http://stackoverflow.com/users/5907364/j%c3%a9r%c3%a9mie", "user_type": "registered", "reputation": 46}, "body": "<p>Your logs suggest that you are running a nutch 2.3.1 version and you said that you have a 2.3 version.</p>\n\n<p>The 2.3.1 version added a lot of new compatibilities with the others technologies (see <a href=\"https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=10680&amp;version=12329371\" rel=\"nofollow\">release notes</a>).</p>\n\n<p>Maybe you use 2 incompatibles version.</p>\n", "creation_date": 1455109422, "is_accepted": false, "score": 0, "last_activity_date": 1455109422, "answer_id": 35316090}, {"question_id": 35306975, "owner": {"user_id": 5907364, "link": "http://stackoverflow.com/users/5907364/j%c3%a9r%c3%a9mie", "user_type": "registered", "reputation": 46}, "body": "<p>Can you check your space on the disk for your database ?\nBecause in nutch 2.X you use an extra database (Hbase, Cassandra...) and maybe you don't have enough space for the data, tempory files or others...</p>\n", "creation_date": 1455181998, "is_accepted": false, "score": 0, "last_activity_date": 1455181998, "answer_id": 35335031}], "question_id": 35306975, "tags": ["java", "apache", "web-crawler", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/35306975/nutch-2-3-not-generate-crawling", "last_activity_date": 1455181998, "owner": {"user_id": 5178563, "answer_count": 0, "creation_date": 1438358009, "accept_rate": 0, "view_count": 6, "reputation": 16}, "body": "<p>I am new to Nutch. I have installed Nutch 2.3 and have got it working up until injecting the seed urls ($NUTCH_ROOT/runtime/local/bin/nutch inject file:///path/to/seed/).</p>\n\n<p>When I do the next command:\n<strong>$NUTCH_ROOT/runtime/local/bin/nutch generate -topN 10</strong><br>\nI get this error:</p>\n\n<pre><code>GeneratorJob: starting at 2016-02-09 23:31:01\nGeneratorJob: Selecting best-scoring urls due for fetch.\nGeneratorJob: starting\nGeneratorJob: filtering: true\nGeneratorJob: normalizing: true\nGeneratorJob: topN: 10\nGeneratorJob: java.lang.RuntimeException: job failed: name=apache-nutch-2.3.1.jar, jobid=job_local1073670973_0001\n    at org.apache.nutch.util.NutchJob.waitForCompletion(NutchJob.java:120)\n    at org.apache.nutch.crawl.GeneratorJob.run(GeneratorJob.java:227)\n    at org.apache.nutch.crawl.GeneratorJob.generate(GeneratorJob.java:256)\n    at org.apache.nutch.crawl.GeneratorJob.run(GeneratorJob.java:322)\n    at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)\n    at org.apache.nutch.crawl.GeneratorJob.main(GeneratorJob.java:330)\n</code></pre>\n\n<p>Any suggestions on where to go from here or what to look for?</p>\n\n<p>Any help on this issue would be greatly appreciated!</p>\n", "creation_date": 1455079391, "score": 0},
{"title": "Crawl delay based on IP address vs hostname vs domain name", "view_count": 117, "owner": {"user_id": 5689154, "view_count": 24, "answer_count": 0, "creation_date": 1450319539, "reputation": 44}, "is_answered": true, "answers": [{"question_id": 35324707, "owner": {"user_id": 432844, "link": "http://stackoverflow.com/users/432844/julien-nioche", "user_type": "registered", "reputation": 939}, "body": "<p>It is good practice to partition by IP with Nutch. The generation step takes a bit longer because of the IP resolution but you'll get a guarantee that the Fetcher will behave politely while at the same time keeping good performance. The politeness settings from robots.txt will be enforced anyway.</p>\n\n<p>I have done multi billion page crawls with Nutch and from experience grouping URLs by IP is the best option. The last thing you want is to be blacklisted by websites or worse have AWS (or whichever cloud provider you are running on) kicking you out. Many webmasters do not even know about robots.txt and will feel very defensive it they perceive your crawler as abusive - even if you intend to crawl politely. The larger the scale, the more cautious you should be.</p>\n", "creation_date": 1455181159, "is_accepted": true, "score": 2, "last_activity_date": 1455181159, "answer_id": 35334772}], "question_id": 35324707, "tags": ["web-crawler", "nutch", "ddos"], "answer_count": 1, "link": "http://stackoverflow.com/questions/35324707/crawl-delay-based-on-ip-address-vs-hostname-vs-domain-name", "last_activity_date": 1455181159, "accepted_answer_id": 35334772, "body": "<p>For example in the case of crawling stackoverflow, it makes sense to delay based on hostname/domain name (e.g. send a request to stackoverflow.com every 10 minutes)</p>\n\n<p>In the case of *.blogspot.com it only makes sense to delay requests based on domain name since there are millions of hostnames ending with .blogspot.com and delaying based on that will flood the server with millions of requests.</p>\n\n<p>When crawling a wide range of websites (web scale crawls) what is the best practice in terms of imposing delays between requests? Should I delay requests based on IP address, hostname or domain name?</p>\n", "creation_date": 1455134238, "score": 0},
{"title": "Nutch IllegalArgumentException: Row length 41221 is &gt; 32767", "view_count": 333, "owner": {"user_id": 1784848, "answer_count": 13, "creation_date": 1351581947, "accept_rate": 57, "view_count": 32, "location": "NY, United States", "reputation": 52}, "is_answered": true, "answers": [{"question_id": 34893658, "owner": {"user_id": 5907364, "link": "http://stackoverflow.com/users/5907364/j%c3%a9r%c3%a9mie", "user_type": "registered", "reputation": 46}, "body": "<p>I had exactly the same problem with the same techno. </p>\n\n<p>I resolved this by changing the file HConstants.java in the hbase-common-0.98.17-hadoop2.jar (in nutch : nutch/build/lib and in hbase : /hbase/lib).</p>\n\n<p>I removed this line :  </p>\n\n<pre><code>public static final short MAX_ROW_LENGTH = Short.MAX_VALUE;\n</code></pre>\n\n<p>And added this line : </p>\n\n<pre><code>public static final long MAX_ROW_LENGTH = Long.MAX_VALUE;\n</code></pre>\n\n<p>Now, it works like a charm.</p>\n\n<p>Please say me if it's running for you too or if you need help for the jar. </p>\n", "creation_date": 1455095134, "is_accepted": true, "score": 1, "last_activity_date": 1455095134, "answer_id": 35310770}], "question_id": 34893658, "tags": ["hadoop", "solr", "hbase", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/34893658/nutch-illegalargumentexception-row-length-41221-is-32767", "last_activity_date": 1455095134, "accepted_answer_id": 35310770, "body": "<p>I have added a set of seeds to crawl using this command</p>\n\n<p>./bin/crawl /largeSeeds 1 <a href=\"http://localhost:8983/solr/ddcd\" rel=\"nofollow\">http://localhost:8983/solr/ddcd</a> 4</p>\n\n<p>For first iteration all of the commands(inject, generate, fetch, parse, update-table, Indexer &amp; delete duplicates.) got executed successfully.\nFor second iteration, \"CrawlDB update\" command got failed (please see error log for reference), because of failure of this command the whole process gets terminated.</p>\n\n<p>Software stack is nutch-branch-2.3.1, gora-hbase 0.6.1 Hadoop 2.5.2, hbase-0.98.8-hadoop2</p>\n\n<pre><code>16/01/20 02:45:19 INFO parse.ParserJob: ParserJob: finished at 2016-01-20 02:45:19, time elapsed: 00:06:57\nCrawlDB update for 1\n/usr/share/searchEngine/nutch-branch-2.3.1/runtime/deploy/bin/nutch updatedb -D mapred.reduce.tasks=2 -D mapred.child.java.opts=-Xmx1000m -D mapred.reduce.tasks.speculative.execution=false -D mapred.map.tasks.speculative.execution=false -D mapred.compress.map.output=true 1453230757-13191 -crawlId 1\n16/01/20 02:45:27 INFO crawl.DbUpdaterJob: DbUpdaterJob: starting at 2016-01-20 02:45:27\n16/01/20 02:45:27 INFO crawl.DbUpdaterJob: DbUpdaterJob: batchId: 1453230757-13191\n16/01/20 02:45:27 INFO plugin.PluginRepository: Plugins: looking in: /tmp/hadoop-root/hadoop-unjar5654418190157422003/classes/plugins\n16/01/20 02:45:28 INFO plugin.PluginRepository: Plugin Auto-activation mode: [true]\n16/01/20 02:45:28 INFO plugin.PluginRepository: Registered Plugins:\n16/01/20 02:45:28 INFO plugin.PluginRepository:     HTTP Framework (lib-http)\n16/01/20 02:45:28 INFO plugin.PluginRepository:     Html Parse Plug-in (parse-html)\n16/01/20 02:45:28 INFO plugin.PluginRepository:     MetaTags (parse-metatags)\n16/01/20 02:45:28 INFO plugin.PluginRepository:     the nutch core extension points (nutch-extensionpoints)\n16/01/20 02:45:28 INFO plugin.PluginRepository:     Basic Indexing Filter (index-basic)\n16/01/20 02:45:28 INFO plugin.PluginRepository:     XML Libraries (lib-xml)\n16/01/20 02:45:28 INFO plugin.PluginRepository:     Anchor Indexing Filter (index-anchor)\n16/01/20 02:45:28 INFO plugin.PluginRepository:     Basic URL Normalizer (urlnormalizer-basic)\n16/01/20 02:45:28 INFO plugin.PluginRepository:     Language Identification Parser/Filter (language-identifier)\n16/01/20 02:45:28 INFO plugin.PluginRepository:     Metadata Indexing Filter (index-metadata)\n16/01/20 02:45:28 INFO plugin.PluginRepository:     CyberNeko HTML Parser (lib-nekohtml)\n16/01/20 02:45:28 INFO plugin.PluginRepository:     Subcollection indexing and query filter (subcollection)\n16/01/20 02:45:28 INFO plugin.PluginRepository:     SOLRIndexWriter (indexer-solr)\n16/01/20 02:45:28 INFO plugin.PluginRepository:     Rel-Tag microformat Parser/Indexer/Querier (microformats-reltag)\n16/01/20 02:45:28 INFO plugin.PluginRepository:     Http / Https Protocol Plug-in (protocol-httpclient)\n16/01/20 02:45:28 INFO plugin.PluginRepository:     JavaScript Parser (parse-js)\n16/01/20 02:45:28 INFO plugin.PluginRepository:     Tika Parser Plug-in (parse-tika)\n16/01/20 02:45:28 INFO plugin.PluginRepository:     Top Level Domain Plugin (tld)\n16/01/20 02:45:28 INFO plugin.PluginRepository:     Regex URL Filter Framework (lib-regex-filter)\n16/01/20 02:45:28 INFO plugin.PluginRepository:     Regex URL Normalizer (urlnormalizer-regex)\n16/01/20 02:45:28 INFO plugin.PluginRepository:     Link Analysis Scoring Plug-in (scoring-link)\n16/01/20 02:45:28 INFO plugin.PluginRepository:     OPIC Scoring Plug-in (scoring-opic)\n16/01/20 02:45:28 INFO plugin.PluginRepository:     More Indexing Filter (index-more)\n16/01/20 02:45:28 INFO plugin.PluginRepository:     Http Protocol Plug-in (protocol-http)\n16/01/20 02:45:28 INFO plugin.PluginRepository:     Creative Commons Plugins (creativecommons)\n16/01/20 02:45:28 INFO plugin.PluginRepository: Registered Extension-Points:\n16/01/20 02:45:28 INFO plugin.PluginRepository:     Parse Filter (org.apache.nutch.parse.ParseFilter)\n16/01/20 02:45:28 INFO plugin.PluginRepository:     Nutch Index Cleaning Filter (org.apache.nutch.indexer.IndexCleaningFilter)\n16/01/20 02:45:28 INFO plugin.PluginRepository:     Nutch Content Parser (org.apache.nutch.parse.Parser)\n16/01/20 02:45:28 INFO plugin.PluginRepository:     Nutch URL Filter (org.apache.nutch.net.URLFilter)\n16/01/20 02:45:28 INFO plugin.PluginRepository:     Nutch Scoring (org.apache.nutch.scoring.ScoringFilter)\n16/01/20 02:45:28 INFO plugin.PluginRepository:     Nutch URL Normalizer (org.apache.nutch.net.URLNormalizer)\n16/01/20 02:45:28 INFO plugin.PluginRepository:     Nutch Protocol (org.apache.nutch.protocol.Protocol)\n16/01/20 02:45:28 INFO plugin.PluginRepository:     Nutch Index Writer (org.apache.nutch.indexer.IndexWriter)\n16/01/20 02:45:28 INFO plugin.PluginRepository:     Nutch Indexing Filter (org.apache.nutch.indexer.IndexingFilter)\n16/01/20 02:45:29 INFO Configuration.deprecation: mapred.map.tasks.speculative.execution is deprecated. Instead, use mapreduce.map.speculative\n16/01/20 02:45:29 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative\n16/01/20 02:45:29 INFO Configuration.deprecation: mapred.compress.map.output is deprecated. Instead, use mapreduce.map.output.compress\n16/01/20 02:45:29 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n16/01/20 02:45:29 INFO zookeeper.RecoverableZooKeeper: Process identifier=hconnection-0x60a2630a connecting to ZooKeeper ensemble=localhost:2181\n16/01/20 02:45:29 INFO zookeeper.ZooKeeper: Client environment:zookeeper.version=3.4.6-1569965, built on 02/20/2014 09:09 GMT\n16/01/20 02:45:29 INFO zookeeper.ZooKeeper: Client environment:host.name=cism479\n16/01/20 02:45:29 INFO zookeeper.ZooKeeper: Client environment:java.version=1.8.0_65\n16/01/20 02:45:29 INFO zookeeper.ZooKeeper: Client environment:java.vendor=Oracle Corporation\n16/01/20 02:45:29 INFO zookeeper.ZooKeeper: Client environment:java.home=/usr/lib/jvm/jdk1.8.0_65/jre\n16/01/20 02:45:35 INFO zookeeper.ClientCnxn: EventThread shut down\n16/01/20 02:45:35 INFO mapreduce.JobSubmitter: number of splits:2\n16/01/20 02:45:36 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1453210838763_0011\n16/01/20 02:45:36 INFO impl.YarnClientImpl: Submitted application application_1453210838763_0011\n16/01/20 02:45:36 INFO mapreduce.Job: The url to track the job: http://cism479:8088/proxy/application_1453210838763_0011/\n16/01/20 02:45:36 INFO mapreduce.Job: Running job: job_1453210838763_0011\n16/01/20 02:45:48 INFO mapreduce.Job: Job job_1453210838763_0011 running in uber mode : false\n16/01/20 02:45:48 INFO mapreduce.Job:  map 0% reduce 0%\n16/01/20 02:47:31 INFO mapreduce.Job:  map 33% reduce 0%\n16/01/20 02:47:47 INFO mapreduce.Job:  map 50% reduce 0%\n16/01/20 02:48:08 INFO mapreduce.Job:  map 83% reduce 0%\n16/01/20 02:48:16 INFO mapreduce.Job:  map 100% reduce 0%\n16/01/20 02:48:31 INFO mapreduce.Job:  map 100% reduce 31%\n16/01/20 02:48:34 INFO mapreduce.Job:  map 100% reduce 33%\n16/01/20 02:50:30 INFO mapreduce.Job:  map 100% reduce 34%\n16/01/20 03:01:18 INFO mapreduce.Job:  map 100% reduce 35%\n16/01/20 03:11:58 INFO mapreduce.Job:  map 100% reduce 36%\n16/01/20 03:22:50 INFO mapreduce.Job:  map 100% reduce 37%\n16/01/20 03:24:22 INFO mapreduce.Job:  map 100% reduce 50%\n16/01/20 03:24:35 INFO mapreduce.Job:  map 100% reduce 82%\n16/01/20 03:24:38 INFO mapreduce.Job:  map 100% reduce 83%\n16/01/20 03:26:33 INFO mapreduce.Job:  map 100% reduce 84%\n16/01/20 03:37:35 INFO mapreduce.Job:  map 100% reduce 85%\n16/01/20 03:39:38 INFO mapreduce.Job: Task Id : attempt_1453210838763_0011_r_000001_0, Status : FAILED\nError: java.lang.IllegalArgumentException: Row length 41221 is &gt; 32767\n    at org.apache.hadoop.hbase.client.Mutation.checkRow(Mutation.java:506)\n    at org.apache.hadoop.hbase.client.Mutation.checkRow(Mutation.java:487)\n    at org.apache.hadoop.hbase.client.Get.&lt;init&gt;(Get.java:89)\n    at org.apache.gora.hbase.store.HBaseStore.get(HBaseStore.java:208)\n    at org.apache.gora.hbase.store.HBaseStore.get(HBaseStore.java:79)\n    at org.apache.gora.store.impl.DataStoreBase.get(DataStoreBase.java:156)\n    at org.apache.gora.store.impl.DataStoreBase.get(DataStoreBase.java:56)\n    at org.apache.nutch.crawl.DbUpdateReducer.reduce(DbUpdateReducer.java:114)\n    at org.apache.nutch.crawl.DbUpdateReducer.reduce(DbUpdateReducer.java:42)\n    at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:171)\n    at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:627)\n    at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:389)\n    at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)\n    at java.security.AccessController.doPrivileged(Native Method)\n    at javax.security.auth.Subject.doAs(Subject.java:422)\n    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1614)\n    at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:163)\n\n16/01/20 03:39:39 INFO mapreduce.Job:  map 100% reduce 50%\n16/01/20 03:39:52 INFO mapreduce.Job:  map 100% reduce 82%\n16/01/20 03:39:55 INFO mapreduce.Job:  map 100% reduce 83%\n16/01/20 03:41:56 INFO mapreduce.Job:  map 100% reduce 84%\n16/01/20 03:53:39 INFO mapreduce.Job:  map 100% reduce 85%\n16/01/20 03:55:49 INFO mapreduce.Job: Task Id : attempt_1453210838763_0011_r_000001_1, Status : FAILED\nError: java.lang.IllegalArgumentException: Row length 41221 is &gt; 32767\n    at org.apache.hadoop.hbase.client.Mutation.checkRow(Mutation.java:506)\n    at org.apache.hadoop.hbase.client.Mutation.checkRow(Mutation.java:487)\n    at org.apache.hadoop.hbase.client.Get.&lt;init&gt;(Get.java:89)\n    at org.apache.gora.hbase.store.HBaseStore.get(HBaseStore.java:208)\n    at org.apache.gora.hbase.store.HBaseStore.get(HBaseStore.java:79)\n    at org.apache.gora.store.impl.DataStoreBase.get(DataStoreBase.java:156)\n    at org.apache.gora.store.impl.DataStoreBase.get(DataStoreBase.java:56)\n    at org.apache.nutch.crawl.DbUpdateReducer.reduce(DbUpdateReducer.java:114)\n    at org.apache.nutch.crawl.DbUpdateReducer.reduce(DbUpdateReducer.java:42)\n    at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:171)\n    at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:627)\n    at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:389)\n    at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)\n    at java.security.AccessController.doPrivileged(Native Method)\n    at javax.security.auth.Subject.doAs(Subject.java:422)\n    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1614)\n    at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:163)\n\n16/01/20 03:55:50 INFO mapreduce.Job:  map 100% reduce 50%\n16/01/20 03:56:01 INFO mapreduce.Job:  map 100% reduce 83%\n16/01/20 03:58:02 INFO mapreduce.Job:  map 100% reduce 84%\n16/01/20 04:10:09 INFO mapreduce.Job:  map 100% reduce 85%\n16/01/20 04:12:33 INFO mapreduce.Job: Task Id : attempt_1453210838763_0011_r_000001_2, Status : FAILED\nError: java.lang.IllegalArgumentException: Row length 41221 is &gt; 32767\n    at org.apache.hadoop.hbase.client.Mutation.checkRow(Mutation.java:506)\n    at org.apache.hadoop.hbase.client.Mutation.checkRow(Mutation.java:487)\n    at org.apache.hadoop.hbase.client.Get.&lt;init&gt;(Get.java:89)\n    at org.apache.gora.hbase.store.HBaseStore.get(HBaseStore.java:208)\n    at org.apache.gora.hbase.store.HBaseStore.get(HBaseStore.java:79)\n    at org.apache.gora.store.impl.DataStoreBase.get(DataStoreBase.java:156)\n    at org.apache.gora.store.impl.DataStoreBase.get(DataStoreBase.java:56)\n    at org.apache.nutch.crawl.DbUpdateReducer.reduce(DbUpdateReducer.java:114)\n    at org.apache.nutch.crawl.DbUpdateReducer.reduce(DbUpdateReducer.java:42)\n    at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:171)\n    at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:627)\n    at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:389)\n    at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)\n    at java.security.AccessController.doPrivileged(Native Method)\n    at javax.security.auth.Subject.doAs(Subject.java:422)\n    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1614)\n    at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:163)\n\n16/01/20 04:12:34 INFO mapreduce.Job:  map 100% reduce 50%\n16/01/20 04:12:45 INFO mapreduce.Job:  map 100% reduce 82%\n16/01/20 04:12:48 INFO mapreduce.Job:  map 100% reduce 83%\n16/01/20 04:14:46 INFO mapreduce.Job:  map 100% reduce 84%\n16/01/20 04:26:53 INFO mapreduce.Job:  map 100% reduce 85%\n16/01/20 04:29:09 INFO mapreduce.Job:  map 100% reduce 100%\n16/01/20 04:29:10 INFO mapreduce.Job: Job job_1453210838763_0011 failed with state FAILED due to: Task failed task_1453210838763_0011_r_000001\nJob failed as tasks failed. failedMaps:0 failedReduces:1\n\n16/01/20 04:29:11 INFO mapreduce.Job: Counters: 50\n    File System Counters\n        FILE: Number of bytes read=38378343\n        FILE: Number of bytes written=115957636\n        FILE: Number of read operations=0\n        FILE: Number of large read operations=0\n        FILE: Number of write operations=0\n        HDFS: Number of bytes read=2382\n        HDFS: Number of bytes written=0\n        HDFS: Number of read operations=2\n        HDFS: Number of large read operations=0\n        HDFS: Number of write operations=0\n    Job Counters\n        Failed reduce tasks=4\n        Launched map tasks=2\n        Launched reduce tasks=5\n        Data-local map tasks=2\n        Total time spent by all maps in occupied slots (ms)=789909\n        Total time spent by all reduces in occupied slots (ms)=30215090\n        Total time spent by all map tasks (ms)=263303\n        Total time spent by all reduce tasks (ms)=6043018\n        Total vcore-seconds taken by all map tasks=263303\n        Total vcore-seconds taken by all reduce tasks=6043018\n        Total megabyte-seconds taken by all map tasks=808866816\n        Total megabyte-seconds taken by all reduce tasks=30940252160\n    Map-Reduce Framework\n        Map input records=49929\n        Map output records=1777904\n        Map output bytes=382773368\n        Map output materialized bytes=77228942\n        Input split bytes=2382\n        Combine input records=0\n        Combine output records=0\n        Reduce input groups=754170\n        Reduce shuffle bytes=38318183\n        Reduce input records=881156\n        Reduce output records=754170\n        Spilled Records=2659060\n        Shuffled Maps =2\n        Failed Shuffles=0\n        Merged Map outputs=2\n        GC time elapsed (ms)=17993\n        CPU time spent (ms)=819690\n        Physical memory (bytes) snapshot=4080136192\n        Virtual memory (bytes) snapshot=15234293760\n        Total committed heap usage (bytes)=4149739520\n    Shuffle Errors\n        BAD_ID=0\n        CONNECTION=0\n        IO_ERROR=0\n        WRONG_LENGTH=0\n        WRONG_MAP=0\n        WRONG_REDUCE=0\n    File Input Format Counters\n        Bytes Read=0\n    File Output Format Counters\n        Bytes Written=0\nException in thread \"main\" java.lang.RuntimeException: job failed: name=[1]update-table, jobid=job_1453210838763_0011\n    at org.apache.nutch.util.NutchJob.waitForCompletion(NutchJob.java:120)\n    at org.apache.nutch.crawl.DbUpdaterJob.run(DbUpdaterJob.java:111)\n    at org.apache.nutch.crawl.DbUpdaterJob.updateTable(DbUpdaterJob.java:140)\n    at org.apache.nutch.crawl.DbUpdaterJob.run(DbUpdaterJob.java:174)\n    at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)\n    at org.apache.nutch.crawl.DbUpdaterJob.main(DbUpdaterJob.java:178)\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n    at java.lang.reflect.Method.invoke(Method.java:497)\n    at org.apache.hadoop.util.RunJar.main(RunJar.java:212)\nError running:\n  /usr/share/searchEngine/nutch-branch-2.3.1/runtime/deploy/bin/nutch updatedb -D mapred.reduce.tasks=2 -D mapred.child.java.opts=-Xmx1000m -D mapred.reduce.tasks.speculative.execution=false -D mapred.map.tasks.speculative.execution=false -D mapred.compress.map.output=true 1453230757-13191 -crawlId 1\nFailed with exit value 1.\n</code></pre>\n\n<p>Please advise.</p>\n", "creation_date": 1453273914, "score": 0},
{"title": "Can I Define a Custom extension point in Apacahe Nutch 1.8", "view_count": 20, "owner": {"user_id": 4644454, "view_count": 4, "answer_count": 0, "creation_date": 1425741586, "reputation": 10}, "is_answered": true, "answers": [{"question_id": 35301335, "owner": {"user_id": 432844, "link": "http://stackoverflow.com/users/432844/julien-nioche", "user_type": "registered", "reputation": 939}, "body": "<p>There is no extension point for doing this. Either export the URLs from Postgres into a text file or rewrite the injector so that it can read from it - which would be more complex.</p>\n\n<p>If you are on Nutch 2.x, GORA has a SQL connector which might work with Postgres.</p>\n", "creation_date": 1455052753, "is_accepted": true, "score": 0, "last_activity_date": 1455052753, "answer_id": 35302380}], "question_id": 35301335, "tags": ["java", "apache", "web-crawler", "nutch", "information-retrieval"], "answer_count": 1, "link": "http://stackoverflow.com/questions/35301335/can-i-define-a-custom-extension-point-in-apacahe-nutch-1-8", "last_activity_date": 1455052753, "accepted_answer_id": 35302380, "body": "<p>I need to use a postgre sql database instead of txt file for seed urls before running injector job. Can i achieve this problem by using plugin system ? If I can, which extension point I should use or Should I define a custpm extension point ?  </p>\n", "creation_date": 1455048931, "score": 0},
{"title": "Nutch Parser Plugin Collect Contact Information", "view_count": 44, "is_answered": false, "answers": [{"question_id": 35299744, "owner": {"user_id": 432844, "link": "http://stackoverflow.com/users/432844/julien-nioche", "user_type": "registered", "reputation": 939}, "body": "<p>You should not need to write a custom map reduce job. Just implement a bespoke HTMLParseFilter which will give you a DOM to run XPath expressions on + the text of the document if you want regular expressions.</p>\n\n<p>Having worked on something similar for a customer a few years ago, I found that there were many pages implementing schema.org. You could write a custom HTMLParse filter with Xpath to extract normalised info from the microdata. You can look at the <a href=\"https://github.com/PopSugar/storm-crawler-extensions/tree/master/microdata-parser\" rel=\"nofollow\">microdata parser</a> which is for StormCrawler as a an example of how to leverage Apache Any23 to extract microdata.</p>\n\n<p>If you want a more NLP-intensive method, you could use <a href=\"https://github.com/DigitalPebble/behemoth\" rel=\"nofollow\">Behemoth</a> to process Nutch segments with tools such as Apache UIMA or GATE.</p>\n\n<p>HTH</p>\n", "creation_date": 1455052462, "is_accepted": false, "score": 0, "last_activity_date": 1455052462, "answer_id": 35302309}], "question_id": 35299744, "tags": ["plugins", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/35299744/nutch-parser-plugin-collect-contact-information", "last_activity_date": 1455052462, "owner": {"age": 27, "answer_count": 159, "creation_date": 1357505010, "user_id": 1953475, "accept_rate": 72, "view_count": 540, "location": "Denver, CO", "reputation": 5595}, "body": "<p>I am working on a project that need to identify contact points on company's website and used for the purpose of enhancing security. </p>\n\n<p>Right now, I managed to use Apache Nutch to crawl several rounds of sites. The next step will be to parse the HTML pages and locate where the contact information is. In this case, I am only interested in email addresses and phone numbers.... </p>\n\n<p>Here is what I am planning to do, we can write a map reduce jobs to parse HTML file and use things like regular expression in combo with Jsoup/Beautifulsoup HTML parsers to find the regular expression.  </p>\n\n<p>However, I am wondering is there any parser plugin that has already been implemented and maybe tested used for this purpose? </p>\n", "creation_date": 1455043394, "score": 0},
{"title": "How to make Nutch crawler to crawl only specific URLS?", "view_count": 50, "is_answered": false, "answers": [{"question_id": 35060863, "owner": {"user_id": 432844, "link": "http://stackoverflow.com/users/432844/julien-nioche", "user_type": "registered", "reputation": 939}, "body": "<p>As you pointed out, the URLFilters aren't much use as they deal with the URL string only. You could achieve what you described by implementing a custom HTMLParseFilter in which you'd access the <a href=\"https://github.com/apache/nutch/blob/trunk/src/java/org/apache/nutch/parse/ParseData.java\" rel=\"nofollow\">ParseData</a> for the current document. It contains the Outlinks which you could filter based on their anchor value.</p>\n\n<p>There are loads of examples online on how to write a plugin and/or custom HTMLParseFilter, see for instance <a href=\"https://github.com/apache/nutch/blob/trunk/src/plugin/parse-metatags/src/java/org/apache/nutch/parse/metatags/MetaTagsParser.java\" rel=\"nofollow\">MetaTagsParser</a>.</p>\n", "creation_date": 1454512483, "is_accepted": false, "score": 0, "last_activity_date": 1454512483, "answer_id": 35180783}], "question_id": 35060863, "tags": ["nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/35060863/how-to-make-nutch-crawler-to-crawl-only-specific-urls", "last_activity_date": 1454512483, "owner": {"user_id": 5505650, "view_count": 3, "answer_count": 0, "creation_date": 1446176066, "reputation": 6}, "body": "<p>I'm aware that regex can be used to restrict the pages that will be downloaded. But, I would like to crawl only those pages where the anchor link in a given page is in a set of urls. For example, i have an array with words like ['computer','software','hardware','operating system','thread'], I would only like to crawl those urls whose anchor text contains one of these words in the array. Where should I implement this kind of logic in Nutch? Thank you.</p>\n", "creation_date": 1453982260, "score": 0},
{"title": "Nutch plugin with phantomjs and selenium on Ubuntu 14.04", "view_count": 264, "is_answered": false, "question_id": 33518344, "tags": ["ubuntu", "selenium", "ant", "phantomjs", "nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/33518344/nutch-plugin-with-phantomjs-and-selenium-on-ubuntu-14-04", "last_activity_date": 1454435036, "owner": {"user_id": 4431527, "view_count": 4, "answer_count": 0, "creation_date": 1420694614, "reputation": 6}, "body": "<p>I was trying to add selenium plugin to Nutch crawler of nutch 1.10 to load dynamic content of webpages and I get this error when I execute <code>bin/nutch fetch $s1</code> as introduced in <a href=\"https://wiki.apache.org/nutch/NutchTutorial\" rel=\"nofollow\">Nutch tutorial</a>:</p>\n\n<pre><code>fetch of http://www.ebay.com/sch/i.html?_from=R40&amp;_trksid=p2050601.m570.l1313.TR0.TRC0.H0.Xtest.TRS0&amp;_nkw=test&amp;_sacat=0 \nfailed with: java.lang.NoSuchFieldError: INSTANCE\nThread FetcherThread has no more work available\n-finishing thread FetcherThread, activeThreads=0\n-activeThreads=0, spinWaiting=0, fetchQueues.totalSize=0, fetchQueues.getQueueCount=0\n-activeThreads=0\nFetcher: finished at 2015-11-04 16:54:24, elapsed: 00:00:05\n</code></pre>\n\n<p>I'm using <strong>phantomjs 1.9.0</strong> and installed with <code>sudo apt-get install phantomjs</code>, and downloaded <code>selenium-server-standalone-2.48.2.jar</code><br/>\nI originally followed momer's github nutch selenium plugin <a href=\"https://github.com/momer/nutch-selenium\" rel=\"nofollow\">here</a>, but feel Firefox is kind of troublesome with old version and display for Xvfb. <br />\nI modified the <strong>HttpWebClient.java</strong> in src/plugin/lib-selenium<br /></p>\n\n<pre><code>protected WebDriver initialValue()\n{\n  // DesiredCapabilities caps = new DesiredCapabilities();\n  // caps.setCapability(\"phantomjs.binary.path\",\n  //               \"/home/test/NutchInstall/phantomjs/phantomjs/bin/phantomjs\");                  \n  // WebDriver driver = new PhantomJSDriver(caps);\n  WebDriver driver = new PhantomJSDriver();\n  return driver;\n};\n</code></pre>\n\n<p>It didn't report any error of cannot start phantomjs so I assume it successfully started the phantomjs driver? <br />\nSome other modifications:<br />\n<strong>lib-selenium/ivy.xml</strong> and <strong>lib-selenium/plugin.xml</strong></p>\n\n<pre><code>&lt;library name=\"selenium-server-standalone-2.48.2.jar\"&gt;\n   &lt;export name=\"*\"/&gt;\n&lt;/library&gt;\n</code></pre>\n\n<p><strong>protocol-selenium/plugin.xml</strong></p>\n\n<pre><code>&lt;!-- &lt;library name=\"operadriver-1.5.jar\"/&gt; --&gt;\n  &lt;!-- &lt;library name=\"operalaunchers-1.1.jar\"/&gt; --&gt;\n  &lt;library name=\"platform-3.4.0.jar\"/&gt;\n  &lt;library name=\"protobuf-java-2.4.1.jar\"/&gt;\n  &lt;library name=\"sac-1.3.jar\"/&gt;\n&lt;!--       &lt;library name=\"selenium-api-2.44.0.jar\"/&gt;\n  &lt;library name=\"selenium-chrome-driver-2.44.0.jar\"/&gt;\n  &lt;library name=\"selenium-firefox-driver-2.44.0.jar\"/&gt;\n  &lt;library name=\"selenium-htmlunit-driver-2.44.0.jar\"/&gt;\n  &lt;library name=\"selenium-ie-driver-2.44.0.jar\"/&gt;\n  &lt;library name=\"selenium-java-2.44.0.jar\"/&gt;\n  &lt;library name=\"selenium-remote-driver-2.44.0.jar\"/&gt;\n  &lt;library name=\"selenium-safari-driver-2.44.0.jar\"/&gt;\n  &lt;library name=\"selenium-support-2.44.0.jar\"/&gt;\n  &lt;library name=\"serializer-2.7.1.jar\"/&gt; --&gt;\n  &lt;library name=\"selenium-server-standalone-2.48.2.jar\" /&gt;\n</code></pre>\n\n<p><strong>protocol-selenium/ivy.xml</strong></p>\n\n<pre><code>&lt;dependency org=\"org.seleniumhq.selenium\" name=\"selenium-server\" rev=\"2.48.2\" conf=\"test-&gt;default\"&gt;\n      &lt;artifact name=\"selenium-server-standalone\" type=\"jar\" url=\"http://selenium-release.storage.googleapis.com/2.48/selenium-server-standalone-2.48.2.jar\"/&gt;\n&lt;/dependency&gt;\n</code></pre>\n\n<p>and also in <strong>${NUTCH_HOME}/ivy/ivy.xml</strong>:</p>\n\n<pre><code>&lt;!-- begin selenium dependencies --&gt;\n    &lt;dependency org=\"org.seleniumhq.selenium\" name=\"selenium-server\" rev=\"2.48.2\" conf=\"test-&gt;default\"&gt;\n        &lt;artifact name=\"selenium-server-standalone\" type=\"jar\" url=\"http://selenium-release.storage.googleapis.com/2.48/selenium-server-standalone-2.48.2.jar\"/&gt;\n    &lt;/dependency&gt;\n    &lt;!-- end selenium dependencies --&gt;\n</code></pre>\n\n<p>I saw some <a href=\"http://stackoverflow.com/questions/29200736/jmeter-and-webdriver-set-do-not-work\">post</a> saying that there are multiple versions of .jar but I've checked there's none. <br />\n<strong>ant</strong> also built successfully the runtime of Nutch so I think those xml files are correct?<hr />\nBTW, initially I also tried with the following code in <strong>HttpWebClient.java</strong> with <strong>phantomjs 2.0.0</strong> cloned from github:</p>\n\n<pre><code>DesiredCapabilities caps = new DesiredCapabilities();\ncaps.setCapability(\"phantomjs.binary.path\",\n           \"/home/test/NutchInstall/phantomjs/phantomjs/bin/phantomjs\");                  \nWebDriver driver = new PhantomJSDriver(caps);\n</code></pre>\n\n<p>but got the error:</p>\n\n<pre><code>The path to the driver executable must be set by the phantomjs.binary.path capability/system property/PATH variable\n</code></pre>\n\n<p>that's why I installed phantomjs directly from apt-get.</p>\n", "creation_date": 1446629987, "score": 0},
{"title": "nutch configuration to crawl entire website without specifying depth", "view_count": 315, "owner": {"user_id": 5836990, "view_count": 1, "answer_count": 1, "creation_date": 1453724636, "reputation": 8}, "is_answered": true, "answers": [{"question_id": 34992942, "owner": {"user_id": 1699191, "link": "http://stackoverflow.com/users/1699191/ameertawfik", "user_type": "registered", "reputation": 618}, "body": "<p>I do not think you can do that. But the work around is to configure nutch to crawl only links from the same domain, then put the crawling depth to very huge number (say 1 million). Nutch will keep crawling while there are links to crawl. Once there are no urls to crawl, it will stop regardless of the depth limit.</p>\n", "creation_date": 1453733698, "is_accepted": false, "score": 0, "last_activity_date": 1453733698, "answer_id": 34995537}, {"question_id": 34992942, "owner": {"user_id": 432844, "link": "http://stackoverflow.com/users/432844/julien-nioche", "user_type": "registered", "reputation": 939}, "body": "<p>You can set a value of -1 to the depth since Nutch 1.11. See <a href=\"https://github.com/apache/nutch/blob/trunk/src/bin/crawl#L180\" rel=\"nofollow\">crawl script</a> </p>\n", "creation_date": 1453928874, "is_accepted": true, "score": 1, "last_activity_date": 1453928874, "answer_id": 35048013}], "question_id": 34992942, "tags": ["web-crawler", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/34992942/nutch-configuration-to-crawl-entire-website-without-specifying-depth", "last_activity_date": 1454237088, "accepted_answer_id": 35048013, "body": "<p>I am using Nutch-1.8 for crawling website and solr for indexing.i need to crawl entire website till the last child link without specifying depth parameter(-depth)</p>\n\n<p>this is the command i am using to crawl and index urls</p>\n\n<p><strong>command :</strong> bin/crawl seeds brainiademo <a href=\"http://localhost:8983/solr/\" rel=\"nofollow\">http://localhost:8983/solr/</a> 10</p>\n\n<p><strong>syntax</strong> : <code>bin/nutch crawl &lt;urlDir&gt; [-solr &lt;solrURL&gt;] [-dir d] [-threads n] [-depth i] [-topN N]</code></p>\n\n<p>in above command I don't want to specify depth parameter i.e., 10.</p>\n\n<p>what are the configuration changes need to be done in order to crawl entire website without specifying depth parameter?</p>\n", "creation_date": 1453725808, "score": 1},
{"title": "Compiling error Nutch on HDP", "view_count": 37, "is_answered": false, "answers": [{"question_id": 35087464, "owner": {"user_id": 1784848, "accept_rate": 57, "link": "http://stackoverflow.com/users/1784848/code-wrangler", "user_type": "registered", "reputation": 52}, "body": "<p>Try these steps :</p>\n\n<ol>\n<li>copy $HABSE_HOME/lib/ to nutch.x.x.x/lib </li>\n<li>make sure to copy the hbase-site.xml from hbase config to nutch cofig dir. </li>\n<li>run \"ant clean\" and \"ant runtime\"</li>\n</ol>\n\n<p>hth.</p>\n", "creation_date": 1454152467, "is_accepted": false, "score": 0, "last_activity_date": 1454152467, "answer_id": 35101090}], "question_id": 35087464, "tags": ["hadoop", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/35087464/compiling-error-nutch-on-hdp", "last_activity_date": 1454152467, "owner": {"user_id": 649923, "answer_count": 15, "creation_date": 1299593695, "accept_rate": 86, "view_count": 44, "reputation": 670}, "body": "<p>I just started using nutch, and after spending yesterday to figure out\nhow to run nutch on the newest HDP (2.3.2) vm i ran into some\nproblems.</p>\n\n<p>Building the source directly, went fine,\nbut after the local first run i ran into the</p>\n\n<pre><code>java.lang.ClassNotFoundException: org.apache.gora.hbase.store.HBaseStore\n</code></pre>\n\n<p>i got this error solved with also adding the hbase jars to the\nclasspath but now i always run into an error i can't really get my\nhead around</p>\n\n<pre><code>java.lang.NoSuchMethodError:\norg.apache.hadoop.hbase.HTableDescriptor.addFamily(Lorg/apache/hadoop/hbase/HColumnDescriptor;)V\n</code></pre>\n\n<p>on stack overflow it is suggested to add the hbase libs to the class\npath, but i already did that, to fix the classnotfound exception.</p>\n\n<p>I just started with hadoop, so it might be an error spawned from the\nlack of knowledge of the system.</p>\n\n<p>Anybody got an idea how to get nutch up and running on HDP?</p>\n\n<p>Thanks</p>\n", "creation_date": 1454079456, "score": 0},
{"title": "Nutch cause error while using solrindex command", "view_count": 156, "is_answered": false, "question_id": 35062030, "tags": ["nutch", "solr5"], "answer_count": 0, "link": "http://stackoverflow.com/questions/35062030/nutch-cause-error-while-using-solrindex-command", "last_activity_date": 1453985493, "owner": {"user_id": 5850189, "view_count": 0, "answer_count": 1, "creation_date": 1453956322, "reputation": 11}, "body": "<p>I am using nutch 1.11 (published in 07 December 2015) and using bin/crawl command to help me do the work and everything is ok until it reaches to <strong>solrindex</strong> command to put the data to solr search engine, it cause error:</p>\n\n<pre><code>SolrIndexWriter\n    solr.server.type : Type of SolrServer to communicate with (default 'http' however options include 'cloud', 'lb' and 'concurrent')\n    solr.server.url : URL of the Solr instance (mandatory)\n    solr.zookeeper.url : URL of the Zookeeper URL (mandatory if 'cloud' value for solr.server.type)\n    solr.loadbalance.urls : Comma-separated string of Solr server strings to be used (madatory if 'lb' value for solr.server.type)\n    solr.mapping.file : name of the mapping file for fields (default solrindex-mapping.xml)\n    solr.commit.size : buffer size when sending to Solr (default 1000)\n    solr.auth : use authentication (default false)\n    solr.auth.username : username for authentication\n    solr.auth.password : password for authentication\n\n\n2016-01-28 02:49:41,422 INFO  indexer.IndexerMapReduce - IndexerMapReduce: crawldb: nutchweb/crawldb\n2016-01-28 02:49:41,425 INFO  indexer.IndexerMapReduce - IndexerMapReduce: linkdb: nutchweb/linkdb\n2016-01-28 02:49:41,425 INFO  indexer.IndexerMapReduce - IndexerMapReduces: adding segment: nutchweb/segments/20160127234706\n2016-01-28 02:49:41,652 WARN  util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n2016-01-28 02:49:42,586 WARN  conf.Configuration - file:/tmp/hadoop-micky/mapred/staging/micky810285982/.staging/job_local810285982_0001/job.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.\n2016-01-28 02:49:42,587 WARN  conf.Configuration - file:/tmp/hadoop-micky/mapred/staging/micky810285982/.staging/job_local810285982_0001/job.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.\n2016-01-28 02:49:42,751 WARN  conf.Configuration - file:/tmp/hadoop-micky/mapred/local/localRunner/micky/job_local810285982_0001/job_local810285982_0001.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.\n2016-01-28 02:49:42,752 WARN  conf.Configuration - file:/tmp/hadoop-micky/mapred/local/localRunner/micky/job_local810285982_0001/job_local810285982_0001.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.\n2016-01-28 02:49:43,342 INFO  anchor.AnchorIndexingFilter - Anchor deduplication is: off\n2016-01-28 02:49:49,230 INFO  indexer.IndexWriters - Adding org.apache.nutch.indexwriter.solr.SolrIndexWriter\n2016-01-28 02:49:50,627 INFO  solr.SolrMappingReader - source: content dest: content\n2016-01-28 02:49:50,627 INFO  solr.SolrMappingReader - source: title dest: title\n2016-01-28 02:49:50,627 INFO  solr.SolrMappingReader - source: host dest: host\n2016-01-28 02:49:50,627 INFO  solr.SolrMappingReader - source: segment dest: segment\n2016-01-28 02:49:50,627 INFO  solr.SolrMappingReader - source: boost dest: boost\n2016-01-28 02:49:50,627 INFO  solr.SolrMappingReader - source: digest dest: digest\n2016-01-28 02:49:50,627 INFO  solr.SolrMappingReader - source: tstamp dest: tstamp\n2016-01-28 02:49:50,959 INFO  solr.SolrIndexWriter - Indexing 250 documents\n2016-01-28 02:49:50,960 INFO  solr.SolrIndexWriter - Deleting 0 documents\n2016-01-28 02:49:54,346 INFO  solr.SolrIndexWriter - Indexing 250 documents\n2016-01-28 02:50:06,471 WARN  mapred.LocalJobRunner - job_local810285982_0001\njava.lang.Exception: org.apache.solr.client.solrj.impl.HttpSolrServer$RemoteSolrException: Exception writing document id http://nutch.apache.org/apidocs/apidocs-1.1/overview-tree.html to the index; possible analysis error.\n    at org.apache.hadoop.mapred.LocalJobRunner$Job.runTasks(LocalJobRunner.java:462)\n    at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:529)\nCaused by: org.apache.solr.client.solrj.impl.HttpSolrServer$RemoteSolrException: Exception writing document id http://nutch.apache.org/apidocs/apidocs-1.1/overview-tree.html to the index; possible analysis error.\n    at org.apache.solr.client.solrj.impl.HttpSolrServer.executeMethod(HttpSolrServer.java:552)\n    at org.apache.solr.client.solrj.impl.HttpSolrServer.request(HttpSolrServer.java:210)\n    at org.apache.solr.client.solrj.impl.HttpSolrServer.request(HttpSolrServer.java:206)\n    at org.apache.solr.client.solrj.request.AbstractUpdateRequest.process(AbstractUpdateRequest.java:124)\n    at org.apache.nutch.indexwriter.solr.SolrIndexWriter.write(SolrIndexWriter.java:134)\n    at org.apache.nutch.indexer.IndexWriters.write(IndexWriters.java:85)\n    at org.apache.nutch.indexer.IndexerOutputFormat$1.write(IndexerOutputFormat.java:50)\n    at org.apache.nutch.indexer.IndexerOutputFormat$1.write(IndexerOutputFormat.java:41)\n    at org.apache.hadoop.mapred.ReduceTask$OldTrackingRecordWriter.write(ReduceTask.java:493)\n    at org.apache.hadoop.mapred.ReduceTask$3.collect(ReduceTask.java:422)\n    at org.apache.nutch.indexer.IndexerMapReduce.reduce(IndexerMapReduce.java:356)\n    at org.apache.nutch.indexer.IndexerMapReduce.reduce(IndexerMapReduce.java:56)\n    at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:444)\n    at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)\n    at org.apache.hadoop.mapred.LocalJobRunner$Job$ReduceTaskRunnable.run(LocalJobRunner.java:319)\n    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)\n    at java.util.concurrent.FutureTask.run(FutureTask.java:262)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n    at java.lang.Thread.run(Thread.java:745)\n2016-01-28 02:50:07,330 ERROR indexer.IndexingJob - Indexer: java.io.IOException: Job failed!\n    at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:836)\n    at org.apache.nutch.indexer.IndexingJob.index(IndexingJob.java:145)\n    at org.apache.nutch.indexer.IndexingJob.run(IndexingJob.java:222)\n    at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)\n    at org.apache.nutch.indexer.IndexingJob.main(IndexingJob.java:231)\n</code></pre>\n\n<p>I found out that the error is </p>\n\n<blockquote>\n  <p>Exception writing document id\n  <a href=\"http://nutch.apache.org/apidocs/apidocs-1.1/overview-tree.html\" rel=\"nofollow\">http://nutch.apache.org/apidocs/apidocs-1.1/overview-tree.html</a> to the\n  index; possible analysis error.</p>\n</blockquote>\n\n<p>And it seems that no one encounter this error before, please help.</p>\n", "creation_date": 1453985493, "score": 2},
{"title": "Ant failed when I run ant runtime in $NUTCH_HOME", "view_count": 72, "owner": {"user_id": 4983473, "answer_count": 0, "creation_date": 1433685399, "accept_rate": 33, "view_count": 6, "reputation": 16}, "is_answered": true, "answers": [{"question_id": 35007970, "owner": {"user_id": 1784848, "accept_rate": 57, "link": "http://stackoverflow.com/users/1784848/code-wrangler", "user_type": "registered", "reputation": 52}, "body": "<p>Seems an file write permission issue, either try running command with sudo Or give the folder write permission. hth.</p>\n", "creation_date": 1453975547, "is_accepted": true, "score": 0, "last_activity_date": 1453975547, "answer_id": 35058367}], "question_id": 35007970, "tags": ["java", "ant", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/35007970/ant-failed-when-i-run-ant-runtime-in-nutch-home", "last_activity_date": 1453975547, "accepted_answer_id": 35058367, "body": "<p>Ant failed when I run ant runtime in <strong>$NUTCH_HOME</strong>, error information:</p>\n\n<p><strong><em>BUILD FAILED</em></strong>    </p>\n\n<blockquote>\n  <p>/home/gannyee/nutch/build.xml:69: Directory /home/gannyee/nutch/build\n  creation was not successful for an unknown reason</p>\n</blockquote>\n", "creation_date": 1453788846, "score": 0},
{"title": "Crawl PDF documents using nutch", "view_count": 2506, "is_answered": false, "answers": [{"question_id": 18054889, "owner": {"user_id": 2695269, "link": "http://stackoverflow.com/users/2695269/mgs", "user_type": "registered", "reputation": 709}, "body": "<p>Use Nutch's <a href=\"http://wiki.apache.org/nutch/TikaPlugin\" rel=\"nofollow\">parse-tika</a> plugin. Plain text, XML, OpenDocument (OpenOffice.org), Microsoft Office (Word, Excel, Powerpoint), PDF, RTF, MP3 (ID3 tags) are all parsed by the Tika plugin</p>\n", "creation_date": 1377067786, "is_accepted": false, "score": 0, "last_activity_date": 1377067786, "answer_id": 18350596}, {"question_id": 18054889, "owner": {"user_id": 3568831, "accept_rate": 71, "link": "http://stackoverflow.com/users/3568831/nwawel-a-iroume", "user_type": "registered", "reputation": 137}, "body": "<ol>\n<li>Edit <strong>regex-urlfilter.txt</strong> and remove any occurence of \"pdf\" </li>\n<li>Edit <strong>suffix-urlfilter.txt</strong> and remove any occurence of \"pdf\" </li>\n<li>Edit <strong>nutch-site.xml</strong>, add \"parse-tika\" and \"parse-html\" in the\nplugin.includes section. this should look like this</li>\n</ol>\n\n<p>this answer came from <a href=\"http://stackoverflow.com/questions/17442052/how-to-crawl-pdf-links-using-apache-nutch\">here</a> . I have tested it  when working on Nutch</p>\n\n<pre><code>&lt;property&gt;\n\n\n&lt;name&gt;plugin.includes&lt;/name&gt;\n  &lt;value&gt;protocol-http|urlfilter-regex|parse-(html|tika|text)|index-(basic|anchor)|scoring-opic|urlnormalizer-(pass|regex|basic)&lt;/value&gt;\n  &lt;description&gt;\n    ...\n  &lt;/description&gt;\n&lt;/property&gt;\n</code></pre>\n", "creation_date": 1419883283, "is_accepted": false, "score": 0, "last_activity_date": 1419883283, "answer_id": 27695217}, {"question_id": 18054889, "owner": {"user_id": 5850189, "link": "http://stackoverflow.com/users/5850189/mickyhelloworld", "user_type": "registered", "reputation": 11}, "body": "<p>I found that even you used the tika plugin, it still can't crawl the pdf or any ms office file into the crawldb. you need to <strong>add the url</strong> that you want to crawl <strong>in the</strong> <strong>white-list in nutch-site.xml</strong> in order to get the pdf and any ms office file:</p>\n\n<pre><code>&lt;property&gt;\n  &lt;name&gt;http.robot.rules.whitelist&lt;/name&gt;\n  &lt;value&gt;xxx.xxx.xxx.xxx&lt;/value&gt;\n  &lt;description&gt;Comma separated list of hostnames or IP addresses to ignore \n  robot rules parsing for. Use with care and only if you are explicitly\n  allowed by the site owner to ignore the site's robots.txt!\n  &lt;/description&gt;\n&lt;/property&gt;\n</code></pre>\n", "creation_date": 1453956694, "is_accepted": false, "score": 0, "last_activity_date": 1453956694, "answer_id": 35053108}], "question_id": 18054889, "tags": ["pdf", "nutch"], "answer_count": 3, "link": "http://stackoverflow.com/questions/18054889/crawl-pdf-documents-using-nutch", "last_activity_date": 1453956694, "owner": {"user_id": 2478392, "answer_count": 1, "creation_date": 1371041548, "accept_rate": 17, "view_count": 22, "reputation": 54}, "body": "<p>I have to crawl PDF documents too from given URL...\nsuggest any tool/API to crawl PDF docs also...\nnow I am using nutch to crawl but I couldn't crawl PDF from given URL...should I use any plugin to crawl PDF in nutch?</p>\n\n<p>seed.txt --> <a href=\"http://nutch.apache.org\" rel=\"nofollow\">http://nutch.apache.org</a>\nregex-urlfilter.txt--->+^http://([a-z0-9]*.)*nutch.apache.org/</p>\n\n<p>Thanks in Advance</p>\n", "creation_date": 1375695344, "score": 3},
{"title": "Crawl/scrap websites/webpages containing a specific text, with no prior information about any such websites/webpages", "view_count": 183, "is_answered": true, "answers": [{"last_edit_date": 1453301021, "owner": {"user_type": "does_not_exist"}, "body": "<p>You can use the Google search API (<a href=\"https://developers.google.com/custom-search/json-api/v1/overview?csw=1\" rel=\"nofollow\">https://developers.google.com/custom-search/json-api/v1/overview?csw=1</a>) for 100 free queries/day. The search results will be in JSON format, which you can use to feed the links to your scraper.</p>\n", "question_id": 34902713, "creation_date": 1453300307, "is_accepted": false, "score": 0, "last_activity_date": 1453301021, "answer_id": 34902815}, {"question_id": 34902713, "owner": {"user_id": 5408918, "accept_rate": 40, "link": "http://stackoverflow.com/users/5408918/muhammad-zeeshan", "user_type": "registered", "reputation": 310}, "body": "<p>Well you can use requests module to get data.</p>\n\n<p>Here in below example I am getting data from all sites having \"pizza\" word in those.</p>\n\n<pre><code>import requests\nurl = 'http://www.google.com/search'\nmy_headers = { 'User-agent' : 'Mozilla/11.0' }\npayload = { 'q' : 'pizza', 'start' : '0' }\nr = requests.get( url, params = payload, headers = my_headers )\n</code></pre>\n\n<p>You can use BeautifulSoup library to extract any type of information from retrieved data (HTML data)</p>\n\n<pre><code>from bs4 import BeautifulSoup\nsoup = BeautifulSoup( r.text, 'html.parser' )\n</code></pre>\n\n<p>Now if you want text data you can use this function</p>\n\n<pre><code>soup.getText()\n</code></pre>\n", "creation_date": 1453366141, "is_accepted": false, "score": 0, "last_activity_date": 1453366141, "answer_id": 34919063}, {"question_id": 34902713, "owner": {"user_id": 432844, "link": "http://stackoverflow.com/users/432844/julien-nioche", "user_type": "registered", "reputation": 939}, "body": "<p>You could parse <a href=\"https://commoncrawl.org/\" rel=\"nofollow\">the commoncrawl dataset</a>. It contains billions of webpages. Their site contains examples on how to do it with MapReduce.\nOther than that any web crawler needs to have some starting point.</p>\n", "creation_date": 1453929127, "is_accepted": false, "score": 1, "last_activity_date": 1453929127, "answer_id": 35048100}], "question_id": 34902713, "tags": ["python", "web-scraping", "scrapy", "screen-scraping", "nutch"], "answer_count": 3, "link": "http://stackoverflow.com/questions/34902713/crawl-scrap-websites-webpages-containing-a-specific-text-with-no-prior-informat", "last_activity_date": 1453929127, "owner": {"user_id": 4728385, "answer_count": 27, "creation_date": 1427695136, "accept_rate": 67, "view_count": 52, "reputation": 682}, "body": "<p>I used nutch and scrapy.  They need seed URLs to crawl.  That means, one should be already aware of the websites/webpages which would contain the text that is being searched for.</p>\n\n<p>My case is different, I do not have the prior information about the websites/webpages which contain the text I am searching for.  So I won't be able to use seed URLs to be crawled by tools such as nutch and scrapy.</p>\n\n<p>Is there a way to crawl websites/webpages for a given text, without knowing any websites/webpages that would possibly contain that text?</p>\n", "creation_date": 1453300048, "score": 0},
{"title": "Nutch processing on the content details of html for solr", "view_count": 26, "is_answered": false, "question_id": 35033968, "tags": ["solr", "web-crawler", "nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/35033968/nutch-processing-on-the-content-details-of-html-for-solr", "last_activity_date": 1453891654, "owner": {"user_id": 3732294, "answer_count": 0, "creation_date": 1402537534, "accept_rate": 0, "view_count": 10, "reputation": 16}, "body": "<p>I was not clear  how nutch parses the content of a page based on <code>solrindex-mapping.xml</code> and push to solr intance. And <code>id</code> is taken as url in which configuration of nutch is this mentioned</p>\n\n<p>I see the below mapping in <code>solrindex-mapping.xml</code> what does nutch do.say for content will it go to the html and read all the body related div and then internally have a mechanism to get the required content.please provide input.</p>\n\n<p><strong>solrindex-mapping.xml configuration</strong></p>\n\n<pre><code> &lt;field dest=\"content\" source=\"content\"/&gt;\n                &lt;field dest=\"title\" source=\"title\"/&gt;\n                &lt;field dest=\"host\" source=\"host\"/&gt;\n                &lt;field dest=\"segment\" source=\"segment\"/&gt;\n                &lt;field dest=\"boost\" source=\"boost\"/&gt;\n                &lt;field dest=\"digest\" source=\"digest\"/&gt;\n                &lt;field dest=\"tstamp\" source=\"tstamp\"/&gt;\n</code></pre>\n", "creation_date": 1453888465, "score": 0},
{"title": "Syntax for specifying regular expression for the index.replace.regexp plugin of Nutch?", "view_count": 33, "is_answered": true, "answers": [{"question_id": 35005575, "owner": {"user_id": 5831580, "link": "http://stackoverflow.com/users/5831580/ondrej-k", "user_type": "registered", "reputation": 11}, "body": "<p>The regex worked correctly, but the problem was that the second regex overwrote the effect of the first regex. The following gave the desired effect (note that the regex is applied only when the urlmatch is evaluated as true):</p>\n\n<pre><code>&lt;property&gt;\n  &lt;name&gt;index.replace.regexp&lt;/name&gt;\n  &lt;value&gt;\n    urlmatch=.*wiki.example.com.*\n    url:content_type=/.*wiki.example.com.*/wiki/\n    urlmatch=.*www.example.com.*\n    url:content_type=/.*www.example.com.*/website/\n  &lt;/value&gt;\n&lt;/property&gt;\n</code></pre>\n", "creation_date": 1453840529, "is_accepted": false, "score": 1, "last_activity_date": 1453840529, "answer_id": 35023439}], "question_id": 35005575, "tags": ["regex", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/35005575/syntax-for-specifying-regular-expression-for-the-index-replace-regexp-plugin-of", "last_activity_date": 1453840529, "owner": {"user_id": 5831580, "view_count": 0, "answer_count": 2, "creation_date": 1453590404, "reputation": 11}, "body": "<p>I am following the documentation for the Nutch IndexReplace plugin posted at <a href=\"https://wiki.apache.org/nutch/IndexReplace\" rel=\"nofollow\">https://wiki.apache.org/nutch/IndexReplace</a> and trying to setup regular expression that will create additional field storing information about the content type that will derived from the url.</p>\n\n<p>Below is the property that has been added to my conf/nutch-site.xml file:</p>\n\n<pre><code>&lt;property&gt;\n  &lt;name&gt;index.replace.regexp&lt;/name&gt;\n  &lt;value&gt;\n    url:content_type=/.*wiki.example.com.*/wiki/\n    url:content_type=/.*www.example.com.*/website/\n  &lt;/value&gt;\n&lt;/property&gt;\n</code></pre>\n\n<p>The goal is to create and populate the additional field content_type by either wiki or website, depending on which url the page has been fetched from. Both fields url and content_type get populated in my solr instance, but they both contain the full url, such as</p>\n\n<pre><code>sample fetched url: https://wiki.example.com/home\nvalue of Solr field url: https://wiki.example.com/home\nvalue of Solr field content_type: https://wiki.example.com/home\n</code></pre>\n\n<p>So it appears that the regular expression does not evaluate as expected in Nutch, although it evaluates as expected in the online regex tester at <a href=\"http://www.ocpsoft.org/tutorials/regular-expressions/java-visual-regex-tester/\" rel=\"nofollow\">http://www.ocpsoft.org/tutorials/regular-expressions/java-visual-regex-tester/</a> .</p>\n\n<p>Could you please clarify what is the correct regex syntax, such that for sample input url stated above, the fields are evaluated as follows?</p>\n\n<pre><code>url: http://wiki.example.com/home\ncontent_type: wiki\n</code></pre>\n", "creation_date": 1453771542, "score": 0},
{"title": "nutch configuration for multiple solr cores", "view_count": 78, "is_answered": false, "answers": [{"question_id": 34938855, "owner": {"user_id": 1699191, "link": "http://stackoverflow.com/users/1699191/ameertawfik", "user_type": "registered", "reputation": 618}, "body": "<p>You need to create two instances of nutch to push to different solr cores (or servers for that matter). The issue is that you want to use different <strong>regex-urlfilter.txt</strong> files. Technically, you can do that using a single nutch instance, but it requires you to write a lot of code. Therefore, the shortest and easiest way is to set up two nutch instances.</p>\n\n<p>With regards to the crawl command arguments,</p>\n\n<pre><code>./bin/crawl conf/core0urls crawl http://solrhost:8085/solr/core0 1\n</code></pre>\n\n<p>The <strong>1</strong> value here means crawl urls in <strong>core0urls</strong> only. Do not go to the second depth and crawl urls generated from the first crawl. Basically, you are lunching one crawl.</p>\n", "creation_date": 1453599139, "is_accepted": false, "score": 0, "last_activity_date": 1453599139, "answer_id": 34971214}], "question_id": 34938855, "tags": ["solr", "web-crawler", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/34938855/nutch-configuration-for-multiple-solr-cores", "last_activity_date": 1453599139, "owner": {"user_id": 3732294, "answer_count": 0, "creation_date": 1402537534, "accept_rate": 0, "view_count": 10, "reputation": 16}, "body": "<p>I am using Nutch 1.9 and Solr 4.10 on Linux. I need to crawl and index the content of a big website and would like to do this using separate Solr cores. The below is Nutch configuration as part of a cronjob...</p>\n\n<pre><code>./bin/crawl conf/core0urls crawl http://solrhost:8085/solr/core0 1\n./bin/crawl conf/core1urls crawl http://solrhost:8085/solr/core1 2\n</code></pre>\n\n<p>I have a cronjob set up for one minute for the above. Could someone help me on the below...</p>\n\n<ol>\n<li>I want to know for <code>core0</code> what does <code>1</code> mean and for <code>core1</code> what does <code>2</code> mean?</li>\n<li>I have created separate <code>seed.txt</code> files using <code>conf/core0urls</code> and <code>conf/core1urls</code>. That works, but i want to have separate <code>regex-urlfilter.txt</code>  files for each core and Nutch should detect it. Please let me know how I can achieve it.</li>\n<li>If the above is not possible in single Nutch, should I set up separate Nutch instances for each Solr core?</li>\n</ol>\n", "creation_date": 1453435793, "score": 0},
{"title": "Can I crawl with Nutch, store in Cassandra, index using Solr?", "view_count": 561, "owner": {"age": 22, "answer_count": 0, "creation_date": 1387817585, "user_id": 3130223, "view_count": 12, "location": "Chennai, India", "reputation": 12}, "is_answered": true, "answers": [{"question_id": 20869191, "owner": {"user_id": 582789, "link": "http://stackoverflow.com/users/582789/alfonso-nishikawa", "user_type": "registered", "reputation": 1263}, "body": "<p>I think you can, but I am not a Cassandra user, so never tried.</p>\n\n<p>You will have to configure gora.properties (<a href=\"http://svn.apache.org/repos/asf/nutch/tags/release-2.2.1/conf/gora.properties\" rel=\"nofollow\">http://svn.apache.org/repos/asf/nutch/tags/release-2.2.1/conf/gora.properties</a>) to enable Cassandra. In Nutch 2 Tutorial (<a href=\"http://wiki.apache.org/nutch/Nutch2Tutorial\" rel=\"nofollow\">http://wiki.apache.org/nutch/Nutch2Tutorial</a>) the do that for HBase.</p>\n\n<p>To know where is the data mapped in Cassandra you will need to take a look at the mappings at <a href=\"http://svn.apache.org/repos/asf/nutch/tags/release-2.2.1/conf/gora-cassandra-mapping.xml\" rel=\"nofollow\">http://svn.apache.org/repos/asf/nutch/tags/release-2.2.1/conf/gora-cassandra-mapping.xml</a></p>\n\n<p>Nutch will store the data in Cassandra. About Solr I don't know (I never used Solr).</p>\n", "creation_date": 1388677467, "is_accepted": true, "score": 0, "last_activity_date": 1388677467, "answer_id": 20886439}, {"question_id": 20869191, "owner": {"user_id": 1549741, "accept_rate": 82, "link": "http://stackoverflow.com/users/1549741/kee", "user_type": "registered", "reputation": 2000}, "body": "<p>If you use Datastax's Cassandra, indexing Cassandra table(s) into Solr is much easier. Here is a link at <a href=\"http://www.datastax.com/what-we-offer/products-services/datastax-enterprise/apache-solr\" rel=\"nofollow\">http://www.datastax.com/what-we-offer/products-services/datastax-enterprise/apache-solr</a></p>\n", "creation_date": 1388692807, "is_accepted": false, "score": 1, "last_activity_date": 1388692807, "answer_id": 20890823}, {"question_id": 20869191, "owner": {"user_id": 3420475, "link": "http://stackoverflow.com/users/3420475/kanishka-vatsa", "user_type": "registered", "reputation": 47}, "body": "<p>Programmatically its possible .... you can get the result from solr indexes ... keep the unique id in both cassandra and Solr ... fetch that id from solr and fetch the entire result from cassandra ..... </p>\n", "creation_date": 1453440536, "is_accepted": false, "score": 0, "last_activity_date": 1453440536, "answer_id": 34939625}], "question_id": 20869191, "tags": ["solr", "cassandra", "nutch"], "answer_count": 3, "link": "http://stackoverflow.com/questions/20869191/can-i-crawl-with-nutch-store-in-cassandra-index-using-solr", "last_activity_date": 1453440536, "accepted_answer_id": 20886439, "body": "<p>I'm developing a keyword analytics app. I wish to crawl the web using Nutch, index the output using Solr and finally store the data in Cassandra.</p>\n\n<p>I should later be able to do search queries and analytics on Solr and it must fetch the relevant data from Cassandra.</p>\n\n<p>Is this setup possible? If yes, is there anything that I should keep in mind?</p>\n", "creation_date": 1388583575, "score": 0},
{"title": "Nutch and Solr: Sites with timeouts", "view_count": 19, "is_answered": false, "question_id": 34928437, "tags": ["apache", "solr", "nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/34928437/nutch-and-solr-sites-with-timeouts", "last_activity_date": 1453391980, "owner": {"age": 27, "answer_count": 1, "creation_date": 1373145571, "user_id": 2557061, "accept_rate": 57, "view_count": 11, "reputation": 56}, "body": "<p>If I try to crawl several sites which a depth of 3 or more it happens that after the nutch processes in nutch the connection to solr fails. There is an exception that states that a fetched site has a timeout and so the Solr is throwing a Bad Request.</p>\n\n<p>I am very confused about the error message because they appear randomly. Here is the log:</p>\n\n<pre><code>2016-01-21 16:20:42,012 DEBUG wire.content - &gt;&gt; \"[\\r][\\n]\"\n2016-01-21 16:20:42,012 DEBUG wire.content - &gt;&gt; \"0\"\n2016-01-21 16:20:42,012 DEBUG wire.content - &gt;&gt; \"[\\r][\\n]\"\n2016-01-21 16:20:42,012 DEBUG wire.content - &gt;&gt; \"[\\r][\\n]\"\n2016-01-21 16:20:42,012 DEBUG methods.EntityEnclosingMethod - Request body sent\n2016-01-21 16:20:42,095 DEBUG wire.header - &lt;&lt; \"HTTP/1.1 400 Bad Request[\\r][\\n]\"\n2016-01-21 16:20:42,095 DEBUG wire.header - &lt;&lt; \"HTTP/1.1 400 Bad Request[\\r][\\n]\"\n2016-01-21 16:20:42,095 DEBUG wire.header - &lt;&lt; \"Content-Type: application/octet-stream[\\r][\\n]\"\n2016-01-21 16:20:42,096 DEBUG wire.header - &lt;&lt; \"Content-Length: 227[\\r][\\n]\"\n2016-01-21 16:20:42,096 DEBUG wire.header - &lt;&lt; \"[\\r][\\n]\"\n2016-01-21 16:20:42,096 DEBUG wire.content - &lt;&lt; \"[0x2][0xa2][0xe0].responseHeader[0xa2][0xe0]&amp;statusP[0x19][0xe0]%QTimeU[0xc][0xe0]%error[0xa2][0xe0]#msg?[0x86][0x1]ERROR: [doc=http://auslagerungsmagazin.ub.rub.de/formular/] multiple values encountered for non multiValued field metatag.keywords: [, University of Bochum, Library][0xe0]$codeP[0x19]\"\n2016-01-21 16:20:42,096 DEBUG httpclient.HttpMethodBase - Resorting to protocol version default close connection policy\n2016-01-21 16:20:42,096 DEBUG httpclient.HttpMethodBase - Should NOT close connection, using HTTP/1.1\n2016-01-21 16:20:42,096 DEBUG httpclient.HttpConnection - Releasing connection back to connection manager.\n2016-01-21 16:20:42,103 WARN  mapred.LocalJobRunner - job_local892720012_0001\norg.apache.solr.common.SolrException: Bad Request\n\nBad Request\n\nrequest: http://localhost:8983/solr/basic/update?wt=javabin&amp;version=2\n    at org.apache.solr.client.solrj.impl.CommonsHttpSolrServer.request(CommonsHttpSolrServer.java:430)\n    at org.apache.solr.client.solrj.impl.CommonsHttpSolrServer.request(CommonsHttpSolrServer.java:244)\n    at org.apache.solr.client.solrj.request.AbstractUpdateRequest.process(AbstractUpdateRequest.java:105)\n    at org.apache.nutch.indexwriter.solr.SolrIndexWriter.write(SolrIndexWriter.java:134)\n    at org.apache.nutch.indexer.IndexWriters.write(IndexWriters.java:85)\n    at org.apache.nutch.indexer.IndexerOutputFormat$1.write(IndexerOutputFormat.java:50)\n    at org.apache.nutch.indexer.IndexerOutputFormat$1.write(IndexerOutputFormat.java:41)\n    at org.apache.hadoop.mapred.ReduceTask$OldTrackingRecordWriter.write(ReduceTask.java:458)\n    at org.apache.hadoop.mapred.ReduceTask$3.collect(ReduceTask.java:500)\n    at org.apache.nutch.indexer.IndexerMapReduce.reduce(IndexerMapReduce.java:337)\n    at org.apache.nutch.indexer.IndexerMapReduce.reduce(IndexerMapReduce.java:53)\n    at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:522)\n    at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:421)\n    at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:398)\n2016-01-21 16:20:42,974 ERROR indexer.IndexingJob - Indexer: java.io.IOException: Job failed!\n    at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:1357)\n    at org.apache.nutch.indexer.IndexingJob.index(IndexingJob.java:113)\n    at org.apache.nutch.indexer.IndexingJob.run(IndexingJob.java:177)\n    at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n    at org.apache.nutch.indexer.IndexingJob.main(IndexingJob.java:187)\n</code></pre>\n\n<p>If I retry the fetching process several times then this error will not occur anymore. Is it possible that my calls where blocked from servers or that my hardward is overloaded by crawling?</p>\n\n<p>Here is the call from the terminal:</p>\n\n<pre><code>Indexer: starting at 2016-01-21 16:33:17\nIndexer: deleting gone documents: false\nIndexer: URL filtering: false\nIndexer: URL normalizing: false\nActive IndexWriters :\nSOLRIndexWriter\n    solr.server.url : URL of the SOLR instance (mandatory)\n    solr.commit.size : buffer size when sending to SOLR (default 1000)\n    solr.mapping.file : name of the mapping file for fields (default solrindex-mapping.xml)\n    solr.auth : use authentication (default false)\n    solr.auth.username : username for authentication\n    solr.auth.password : password for authentication\n\n\nIndexer: java.io.IOException: Job failed!\nat org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:1357)\nat org.apache.nutch.indexer.IndexingJob.index(IndexingJob.java:113)\nat org.apache.nutch.indexer.IndexingJob.run(IndexingJob.java:177)\nat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\nat  org.apache.nutch.indexer.IndexingJob.main(IndexingJob.java:187)\n</code></pre>\n", "creation_date": 1453391980, "score": 0},
{"title": "How to write regular expression in nutch?", "view_count": 63, "owner": {"user_id": 5248320, "view_count": 0, "answer_count": 1, "creation_date": 1440085772, "reputation": 23}, "is_answered": true, "answers": [{"question_id": 34851699, "owner": {"user_id": 4453262, "accept_rate": 86, "link": "http://stackoverflow.com/users/4453262/karthik-manchala", "user_type": "registered", "reputation": 10437}, "body": "<p>You ca use the following:</p>\n\n<pre><code>+^https://www\\.practo\\.com.*cardiologist\n</code></pre>\n", "creation_date": 1453110927, "is_accepted": false, "score": 0, "last_activity_date": 1453110927, "answer_id": 34851767}, {"question_id": 34851699, "owner": {"user_id": 5248320, "link": "http://stackoverflow.com/users/5248320/uday-kumar-singh", "user_type": "registered", "reputation": 23}, "body": "<p>I got answer to my question. Problem was getting the correct regular expression.</p>\n\n<p>+^(https|http)://([a-zA-Z0-9./-]+)cardiologist([a-zA-Z0-9-#?=])*</p>\n\n<p>The following site help me a lot to get to the correct expression : <a href=\"https://regex101.com/\" rel=\"nofollow\">https://regex101.com/</a></p>\n", "creation_date": 1453374046, "is_accepted": true, "score": 1, "last_activity_date": 1453374046, "answer_id": 34921944}], "question_id": 34851699, "tags": ["regex", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/34851699/how-to-write-regular-expression-in-nutch", "last_activity_date": 1453374046, "accepted_answer_id": 34921944, "body": "<p>I am using Nutch for crawling web pages. I am getting problem in writing the regular expression.</p>\n\n<p>It is working fine with the following configuration:\nSeed url :\nwww.practo.com \n(preceded with https:// )</p>\n\n<p>Regex-urlfilter.txt: \n+^<a href=\"https://www.practo.com/\" rel=\"nofollow\">https://www.practo.com/</a></p>\n\n<p>But I want to fetch only specific pages such as pages that contain information about 'cardiologist'\nExample: I want to fetch pages like:\nwww.practo.com/hyderabad/doctor/some-name-cardiologist\ni.e. I want to fetch pages ending in certain keyword.</p>\n\n<p>I am using following regular expression:\n+^<a href=\"https://www.practo.com(/[a-z0-9]\" rel=\"nofollow\">https://www.practo.com(/[a-z0-9]</a>*)*cardiologist</p>\n\n<p>Please help me out in writing the regular expression.</p>\n", "creation_date": 1453110716, "score": 1},
{"title": "nuch with solr authentication issue", "view_count": 17, "is_answered": false, "question_id": 34916797, "tags": ["solr", "lucene", "web-crawler", "bigdata", "nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/34916797/nuch-with-solr-authentication-issue", "last_activity_date": 1453357711, "owner": {"user_id": 5315596, "answer_count": 3, "creation_date": 1441777630, "view_count": 4, "location": "Delhi, India", "reputation": 21}, "body": "<p>I have this issue, I did try with many option and even asked several places but out of luck.  I want to create my own search option on my website. For that I have crawled website using nutch and pushed the data to solr. That was ok for one time. Now solr is password protected and my website is pretty dynamic interms of content so I run the nutch periodically and then I have to push the data to solr. Since solr is password protected so I used the nutch configuration to use that authentication no matter how many times I try with different configuration but I get always same error and even log is not giving me any hint. </p>\n\n<p>I configured nutch-default.xml and changed 3 things</p>\n\n<blockquote>\n  <p>solr.auth = true ,  solr.auth.username = abc  , solr.auth.password =\n  etcccc</p>\n</blockquote>\n\n<p>for sure i used the actual username and password but i never get success. Please point me out where I did the mistake. </p>\n\n<p>thanks in advance. </p>\n\n<p>Also my solr is 5.4 and nutch 1.11</p>\n", "creation_date": 1453357711, "score": 0},
{"title": "how can I configure mongoDB for nutch?", "view_count": 173, "owner": {"user_id": 4983473, "answer_count": 0, "creation_date": 1433685399, "accept_rate": 33, "view_count": 6, "reputation": 16}, "is_answered": true, "answers": [{"question_id": 34855443, "owner": {"user_id": 1699191, "link": "http://stackoverflow.com/users/1699191/ameertawfik", "user_type": "registered", "reputation": 618}, "body": "<p>Nutch 2.x support for MongoDB is not for storing extracted and structured result but to store nutch's internal database in MongoDB.</p>\n\n<p>Currently, nutch supports pushing data to Apache Solr, Elasticsearch and Amazon Cloud service. If you want to push the data to MongoDB, then you need to create a new indexer plugin. Look at <strong>indexer-elastic</strong> or <strong>indexer-solr</strong> to understand how to write a new indexer plugin.</p>\n", "creation_date": 1453189204, "is_accepted": true, "score": 0, "last_activity_date": 1453189204, "answer_id": 34871047}], "question_id": 34855443, "tags": ["mongodb", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/34855443/how-can-i-configure-mongodb-for-nutch", "last_activity_date": 1453189204, "accepted_answer_id": 34871047, "body": "<p>Recently I try to finish a web-spider, I use nutch-1.10, I want to load data into mongoDB, which data gotten by nutch/crawl, I don't how to configure mongoDB for nutch, I can't find relative materials. I know that from the some blogs that nutch2.x is must while 1.x can not achieve my purpose! But the details for configuring still unclear to me! Can someone clear that!Thank you!</p>\n", "creation_date": 1453122112, "score": 0},
{"title": "Apache Nutch Indexing using elasticsearch", "view_count": 510, "owner": {"age": 21, "answer_count": 0, "creation_date": 1351572204, "user_id": 1784574, "accept_rate": 91, "view_count": 8, "location": "Los Ba&#241;os, Philippines", "reputation": 41}, "is_answered": true, "answers": [{"last_edit_date": 1453187975, "owner": {"user_id": 1699191, "link": "http://stackoverflow.com/users/1699191/ameertawfik", "user_type": "registered", "reputation": 618}, "body": "<p>Make sure you are running the same versions in nutch elastic dependency and your local server. </p>\n\n<p>If they are not the same, then do not waste your time, and use the http protocol to push directly to elastic from nutch instead of the Java api.</p>\n", "question_id": 34806481, "creation_date": 1452926000, "is_accepted": true, "score": 0, "last_activity_date": 1453187975, "answer_id": 34824266}], "question_id": 34806481, "tags": ["apache", "indexing", "elasticsearch", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/34806481/apache-nutch-indexing-using-elasticsearch", "last_activity_date": 1453187975, "accepted_answer_id": 34824266, "body": "<p>I am currently making a search engine using the Apache Nutch and ElasticSearch stack. I am using Apache Nutch 2.1 and ElasticSearch 1.7.3.</p>\n\n<p>I am currently trying to index directly from Nutch by following the instructions here: <a href=\"https://www.mind-it.info/2013/09/26/integrating-nutch-1-7-elasticsearch/\" rel=\"nofollow\">https://www.mind-it.info/2013/09/26/integrating-nutch-1-7-elasticsearch/</a>. Both Nutch and Elasticsearch runs on my localhost, with cluster name \"elasticsearch\".</p>\n\n<p>These are some of the parts of nutch-site.xml that I changed:</p>\n\n<pre><code>&lt;property&gt;\n    &lt;name&gt;plugin.includes&lt;/name&gt;\n    &lt;value&gt;protocol-selenium|protocol-httpclient|urlfilter-regex|parse-(html|tika|metatags)|index-(basic|anchor|metadata)|indexer-elastic|scoring-opic|urlnormalizer-(pass|regex|basic)&lt;/value&gt;\n    &lt;description&gt;Regular expression naming plugin directory names to\n    include.  Any plugin not matching this expression is excluded.\n    In any case you need at least include the nutch-extensionpoints plugin. By\n    default Nutch includes crawling just HTML and plain text via HTTP,\n    and basic indexing and search plugins. In order to use HTTPS please enable\n    protocol-httpclient, but be aware of possible intermittent problems with the\n    underlying commons-httpclient library.\n    &lt;/description&gt;\n&lt;/property&gt;\n</code></pre>\n\n<p>After running the command <strong>ant runtime</strong>, I tried issuing the command</p>\n\n<pre><code>bin/nutch elasticindex elasticsearch -all\n</code></pre>\n\n<p>But it returned this:</p>\n\n<pre><code>Exception in thread \"main\" java.lang.RuntimeException: job failed: name=elastic-index [elasticsearch], jobid=job_local_0001\nat org.apache.nutch.util.NutchJob.waitForCompletion(NutchJob.java:54)\nat org.apache.nutch.indexer.elastic.ElasticIndexerJob.run(ElasticIndexerJob.java:52)\nat org.apache.nutch.indexer.elastic.ElasticIndexerJob.indexElastic(ElasticIndexerJob.java:60)\nat org.apache.nutch.indexer.elastic.ElasticIndexerJob.run(ElasticIndexerJob.java:73)\nat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\nat org.apache.nutch.indexer.elastic.ElasticIndexerJob.main(ElasticIndexerJob.java:78)\n</code></pre>\n\n<p>I'm not sure where I went wrong. Here is my hadoop.log:</p>\n\n<pre><code>    2016-01-15 15:46:24,106 INFO  elastic.ElasticIndexerJob - Starting\n2016-01-15 15:46:24,733 INFO  plugin.PluginRepository - Plugins: looking in: /home/gabrielgagno/apache-nutch-2.1/runtime/local/plugins\n2016-01-15 15:46:24,817 INFO  plugin.PluginRepository - Plugin Auto-activation mode: [true]\n2016-01-15 15:46:24,817 INFO  plugin.PluginRepository - Registered Plugins:\n2016-01-15 15:46:24,817 INFO  plugin.PluginRepository -     the nutch core extension points (nutch-extensionpoints)\n2016-01-15 15:46:24,817 INFO  plugin.PluginRepository -     Basic URL Normalizer (urlnormalizer-basic)\n2016-01-15 15:46:24,817 INFO  plugin.PluginRepository -     Basic Indexing Filter (index-basic)\n2016-01-15 15:46:24,817 INFO  plugin.PluginRepository -     Html Parse Plug-in (parse-html)\n2016-01-15 15:46:24,817 INFO  plugin.PluginRepository -     Http / Https Protocol Plug-in (protocol-httpclient)\n2016-01-15 15:46:24,817 INFO  plugin.PluginRepository -     HTTP Framework (lib-http)\n2016-01-15 15:46:24,817 INFO  plugin.PluginRepository -     Regex URL Filter (urlfilter-regex)\n2016-01-15 15:46:24,817 INFO  plugin.PluginRepository -     Pass-through URL Normalizer (urlnormalizer-pass)\n2016-01-15 15:46:24,817 INFO  plugin.PluginRepository -     Regex URL Normalizer (urlnormalizer-regex)\n2016-01-15 15:46:24,817 INFO  plugin.PluginRepository -     Tika Parser Plug-in (parse-tika)\n2016-01-15 15:46:24,817 INFO  plugin.PluginRepository -     OPIC Scoring Plug-in (scoring-opic)\n2016-01-15 15:46:24,817 INFO  plugin.PluginRepository -     CyberNeko HTML Parser (lib-nekohtml)\n2016-01-15 15:46:24,817 INFO  plugin.PluginRepository -     Anchor Indexing Filter (index-anchor)\n2016-01-15 15:46:24,817 INFO  plugin.PluginRepository -     Regex URL Filter Framework (lib-regex-filter)\n2016-01-15 15:46:24,818 INFO  plugin.PluginRepository - Registered Extension-Points:\n2016-01-15 15:46:24,818 INFO  plugin.PluginRepository -     Nutch URL Normalizer (org.apache.nutch.net.URLNormalizer)\n2016-01-15 15:46:24,818 INFO  plugin.PluginRepository -     Nutch Protocol (org.apache.nutch.protocol.Protocol)\n2016-01-15 15:46:24,818 INFO  plugin.PluginRepository -     Parse Filter (org.apache.nutch.parse.ParseFilter)\n2016-01-15 15:46:24,818 INFO  plugin.PluginRepository -     Nutch URL Filter (org.apache.nutch.net.URLFilter)\n2016-01-15 15:46:24,818 INFO  plugin.PluginRepository -     Nutch Indexing Filter (org.apache.nutch.indexer.IndexingFilter)\n2016-01-15 15:46:24,818 INFO  plugin.PluginRepository -     Nutch Content Parser (org.apache.nutch.parse.Parser)\n2016-01-15 15:46:24,818 INFO  plugin.PluginRepository -     Nutch Scoring (org.apache.nutch.scoring.ScoringFilter)\n2016-01-15 15:46:24,822 INFO  basic.BasicIndexingFilter - Maximum title length for indexing set to: 100\n2016-01-15 15:46:24,822 INFO  indexer.IndexingFilters - Adding org.apache.nutch.indexer.basic.BasicIndexingFilter\n2016-01-15 15:46:24,824 INFO  anchor.AnchorIndexingFilter - Anchor deduplication is: off\n2016-01-15 15:46:24,824 INFO  indexer.IndexingFilters - Adding org.apache.nutch.indexer.anchor.AnchorIndexingFilter\n2016-01-15 15:46:25,827 WARN  util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n2016-01-15 15:46:26,521 INFO  mapreduce.GoraRecordReader - gora.buffer.read.limit = 10000\n2016-01-15 15:46:26,727 INFO  elasticsearch.node - [Layla Miller] version[1.7.3], pid[18188], build[05d4530/2015-10-15T09:14:17Z]\n2016-01-15 15:46:26,727 INFO  elasticsearch.node - [Layla Miller] initializing ...\n2016-01-15 15:46:26,852 INFO  elasticsearch.plugins - [Layla Miller] loaded [], sites []\n2016-01-15 15:46:28,229 WARN  elasticsearch.bootstrap - JNA not found. native methods will be disabled.\n2016-01-15 15:46:28,756 INFO  elasticsearch.node - [Layla Miller] initialized\n2016-01-15 15:46:28,756 INFO  elasticsearch.node - [Layla Miller] starting ...\n2016-01-15 15:46:28,824 INFO  elasticsearch.transport - [Layla Miller] bound_address {inet[/0:0:0:0:0:0:0:0:9301]}, publish_address {inet[/172.16.3.72:9301]}\n2016-01-15 15:46:28,836 INFO  elasticsearch.discovery - [Layla Miller] elasticsearch/_tzxV-I7SSeduY9b8enpPw\n2016-01-15 15:46:58,836 WARN  elasticsearch.discovery - [Layla Miller] waited for 30s and no initial state was set by the discovery\n2016-01-15 15:46:58,845 INFO  elasticsearch.http - [Layla Miller] bound_address {inet[/0:0:0:0:0:0:0:0:9201]}, publish_address {inet[/172.16.3.72:9201]}\n2016-01-15 15:46:58,845 INFO  elasticsearch.node - [Layla Miller] started\n2016-01-15 15:46:58,848 INFO  basic.BasicIndexingFilter - Maximum title length for indexing set to: 100\n2016-01-15 15:46:58,848 INFO  indexer.IndexingFilters - Adding org.apache.nutch.indexer.basic.BasicIndexingFilter\n2016-01-15 15:46:58,848 INFO  anchor.AnchorIndexingFilter - Anchor deduplication is: off\n2016-01-15 15:46:58,848 INFO  indexer.IndexingFilters - Adding org.apache.nutch.indexer.anchor.AnchorIndexingFilter\n2016-01-15 15:46:59,438 INFO  elastic.ElasticWriter - Processing remaining requests [docs = 147, length = 1011442, total docs = 147]\n2016-01-15 15:46:59,445 INFO  elastic.ElasticWriter - Processing to finalize last execute\n2016-01-15 15:47:59,452 WARN  mapred.FileOutputCommitter - Output path is null in cleanup\n2016-01-15 15:47:59,453 WARN  mapred.LocalJobRunner - job_local_0001\norg.elasticsearch.cluster.block.ClusterBlockException: blocked by: [SERVICE_UNAVAILABLE/1/state not recovered / initialized];[SERVICE_UNAVAILABLE/2/no master];\n    at org.elasticsearch.cluster.block.ClusterBlocks.globalBlockedException(ClusterBlocks.java:151)\n    at org.elasticsearch.cluster.block.ClusterBlocks.globalBlockedRaiseException(ClusterBlocks.java:141)\n    at org.elasticsearch.action.bulk.TransportBulkAction.executeBulk(TransportBulkAction.java:215)\n    at org.elasticsearch.action.bulk.TransportBulkAction.access$000(TransportBulkAction.java:67)\n    at org.elasticsearch.action.bulk.TransportBulkAction$1.onFailure(TransportBulkAction.java:153)\n    at org.elasticsearch.action.support.TransportAction$ThreadedActionListener$2.run(TransportAction.java:137)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n    at java.lang.Thread.run(Thread.java:745)\n</code></pre>\n\n<p>Can anyone help me with this? Thanks!</p>\n", "creation_date": 1452844386, "score": 0},
{"title": "Nutch fetch command not fetching data", "view_count": 106, "owner": {"user_id": 1784848, "answer_count": 13, "creation_date": 1351581947, "accept_rate": 57, "view_count": 32, "location": "NY, United States", "reputation": 52}, "is_answered": true, "answers": [{"question_id": 34677993, "owner": {"user_id": 2014559, "accept_rate": 100, "link": "http://stackoverflow.com/users/2014559/andrew-butkus", "user_type": "registered", "reputation": 425}, "body": "<p>It's been a while since i worked with nutch, but from memory there is a time to live on fetching a page. for instance if you crawl <a href=\"http://helloworld.com\" rel=\"nofollow\">http://helloworld.com</a> today, and try to issue the fetch command again today, then it will probably just finish without fetching anything as the timetolive on the url <a href=\"http://helloworld.com\" rel=\"nofollow\">http://helloworld.com</a> is belated by x amount of days (forgot the default time to live).</p>\n\n<p>I think you can fix this by clearing the crawl_db and trying again - or there may be a command now to set the timetolive to 0.</p>\n", "creation_date": 1452259193, "is_accepted": false, "score": 1, "last_activity_date": 1452259193, "answer_id": 34678092}, {"last_edit_date": 1453062619, "owner": {"user_id": 1784848, "accept_rate": 57, "link": "http://stackoverflow.com/users/1784848/code-wrangler", "user_type": "registered", "reputation": 52}, "body": "<p>Finally after several hours r&amp;d I fond the problem was because of a bug in nutch, which is like \"The batch id passed to GeneratorJob by option/argument <code>-batchId &lt;id&gt;</code> is ignored and a generated batch id is used to mark the current batch.\".  Listed here as an issue <a href=\"https://issues.apache.org/jira/browse/NUTCH-2143\" rel=\"nofollow\">https://issues.apache.org/jira/browse/NUTCH-2143</a></p>\n\n<p>Special thanks to <a href=\"http://stackoverflow.com/users/2014559/andrew-butkus\">andrew-butkus</a> :)</p>\n", "question_id": 34677993, "creation_date": 1452582548, "is_accepted": true, "score": 0, "last_activity_date": 1453062619, "answer_id": 34737484}], "question_id": 34677993, "tags": ["hadoop", "hbase", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/34677993/nutch-fetch-command-not-fetching-data", "last_activity_date": 1453062619, "accepted_answer_id": 34737484, "body": "<p>I have a cluster setup with the following software stack :</p>\n\n<p><strong>nutch-branch-2.3.1,\ngora-hbase 0.6.1\nHadoop 2.5.2,\nhbase-0.98.8-hadoop2</strong></p>\n\n<p>So initial command is : inject, generate, fetch, parse, updatedb \nOut of which first 2 i.e. inject, generate are working fine, but for nutch command (even though its executing successfully) its not fetching any data, and because fetch process is failing its subsequent processes also getting failed.</p>\n\n<p>Please find the logs for counters for each process :</p>\n\n<p><strong>Inject job:</strong></p>\n\n<pre><code>2016-01-08 14:12:45,649 INFO  [main] mapreduce.Job: Counters: 31\n    File System Counters\n        FILE: Number of bytes read=0\n        FILE: Number of bytes written=114853\n        FILE: Number of read operations=0\n        FILE: Number of large read operations=0\n        FILE: Number of write operations=0\n        HDFS: Number of bytes read=836443\n        HDFS: Number of bytes written=0\n        HDFS: Number of read operations=2\n        HDFS: Number of large read operations=0\n        HDFS: Number of write operations=0\n    Job Counters \n        Launched map tasks=1\n        Data-local map tasks=1\n        Total time spent by all maps in occupied slots (ms)=179217\n        Total time spent by all reduces in occupied slots (ms)=0\n        Total time spent by all map tasks (ms)=59739\n        Total vcore-seconds taken by all map tasks=59739\n        Total megabyte-seconds taken by all map tasks=183518208\n    Map-Reduce Framework\n        Map input records=29973\n        Map output records=29973\n        Input split bytes=94\n        Spilled Records=0\n        Failed Shuffles=0\n        Merged Map outputs=0\n        GC time elapsed (ms)=318\n        CPU time spent (ms)=24980\n        Physical memory (bytes) snapshot=427704320\n        Virtual memory (bytes) snapshot=5077356544\n        Total committed heap usage (bytes)=328728576\n    injector\n        urls_injected=29973\n    File Input Format Counters \n        Bytes Read=836349\n    File Output Format Counters \n        Bytes Written=0\n</code></pre>\n\n<p><strong>generate job:</strong></p>\n\n<pre><code>2016-01-08 14:14:38,257 INFO  [main] mapreduce.Job: Counters: 50\n    File System Counters\n        FILE: Number of bytes read=137140\n        FILE: Number of bytes written=623942\n        FILE: Number of read operations=0\n        FILE: Number of large read operations=0\n        FILE: Number of write operations=0\n        HDFS: Number of bytes read=937\n        HDFS: Number of bytes written=0\n        HDFS: Number of read operations=1\n        HDFS: Number of large read operations=0\n        HDFS: Number of write operations=0\n    Job Counters \n        Launched map tasks=1\n        Launched reduce tasks=2\n        Data-local map tasks=1\n        Total time spent by all maps in occupied slots (ms)=43788\n        Total time spent by all reduces in occupied slots (ms)=305690\n        Total time spent by all map tasks (ms)=14596\n        Total time spent by all reduce tasks (ms)=61138\n        Total vcore-seconds taken by all map tasks=14596\n        Total vcore-seconds taken by all reduce tasks=61138\n        Total megabyte-seconds taken by all map tasks=44838912\n        Total megabyte-seconds taken by all reduce tasks=313026560\n    Map-Reduce Framework\n        Map input records=14345\n        Map output records=14342\n        Map output bytes=1261921\n        Map output materialized bytes=137124\n        Input split bytes=937\n        Combine input records=0\n        Combine output records=0\n        Reduce input groups=14342\n        Reduce shuffle bytes=137124\n        Reduce input records=14342\n        Reduce output records=14342\n        Spilled Records=28684\n        Shuffled Maps =2\n        Failed Shuffles=0\n        Merged Map outputs=2\n        GC time elapsed (ms)=1299\n        CPU time spent (ms)=39600\n        Physical memory (bytes) snapshot=2060779520\n        Virtual memory (bytes) snapshot=15215738880\n        Total committed heap usage (bytes)=1864892416\n    Generator\n        GENERATE_MARK=14342\n    Shuffle Errors\n        BAD_ID=0\n        CONNECTION=0\n        IO_ERROR=0\n        WRONG_LENGTH=0\n        WRONG_MAP=0\n        WRONG_REDUCE=0\n    File Input Format Counters \n        Bytes Read=0\n    File Output Format Counters \n        Bytes Written=0\n2016-01-08 14:14:38,429 INFO  [main] crawl.GeneratorJob: GeneratorJob: finished at 2016-01-08 14:14:38, time elapsed: 00:01:47\n2016-01-08 14:14:38,431 INFO  [main] crawl.GeneratorJob: GeneratorJob: generated batch id: 1452242570-1295749106 containing 14342 URLs\n</code></pre>\n\n<p><strong>Fetching :</strong></p>\n\n<pre><code>../nutch fetch -D mapred.reduce.tasks=2 -D mapred.child.java.opts=-Xmx1000m -D mapred.reduce.tasks.speculative.execution=false -D mapred.map.tasks.speculative.execution=false -D mapred.compress.map.output=true -D fetcher.timelimit.mins=180 1452242566-14060 -crawlId 1 -threads 50\n\n\n2016-01-08 14:14:43,142 INFO  [main] fetcher.FetcherJob: FetcherJob: starting at 2016-01-08 14:14:43\n2016-01-08 14:14:43,145 INFO  [main] fetcher.FetcherJob: FetcherJob: batchId: 1452242566-14060\n2016-01-08 14:15:53,837 INFO  [main] mapreduce.Job: Job job_1452239500353_0024 completed successfully\n2016-01-08 14:15:54,286 INFO  [main] mapreduce.Job: Counters: 50\n    File System Counters\n        FILE: Number of bytes read=44\n        FILE: Number of bytes written=349279\n        FILE: Number of read operations=0\n        FILE: Number of large read operations=0\n        FILE: Number of write operations=0\n        HDFS: Number of bytes read=1087\n        HDFS: Number of bytes written=0\n        HDFS: Number of read operations=1\n        HDFS: Number of large read operations=0\n        HDFS: Number of write operations=0\n    Job Counters \n        Launched map tasks=1\n        Launched reduce tasks=2\n        Data-local map tasks=1\n        Total time spent by all maps in occupied slots (ms)=30528\n        Total time spent by all reduces in occupied slots (ms)=136535\n        Total time spent by all map tasks (ms)=10176\n        Total time spent by all reduce tasks (ms)=27307\n        Total vcore-seconds taken by all map tasks=10176\n        Total vcore-seconds taken by all reduce tasks=27307\n        Total megabyte-seconds taken by all map tasks=31260672\n        Total megabyte-seconds taken by all reduce tasks=139811840\n    Map-Reduce Framework\n        Map input records=0\n        Map output records=0\n        Map output bytes=0\n        Map output materialized bytes=28\n        Input split bytes=1087\n        Combine input records=0\n        Combine output records=0\n        Reduce input groups=0\n        Reduce shuffle bytes=28\n        Reduce input records=0\n        Reduce output records=0\n        Spilled Records=0\n        Shuffled Maps =2\n        Failed Shuffles=0\n        Merged Map outputs=2\n        GC time elapsed (ms)=426\n        CPU time spent (ms)=11140\n        Physical memory (bytes) snapshot=1884893184\n        Virtual memory (bytes) snapshot=15245959168\n        Total committed heap usage (bytes)=1751646208\n    FetcherStatus\n        HitByTimeLimit-QueueFeeder=0\n    Shuffle Errors\n        BAD_ID=0\n        CONNECTION=0\n        IO_ERROR=0\n        WRONG_LENGTH=0\n        WRONG_MAP=0\n        WRONG_REDUCE=0\n    File Input Format Counters \n        Bytes Read=0\n    File Output Format Counters \n        Bytes Written=0\n2016-01-08 14:15:54,314 INFO  [main] fetcher.FetcherJob: FetcherJob: finished at 2016-01-08 14:15:54, time elapsed: 00:01:11\n</code></pre>\n\n<p>Please advise.</p>\n", "creation_date": 1452258845, "score": 0},
{"title": "Configure Solr to index metadata included in seed.txt", "view_count": 24, "is_answered": false, "question_id": 34833883, "tags": ["solr", "field", "metadata", "nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/34833883/configure-solr-to-index-metadata-included-in-seed-txt", "last_activity_date": 1452991845, "owner": {"user_id": 3796038, "view_count": 1, "answer_count": 0, "creation_date": 1404269669, "reputation": 1}, "body": "<p>I am currently running Nutch 1.10 and solr 5.3.1, and I am attempting to crawl and index a few sites.  These sites have an id and name associated (on the same line as the url in the seed.txt file) which I would like to be included along side other fields in the solr search results (such as host, segment, etc).  Is this possible? If so, would I need to modify any files other than the seed.txt and schema-solr4?</p>\n\n<p>Example of what I have in seed.txt: </p>\n\n<p>www.exampleSite.com       id=3         name=exampleSite</p>\n", "creation_date": 1452991845, "score": 0},
{"title": "Have you indexed nutch crawl results using elasticsearch before?", "view_count": 2666, "owner": {"age": 35, "answer_count": 0, "creation_date": 1305503692, "user_id": 754938, "view_count": 17, "location": "Burlington, VT", "reputation": 40}, "is_answered": true, "answers": [{"question_id": 6012093, "owner": {"user_id": 432844, "link": "http://stackoverflow.com/users/432844/julien-nioche", "user_type": "registered", "reputation": 939}, "body": "<p>Haven't done it but this is definitely doable but would require to piggyback the SOLR code (src/java/org/apache/nutch/indexer/solr) and adapt it to ElasticSearch. Would be a nice contrib to Nutch BTW</p>\n", "creation_date": 1306336927, "is_accepted": true, "score": 2, "last_activity_date": 1306336927, "answer_id": 6126897}, {"question_id": 6012093, "owner": {"user_id": 1057905, "link": "http://stackoverflow.com/users/1057905/ctjmorgan", "user_type": "unregistered", "reputation": 31}, "body": "<p>I know that Nutch will be adding pluggable backends and glad to see it.  I had a need to integrate elasticsearch with Nutch 1.3.  Code is posted here.  Piggybacked off the (src/java/org/apache/nutch/indexer/solr) code.</p>\n\n<p><a href=\"https://github.com/ctjmorgan/nutch-elasticsearch-indexer\" rel=\"nofollow\">https://github.com/ctjmorgan/nutch-elasticsearch-indexer</a></p>\n", "creation_date": 1321883553, "is_accepted": false, "score": 3, "last_activity_date": 1321883553, "answer_id": 8212897}, {"question_id": 6012093, "owner": {"user_id": 1200588, "link": "http://stackoverflow.com/users/1200588/matt-weber", "user_type": "registered", "reputation": 334}, "body": "<p>I wrote an ElasticSearch plugin that mocks the Solr api.  Using this plugin and the standard Nutch Solr indexer you can easily send crawled data into ElasticSearch.  Plugin and an example of how to use it with Nutch can be found on GitHub:</p>\n\n<p><a href=\"https://github.com/mattweber/elasticsearch-mocksolrplugin\">https://github.com/mattweber/elasticsearch-mocksolrplugin</a></p>\n", "creation_date": 1328819374, "is_accepted": false, "score": 10, "last_activity_date": 1328819374, "answer_id": 9218268}, {"question_id": 6012093, "owner": {"user_id": 3422238, "link": "http://stackoverflow.com/users/3422238/duong-nguyen", "user_type": "registered", "reputation": 310}, "body": "<p>Time goes by and now Nucth is already integrated well with ElasticSearch.  <a href=\"http://www.aossama.com/search-engine-with-apache-nutch-mongodb-and-elasticsearch/\" rel=\"nofollow\">Here</a> is a nice tutorial.</p>\n", "creation_date": 1452848595, "is_accepted": false, "score": 0, "last_activity_date": 1452848595, "answer_id": 34807530}], "question_id": 6012093, "tags": ["lucene", "full-text-search", "web-crawler", "nutch", "elasticsearch"], "answer_count": 4, "link": "http://stackoverflow.com/questions/6012093/have-you-indexed-nutch-crawl-results-using-elasticsearch-before", "last_activity_date": 1452848595, "accepted_answer_id": 6126897, "body": "<p>Has anyone had any luck writing custom indexers for nutch to index the crawl results with elasticsearch? Or do you know of any that already exist?</p>\n", "creation_date": 1305503895, "score": 7},
{"title": "Reading JavaScript content in Apache Nutch by prolonging Network Timeout", "view_count": 42, "owner": {"age": 21, "answer_count": 0, "creation_date": 1351572204, "user_id": 1784574, "accept_rate": 91, "view_count": 8, "location": "Los Ba&#241;os, Philippines", "reputation": 41}, "is_answered": true, "answers": [{"question_id": 34734554, "owner": {"user_id": 1699191, "link": "http://stackoverflow.com/users/1699191/ameertawfik", "user_type": "registered", "reputation": 618}, "body": "<p>Try to use nutch selenium plugin. It was recently released with nutch 1.11. Look at the protocol-selenium and protocol-interactiveselenium plugins to understand better how they are working.</p>\n", "creation_date": 1452573399, "is_accepted": true, "score": 0, "last_activity_date": 1452573399, "answer_id": 34735681}], "question_id": 34734554, "tags": ["ajax", "apache", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/34734554/reading-javascript-content-in-apache-nutch-by-prolonging-network-timeout", "last_activity_date": 1452747165, "accepted_answer_id": 34735681, "body": "<p>I am crawling a single domain using <code>Apache Nutch</code>. However, majority of its pages are using <code>JavaScript/AJAX</code> operations to load some links I want fetched. Will prolonging network timeout setting in <code>nutch-site.xml</code> help in prolonging waiting times for the crawler (and possibly allowing the Ajax call to finish and render itself)?</p>\n\n<p>Thanks!</p>\n", "creation_date": 1452565235, "score": 0},
{"title": "Error: Could not find or load main class org.apache.nutch.crawl.InjectorJob", "view_count": 517, "owner": {"age": 23, "answer_count": 17, "creation_date": 1444830614, "user_id": 5445436, "accept_rate": 80, "view_count": 26, "location": "Pune, India", "reputation": 131}, "is_answered": true, "answers": [{"question_id": 34737595, "owner": {"user_id": 5445436, "accept_rate": 80, "link": "http://stackoverflow.com/users/5445436/lokesh-kakran", "user_type": "registered", "reputation": 131}, "body": "<p>This is because class may or may not be present in your system, but it is not present at required place.</p>\n\n<p>Firstly, complete the below step</p>\n\n<pre><code>wget http://rdf.dmoz.org/rdf/content.rdf.u8.gz\ngunzip content.rdf.u8.gz\n</code></pre>\n\n<p>DMOZ contains around three million URLs. We select one out of every 5,000, so that we end up with around 1,000 URLs: </p>\n\n<pre><code>mkdir dmoz\nbin/nutch org.apache.nutch.tools.DmozParser content.rdf.u8 -subset 5000 &gt; dmoz/urls\n</code></pre>\n\n<p>The parser also takes a few minutes, as it must parse the full file. Finally, we initialize the crawldb with the selected URLs.</p>\n\n<pre><code>bin/nutch inject crawl/crawldb dmoz\n</code></pre>\n\n<p>&amp; then rest of steps you want to hit.</p>\n", "creation_date": 1452687929, "is_accepted": true, "score": 0, "last_activity_date": 1452687929, "answer_id": 34766693}], "question_id": 34737595, "tags": ["apache", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/34737595/error-could-not-find-or-load-main-class-org-apache-nutch-crawl-injectorjob", "last_activity_date": 1452687929, "accepted_answer_id": 34766693, "body": "<p><strong>i am following url</strong> <a href=\"https://wiki.apache.org/nutch/NutchTutorial#Crawl_your_first_website\" rel=\"nofollow\">https://wiki.apache.org/nutch/NutchTutorial#Crawl_your_first_website</a> <strong>to crawl data..</strong></p>\n\n<p>when i reached to the below command , I am facing the Error</p>\n\n<pre><code>bin/nutch inject crawl/crawldb dmoz\n</code></pre>\n\n<p>my java path is set .. &amp; i am having also the class DmozParser in path</p>\n\n<p>/home/admin/Desktop/nutch-solr/apache-nutch-2.3/src/java/org/apache/nutch</p>\n", "creation_date": 1452583029, "score": 0},
{"title": "Thread FetcherThread has no more work available. fetch of .com/ failed with: java.net.SocketTimeoutException: connect timed out", "view_count": 126, "owner": {"age": 23, "answer_count": 17, "creation_date": 1444830614, "user_id": 5445436, "accept_rate": 80, "view_count": 26, "location": "Pune, India", "reputation": 131}, "is_answered": true, "answers": [{"question_id": 34762930, "owner": {"user_id": 5445436, "accept_rate": 80, "link": "http://stackoverflow.com/users/5445436/lokesh-kakran", "user_type": "registered", "reputation": 131}, "body": "<p><strong>remove your crawl folder first.</strong></p>\n\n<p><strong>run the below commands:</strong></p>\n\n<pre><code>bin/nutch inject crawl/crawldb dmoz\nbin/nutch inject crawl/crawldb urls\nbin/nutch generate crawl/crawldb crawl/segments\ns1=`ls -d crawl/segments/2* | tail -1`\necho $s1\nbin/nutch fetch $s1\nbin/nutch parse $s1\nbin/nutch updatedb crawl/crawldb $s1\n</code></pre>\n\n<p>initiate all the configuration like url under dmoz, nutch-site.xml, etc</p>\n", "creation_date": 1452686813, "is_accepted": true, "score": 0, "last_activity_date": 1452686813, "answer_id": 34766297}], "question_id": 34762930, "tags": ["nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/34762930/thread-fetcherthread-has-no-more-work-available-fetch-of-com-failed-with-jav", "last_activity_date": 1452686813, "accepted_answer_id": 34766297, "body": "<p>While giving <strong>bin/nutch fetch $s1</strong> while following <a href=\"https://wiki.apache.org/nutch/NutchTutorial\" rel=\"nofollow\">https://wiki.apache.org/nutch/NutchTutorial</a> facing the below issue</p>\n\n<pre><code>Thread FetcherThread has no more work available\n-finishing thread FetcherThread, activeThreads=1\n-activeThreads=1, spinWaiting=0, fetchQueues.totalSize=0, fetchQueues.getQueueCount=1\n-activeThreads=1, spinWaiting=0, fetchQueues.totalSize=0, fetchQueues.getQueueCount=1\n-activeThreads=1, spinWaiting=0, fetchQueues.totalSize=0, fetchQueues.getQueueCount=1\n-activeThreads=1, spinWaiting=0, fetchQueues.totalSize=0, fetchQueues.getQueueCount=1\n-activeThreads=1, spinWaiting=0, fetchQueues.totalSize=0, fetchQueues.getQueueCount=1\n-activeThreads=1, spinWaiting=0, fetchQueues.totalSize=0, fetchQueues.getQueueCount=1\n-activeThreads=1, spinWaiting=0, fetchQueues.totalSize=0, fetchQueues.getQueueCount=1\n-activeThreads=1, spinWaiting=0, fetchQueues.totalSize=0, fetchQueues.getQueueCount=1\n-activeThreads=1, spinWaiting=0, fetchQueues.totalSize=0, fetchQueues.getQueueCount=1\nfetch of http://nutch.apache.org/ failed with: java.net.SocketTimeoutException: connect timed out\n</code></pre>\n\n<p>please sort out what is this happening?</p>\n", "creation_date": 1452677550, "score": 0},
{"title": "Can&#39;t crawl with Nutch in Ubuntu 14.04", "view_count": 214, "is_answered": false, "answers": [{"question_id": 25266841, "owner": {"user_id": 5445436, "accept_rate": 80, "link": "http://stackoverflow.com/users/5445436/lokesh-kakran", "user_type": "registered", "reputation": 131}, "body": "<p><strong>If you are linux user > ></strong>   </p>\n\n<p>export JAVA_HOME=$(readlink -f /your/jdk/path | sed \"s:bin/java::\")</p>\n\n<p><strong>else</strong></p>\n\n<pre><code>export JAVA_HOME=your jdk path directly\n</code></pre>\n", "creation_date": 1452576636, "is_accepted": false, "score": 0, "last_activity_date": 1452576636, "answer_id": 34736214}], "question_id": 25266841, "tags": ["apache", "nutch", "ubuntu-14.04"], "answer_count": 1, "link": "http://stackoverflow.com/questions/25266841/cant-crawl-with-nutch-in-ubuntu-14-04", "last_activity_date": 1452576636, "owner": {"age": 24, "answer_count": 1, "creation_date": 1407852731, "user_id": 3933787, "accept_rate": 0, "view_count": 19, "location": "Istanbul", "reputation": 24}, "body": "<p>I use this code in Terminal:</p>\n\n<pre><code>bin/nutch crawl urls -dir crawl -depth 3 -topN 5\n</code></pre>\n\n<p>Nutch doesn't work and this is these are the errors written on the Terminal;</p>\n\n<pre><code>bash:bin/nutch: No such file or directory\n</code></pre>\n\n<p>and</p>\n\n<pre><code>Error: JAVA_HOME is not set\n</code></pre>\n", "creation_date": 1407853474, "score": 1},
{"title": "nutch solr integration on cloudera", "view_count": 351, "is_answered": false, "question_id": 26435350, "tags": ["solr", "nutch", "cloudera"], "answer_count": 0, "link": "http://stackoverflow.com/questions/26435350/nutch-solr-integration-on-cloudera", "last_activity_date": 1452491222, "owner": {"user_id": 1058511, "answer_count": 22, "creation_date": 1321905325, "accept_rate": 47, "view_count": 109, "reputation": 591}, "body": "<p>Has any one been able to get nutch and solr to work together on a Cloudera cluster?</p>\n\n<p>The usual instruction is to get the schema.xml from nutch and replace the orginal one in solr. After that </p>\n\n<pre><code>/opt/apache-nutch-1.9/bin/crawl urls crawl http://localhost:8983/solr 2 -depth 3 -topN 100\n</code></pre>\n\n<p>will index the crawled pages in solr. However, in the Cloudera cluster, the schema.xml sits in zookeeper. After I replace the zookeeper copy with the one from nutch, the solr cluster no long works. One out the two solr servers in the cluster crashes right after restart. </p>\n\n<p>I can not find any instructions on the web. Does any one know about this?</p>\n", "creation_date": 1413594375, "score": 1},
{"title": "how can I integrate solr and nutch in Cloudera", "view_count": 354, "is_answered": false, "answers": [{"question_id": 29768111, "owner": {"user_id": 5445436, "accept_rate": 80, "link": "http://stackoverflow.com/users/5445436/lokesh-kakran", "user_type": "registered", "reputation": 131}, "body": "<p><strong>Setup</strong></p>\n\n<p>The first step to get started is to download the required software components, namely Apache Solr and Nutch.</p>\n\n<ol>\n<li><p>Download Solr version 1.3.0 or LucidWorks for Solr from Download page</p></li>\n<li><p>Extract Solr package</p></li>\n<li><p>Download Nutch version 1.0 or later (Alternatively download the the nightly version of Nutch that contains the required functionality)</p></li>\n<li><p>Extract the Nutch package</p>\n\n<p>tar xzf apache-nutch-1.0.tar.gz</p></li>\n<li><p>Configure Solr</p></li>\n</ol>\n\n<p>For the sake of simplicity we are going to use the example\nconfiguration of Solr as a base.</p>\n\n<p>a. Copy the provided Nutch schema from directory\napache-nutch-1.0/conf to directory apache-solr-1.3.0/example/solr/conf (override the existing file)</p>\n\n<p>We want to allow Solr to create the snippets for search results so we need to store the content in addition to indexing it:</p>\n\n<p>b. Change schema.xml so that the stored attribute of field \u201ccontent\u201d is true.</p>\n\n<pre><code>&lt;field name=\u201dcontent\u201d type=\u201dtext\u201d stored=\u201dtrue\u201d indexed=\u201dtrue\u201d/&gt;\n</code></pre>\n\n<p>We want to be able to tweak the relevancy of queries easily so we\u2019ll create new dismax request handler configuration for our use case:</p>\n\n<p>d. Open apache-solr-1.3.0/example/solr/conf/solrconfig.xml and paste following fragment to it</p>\n\n<pre><code>&lt;requestHandler name=\"/nutch\" class=\"solr.SearchHandler\" &gt;\n&lt;lst name=\"defaults\"&gt;\n&lt;str name=\"defType\"&gt;dismax&lt;/str&gt;\n&lt;str name=\"echoParams\"&gt;explicit&lt;/str&gt;\n&lt;float name=\"tie\"&gt;0.01&lt;/float&gt;\n&lt;str name=\"qf\"&gt;\ncontent^0.5 anchor^1.0 title^1.2\n&lt;/str&gt;\n&lt;str name=\"pf\"&gt;\ncontent^0.5 anchor^1.5 title^1.2 site^1.5\n&lt;/str&gt;\n&lt;str name=\"fl\"&gt;\nurl\n&lt;/str&gt;\n&lt;str name=\"mm\"&gt;\n2&amp;lt;-1 5&amp;lt;-2 6&amp;lt;90%\n&lt;/str&gt;\n&lt;int name=\"ps\"&gt;100&lt;/int&gt;\n&lt;bool hl=\"true\"/&gt;\n&lt;str name=\"q.alt\"&gt;*:*&lt;/str&gt;\n&lt;str name=\"hl.fl\"&gt;title url content&lt;/str&gt;\n&lt;str name=\"f.title.hl.fragsize\"&gt;0&lt;/str&gt;\n&lt;str name=\"f.title.hl.alternateField\"&gt;title&lt;/str&gt;\n&lt;str name=\"f.url.hl.fragsize\"&gt;0&lt;/str&gt;\n&lt;str name=\"f.url.hl.alternateField\"&gt;url&lt;/str&gt;\n&lt;str name=\"f.content.hl.fragmenter\"&gt;regex&lt;/str&gt;\n&lt;/lst&gt;\n&lt;/requestHandler&gt;\n</code></pre>\n\n<ol start=\"6\">\n<li><p>Start Solr</p>\n\n<p>cd apache-solr-1.3.0/example\njava -jar start.jar</p></li>\n<li><p>Configure Nutch</p></li>\n</ol>\n\n<p>a. Open nutch-site.xml in directory apache-nutch-1.0/conf, replace it\u2019s contents with the following (we specify our crawler name, active plugins and limit maximum url count for single host per run to be 100) :</p>\n\n<pre><code>&lt;?xml version=\"1.0\"?&gt;\n&lt;configuration&gt;\n&lt;property&gt;\n&lt;name&gt;http.agent.name&lt;/name&gt;\n&lt;value&gt;nutch-solr-integration&lt;/value&gt;\n&lt;/property&gt;\n&lt;property&gt;\n&lt;name&gt;generate.max.per.host&lt;/name&gt;\n&lt;value&gt;100&lt;/value&gt;\n&lt;/property&gt;\n&lt;property&gt;\n&lt;name&gt;plugin.includes&lt;/name&gt;\n&lt;value&gt;protocol-http|urlfilter-regex|parse-html|index-(basic|anchor)|query-(basic|site|url)|response-(json|xml)|summary-basic|scoring-opic|urlnormalizer-(pass|regex|basic)&lt;/value&gt;\n&lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre>\n\n<p>b. Open regex-urlfilter.txt in directory apache-nutch-1.0/conf,\nreplace it\u2019s content with following:</p>\n\n<pre><code>-^(https|telnet|file|ftp|mailto):\n</code></pre>\n\n<h1>skip some suffixes</h1>\n\n<pre><code>-.(swf|SWF|doc|DOC|mp3|MP3|WMV|wmv|txt|TXT|rtf|RTF|avi|AVI|m3u|M3U|flv|FLV|WAV|wav|mp4|MP4|avi|AVI|rss|RSS|xml|XML|pdf|PDF|js|JS|gif|GIF|jpg|JPG|png|PNG|ico|ICO|css|sit|eps|wmf|zip|ppt|mpg|xls|gz|rpm|tgz|mov|MOV|exe|jpeg|JPEG|bmp|BMP)$\n</code></pre>\n\n<h1>skip URLs containing certain characters as probable queries, etc.</h1>\n\n<pre><code>-[?*!@=]\n</code></pre>\n\n<h1>allow urls in foofactory.fi domain</h1>\n\n<p>+^<a href=\"http://([a-z0-9-A-Z]\" rel=\"nofollow\">http://([a-z0-9-A-Z]</a>*.)*lucidimagination.com/</p>\n\n<h1>deny anything else</h1>\n\n<p>-.\n8. Create a seed list (the initial urls to fetch)</p>\n\n<p>mkdir urls</p>\n\n<pre><code>echo \"http://www.lucidimagination.com/\" &gt; urls/seed.txt\n</code></pre>\n\n<ol start=\"9\">\n<li><p>Inject seed url(s) to nutch crawldb (execute in nutch directory)</p>\n\n<p>bin/nutch inject crawl/crawldb urls</p></li>\n<li><p>Generate fetch list, fetch and parse content</p>\n\n<p>bin/nutch generate crawl/crawldb crawl/segments</p></li>\n</ol>\n\n<p>The above command will generate a new segment directory under crawl/segments that at this point contains files that store the url(s) to be fetched. In the following commands we need the latest segment dir as parameter so we\u2019ll store it in an environment variable:</p>\n\n<pre><code>export SEGMENT=crawl/segments/`ls -tr crawl/segments|tail -1`\n</code></pre>\n\n<p>Now I launch the fetcher that actually goes to get the content:</p>\n\n<pre><code>bin/nutch fetch $SEGMENT -noParsing\n</code></pre>\n\n<p>Next I parse the content:</p>\n\n<pre><code>bin/nutch parse $SEGMENT\n</code></pre>\n\n<p>Then I update the Nutch crawldb. The updatedb command wil store all new urls discovered during the fetch and parse of the previous segment into Nutch database so they can be fetched later. Nutch also stores information about the pages that were fetched so the same urls won\u2019t be fetched again and again.</p>\n\n<pre><code>bin/nutch updatedb crawl/crawldb $SEGMENT -filter -normalize\n</code></pre>\n\n<p>Now a full Fetch cycle is completed. Next you can repeat step 10 couple of more times to get some more content.</p>\n\n<ol start=\"11\">\n<li><p>Create linkdb</p>\n\n<p>bin/nutch invertlinks crawl/linkdb -dir crawl/segments</p></li>\n<li><p>Finally index all content from all segments to Solr</p></li>\n</ol>\n\n<p>bin/nutch solrindex <a href=\"http://127.0.0.1:8983/solr/\" rel=\"nofollow\">http://127.0.0.1:8983/solr/</a> crawl/crawldb crawl/linkdb crawl/segments/*\nNow the indexed content is available through Solr. You can try to execute searches from the Solr admin ui from</p>\n\n<pre><code>http://127.0.0.1:8983/solr/admin\n</code></pre>\n\n<p>, or directly with url like</p>\n\n<pre><code>http://127.0.0.1:8983/solr/nutch/?q=solr&amp;version=2.2&amp;start=0&amp;rows=10&amp;indent=on&amp;wt=json\n</code></pre>\n", "creation_date": 1452491002, "is_accepted": false, "score": 0, "last_activity_date": 1452491002, "answer_id": 34714888}], "question_id": 29768111, "tags": ["solr", "hdfs", "cloudera", "nutch", "zookeeper"], "answer_count": 1, "link": "http://stackoverflow.com/questions/29768111/how-can-i-integrate-solr-and-nutch-in-cloudera", "last_activity_date": 1452491002, "owner": {"user_id": 4662134, "answer_count": 1, "creation_date": 1426152609, "accept_rate": 0, "view_count": 11, "reputation": 21}, "body": "<p>I get Starting Zookeeper and solr service and in Cloudera Manager also I had create a HDFS.But i still not able to get working nutch and solr together in Cloudera.\nI do not know the following steps in order to get crawling and indexing new urls and get Query Result of solr index.</p>\n\n<p>Does anyone know how to proceed?</p>\n", "creation_date": 1429608439, "score": 0},
{"title": "Why Nutch Solrdedup can not remove duplicate webpage", "view_count": 473, "is_answered": true, "answers": [{"last_edit_date": 1452425997, "owner": {"user_id": 12016, "accept_rate": 77, "link": "http://stackoverflow.com/users/12016/mana", "user_type": "registered", "reputation": 2720}, "body": "<p>The nutch schema defines id (= url) as the unique key. If that is not ok for you, then change it. The corresponding line in the <code>schema.xml</code> is:</p>\n\n<pre><code>&lt;uniqueKey&gt;url&lt;/uniqueKey&gt;\n</code></pre>\n\n<p>But the better solution may be to do the following: If you can access your server by</p>\n\n<pre><code>http://www.example.com\n</code></pre>\n\n<p>and by</p>\n\n<pre><code>http://example.com\n</code></pre>\n\n<p>you should consider crawling only one of them using regex-url filters to prevent duplicates.</p>\n", "question_id": 9448267, "creation_date": 1341263163, "is_accepted": false, "score": 3, "last_activity_date": 1452425997, "answer_id": 11301169}], "question_id": 9448267, "tags": ["nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/9448267/why-nutch-solrdedup-can-not-remove-duplicate-webpage", "last_activity_date": 1452425997, "owner": {"user_id": 561629, "answer_count": 2, "creation_date": 1294080535, "accept_rate": 61, "view_count": 48, "reputation": 215}, "body": "<p>I have two webpages with identical contents but different in URL. One URL is started with <a href=\"http://www.XXX.com\" rel=\"nofollow\">http://www.XXX.com</a> The other is started with <a href=\"http://xxx.com\" rel=\"nofollow\">http://xxx.com</a>. After I use Solrdedup to remove duplicate data in Solr. I find that the two are remain there. Does anyone know what's going on here?</p>\n", "creation_date": 1330205481, "score": 1},
{"title": "How to give depth in nutch 2.3 crawl", "view_count": 569, "is_answered": false, "answers": [{"question_id": 31892839, "owner": {"user_id": 1577405, "link": "http://stackoverflow.com/users/1577405/aperfectpoint", "user_type": "registered", "reputation": 41}, "body": "<p>The documentation for the script is not updated, I think the script there is relevant for Nutch 1.4.</p>\n\n<p>You can always check Nutch formal repository at <a href=\"https://github.com/apache/nutch\" rel=\"nofollow\">github</a>, of course make sure that you're looking at the right branch.</p>\n\n<p>Anyway, Nutch 2.X crawl script is as follows (and you can see the 2.3 source <a href=\"https://github.com/apache/nutch/blob/branch-2.3/src/bin/crawl\" rel=\"nofollow\">here</a>):</p>\n\n<pre><code>crawl &lt;seedDir&gt; &lt;crawlId&gt; &lt;solrURL&gt; &lt;numberOfRounds&gt;\n</code></pre>\n\n<p>Where <em>seedDir</em> is the directory where your seed urls' files are, <em>crawlId</em> is a name you wish to call your crawl job, <em>solrURL</em> is self-explantory and <em>numberOfRounds</em> is what you are looking for.</p>\n\n<p>Notice however that the script also has parameters that you can change (within the script), like the number of fetched pages per level (i.e. the breadth of crawl).</p>\n", "creation_date": 1439714775, "is_accepted": false, "score": 0, "last_activity_date": 1439714775, "answer_id": 32033322}, {"question_id": 31892839, "owner": {"user_id": 2938811, "link": "http://stackoverflow.com/users/2938811/user2938811", "user_type": "registered", "reputation": 1}, "body": "<p>it's in the nutch-default.xml in the conf folder </p>\n", "creation_date": 1451961768, "is_accepted": false, "score": 0, "last_activity_date": 1451961768, "answer_id": 34603378}], "question_id": 31892839, "tags": ["web-crawler", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/31892839/how-to-give-depth-in-nutch-2-3-crawl", "last_activity_date": 1451961768, "owner": {"age": 26, "answer_count": 501, "creation_date": 1294499143, "user_id": 568109, "accept_rate": 64, "view_count": 1841, "location": "New Delhi, India", "reputation": 29972}, "body": "<p>In v2.2.1 there is a <a href=\"http://wiki.apache.org/nutch/bin/nutch_crawl\" rel=\"nofollow\">nutch crawl</a> command in which you can give depth to crawl</p>\n\n<blockquote>\n  <p>bin/nutch crawl  [-solr ] [-dir d] [-threads n]\n  [-depth i] [-topN N]</p>\n</blockquote>\n\n<p>How to do the same for nutch v2.3 ?</p>\n", "creation_date": 1439034758, "score": 0},
{"title": "Assign a short name for a full directory path in ubuntu", "view_count": 19, "owner": {"age": 19, "answer_count": 0, "creation_date": 1436980120, "user_id": 5120449, "accept_rate": 83, "view_count": 25, "location": "Patna, India", "reputation": 73}, "is_answered": true, "answers": [{"question_id": 34553634, "owner": {"user_id": 3027320, "link": "http://stackoverflow.com/users/3027320/denny", "user_type": "registered", "reputation": 38}, "body": "<p>Open the file <code>~/.bashrc</code> ,\nAdd following lines at the end of the file:</p>\n\n<pre><code># for example: \n# export JAVA_HOME=/usr/lib/jvm/java-7-openjdk-amd64\nexport SHORT_NAME=/the/whole/directory\n</code></pre>\n\n<p>Run command <code>source .bashrc</code></p>\n\n<p>Then you can cd into <code>/the/whole/directory</code> by command <code>cd $SHORT_NAME</code></p>\n\n<p>This way works in bash =]</p>\n", "creation_date": 1451748407, "is_accepted": true, "score": 1, "last_activity_date": 1451748407, "answer_id": 34567505}], "question_id": 34553634, "tags": ["ubuntu", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/34553634/assign-a-short-name-for-a-full-directory-path-in-ubuntu", "last_activity_date": 1451748407, "accepted_answer_id": 34567505, "body": "<p>I am new on nutch. When I was installing nutch, I am stuck at line \n\" ${NUTCH_RUNTIME_HOME} to refer to the current directory (apache-nutch-1.X/)\".\nThen How will I assign the variable name to current directory ?</p>\n", "creation_date": 1451624057, "score": 0},
{"title": "HBase master not running exception", "view_count": 5261, "is_answered": false, "answers": [{"question_id": 19495612, "owner": {"user_id": 582789, "link": "http://stackoverflow.com/users/582789/alfonso-nishikawa", "user_type": "registered", "reputation": 1263}, "body": "<p>It seems your HBase Master is not running. Execute <code>$ jps</code> to see if you have HMaster running.</p>\n", "creation_date": 1382364486, "is_accepted": false, "score": 0, "last_activity_date": 1382364486, "answer_id": 19496795}, {"last_edit_date": 1451620244, "owner": {"user_id": 1817615, "accept_rate": 0, "link": "http://stackoverflow.com/users/1817615/sujit-rai", "user_type": "registered", "reputation": 64}, "body": "<p>Correction. By mistake I had written wrong host name in hbase-site.xml</p>\n\n<pre><code>&lt;configuration&gt;\n  &lt;property&gt;\n     &lt;name&gt;hbase.rootdir&lt;/name&gt;\n     &lt;value&gt;hdfs://hadoop:9000/hbase&lt;/value&gt;  // the actual host name was hadoop1\n  &lt;/property&gt;\n</code></pre>\n", "question_id": 19495612, "creation_date": 1382441157, "is_accepted": false, "score": 0, "last_activity_date": 1451620244, "answer_id": 19516560}], "question_id": 19495612, "tags": ["hadoop", "hbase", "nutch", "gora"], "answer_count": 2, "link": "http://stackoverflow.com/questions/19495612/hbase-master-not-running-exception", "last_activity_date": 1451620244, "owner": {"age": 27, "answer_count": 11, "creation_date": 1352710997, "user_id": 1817615, "accept_rate": 0, "view_count": 31, "location": "Mumbai", "reputation": 64}, "body": "<p>I am getting following error. I am trying to connect HBase as a back-end for Nutch crawler.</p>\n\n<pre><code>13/10/21 13:11:13 INFO client.HConnectionManager$HConnectionImplementation: getMaster attempt 0 of 10 failed; retrying after sleep of 1000\norg.apache.hadoop.hbase.MasterNotRunningException\n        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.getMaster(HConnectionManager.java:357)\n        at org.apache.hadoop.hbase.client.HBaseAdmin.&lt;init&gt;(HBaseAdmin.java:94)\n        at org.apache.gora.hbase.store.HBaseStore.initialize(HBaseStore.java:109)\n        at org.apache.gora.store.DataStoreFactory.initializeDataStore(DataStoreFactory.java:102)\n        at org.apache.gora.store.DataStoreFactory.createDataStore(DataStoreFactory.java:161)\n        at org.apache.gora.store.DataStoreFactory.createDataStore(DataStoreFactory.java:135)\n        at org.apache.nutch.storage.StorageUtils.createWebStore(StorageUtils.java:75)\n        at org.apache.nutch.crawl.InjectorJob.run(InjectorJob.java:221)\n        at org.apache.nutch.crawl.InjectorJob.inject(InjectorJob.java:251)\n        at org.apache.nutch.crawl.InjectorJob.run(InjectorJob.java:273)\n        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n        at org.apache.nutch.crawl.InjectorJob.main(InjectorJob.java:282)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n        at java.lang.reflect.Method.invoke(Method.java:597)\n        at org.apache.hadoop.util.RunJar.main(RunJar.java:160)\n13/10/21 13:11:14 INFO client.HConnectionManager$HConnectionImplementation: ZooKeeper available but no active master location found\n13/10/21 13:11:14 INFO client.HConnectionManager$HConnectionImplementation: getMaster attempt 1 of 10 failed; retrying after sleep of 1000\norg.apache.hadoop.hbase.MasterNotRunningException\n        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.getMaster(HConnectionManager.java:357)\n        at org.apache.hadoop.hbase.client.HBaseAdmin.&lt;init&gt;(HBaseAdmin.java:94)\n        at org.apache.gora.hbase.store.HBaseStore.initialize(HBaseStore.java:109)\n        at org.apache.gora.store.DataStoreFactory.initializeDataStore(DataStoreFactory.java:102)\n        at org.apache.gora.store.DataStoreFactory.createDataStore(DataStoreFactory.java:161)\n        at org.apache.gora.store.DataStoreFactory.createDataStore(DataStoreFactory.java:135)\n        at org.apache.nutch.storage.StorageUtils.createWebStore(StorageUtils.java:75)\n        at org.apache.nutch.crawl.InjectorJob.run(InjectorJob.java:221)\n        at org.apache.nutch.crawl.InjectorJob.inject(InjectorJob.java:251)\n        at org.apache.nutch.crawl.InjectorJob.run(InjectorJob.java:273)\n        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n        at org.apache.nutch.crawl.InjectorJob.main(InjectorJob.java:282)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n        at java.lang.reflect.Method.invoke(Method.java:597)\n        at org.apache.hadoop.util.RunJar.main(RunJar.java:160)\n13/10/21 13:11:15 INFO client.HConnectionManager$HConnectionImplementation: ZooKeeper available but no active master location found\n13/10/21 13:11:15 INFO client.HConnectionManager$HConnectionImplementation: getMaster attempt 2 of 10 failed; retrying after sleep of 1000\norg.apache.hadoop.hbase.MasterNotRunningException\n        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.getMaster(HConnectionManager.java:357)\n        at org.apache.hadoop.hbase.client.HBaseAdmin.&lt;init&gt;(HBaseAdmin.java:94)\n        at org.apache.gora.hbase.store.HBaseStore.initialize(HBaseStore.java:109)\n        at org.apache.gora.store.DataStoreFactory.initializeDataStore(DataStoreFactory.java:102)\n        at org.apache.gora.store.DataStoreFactory.createDataStore(DataStoreFactory.java:161)\n        at org.apache.gora.store.DataStoreFactory.createDataStore(DataStoreFactory.java:135)\n        at org.apache.nutch.storage.StorageUtils.createWebStore(StorageUtils.java:75)\n        at org.apache.nutch.crawl.InjectorJob.run(InjectorJob.java:221)\n        at org.apache.nutch.crawl.InjectorJob.inject(InjectorJob.java:251)\n        at org.apache.nutch.crawl.InjectorJob.run(InjectorJob.java:273)\n        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n        at org.apache.nutch.crawl.InjectorJob.main(InjectorJob.java:282)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n        at java.lang.reflect.Method.invoke(Method.java:597)\n        at org.apache.hadoop.util.RunJar.main(RunJar.java:160)\n13/10/21 13:11:16 INFO client.HConnectionManager$HConnectionImplementation: ZooKeeper available but no active master location found\n13/10/21 13:11:16 INFO client.HConnectionManager$HConnectionImplementation: getMaster attempt 3 of 10 failed; retrying after sleep of 2000\norg.apache.hadoop.hbase.MasterNotRunningException\n        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.getMaster(HConnectionManager.java:357)\n        at org.apache.hadoop.hbase.client.HBaseAdmin.&lt;init&gt;(HBaseAdmin.java:94)\n        at org.apache.gora.hbase.store.HBaseStore.initialize(HBaseStore.java:109)\n        at org.apache.gora.store.DataStoreFactory.initializeDataStore(DataStoreFactory.java:102)\n        at org.apache.gora.store.DataStoreFactory.createDataStore(DataStoreFactory.java:161)\n        at org.apache.gora.store.DataStoreFactory.createDataStore(DataStoreFactory.java:135)\n        at org.apache.nutch.storage.StorageUtils.createWebStore(StorageUtils.java:75)\n        at org.apache.nutch.crawl.InjectorJob.run(InjectorJob.java:221)\n        at org.apache.nutch.crawl.InjectorJob.inject(InjectorJob.java:251)\n        at org.apache.nutch.crawl.InjectorJob.run(InjectorJob.java:273)\n        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n        at org.apache.nutch.crawl.InjectorJob.main(InjectorJob.java:282)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n        at java.lang.reflect.Method.invoke(Method.java:597)\n        at org.apache.hadoop.util.RunJar.main(RunJar.java:160)\n13/10/21 13:11:18 INFO client.HConnectionManager$HConnectionImplementation: ZooKeeper available but no active master location found\n13/10/21 13:11:18 INFO client.HConnectionManager$HConnectionImplementation: getMaster attempt 4 of 10 failed; retrying after sleep of 2000\norg.apache.hadoop.hbase.MasterNotRunningException\n        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.getMaster(HConnectionManager.java:357)\n        at org.apache.hadoop.hbase.client.HBaseAdmin.&lt;init&gt;(HBaseAdmin.java:94)\n        at org.apache.gora.hbase.store.HBaseStore.initialize(HBaseStore.java:109)\n        at org.apache.gora.store.DataStoreFactory.initializeDataStore(DataStoreFactory.java:102)\n        at org.apache.gora.store.DataStoreFactory.createDataStore(DataStoreFactory.java:161)\n        at org.apache.gora.store.DataStoreFactory.createDataStore(DataStoreFactory.java:135)\n        at org.apache.nutch.storage.StorageUtils.createWebStore(StorageUtils.java:75)\n        at org.apache.nutch.crawl.InjectorJob.run(InjectorJob.java:221)\n        at org.apache.nutch.crawl.InjectorJob.inject(InjectorJob.java:251)\n        at org.apache.nutch.crawl.InjectorJob.run(InjectorJob.java:273)\n        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n        at org.apache.nutch.crawl.InjectorJob.main(InjectorJob.java:282)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n        at java.lang.reflect.Method.invoke(Method.java:597)\n        at org.apache.hadoop.util.RunJar.main(RunJar.java:160)\n13/10/21 13:11:20 INFO client.HConnectionManager$HConnectionImplementation: ZooKeeper available but no active master location found\n13/10/21 13:11:20 INFO client.HConnectionManager$HConnectionImplementation: getMaster attempt 5 of 10 failed; retrying after sleep of 4000\norg.apache.hadoop.hbase.MasterNotRunningException\n        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.getMaster(HConnectionManager.java:357)\n        at org.apache.hadoop.hbase.client.HBaseAdmin.&lt;init&gt;(HBaseAdmin.java:94)\n        at org.apache.gora.hbase.store.HBaseStore.initialize(HBaseStore.java:109)\n        at org.apache.gora.store.DataStoreFactory.initializeDataStore(DataStoreFactory.java:102)\n        at org.apache.gora.store.DataStoreFactory.createDataStore(DataStoreFactory.java:161)\n        at org.apache.gora.store.DataStoreFactory.createDataStore(DataStoreFactory.java:135)\n        at org.apache.nutch.storage.StorageUtils.createWebStore(StorageUtils.java:75)\n        at org.apache.nutch.crawl.InjectorJob.run(InjectorJob.java:221)\n        at org.apache.nutch.crawl.InjectorJob.inject(InjectorJob.java:251)\n        at org.apache.nutch.crawl.InjectorJob.run(InjectorJob.java:273)\n        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n        at org.apache.nutch.crawl.InjectorJob.main(InjectorJob.java:282)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n        at java.lang.reflect.Method.invoke(Method.java:597)\n        at org.apache.hadoop.util.RunJar.main(RunJar.java:160)\n13/10/21 13:11:24 INFO client.HConnectionManager$HConnectionImplementation: ZooKeeper available but no active master location found\n13/10/21 13:11:24 INFO client.HConnectionManager$HConnectionImplementation: getMaster attempt 6 of 10 failed; retrying after sleep of 4000\norg.apache.hadoop.hbase.MasterNotRunningException\n        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.getMaster(HConnectionManager.java:357)\n        at org.apache.hadoop.hbase.client.HBaseAdmin.&lt;init&gt;(HBaseAdmin.java:94)\n        at org.apache.gora.hbase.store.HBaseStore.initialize(HBaseStore.java:109)\n        at org.apache.gora.store.DataStoreFactory.initializeDataStore(DataStoreFactory.java:102)\n        at org.apache.gora.store.DataStoreFactory.createDataStore(DataStoreFactory.java:161)\n        at org.apache.gora.store.DataStoreFactory.createDataStore(DataStoreFactory.java:135)\n        at org.apache.nutch.storage.StorageUtils.createWebStore(StorageUtils.java:75)\n        at org.apache.nutch.crawl.InjectorJob.run(InjectorJob.java:221)\n        at org.apache.nutch.crawl.InjectorJob.inject(InjectorJob.java:251)\n        at org.apache.nutch.crawl.InjectorJob.run(InjectorJob.java:273)\n        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n        at org.apache.nutch.crawl.InjectorJob.main(InjectorJob.java:282)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n        at java.lang.reflect.Method.invoke(Method.java:597)\n        at org.apache.hadoop.util.RunJar.main(RunJar.java:160)\n13/10/21 13:11:28 INFO client.HConnectionManager$HConnectionImplementation: ZooKeeper available but no active master location found\n13/10/21 13:11:28 INFO client.HConnectionManager$HConnectionImplementation: getMaster attempt 7 of 10 failed; retrying after sleep of 8000\norg.apache.hadoop.hbase.MasterNotRunningException\n        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.getMaster(HConnectionManager.java:357)\n        at org.apache.hadoop.hbase.client.HBaseAdmin.&lt;init&gt;(HBaseAdmin.java:94)\n        at org.apache.gora.hbase.store.HBaseStore.initialize(HBaseStore.java:109)\n        at org.apache.gora.store.DataStoreFactory.initializeDataStore(DataStoreFactory.java:102)\n        at org.apache.gora.store.DataStoreFactory.createDataStore(DataStoreFactory.java:161)\n        at org.apache.gora.store.DataStoreFactory.createDataStore(DataStoreFactory.java:135)\n        at org.apache.nutch.storage.StorageUtils.createWebStore(StorageUtils.java:75)\n        at org.apache.nutch.crawl.InjectorJob.run(InjectorJob.java:221)\n        at org.apache.nutch.crawl.InjectorJob.inject(InjectorJob.java:251)\n        at org.apache.nutch.crawl.InjectorJob.run(InjectorJob.java:273)\n        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n        at org.apache.nutch.crawl.InjectorJob.main(InjectorJob.java:282)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n        at java.lang.reflect.Method.invoke(Method.java:597)\n        at org.apache.hadoop.util.RunJar.main(RunJar.java:160)\n13/10/21 13:11:36 INFO client.HConnectionManager$HConnectionImplementation: ZooKeeper available but no active master location found\n13/10/21 13:11:36 INFO client.HConnectionManager$HConnectionImplementation: getMaster attempt 8 of 10 failed; retrying after sleep of 16000\norg.apache.hadoop.hbase.MasterNotRunningException\n        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.getMaster(HConnectionManager.java:357)\n        at org.apache.hadoop.hbase.client.HBaseAdmin.&lt;init&gt;(HBaseAdmin.java:94)\n        at org.apache.gora.hbase.store.HBaseStore.initialize(HBaseStore.java:109)\n        at org.apache.gora.store.DataStoreFactory.initializeDataStore(DataStoreFactory.java:102)\n        at org.apache.gora.store.DataStoreFactory.createDataStore(DataStoreFactory.java:161)\n        at org.apache.gora.store.DataStoreFactory.createDataStore(DataStoreFactory.java:135)\n        at org.apache.nutch.storage.StorageUtils.createWebStore(StorageUtils.java:75)\n        at org.apache.nutch.crawl.InjectorJob.run(InjectorJob.java:221)\n        at org.apache.nutch.crawl.InjectorJob.inject(InjectorJob.java:251)\n        at org.apache.nutch.crawl.InjectorJob.run(InjectorJob.java:273)\n        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n        at org.apache.nutch.crawl.InjectorJob.main(InjectorJob.java:282)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n        at java.lang.reflect.Method.invoke(Method.java:597)\n        at org.apache.hadoop.util.RunJar.main(RunJar.java:160)\n13/10/21 13:11:52 INFO client.HConnectionManager$HConnectionImplementation: ZooKeeper available but no active master location found\n13/10/21 13:11:52 INFO client.HConnectionManager$HConnectionImplementation: getMaster attempt 9 of 10 failed; no more retrying.\norg.apache.hadoop.hbase.MasterNotRunningException\n        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.getMaster(HConnectionManager.java:357)\n        at org.apache.hadoop.hbase.client.HBaseAdmin.&lt;init&gt;(HBaseAdmin.java:94)\n        at org.apache.gora.hbase.store.HBaseStore.initialize(HBaseStore.java:109)\n        at org.apache.gora.store.DataStoreFactory.initializeDataStore(DataStoreFactory.java:102)\n        at org.apache.gora.store.DataStoreFactory.createDataStore(DataStoreFactory.java:161)\n        at org.apache.gora.store.DataStoreFactory.createDataStore(DataStoreFactory.java:135)\n        at org.apache.nutch.storage.StorageUtils.createWebStore(StorageUtils.java:75)\n        at org.apache.nutch.crawl.InjectorJob.run(InjectorJob.java:221)\n        at org.apache.nutch.crawl.InjectorJob.inject(InjectorJob.java:251)\n        at org.apache.nutch.crawl.InjectorJob.run(InjectorJob.java:273)\n        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n        at org.apache.nutch.crawl.InjectorJob.main(InjectorJob.java:282)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n        at java.lang.reflect.Method.invoke(Method.java:597)\n        at org.apache.hadoop.util.RunJar.main(RunJar.java:160)\n13/10/21 13:11:52 ERROR crawl.InjectorJob: InjectorJob: org.apache.gora.util.GoraException: java.lang.RuntimeException: org.apache.hadoop.hbase.MasterNotRunningException\n        at org.apache.gora.store.DataStoreFactory.createDataStore(DataStoreFactory.java:167)\n        at org.apache.gora.store.DataStoreFactory.createDataStore(DataStoreFactory.java:135)\n        at org.apache.nutch.storage.StorageUtils.createWebStore(StorageUtils.java:75)\n        at org.apache.nutch.crawl.InjectorJob.run(InjectorJob.java:221)\n        at org.apache.nutch.crawl.InjectorJob.inject(InjectorJob.java:251)\n        at org.apache.nutch.crawl.InjectorJob.run(InjectorJob.java:273)\n        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n        at org.apache.nutch.crawl.InjectorJob.main(InjectorJob.java:282)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n        at java.lang.reflect.Method.invoke(Method.java:597)\n        at org.apache.hadoop.util.RunJar.main(RunJar.java:160)\nCaused by: java.lang.RuntimeException: org.apache.hadoop.hbase.MasterNotRunningException\n        at org.apache.gora.hbase.store.HBaseStore.initialize(HBaseStore.java:127)\n        at org.apache.gora.store.DataStoreFactory.initializeDataStore(DataStoreFactory.java:102)\n        at org.apache.gora.store.DataStoreFactory.createDataStore(DataStoreFactory.java:161)\n        ... 12 more\nCaused by: org.apache.hadoop.hbase.MasterNotRunningException\n        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.getMaster(HConnectionManager.java:394)\n        at org.apache.hadoop.hbase.client.HBaseAdmin.&lt;init&gt;(HBaseAdmin.java:94)\n        at org.apache.gora.hbase.store.HBaseStore.initialize(HBaseStore.java:109)\n</code></pre>\n\n<p>Please help me.</p>\n", "creation_date": 1382361400, "score": 1},
{"title": "Apache nutch in distributed mode not going to crawl from web", "view_count": 99, "owner": {"user_id": 3454410, "answer_count": 20, "creation_date": 1395646154, "accept_rate": 48, "view_count": 174, "reputation": 832}, "is_answered": true, "answers": [{"question_id": 34412927, "owner": {"user_id": 1784848, "accept_rate": 57, "link": "http://stackoverflow.com/users/1784848/code-wrangler", "user_type": "registered", "reputation": 52}, "body": "<p>I am also having the same issue, but with somewhat newer versions of softwares. see this post <a href=\"http://stackoverflow.com/questions/34290214/nutch-solr-dataimport-handler\">Nutch Solr dataimport handler?</a></p>\n\n<p>As a workaround you can count records in hbase by opening hbase shell and run these commands</p>\n\n<pre><code>&gt; list (this will make sure the table is getting created.)\n&gt; count 'hbase_table' (this will give you records in it.)\n</code></pre>\n\n<p>alternatively rather then running all commands as a batch, try running them individually.<br>\notherwise you can paste the log files.</p>\n", "creation_date": 1450778990, "is_accepted": false, "score": 0, "last_activity_date": 1450778990, "answer_id": 34413247}, {"question_id": 34412927, "owner": {"user_id": 2265190, "accept_rate": 50, "link": "http://stackoverflow.com/users/2265190/hms", "user_type": "registered", "reputation": 293}, "body": "<p>In distributed mode of apache nutch, you have to check the logs of your tasktracker. Details of documents crawled should be there and not in mapreduce log. Its url will be like (if your are using default configuration)</p>\n\n<p><code>http://data-node-ip:50060/logs/hadoop-{user-name}-tasktracker-{machine-name}.log</code> </p>\n", "creation_date": 1451542459, "is_accepted": true, "score": 0, "last_activity_date": 1451542459, "answer_id": 34541993}], "question_id": 34412927, "tags": ["java", "hadoop", "solr", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/34412927/apache-nutch-in-distributed-mode-not-going-to-crawl-from-web", "last_activity_date": 1451542459, "accepted_answer_id": 34541993, "body": "<p>I am using apache nutch 2.3, hadoop 1.2.1 (3 datanodes), hbase 0.94, solr 4.8. In order to run apache nutch is distributed mode. I do follwoing steps</p>\n\n<ol>\n<li>go to runtime/deploy directory</li>\n<li>copied apache-nutch-2.3.jar to deploy directory</li>\n<li>Run this command <strong>runtime/deploy/bin/crawl urls/ hbase_table <a href=\"http://solrHost:8983/solr\" rel=\"nofollow\">http://solrHost:8983/solr</a> 1</strong></li>\n</ol>\n\n<p>where hbase_table is the name of hbase table where nutch will store data.\nAfter command starts, Mapreduce jobs starts for each phbase i.e. inject, generate, fetech,dedup,sorlrindex. All these mapreduce jobs finished with out any error. But When I check from HDFS, there was no data in hbase_table. Where is the problem in configuration. Unfortunately, Apache nutch in distributed mode guide is not completely available (according to my search)</p>\n", "creation_date": 1450778004, "score": 0},
{"title": "ERROR setFile(null,true) call failed,Either File or DatePattern options are not set for appender", "view_count": 24, "is_answered": false, "question_id": 34489229, "tags": ["nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/34489229/error-setfilenull-true-call-failed-either-file-or-datepattern-options-are-not", "last_activity_date": 1451287097, "owner": {"age": 19, "answer_count": 0, "creation_date": 1436980120, "user_id": 5120449, "accept_rate": 83, "view_count": 25, "location": "Patna, India", "reputation": 73}, "body": "<p>I am New on Nutch. When I crawled the the first website using instructions mentioned on this <a href=\"https://sites.google.com/site/profileswapnilkulkarni/tech-talk/nutchtutorialonubuntu10easysteps\" rel=\"nofollow\">web page</a></p>\n\n<p>getting following errors.I tried but didn't solve this.</p>\n\n<p><a href=\"http://i.stack.imgur.com/4UCQH.png\" rel=\"nofollow\"><img src=\"http://i.stack.imgur.com/4UCQH.png\" alt=\"enter image description here\"></a></p>\n", "creation_date": 1451287097, "score": 0},
{"title": "Nutch 2.3 + Elasticsearch 2.1.1. Cannot load Elasticsearch dependencies", "view_count": 407, "is_answered": false, "answers": [{"question_id": 34475789, "owner": {"user_id": 432844, "link": "http://stackoverflow.com/users/432844/julien-nioche", "user_type": "registered", "reputation": 939}, "body": "<p>Plugin dependencies should be declared both in the plugin's ivy.xml and in the plugin.xml files. I haven't tested the files you included but cannot see anything immediately wrong with them. As you pointed out, declaring the deps in the main ivy file is not great.</p>\n\n<p>See <a href=\"https://github.com/apache/nutch/blob/trunk/src/plugin/parse-tika/howto_upgrade_tika.txt\" rel=\"nofollow\">this note on how to upgrade the Tika plugin</a>, the same logic applies to all the plugins.</p>\n\n<p>As for resolving conflicts between the main dependencies and the ones from the plugins, unfortunately you'll have to deal with it yourself e.g. force the version you need in the main ivy.xml as Nutch does not handle the plugins as dependencies (in the Maven sense) of the main code.</p>\n", "creation_date": 1451250272, "is_accepted": false, "score": 0, "last_activity_date": 1451250272, "answer_id": 34484801}], "question_id": 34475789, "tags": ["apache", "elasticsearch", "ant", "ivy", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/34475789/nutch-2-3-elasticsearch-2-1-1-cannot-load-elasticsearch-dependencies", "last_activity_date": 1451250272, "owner": {"user_id": 1590654, "view_count": 13, "answer_count": 0, "creation_date": 1344612179, "reputation": 1}, "body": "<p>I'm trying to integrate Nutch 2.3 in order to push data to the latest Elasticsearch 2.1.1.\nI started updating versions and dependencies in the following files:</p>\n\n<p><strong>indexer-elastic/plugin.xml</strong></p>\n\n<pre><code>    &lt;plugin id=\"indexer-elastic\" name=\"ElasticIndexWriter\" version=\"1.0.0\"\n  provider-name=\"nutch.apache.org\"&gt;\n\n  &lt;runtime&gt;\n    &lt;library name=\"indexer-elastic.jar\"&gt;\n      &lt;export name=\"*\" /&gt;\n    &lt;/library&gt;\n\n    &lt;library name=\"elasticsearch-2.1.1.jar\"/&gt;\n\n    &lt;library name=\"hppc-0.7.1.jar\"/&gt;\n    &lt;library name=\"jackson-core-2.6.2.jar\"/&gt;\n    &lt;library name=\"jackson-dataformat-cbor-2.6.2.jar\"/&gt;\n    &lt;library name=\"jackson-dataformat-smile-2.6.2.jar\"/&gt;\n    &lt;library name=\"jackson-dataformat-yaml-2.6.2.jar\"/&gt;\n    &lt;library name=\"guava-18.0.jar\"/&gt;\n    &lt;library name=\"compress-lzf-1.0.2.jar\"/&gt;\n    &lt;library name=\"t-digest-3.0.jar\"/&gt;\n    &lt;library name=\"jsr166e-1.1.0.jar\"/&gt;\n    &lt;library name=\"commons-cli-1.3.1.jar\"/&gt;\n    &lt;library name=\"netty-3.10.5.Final.jar\"/&gt;\n    &lt;library name=\"joda-time-2.8.2.jar\"/&gt;\n\n    &lt;library name=\"lucene-analyzers-common-5.3.1.jar\"/&gt;\n    &lt;library name=\"lucene-backward-codecs-5.3.1.jar\"/&gt;\n    &lt;library name=\"lucene-core-5.3.1.jar\"/&gt;\n    &lt;library name=\"lucene-highlighter-5.3.1.jar\"/&gt;\n    &lt;library name=\"lucene-join-5.3.1.jar\"/&gt;\n    &lt;library name=\"lucene-memory-5.3.1.jar\"/&gt;\n    &lt;library name=\"lucene-queries-5.3.1.jar\"/&gt;\n    &lt;library name=\"lucene-queryparser-5.3.1.jar\"/&gt;\n    &lt;library name=\"lucene-spatial-5.3.1.jar\"/&gt;\n    &lt;library name=\"lucene-suggest-5.3.1.jar\"/&gt;\n\n    &lt;library name=\"HdrHistogram-2.1.6.jar\"/&gt;\n    &lt;library name=\"joda-convert-1.2.jar\"/&gt;\n  &lt;/runtime&gt;\n\n  &lt;requires&gt;\n    &lt;import plugin=\"nutch-extensionpoints\" /&gt;\n  &lt;/requires&gt;\n\n  &lt;extension id=\"org.apache.nutch.indexer.elastic\"\n    name=\"Elasticsearch Index Writer\"\n    point=\"org.apache.nutch.indexer.IndexWriter\"&gt;\n    &lt;implementation id=\"ElasticIndexWriter\"\n      class=\"org.apache.nutch.indexwriter.elastic.ElasticIndexWriter\" /&gt;\n  &lt;/extension&gt;\n\n&lt;/plugin&gt;\n</code></pre>\n\n<p><strong>indexer-elastic/ivy.xml</strong></p>\n\n<pre><code>&lt;ivy-module version=\"1.0\"&gt;\n  &lt;info organisation=\"org.apache.nutch\" module=\"${ant.project.name}\"&gt;\n    &lt;license name=\"Apache 2.0\" /&gt;\n    &lt;ivyauthor name=\"Apache Nutch Team\" url=\"http://nutch.apache.org\" /&gt;\n    &lt;description&gt;Apache Nutch&lt;/description&gt;\n  &lt;/info&gt;\n\n  &lt;configurations&gt;\n    &lt;include file=\"../../..//ivy/ivy-configurations.xml\" /&gt;\n  &lt;/configurations&gt;\n\n  &lt;publications&gt;\n    &lt;!--get the artifact from our module name --&gt;\n    &lt;artifact conf=\"master\" /&gt;\n  &lt;/publications&gt;\n\n  &lt;dependencies&gt;\n    &lt;dependency org=\"org.elasticsearch\" name=\"elasticsearch\"\n      rev=\"2.1.1\" conf=\"*-&gt;default\" /&gt;\n\n    &lt;dependency org=\"com.google.guava\" name=\"guava\" rev=\"18.0\" /&gt;\n  &lt;/dependencies&gt;\n&lt;/ivy-module&gt;\n</code></pre>\n\n<p>I also reworked org.apache.nutch.indexwriter.elastic.ElasticIndexWriter to work against the new interface of the elasticsearch 2.1.1 client.</p>\n\n<p><strong>So what is the problem?</strong></p>\n\n<p>It seems that the dependencies listed in <strong>indexer-elastic/plugin.xml</strong> are not loaded  automatically at runtime. Therefore elasticsearch client cannot benefit from them and throws exceptions..\nSo i tried a different approach adding the dependencies one by one according to the exception it gives me in <strong>$NUTCH_ROOT/ivy/ivy.xml</strong> where the main dependencies of Apache Nutch are listed. That's not the right approach but it's kind of working.</p>\n\n<ol>\n<li>How to deal with plugin dependencies? </li>\n<li>What is the strategy for using newer version of a library in the plugin. For example Nutch uses Guava v11.0.2 but Elasticsearch 2.1.1 requires Guava v18.0. Although i'm specifying it explicitly in <strong>indexer-elastic/ivy.xml</strong> it seems to load the old version at runtime.</li>\n</ol>\n", "creation_date": 1451168466, "score": 0},
{"title": "How to run nutch 1.9 in eclipse on windows?", "view_count": 577, "is_answered": false, "answers": [{"question_id": 25933941, "owner": {"user_id": 4859544, "accept_rate": 19, "link": "http://stackoverflow.com/users/4859544/ron", "user_type": "registered", "reputation": 75}, "body": "<p>Download cygwin, then add that to your path of the environment variables. I think your problem is caused by the fact that windows can't invoke a unix native command. That is what I did however as soon as i got past that problem, I encountered other problems.</p>\n", "creation_date": 1445357070, "is_accepted": false, "score": 0, "last_activity_date": 1445357070, "answer_id": 33241459}], "question_id": 25933941, "tags": ["java", "eclipse", "hadoop", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/25933941/how-to-run-nutch-1-9-in-eclipse-on-windows", "last_activity_date": 1451204969, "owner": {"age": 25, "answer_count": 126, "creation_date": 1396591478, "user_id": 3496666, "accept_rate": 72, "view_count": 618, "location": "Chennai, India", "reputation": 1801}, "body": "<p>I want to run Nutch 1.9 in Eclipse on Windows. I followed the tutorial from <a href=\"http://wiki.apache.org/nutch/RunNutchInEclipse\" rel=\"nofollow\">http://wiki.apache.org/nutch/RunNutchInEclipse</a> and opened the project in Eclipse. </p>\n\n<p>But when I run Nutch, I get the following error:</p>\n\n<pre><code>2014-09-19 17:45:48,039 INFO  crawl.Injector (Injector.java:inject(283)) - Injector: starting at 2014-09-19 17:45:48\n2014-09-19 17:45:48,043 INFO  crawl.Injector (Injector.java:inject(284)) - Injector: crawlDb: K:/kumar/Nutch/apache-nutch-1.9/crawlresult\n2014-09-19 17:45:48,043 INFO  crawl.Injector (Injector.java:inject(285)) - Injector: urlDir: K:/kumar/Nutch/apache-nutch-1.9/urls\n2014-09-19 17:45:48,043 INFO  crawl.Injector (Injector.java:inject(294)) - Injector: Converting injected urls to crawl db entries.\n2014-09-19 17:45:48,207 INFO  jvm.JvmMetrics (JvmMetrics.java:init(71)) - Initializing JVM Metrics with processName=JobTracker, sessionId=\n2014-09-19 17:45:48,252 WARN  mapred.JobClient (JobClient.java:configureCommandLineOptions(661)) - No job jar file set.  User classes may not be found. See JobConf(Class) or JobConf#setJar(String).\n2014-09-19 17:45:48,268 INFO  mapred.FileInputFormat (FileInputFormat.java:listStatus(192)) - Total input paths to process : 1\n2014-09-19 17:45:48,485 INFO  mapred.JobClient (JobClient.java:monitorAndPrintJob(1275)) - Running job: job_local_0001\n2014-09-19 17:45:48,487 INFO  mapred.FileInputFormat (FileInputFormat.java:listStatus(192)) - Total input paths to process : 1\n2014-09-19 17:45:48,526 INFO  mapred.MapTask (MapTask.java:runOldMapper(347)) - numReduceTasks: 0\n2014-09-19 17:45:48,565 INFO  plugin.PluginRepository (PluginManifestParser.java:parsePluginFolder(87)) - Plugins: looking in: K:\\Nutch\\apache-nutch-1.9\\plugins\n2014-09-19 17:45:48,566 WARN  plugin.PluginRepository (PluginManifestParser.java:parsePluginFolder(101)) - java.io.FileNotFoundException: K:\\Nutch\\apache-nutch-1.9\\plugins\\creativecommons\\plugin.xml (The system cannot find the file specified)\n</code></pre>\n\n<p>It seems that Hadoop is the causing error. I don't know how to solve this problem. I know Nutch requires Unix environment. But, I want to run Nutch in Eclipse on Windows.</p>\n\n<p>Can anybody help me to solve this?</p>\n", "creation_date": 1411129645, "score": 1},
{"title": "Maximum number of Apache Nutch worker instances", "view_count": 48, "is_answered": false, "answers": [{"question_id": 34325728, "owner": {"user_id": 432844, "link": "http://stackoverflow.com/users/432844/julien-nioche", "user_type": "registered", "reputation": 939}, "body": "<p>Not clear what you mean by crawler instances. If you want to run the crawl script several times in parallel e.g. you have distinct crawls with separate configs, seeds etc... then they will compete for slots on the Hadoop cluster. It will then boil down to how many mapper / reducer slots are available on your cluster, which itself depends on how many slaves are there.</p>\n\n<p>Handling multiple Nutch crawls in parallel can get very tricky and resource inefficient. Instead re-think your architecture so that all the logical crawlers could run as a single physical one or have a look at <a href=\"https://github.com/DigitalPebble/storm-crawler\" rel=\"nofollow\">StormCrawler</a>, which should be a better fit for doing this.</p>\n", "creation_date": 1450947060, "is_accepted": false, "score": 0, "last_activity_date": 1450947060, "answer_id": 34449880}], "question_id": 34325728, "tags": ["hadoop", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/34325728/maximum-number-of-apache-nutch-worker-instances", "last_activity_date": 1450947060, "owner": {"user_id": 5689154, "view_count": 24, "answer_count": 0, "creation_date": 1450319539, "reputation": 44}, "body": "<p>What is the maximum number of Apache Nutch crawler instances that can run at the same time with one master node?</p>\n", "creation_date": 1450319968, "score": 7},
{"title": "Integrating Nutch and Solr", "view_count": 83, "is_answered": false, "answers": [{"question_id": 34431116, "owner": {"user_id": 3757127, "accept_rate": 83, "link": "http://stackoverflow.com/users/3757127/yann", "user_type": "registered", "reputation": 601}, "body": "<p>Your Solr URL should include the name of the core as well:</p>\n\n<pre><code>bin/nutch solrindex http://127.0.0.1:8983/solr/gettingstarted_shard1_replica1/ crawl/crawldb -linkdb crawl/linkdb crawl/segments/\n</code></pre>\n", "creation_date": 1450904587, "is_accepted": false, "score": 0, "last_activity_date": 1450904587, "answer_id": 34443671}], "question_id": 34431116, "tags": ["apache", "solr", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/34431116/integrating-nutch-and-solr", "last_activity_date": 1450904587, "owner": {"user_id": 1553519, "answer_count": 50, "creation_date": 1343279211, "accept_rate": 82, "view_count": 611, "location": "Shenzhen, China", "reputation": 3479}, "body": "<p>All, I am trying to integrate <code>Nutch</code> and <code>Solr</code> working together by following <a href=\"http://wiki.apache.org/nutch/NutchTutorial\" rel=\"nofollow\">this guide</a></p>\n\n<p>Nutch version is 1.1\nSolr version is 5.3.1</p>\n\n<p>When I tried to index the Nutch crawl result into the soly by running following command in the cygwin.</p>\n\n<p><code>bin/nutch solrindex http://127.0.0.1:8983/solr/ crawl/crawldb -linkdb crawl/linkdb crawl/segments/</code></p>\n\n<p>The command line is running without any exception. \nBut When I tried to run query in the Solr admin . Nothing can be searched .</p>\n\n<p><a href=\"http://i.stack.imgur.com/0WKST.png\" rel=\"nofollow\"><img src=\"http://i.stack.imgur.com/0WKST.png\" alt=\"enter image description here\"></a></p>\n\n<p>I wondered if there is anything exception happened when index into Solr.\nBut I checked the log of Solr . Nothing happened. Anything I missed?</p>\n\n<p>Thanks.</p>\n", "creation_date": 1450857517, "score": 0},
{"title": "Apache nutch error NoClassDefFoundError &quot;com/google/protobuf/Message&quot;", "view_count": 266, "owner": {"user_id": 3454410, "answer_count": 20, "creation_date": 1395646154, "accept_rate": 48, "view_count": 174, "reputation": 832}, "is_answered": true, "answers": [{"question_id": 34390548, "owner": {"user_id": 2265190, "accept_rate": 50, "link": "http://stackoverflow.com/users/2265190/hms", "user_type": "registered", "reputation": 293}, "body": "<p>I think you should add protobuf-java{version}.jar to hadoop lib directory. It should solve your problem.\nFor classpath verification. Run following command</p>\n\n<pre><code>hadoop classpath |grep protobuf\n</code></pre>\n\n<p>If it contains repective jar then it means it has been added to classpath.</p>\n", "creation_date": 1450683443, "is_accepted": true, "score": 1, "last_activity_date": 1450683443, "answer_id": 34390916}], "question_id": 34390548, "tags": ["java", "hadoop", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/34390548/apache-nutch-error-noclassdeffounderror-com-google-protobuf-message", "last_activity_date": 1450683443, "accepted_answer_id": 34390916, "body": "<p>I am using apache nutch 2.3 with hadoop 1.2.1 and hbase 0.94. I have configured apache nutch for distributed mode. When I run crawling, It gives following error</p>\n\n<pre><code>15/12/21 12:06:56 INFO zookeeper.ClientCnxn: Session establishment complete on server node1/1.11.1.2:2181, sessionid = 0x151ba038ac60250, negotiated timeout = 180000\nException in thread \"main\" java.lang.NoClassDefFoundError: com/google/protobuf/Message\n    at org.apache.hadoop.hbase.io.HbaseObjectWritable.&lt;clinit&gt;(HbaseObjectWritable.java:265)\n    at org.apache.hadoop.hbase.ipc.Invocation.write(Invocation.java:139)\n    at org.apache.hadoop.hbase.ipc.HBaseClient$Connection.sendParam(HBaseClient.java:620)\n    at org.apache.hadoop.hbase.ipc.HBaseClient.call(HBaseClient.java:989)\n    at org.apache.hadoop.hbase.ipc.WritableRpcEngine$Invoker.invoke(WritableRpcEngine.java:87)\n    at com.sun.proxy.$Proxy5.getProtocolVersion(Unknown Source)\n    at org.apache.hadoop.hbase.ipc.WritableRpcEngine.getProxy(WritableRpcEngine.java:141)\n    at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.getMaster(HConnectionManager.java:813)\n    at org.apache.hadoop.hbase.client.HBaseAdmin.&lt;init&gt;(HBaseAdmin.java:127)\n    at org.apache.gora.hbase.store.HBaseStore.initialize(HBaseStore.java:115)\n    at org.apache.gora.store.DataStoreFactory.initializeDataStore(DataStoreFactory.java:104)\n    at org.apache.gora.store.DataStoreFactory.createDataStore(DataStoreFactory.java:163)\n    at org.apache.gora.store.DataStoreFactory.createDataStore(DataStoreFactory.java:137)\n    at org.apache.nutch.storage.StorageUtils.createWebStore(StorageUtils.java:78)\n    at org.apache.nutch.crawl.InjectorJob.run(InjectorJob.java:218)\n    at org.apache.nutch.crawl.InjectorJob.inject(InjectorJob.java:252)\n    at org.apache.nutch.crawl.InjectorJob.run(InjectorJob.java:275)\n    at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n    at org.apache.nutch.crawl.InjectorJob.main(InjectorJob.java:284)\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n    at java.lang.reflect.Method.invoke(Method.java:483)\n    at org.apache.hadoop.util.RunJar.main(RunJar.java:160)\nCaused by: java.lang.ClassNotFoundException: com.google.protobuf.Message\n    at java.net.URLClassLoader$1.run(URLClassLoader.java:372)\n    at java.net.URLClassLoader$1.run(URLClassLoader.java:361)\n    at java.security.AccessController.doPrivileged(Native Method)\n    at java.net.URLClassLoader.findClass(URLClassLoader.java:360)\n    at java.lang.ClassLoader.loadClass(ClassLoader.java:424)\n    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)\n    at java.lang.ClassLoader.loadClass(ClassLoader.java:357)\n    ... 24 more\n</code></pre>\n\n<p>In lib of nutch, there is a jar of protobuf-java-2.4.1.jar \nHow to solve it?</p>\n", "creation_date": 1450681920, "score": 1},
{"title": "HBase shell not running windows 7", "view_count": 66, "is_answered": false, "question_id": 34341479, "tags": ["hbase", "nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/34341479/hbase-shell-not-running-windows-7", "last_activity_date": 1450375978, "owner": {"user_id": 5636973, "view_count": 11, "answer_count": 1, "creation_date": 1449177812, "reputation": 16}, "body": "<p>I have HBase copied to C:\\Users\\User5\\Documents\\apache nutch\\hbase-0.90.4.tar\\hbase-0.90.4</p>\n\n<p>I cannot get the HBase shell to execute:</p>\n\n<pre><code>C:\\Users\\User5\\Documents\\apache nutch\\hbase-0.90.4.tar\\hbase-0.90.4&gt;.\\bin\\hbase\nshell\n'.\\bin\\hbase' is not recognized as an internal or external command,\noperable program or batch file.\n\nC:\\Users\\User5\\Documents\\apache nutch\\hbase-0.90.4.tar\\hbase-0.90.4&gt;bin\\hbase shell\n    'bin\\hbase' is not recognized as an internal or external command,\n    operable program or batch file.\n</code></pre>\n\n<p>This is just printout of different things I tried, but I believe the problem is in the above code.</p>\n\n<pre><code>C:\\Users\\User5\\Documents\\apache nutch\\hbase-0.90.4.tar\\hbase-0.90.4&gt;.\\bin\\hbase\nshell\n'.\\bin\\hbase' is not recognized as an internal or external command,\noperable program or batch file.\n\nC:\\Users\\User5\\Documents\\apache nutch\\hbase-0.90.4.tar\\hbase-0.90.4&gt;.\\bin\\hbase\nshell\n'.\\bin\\hbase' is not recognized as an internal or external command,\noperable program or batch file.\n\nC:\\Users\\User5\\Documents\\apache nutch\\hbase-0.90.4.tar\\hbase-0.90.4&gt;.\\bin\\hbase\n'.\\bin\\hbase' is not recognized as an internal or external command,\noperable program or batch file.\n\nC:\\Users\\User5\\Documents\\apache nutch\\hbase-0.90.4.tar\\hbase-0.90.4&gt;cd C:\\Users\\\nUser5\\Documents\\apache nutch\\hbase-0.90.4.tar\\hbase-0.90.4\\bin\n\nC:\\Users\\User5\\Documents\\apache nutch\\hbase-0.90.4.tar\\hbase-0.90.4\\bin&gt;hbase\n'hbase' is not recognized as an internal or external command,\noperable program or batch file.\n\nC:\\Users\\User5\\Documents\\apache nutch\\hbase-0.90.4.tar\\hbase-0.90.4\\bin&gt;bin\\star\nt-hbase.cmd\nThe system cannot find the path specified.\n\nC:\\Users\\User5\\Documents\\apache nutch\\hbase-0.90.4.tar\\hbase-0.90.4\\bin&gt;cd C:\\Us\ners\\User5\\Documents\\apache nutch\\hbase-0.90.4.tar\\hbase-0.90.4\n\nC:\\Users\\User5\\Documents\\apache nutch\\hbase-0.90.4.tar\\hbase-0.90.4&gt;bin\\start-hb\nase.cmd\n'bin\\start-hbase.cmd' is not recognized as an internal or external command,\noperable program or batch file.\n\nC:\\Users\\User5\\Documents\\apache nutch\\hbase-0.90.4.tar\\hbase-0.90.4&gt;\n</code></pre>\n\n<p>I have changed the hbase-site.xml, like:</p>\n\n<pre><code>&lt;?xml version=\"1.0\"?&gt; \n&lt;?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?&gt; &lt;configuration&gt; \n&lt;property&gt; \n&lt;name&gt;hbase.rootdir&lt;/name&gt; \n&lt;value&gt;C:\\Users\\User5\\Documents\\apache nutch\\hbase_store&lt;/value&gt; \n&lt;!\u2014 You need to create one directory and assign a path up to that directory. That directory will be used by Apache Hbase to store all relevant information. \u00e0\n&lt;/property&gt; \n&lt;property&gt; \n&lt;name&gt;hbase.zookeeper.property.dataDir&lt;/name&gt;\n&lt;value&gt;C:\\Users\\User5\\Documents\\apache nutch\\hbase_info&lt;/value&gt; \n&lt;!\u2014 You need to create one directory and assign a path up to that directory. That directory will be used by Apache Hbase to store all relevant information related to Apache zookeeper which comes inbuilt with Apache Hbase. Apache Zookeeper is an opensource server which is used for distributed coordination. You can learn more about Apache Zookeeper from https://cwiki.apache.org/confluence/display/ZOOKEEPER/Index \n&lt;/property&gt; \n&lt;/configuration&gt;\n</code></pre>\n\n<p>The reference I am following said:</p>\n\n<p>\"Just make sure that the hosts file under etc contains the loop back address, which is 127.0.0.1 (in some Linux distributions, it might be 127.0.1.1). Otherwise you might face an issue while running Apache HBase.\"</p>\n\n<p>I don't know what that is referring to honestly, because there is no ect dir or siles in the HBase download.</p>\n\n<p>Any help with this will be very much appreciated!!</p>\n", "creation_date": 1450375978, "score": 0},
{"title": "Nutch Solr dataimport handler?", "view_count": 89, "is_answered": false, "question_id": 34290214, "tags": ["hadoop", "solr", "nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/34290214/nutch-solr-dataimport-handler", "last_activity_date": 1450185176, "owner": {"user_id": 1784848, "answer_count": 13, "creation_date": 1351581947, "accept_rate": 57, "view_count": 32, "location": "NY, United States", "reputation": 52}, "body": "<p>I have setup a nutch crawler on top of hadoop. Below is software stack with their respective versions.\n<strong>apache-nutch-2.3.1</strong>, <strong>hbase-0.98.8-hadoop2</strong> Both on top of <strong>hadoop-2.5.2</strong>.\nAll process till data insertion into Hbase is working fine. The problem is when I try to invoke IndexingJob using class, org.apache.nutch.indexer.IndexingJob the command run successfully but no records got indexed in solr. Solr version is <strong>solr-5.3.1</strong>.</p>\n\n<p>Below is the output of the command I ran :</p>\n\n<pre><code>15/12/15 18:26:32 INFO mapreduce.Job: Running job: job_1450175405767_0007\n15/12/15 18:26:43 INFO mapreduce.Job: Job job_1450175405767_0007 running in uber mode : false\n15/12/15 18:26:43 INFO mapreduce.Job:  map 0% reduce 0%\n15/12/15 18:28:00 INFO mapreduce.Job:  map 50% reduce 0%\n15/12/15 18:28:22 INFO mapreduce.Job:  map 100% reduce 0%\n15/12/15 18:28:22 INFO mapreduce.Job: Job job_1450175405767_0007 completed successfully\n15/12/15 18:28:23 INFO mapreduce.Job: Counters: 31\n    File System Counters\n        FILE: Number of bytes read=0\n        FILE: Number of bytes written=230132\n        FILE: Number of read operations=0\n        FILE: Number of large read operations=0\n        FILE: Number of write operations=0\n        HDFS: Number of bytes read=1324\n        HDFS: Number of bytes written=0\n        HDFS: Number of read operations=2\n        HDFS: Number of large read operations=0\n        HDFS: Number of write operations=0\n    Job Counters \n        Killed map tasks=1\n        Launched map tasks=3\n        Data-local map tasks=3\n        Total time spent by all maps in occupied slots (ms)=192484\n        Total time spent by all reduces in occupied slots (ms)=0\n        Total time spent by all map tasks (ms)=192484\n        Total vcore-seconds taken by all map tasks=192484\n        Total megabyte-seconds taken by all map tasks=197103616\n    Map-Reduce Framework\n        Map input records=3312819\n        Map output records=0\n        Input split bytes=1324\n        Spilled Records=0\n        Failed Shuffles=0\n        Merged Map outputs=0\n        GC time elapsed (ms)=1678\n        CPU time spent (ms)=62560\n        Physical memory (bytes) snapshot=406765568\n        Virtual memory (bytes) snapshot=3877060608\n        Total committed heap usage (bytes)=239075328\n    File Input Format Counters \n        Bytes Read=0\n    File Output Format Counters \n        Bytes Written=0\n15/12/15 18:28:23 INFO indexer.IndexWriters: Adding org.apache.nutch.indexwriter.solr.SolrIndexWriter\n15/12/15 18:28:23 INFO indexer.IndexingJob: Active IndexWriters :\nSOLRIndexWriter\n    solr.server.url : URL of the SOLR instance (mandatory)\n    solr.commit.size : buffer size when sending to SOLR (default 1000)\n    solr.mapping.file : name of the mapping file for fields (default solrindex-mapping.xml)\n    solr.auth : use authentication (default false)\n    solr.auth.username : username for authentication\n    solr.auth.password : password for authentication\n\n\n15/12/15 18:28:23 INFO conf.Configuration: found resource solrindex-mapping.xml at file:/tmp/hadoop-root/hadoop-unjar491190780945254030/solrindex-mapping.xml\n15/12/15 18:28:23 INFO solr.SolrMappingReader: source: content dest: content\n15/12/15 18:28:23 INFO solr.SolrMappingReader: source: title dest: title\n15/12/15 18:28:23 INFO solr.SolrMappingReader: source: host dest: host\n15/12/15 18:28:23 INFO solr.SolrMappingReader: source: batchId dest: batchId\n15/12/15 18:28:23 INFO solr.SolrMappingReader: source: boost dest: boost\n15/12/15 18:28:23 INFO solr.SolrMappingReader: source: digest dest: digest\n15/12/15 18:28:23 INFO solr.SolrMappingReader: source: tstamp dest: tstamp\n15/12/15 18:28:23 INFO solr.SolrIndexWriter: Total 0 document is added.\n15/12/15 18:28:23 INFO indexer.IndexingJob: IndexingJob: done.\n</code></pre>\n", "creation_date": 1450185176, "score": 1},
{"title": "How to parse apache solr database", "view_count": 23, "is_answered": false, "question_id": 34281929, "tags": ["parsing", "hadoop", "solr", "nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/34281929/how-to-parse-apache-solr-database", "last_activity_date": 1450158368, "owner": {"user_id": 3454410, "answer_count": 20, "creation_date": 1395646154, "accept_rate": 48, "view_count": 174, "reputation": 832}, "body": "<p>I am using apache solr 4.10. Its data is provided via crawling by apache nutch (hadoop,hbase system). Solr is using local file system for its index storage. Now I have to parse and remove some bad documents i.e. documents that do not have content etc.</p>\n\n<p>How I can parse it. Is there any way to use hadoop mapreduce for this purpose ?</p>\n", "creation_date": 1450158368, "score": 0},
{"title": "how to get content from Solr to drupal?", "view_count": 17, "owner": {"user_id": 4823891, "answer_count": 11, "creation_date": 1429787038, "accept_rate": 57, "view_count": 18, "reputation": 93}, "is_answered": true, "answers": [{"question_id": 34239006, "owner": {"user_id": 1619004, "accept_rate": 86, "link": "http://stackoverflow.com/users/1619004/andrea", "user_type": "registered", "reputation": 1486}, "body": "<p>I guess the reason why all tutorials say that is because that's the only possible direction. Solr is a search engine and it's not supposed to be a \"source\" system for feeding another storage (like a ln RDBMS): things are supposed to be exactly in the opposite way (e.g. Nutch --> Solr, RDBMS --> Solr)</p>\n", "creation_date": 1449924115, "is_accepted": false, "score": 0, "last_activity_date": 1449924115, "answer_id": 34239946}, {"question_id": 34239006, "owner": {"user_id": 49808, "accept_rate": 67, "link": "http://stackoverflow.com/users/49808/pierre-buyle", "user_type": "registered", "reputation": 4059}, "body": "<p>You would need to build a custom module to query your Solr index using the solr HTTP API, then present the results using Drupal theming and render API.</p>\n", "creation_date": 1450124017, "is_accepted": true, "score": 0, "last_activity_date": 1450124017, "answer_id": 34275763}], "question_id": 34239006, "tags": ["drupal", "solr", "drupal-modules", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/34239006/how-to-get-content-from-solr-to-drupal", "last_activity_date": 1450124017, "accepted_answer_id": 34275763, "body": "<p>I have crawl some website with nutch and and index it with solr.Now i would like to pass these data to drupal and take use of there search interface.\nMost of the tutorial only show indexing from drupal to solr but not from solr to drupal.</p>\n\n<p>So how can i go about passing already indexed data from solr to drupal?</p>\n", "creation_date": 1449917684, "score": -2},
{"title": "Integration between Nutch 1.11(1.x) and Solr 5.3.1(5.x)", "view_count": 1539, "is_answered": false, "answers": [{"question_id": 34262293, "owner": {"user_type": "does_not_exist"}, "body": "<p>Instead, Try this statement to integrate solr and nutch </p>\n\n<pre><code>bin/nutch solrindex http://127.0.0.1:8983/solr/ crawl/crawldb -linkdb crawl/linkdb crawl/segments/\n</code></pre>\n", "creation_date": 1450088901, "is_accepted": false, "score": 0, "last_activity_date": 1450088901, "answer_id": 34264699}, {"question_id": 34262293, "owner": {"user_id": 1977773, "link": "http://stackoverflow.com/users/1977773/jorge-luis", "user_type": "registered", "reputation": 510}, "body": "<p>Have you tried specifying the Solr URL using:</p>\n\n<pre><code>-D solr.server.url=http://localhost:8983/solr/files\n</code></pre>\n\n<p>instead of the <code>-params</code> approach? At least this is the right syntax for the <code>crawl</code> script. And since both invoke an underline java class to do the work should work.</p>\n\n<pre><code>bin/nutch index crawl/crawldb \\\n-linkdb crawl/linkdb \\\n-D solr.server.url=http://127.0.0.1:8983/solr/files \\\n-dir crawl/segments\n</code></pre>\n", "creation_date": 1450093509, "is_accepted": false, "score": 0, "last_activity_date": 1450093509, "answer_id": 34266246}], "question_id": 34262293, "tags": ["solr", "nutch", "solr5"], "answer_count": 2, "link": "http://stackoverflow.com/questions/34262293/integration-between-nutch-1-111-x-and-solr-5-3-15-x", "last_activity_date": 1450093509, "owner": {"user_id": 5566589, "answer_count": 7, "creation_date": 1447658733, "accept_rate": 67, "view_count": 22, "location": "China", "reputation": 262}, "body": "<p>I just started using <strong>Nutch 1.11</strong> and <strong>Solr 5.3.1</strong>.</p>\n\n<p>I want to <strong>crawl data with Nutch</strong>, then <strong>index and prepare for searching with Solr</strong>.</p>\n\n<p>I know how to crawl data from web using <code>Nutch</code>'s <code>bin/crawl</code> command, and successfully got much data from a website in my local.</p>\n\n<p>I also started a new <code>Solr</code> server in local with below command under <code>Solr</code> root folder, </p>\n\n<pre><code>bin/solr start\n</code></pre>\n\n<p>And started the example <code>files</code> core under the example folder with below command:</p>\n\n<pre><code>bin/solr create -c files -d example/files/conf\n</code></pre>\n\n<p>And I can login below admin url and manage the <code>files</code> core,</p>\n\n<pre><code>http://localhost:8983/solr/#/files\n</code></pre>\n\n<p>So I believe I started the <code>Solr</code> correctly, and started to post the <code>Nutch</code> data into <code>Solr</code> with <code>Nutch</code>'s <code>bin/nutch index</code> command:</p>\n\n<pre><code>bin/nutch index crawl/crawldb \\\n-linkdb crawl/linkdb \\\n-params solr.server.url=127.0.0.1:8983/solr/files \\\n-dir crawl/segments\n</code></pre>\n\n<p>Hoping with <code>Solr5</code>'s new <strong>Auto Schema</strong> feature, I can put myself restful, however, I got below error(copy from log file):</p>\n\n<pre><code>WARN  util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\nINFO  segment.SegmentChecker - Segment dir is complete: file:/user/nutch/apache-nutch-1.11/crawl/segments/s1.\nINFO  segment.SegmentChecker - Segment dir is complete: file:/user/nutch/apache-nutch-1.11/crawl/segments/s2.\nINFO  segment.SegmentChecker - Segment dir is complete: file:/user/nutch/apache-nutch-1.11/crawl/segments/s3.\nINFO  indexer.IndexingJob - Indexer: starting at 2015-12-14 15:21:39\nINFO  indexer.IndexingJob - Indexer: deleting gone documents: false\nINFO  indexer.IndexingJob - Indexer: URL filtering: false\nINFO  indexer.IndexingJob - Indexer: URL normalizing: false\nINFO  indexer.IndexWriters - Adding org.apache.nutch.indexwriter.solr.SolrIndexWriter\nINFO  indexer.IndexingJob - Active IndexWriters :\nSolrIndexWriter\n    solr.server.type : Type of SolrServer to communicate with (default 'http' however options include 'cloud', 'lb' and 'concurrent')\n    solr.server.url : URL of the Solr instance (mandatory)\n    solr.zookeeper.url : URL of the Zookeeper URL (mandatory if 'cloud' value for solr.server.type)\n    solr.loadbalance.urls : Comma-separated string of Solr server strings to be used (madatory if 'lb' value for solr.server.type)\n    solr.mapping.file : name of the mapping file for fields (default solrindex-mapping.xml)\n    solr.commit.size : buffer size when sending to Solr (default 1000)\n    solr.auth : use authentication (default false)\n    solr.auth.username : username for authentication\n    solr.auth.password : password for authentication\n\n\nINFO  indexer.IndexerMapReduce - IndexerMapReduce: crawldb: crawl/crawldb\nINFO  indexer.IndexerMapReduce - IndexerMapReduce: linkdb: crawl/linkdb\nINFO  indexer.IndexerMapReduce - IndexerMapReduces: adding segment: file:/user/nutch/apache-nutch-1.11/crawl/segments/s1\nINFO  indexer.IndexerMapReduce - IndexerMapReduces: adding segment: file:/user/nutch/apache-nutch-1.11/crawl/segments/s2\nINFO  indexer.IndexerMapReduce - IndexerMapReduces: adding segment: file:/user/nutch/apache-nutch-1.11/crawl/segments/s3\nWARN  conf.Configuration - file:/tmp/hadoop-user/mapred/staging/user117437667/.staging/job_local117437667_0001/job.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.\nWARN  conf.Configuration - file:/tmp/hadoop-user/mapred/staging/user117437667/.staging/job_local117437667_0001/job.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.\nWARN  conf.Configuration - file:/tmp/hadoop-user/mapred/local/localRunner/user/job_local117437667_0001/job_local117437667_0001.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.\nWARN  conf.Configuration - file:/tmp/hadoop-user/mapred/local/localRunner/user/job_local117437667_0001/job_local117437667_0001.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.\nINFO  anchor.AnchorIndexingFilter - Anchor deduplication is: off\nINFO  indexer.IndexWriters - Adding org.apache.nutch.indexwriter.solr.SolrIndexWriter\nINFO  solr.SolrMappingReader - source: content dest: content\nINFO  solr.SolrMappingReader - source: title dest: title\nINFO  solr.SolrMappingReader - source: host dest: host\nINFO  solr.SolrMappingReader - source: segment dest: segment\nINFO  solr.SolrMappingReader - source: boost dest: boost\nINFO  solr.SolrMappingReader - source: digest dest: digest\nINFO  solr.SolrMappingReader - source: tstamp dest: tstamp\nINFO  solr.SolrIndexWriter - Indexing 250 documents\nINFO  solr.SolrIndexWriter - Deleting 0 documents\nINFO  solr.SolrIndexWriter - Indexing 250 documents\nWARN  mapred.LocalJobRunner - job_local117437667_0001\njava.lang.Exception: org.apache.solr.client.solrj.impl.HttpSolrServer$RemoteSolrException: Expected mime type application/octet-stream but got text/html. &lt;html&gt;\n&lt;head&gt;\n&lt;meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8\"/&gt;\n&lt;title&gt;Error 404 Not Found&lt;/title&gt;\n&lt;/head&gt;\n&lt;body&gt;&lt;h2&gt;HTTP ERROR 404&lt;/h2&gt;\n&lt;p&gt;Problem accessing /solr/update. Reason:\n&lt;pre&gt;    Not Found&lt;/pre&gt;&lt;/p&gt;&lt;hr&gt;&lt;i&gt;&lt;small&gt;Powered by Jetty://&lt;/small&gt;&lt;/i&gt;&lt;hr/&gt;\n\n&lt;/body&gt;\n&lt;/html&gt;\n\n    at org.apache.hadoop.mapred.LocalJobRunner$Job.runTasks(LocalJobRunner.java:462)\n    at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:529)\nCaused by: org.apache.solr.client.solrj.impl.HttpSolrServer$RemoteSolrException: Expected mime type application/octet-stream but got text/html. &lt;html&gt;\n&lt;head&gt;\n&lt;meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8\"/&gt;\n&lt;title&gt;Error 404 Not Found&lt;/title&gt;\n&lt;/head&gt;\n&lt;body&gt;&lt;h2&gt;HTTP ERROR 404&lt;/h2&gt;\n&lt;p&gt;Problem accessing /solr/update. Reason:\n&lt;pre&gt;    Not Found&lt;/pre&gt;&lt;/p&gt;&lt;hr&gt;&lt;i&gt;&lt;small&gt;Powered by Jetty://&lt;/small&gt;&lt;/i&gt;&lt;hr/&gt;\n\n&lt;/body&gt;\n&lt;/html&gt;\n\n    at org.apache.solr.client.solrj.impl.HttpSolrServer.executeMethod(HttpSolrServer.java:512)\n    at org.apache.solr.client.solrj.impl.HttpSolrServer.request(HttpSolrServer.java:210)\n    at org.apache.solr.client.solrj.impl.HttpSolrServer.request(HttpSolrServer.java:206)\n    at org.apache.solr.client.solrj.request.AbstractUpdateRequest.process(AbstractUpdateRequest.java:124)\n    at org.apache.nutch.indexwriter.solr.SolrIndexWriter.write(SolrIndexWriter.java:134)\n    at org.apache.nutch.indexer.IndexWriters.write(IndexWriters.java:85)\n    at org.apache.nutch.indexer.IndexerOutputFormat$1.write(IndexerOutputFormat.java:50)\n    at org.apache.nutch.indexer.IndexerOutputFormat$1.write(IndexerOutputFormat.java:41)\n    at org.apache.hadoop.mapred.ReduceTask$OldTrackingRecordWriter.write(ReduceTask.java:493)\n    at org.apache.hadoop.mapred.ReduceTask$3.collect(ReduceTask.java:422)\n    at org.apache.nutch.indexer.IndexerMapReduce.reduce(IndexerMapReduce.java:356)\n    at org.apache.nutch.indexer.IndexerMapReduce.reduce(IndexerMapReduce.java:56)\n    at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:444)\n    at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)\n    at org.apache.hadoop.mapred.LocalJobRunner$Job$ReduceTaskRunnable.run(LocalJobRunner.java:319)\n    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n    at java.util.concurrent.FutureTask.run(FutureTask.java:266)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n    at java.lang.Thread.run(Thread.java:745)\nERROR indexer.IndexingJob - Indexer: java.io.IOException: Job failed!\n    at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:836)\n    at org.apache.nutch.indexer.IndexingJob.index(IndexingJob.java:145)\n    at org.apache.nutch.indexer.IndexingJob.run(IndexingJob.java:222)\n    at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)\n    at org.apache.nutch.indexer.IndexingJob.main(IndexingJob.java:231)\n</code></pre>\n\n<p>I remember this </p>\n\n<pre><code>org.apache.solr.client.solrj.impl.HttpSolrServer$RemoteSolrException: Expected mime type application/octet-stream but got text/html.\n</code></pre>\n\n<p>Is something related to the <code>Solr</code> url, but I double check with the url I used <code>127.0.0.1:8983/solr/files</code>, I think it is correct.</p>\n\n<p>Does anyone know what the problem is? I search on the web and in here, got nothing useful.</p>\n\n<p>Note: I also tried the way which disabled <code>Solr5</code>'s <strong>Auto Schema</strong> feature in <code>examples/files/conf/solrconfig.xml</code> and replaced <code>examples/files/conf/managed-schema.xml</code> with <code>Nutch</code>'s <code>conf/schema.xml</code>, still hit the same error.</p>\n\n<p><strong>Update</strong>: After trying the <strong>DEPRECATED</strong> command <code>bin/nutch solrindex</code>(Thanks to <code>Thangaperumal</code>), the previous error is gone but hit another error:</p>\n\n<pre><code>bin/nutch solrindex http://127.0.0.1:8983/solr/files crawl/crawldb -linkdb crawl/linkdb crawl/segments/s1\n</code></pre>\n\n<p>Error message:</p>\n\n<pre><code>INFO  solr.SolrIndexWriter - Indexing 250 documents\nINFO  solr.SolrIndexWriter - Deleting 0 documents\nINFO  solr.SolrIndexWriter - Indexing 250 documents\nINFO  solr.SolrIndexWriter - Deleting 0 documents\nINFO  solr.SolrIndexWriter - Indexing 250 documents\nWARN  mapred.LocalJobRunner - job_local1306504137_0001\njava.lang.Exception: org.apache.solr.client.solrj.impl.HttpSolrServer$RemoteSolrException: Unable to invoke function processAdd in script: update-script.js: Can't unambiguously select between fixed arity signatures [(java.lang.String, java.io.Reader), (java.lang.String, java.lang.String)] of the method org.apache.solr.analysis.TokenizerChain.tokenStream for argument types [java.lang.String, null]\n    at org.apache.hadoop.mapred.LocalJobRunner$Job.runTasks(LocalJobRunner.java:462)\n    at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:529)\nCaused by: org.apache.solr.client.solrj.impl.HttpSolrServer$RemoteSolrException: Unable to invoke function processAdd in script: update-script.js: Can't unambiguously select between fixed arity signatures [(java.lang.String, java.io.Reader), (java.lang.String, java.lang.String)] of the method org.apache.solr.analysis.TokenizerChain.tokenStream for argument types [java.lang.String, null]\n    at org.apache.solr.client.solrj.impl.HttpSolrServer.executeMethod(HttpSolrServer.java:552)\n    at org.apache.solr.client.solrj.impl.HttpSolrServer.request(HttpSolrServer.java:210)\n    at org.apache.solr.client.solrj.impl.HttpSolrServer.request(HttpSolrServer.java:206)\n    at org.apache.solr.client.solrj.request.AbstractUpdateRequest.process(AbstractUpdateRequest.java:124)\n    at org.apache.nutch.indexwriter.solr.SolrIndexWriter.write(SolrIndexWriter.java:134)\n    at org.apache.nutch.indexer.IndexWriters.write(IndexWriters.java:85)\n    at org.apache.nutch.indexer.IndexerOutputFormat$1.write(IndexerOutputFormat.java:50)\n    at org.apache.nutch.indexer.IndexerOutputFormat$1.write(IndexerOutputFormat.java:41)\n    at org.apache.hadoop.mapred.ReduceTask$OldTrackingRecordWriter.write(ReduceTask.java:493)\n    at org.apache.hadoop.mapred.ReduceTask$3.collect(ReduceTask.java:422)\n    at org.apache.nutch.indexer.IndexerMapReduce.reduce(IndexerMapReduce.java:356)\n    at org.apache.nutch.indexer.IndexerMapReduce.reduce(IndexerMapReduce.java:56)\n    at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:444)\n    at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)\n    at org.apache.hadoop.mapred.LocalJobRunner$Job$ReduceTaskRunnable.run(LocalJobRunner.java:319)\n    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n    at java.util.concurrent.FutureTask.run(FutureTask.java:266)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n    at java.lang.Thread.run(Thread.java:745)\nERROR indexer.IndexingJob - Indexer: java.io.IOException: Job failed!\n    at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:836)\n    at org.apache.nutch.indexer.IndexingJob.index(IndexingJob.java:145)\n    at org.apache.nutch.indexer.IndexingJob.run(IndexingJob.java:222)\n    at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)\n    at org.apache.nutch.indexer.IndexingJob.main(IndexingJob.java:231)\n</code></pre>\n", "creation_date": 1450080712, "score": 4},
{"title": "Nutch 2.1 urls injection takes forever", "view_count": 630, "owner": {"user_id": 3381124, "view_count": 7, "answer_count": 0, "creation_date": 1393972003, "reputation": 35}, "is_answered": true, "answers": [{"last_edit_date": 1449877398, "owner": {"user_id": 2467397, "accept_rate": 62, "link": "http://stackoverflow.com/users/2467397/frank", "user_type": "registered", "reputation": 121}, "body": "<p>Ubuntu defaults the loopback IP address in hosts to 127.0.1.1. HBase (according to <a href=\"http://hbase.apache.org/book/quickstart.html\" rel=\"nofollow\">this page</a>) requires your loopback IP address be 127.0.0.1.</p>\n\n<p>The Ubuntu <code>/etc/hosts</code> file by default contains (with myComputerName being your computer name):</p>\n\n<pre><code>127.0.0.1   localhost\n127.0.1.1   myComputerName\n</code></pre>\n\n<p>Use <code>sudo gedit /etc/hosts</code> to update your hosts file as follow:</p>\n\n<pre><code>127.0.0.1   localhost\n127.0.0.1   myComputerName\n</code></pre>\n\n<p>Reboot Ubuntu. Nutch should no longer have trouble injecting urls into HBase.</p>\n", "question_id": 23050000, "creation_date": 1402419071, "is_accepted": true, "score": 2, "last_activity_date": 1449877398, "answer_id": 24146669}], "question_id": 23050000, "tags": ["apache", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/23050000/nutch-2-1-urls-injection-takes-forever", "last_activity_date": 1449877398, "accepted_answer_id": 24146669, "body": "<p>I'm trying to deploy nutch 2.1 on Ubuntu 12.04 by following that <a href=\"https://github.com/renepickhardt/metalcon/wiki/simpleNutchSolrSetup/\" rel=\"nofollow\">tutorial</a>. Everything goes well\nuntil I try to inject urls into the database. When I type ($bin/nutch inject urls) and press\nEnter I get</p>\n\n<pre><code>    InjectorJob: starting\n    InjectorJob: urlDir: urls\n</code></pre>\n\n<p>and remains there (for hours) until I decide to cancel the execution. urls is a directory\nthat contains file with urls.  I added proxy and port details in the nutch-site.xml as suggested <a href=\"http://stackoverflow.com/questions/22586950/nutch-2-2-1-doesnt-continue-after-injector-job?answertab=active#tab-top/\">here</a> but it doesn't solve. I tried apache nutch 2.2.1 and the issue continues.</p>\n\n<p>If you know how to fix that issue, please, help me!</p>\n\n<p>Thanks in advance. </p>\n", "creation_date": 1397434020, "score": 4},
{"title": "Nutch (2.2.1) Inject Urls Hangs", "view_count": 764, "owner": {"user_id": 2467397, "answer_count": 3, "creation_date": 1370740223, "accept_rate": 62, "view_count": 14, "reputation": 121}, "is_answered": true, "answers": [{"last_edit_date": 1449877393, "owner": {"user_id": 2467397, "accept_rate": 62, "link": "http://stackoverflow.com/users/2467397/frank", "user_type": "registered", "reputation": 121}, "body": "<p>Ubuntu defaults the loopback IP address in hosts to 127.0.1.1. HBase (according to <a href=\"http://hbase.apache.org/book/quickstart.html\" rel=\"nofollow\">this page</a>) requires your loopback IP address be 127.0.0.1.</p>\n\n<p>The Ubuntu <code>/etc/hosts</code> file by default contains (with myComputerName being your computer name):</p>\n\n<pre><code>127.0.0.1   localhost\n127.0.1.1   myComputerName\n</code></pre>\n\n<p>Use <code>sudo gedit /etc/hosts</code> to update your hosts file as follow:</p>\n\n<pre><code>127.0.0.1   localhost\n127.0.0.1   myComputerName\n</code></pre>\n\n<p>Reboot Ubuntu. Nutch should no longer have trouble injecting urls into HBase.</p>\n", "question_id": 24121987, "creation_date": 1402418690, "is_accepted": true, "score": 2, "last_activity_date": 1449877393, "answer_id": 24146563}], "question_id": 24121987, "tags": ["solr", "hbase", "nutch", "ubuntu-14.04"], "answer_count": 1, "link": "http://stackoverflow.com/questions/24121987/nutch-2-2-1-inject-urls-hangs", "last_activity_date": 1449877393, "accepted_answer_id": 24146563, "body": "<p>I'm running Ubuntu 14.04, I'm tying to get a basic Nutch Web Crawl running to no avail. Following <a href=\"https://github.com/renepickhardt/metalcon/wiki/simpleNutchSolrSetup\" rel=\"nofollow\">this</a> tutorial I set up the following building blocks:</p>\n\n<ul>\n<li>Ubuntu 14.04</li>\n<li>HBase 0.90.4</li>\n<li>Nutch 2.2.1</li>\n<li>Solr 4.3.1</li>\n</ul>\n\n<p>I confirm both HBase and Solr is running, I populate the <code>urls/seed.txt</code> file. Then when I call;</p>\n\n<pre><code>bin/nutch inject urls\n</code></pre>\n\n<p>I'm presented with the following output and then it seems Nutch just hangs.</p>\n\n<pre><code>InjectorJob: starting at 2014-06-09 23:38:49\nInjectorJob: Injecting urlDir: urls/seed.txt\n</code></pre>\n\n<p><a href=\"http://stackoverflow.com/questions/22586950/nutch-2-2-1-doesnt-continue-after-injector-job\">This</a> stackoverflow question seems similar to mine, I am however not behind a proxy so the answer is not applicable.</p>\n\n<p>Any help in resolving this issue would be greatly appreciated.</p>\n", "creation_date": 1402322690, "score": 1},
{"title": "Homework - How to fetch pages only for generating new urls?", "view_count": 20, "is_answered": false, "question_id": 34107692, "tags": ["solr", "web-crawler", "nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/34107692/homework-how-to-fetch-pages-only-for-generating-new-urls", "last_activity_date": 1449332099, "owner": {"user_id": 2811496, "answer_count": 5, "creation_date": 1380032058, "accept_rate": 71, "view_count": 45, "reputation": 114}, "body": "<p>I'm trying to crawl Wikipedia with Nutch for a project and I've noticed that a substantial part of the pages being fetched are good for the crawler (e.g. portals and lists, which link to actual articles), but quite bad for the search engine (Solr) that will use the indexed data (most queries lead to these lists of links and not to actual articles).</p>\n\n<p>Is there a way to tell Nutch to fetch pages matching a specific regex only for acquiring new URLs?</p>\n", "creation_date": 1449332099, "score": 0},
{"title": "Nutch 2.3 has an old version of hbase jar in runtime/lib folder", "view_count": 126, "owner": {"user_id": 1780704, "answer_count": 39, "creation_date": 1351424995, "accept_rate": 80, "view_count": 168, "reputation": 465}, "is_answered": true, "answers": [{"question_id": 32624873, "owner": {"user_id": 47401, "accept_rate": 80, "link": "http://stackoverflow.com/users/47401/diadistis", "user_type": "registered", "reputation": 9116}, "body": "<p>You can't solve it by just replacing jars/bindings. The problem lies with Gora that uses pre-1.0 client that is incompatible with HBase 1.0+ versions. Your options right now are to either update Gora (not just the jars, coding is required), wait for an official update or downgrade your HBase server.</p>\n\n<p><a href=\"https://issues.apache.org/jira/browse/GORA-443\" rel=\"nofollow\">Related jira issue</a></p>\n\n<p><a href=\"http://www.slideshare.net/enissoz/meet-hbase-10\" rel=\"nofollow\">HBase 1.0+ changes</a></p>\n", "creation_date": 1449269312, "is_accepted": true, "score": 1, "last_activity_date": 1449269312, "answer_id": 34098783}], "question_id": 32624873, "tags": ["hadoop", "solr", "hbase", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/32624873/nutch-2-3-has-an-old-version-of-hbase-jar-in-runtime-lib-folder", "last_activity_date": 1449269312, "accepted_answer_id": 34098783, "body": "<p>Nutch 2.3 has an old version of hbase jar file(hbase-0.94.14.jar) in runtime/lib folder. I have downloaded hbase 0.94.14 but it does not start because of \"Server IPC version 9 cannot communicate with client version 4\". I can run Hbase 1.1.2 but jar file does not included.</p>\n\n<p>How can I solve this?</p>\n\n<p>Thanks</p>\n", "creation_date": 1442475372, "score": 1},
{"title": "How can i generate a link graph and page rank from scrapy crawl?", "view_count": 75, "is_answered": false, "question_id": 34074967, "tags": ["python", "solr", "lucene", "scrapy", "nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/34074967/how-can-i-generate-a-link-graph-and-page-rank-from-scrapy-crawl", "last_activity_date": 1449172427, "owner": {"user_id": 5405141, "view_count": 0, "answer_count": 0, "creation_date": 1443898981, "reputation": 33}, "body": "<p>As part of my coursework project in information retrieval , i have developed a small search engine (~ 150k web pages).I used scrapy for crawl and used apache solr for indexing . Now my professor is asking for pagerank to be used for scoring. Now i don't know how i can get the page ranks for pages i crawled  using scrapy. so my question is is there a way to generate the link graph and pagerank from scrapy data. I believe i can generate the page ranks using networkx but before that i think i need to build the link graph. is there a way to  generate the link graph with scrapy data. or should i just replace scrapy with apache nutch for crawl which has support for pagerank?</p>\n", "creation_date": 1449172427, "score": 0},
{"title": "How to modify the index template used by the nutch index writer for elasticsearch?", "view_count": 177, "is_answered": true, "answers": [{"last_edit_date": 1449149308, "owner": {"user_id": 4604579, "link": "http://stackoverflow.com/users/4604579/val", "user_type": "registered", "reputation": 42575}, "body": "<p>Welcome to StackOverflow !!</p>\n\n<p>Here's my take at your questions:</p>\n\n<ol>\n<li><p>It doesn't look like Nutch creates any template. Here is the source code for <a href=\"https://github.com/apache/nutch/blob/c73a416d64a6675e5423ca13be435e284c408ef0/src/plugin/indexer-elastic/src/java/org/apache/nutch/indexwriter/elastic/ElasticIndexWriter.java\" rel=\"nofollow\"><code>ElasticIndexWriter</code></a> and as you can see there's no reference to any template anywhere.</p></li>\n<li><p>Since Nutch doesn't create any index template, you can't change it... but you can definitely create one yourself directly in your ES cluster, if you want/need to control the mapping of certain fields.</p></li>\n</ol>\n\n<p>You can start off the default mapping created by Nutch (i.e. the one you've pasted in your question) and iterate on that. Creating a template out of it is trivial, i.e. you just add the <code>\"template\": \"nutch*\"</code> property (first line below) and you're good to go (some more info available on how to change mappings <a href=\"https://www.elastic.co/guide/en/elasticsearch/reference/current/mapping.html\" rel=\"nofollow\">available here</a>):</p>\n\n<pre><code>curl -XPUT localhost:9200/_template/nutch_template -d '{\n  \"template\": \"nutch*\",\n  \"mappings\": {\n    \"doc\": {\n      \"properties\": {\n        \"anchor\": {\n          \"type\": \"string\"\n        },\n        \"boost\": {\n          \"type\": \"string\"\n        },\n        \"cache\": {\n          \"type\": \"string\"\n        },\n        \"content\": {\n          \"type\": \"string\"\n        },\n        \"contentLength\": {\n          \"type\": \"string\"\n        },\n        \"date\": {\n          \"type\": \"date\",\n          \"format\": \"dateOptionalTime\"\n        },\n        \"digest\": {\n          \"type\": \"string\"\n        },\n        \"host\": {\n          \"type\": \"string\"\n        },\n        \"id\": {\n          \"type\": \"string\"\n        },\n        \"lang\": {\n          \"type\": \"string\"\n        },\n        \"lastModified\": {\n          \"type\": \"date\",\n          \"format\": \"dateOptionalTime\"\n        },\n        \"segment\": {\n          \"type\": \"string\"\n        },\n        \"title\": {\n          \"type\": \"string\"\n        },\n        \"tstamp\": {\n          \"type\": \"date\",\n          \"format\": \"dateOptionalTime\"\n        },\n        \"type\": {\n          \"type\": \"string\"\n        },\n        \"url\": {\n          \"type\": \"string\"\n        }\n      }\n    }\n  }\n}'\n</code></pre>\n\n<p>3-4. There is a description of all the fields indexed/stored by Nutch <a href=\"https://wiki.apache.org/nutch/IndexStructure\" rel=\"nofollow\">in their wiki</a>, so you can modify the mapping above in order to store/index certain fields differently to match your exact needs.</p>\n\n<p>Note: make sure to wipe your current <code>nutch</code> index first, then create your template (point 2 above) and then when Nutch will index its first document, the index will be created automatically.</p>\n\n<p>You might also be interested in looking into the issue <a href=\"https://issues.apache.org/jira/browse/FLUME-2787\" rel=\"nofollow\">FLUME-2787</a> as someone else seems to have gone through template creation himself. You might find some nuggets in there.</p>\n", "question_id": 34065863, "creation_date": 1449148362, "is_accepted": false, "score": 1, "last_activity_date": 1449149308, "answer_id": 34066986}], "question_id": 34065863, "tags": ["templates", "elasticsearch", "nutch", "mappings", "indexwriter"], "answer_count": 1, "link": "http://stackoverflow.com/questions/34065863/how-to-modify-the-index-template-used-by-the-nutch-index-writer-for-elasticsearc", "last_activity_date": 1449149308, "owner": {"user_id": 5634841, "view_count": 7, "answer_count": 5, "creation_date": 1449143354, "reputation": 26}, "body": "<p>Out of the box the nutch index writer for elasticsearch generates an index in elasticsearch with the name provided in nutch-site.xml (or nutch-default.xml) in the property element: </p>\n\n<pre><code>   &lt;property&gt; \n     &lt;name&gt;elastic.index&lt;/name&gt;\n     &lt;value&gt;nutch&lt;/value&gt; \n     &lt;description&gt;Default index to send documents to.&lt;/description&gt;\n   &lt;/property&gt;\n</code></pre>\n\n<p>The mappings section in elasticsearch for such an automatically generated index always has the following structure</p>\n\n<pre><code>   {\n       \"nutch\": {\n           \"mappings\": {\n               \"doc\": {\n                   \"properties\": {\n                       \"anchor\": {\n                           \"type\": \"string\"\n                       },\n                       \"boost\": {\n                           \"type\": \"string\"\n                       },\n                       \"cache\": {\n                           \"type\": \"string\"\n                       },\n                       \"content\": {\n                           \"type\": \"string\"\n                       },\n                       \"contentLength\": {\n                           \"type\": \"string\"\n                       },\n                       \"date\": {\n                           \"type\": \"date\",\n                           \"format\": \"dateOptionalTime\"\n                       },\n                       \"digest\": {\n                           \"type\": \"string\"\n                       },\n                       \"host\": {\n                           \"type\": \"string\"\n                       },\n                       \"id\": {\n                           \"type\": \"string\"\n                       },\n                       \"lang\": {\n                           \"type\": \"string\"\n                       },\n                       \"lastModified\": {\n                           \"type\": \"date\",\n                           \"format\": \"dateOptionalTime\"\n                       },\n                       \"segment\": {\n                           \"type\": \"string\"\n                       },\n                       \"title\": {\n                           \"type\": \"string\"\n                       },\n                       \"tstamp\": {\n                           \"type\": \"date\",\n                           \"format\": \"dateOptionalTime\"\n                       },\n                       \"type\": {\n                           \"type\": \"string\"\n                       },\n                       \"url\": {\n                           \"type\": \"string\"\n                       }\n                   }\n               }\n           }\n       }\n   }\n</code></pre>\n\n<ol>\n<li>Where is the template for this?</li>\n<li>Can it be changed?</li>\n<li>If yes, which fields are mandatory and which are optional?</li>\n<li>Where can I find more information on this?</li>\n</ol>\n\n<p>Any help appreciated!\nThanks, Wolfram</p>\n", "creation_date": 1449144983, "score": 1},
{"title": "Nutch Crawling Path - View Hops in solr", "view_count": 28, "is_answered": true, "answers": [{"question_id": 34062478, "owner": {"user_id": 432844, "link": "http://stackoverflow.com/users/432844/julien-nioche", "user_type": "registered", "reputation": 939}, "body": "<p>Nutch keeps the relations between pages in the linkdb, however it is not used by default when indexing. The easiest way to do this would be to write a custom plugin or hack the scoring-depth one so that it keeps the parent link.</p>\n\n<p>The depth can be tracked by activating the scoring-depth plugin, you could combine it with the index-metadata plugin to store the metadata '<em>depth</em>' into the index.</p>\n\n<p>You might find it easier to do this with <a href=\"https://github.com/DigitalPebble/storm-crawler\" rel=\"nofollow\">StormCrawler</a> instead as it tracks the depth and the full path by default. Those can then be indexed into SOLR simply by specifying the key names in indexer.md.filter.</p>\n", "creation_date": 1449138520, "is_accepted": false, "score": 1, "last_activity_date": 1449138520, "answer_id": 34063490}], "question_id": 34062478, "tags": ["solr", "web-crawler", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/34062478/nutch-crawling-path-view-hops-in-solr", "last_activity_date": 1449138520, "owner": {"age": 27, "answer_count": 1, "creation_date": 1373145571, "user_id": 2557061, "accept_rate": 57, "view_count": 11, "reputation": 56}, "body": "<p>Is there a possibility to track in which depth nutch found the link and what is the parent link of this one. </p>\n\n<p>It would be very interesting for my project to see in solr where did the crawler came from and maybe as a result I can create a dependency tree in which the user can see where how this link is connected to the root.</p>\n\n<p>Is the linked data an option in nutch or do I need another programm to manage this?</p>\n", "creation_date": 1449135622, "score": 0},
{"title": "Nutch2.0 dependency setting", "view_count": 286, "is_answered": false, "question_id": 12798325, "tags": ["hadoop", "nutch", "gora"], "answer_count": 0, "link": "http://stackoverflow.com/questions/12798325/nutch2-0-dependency-setting", "last_activity_date": 1448460741, "owner": {"user_id": 1728541, "view_count": 1, "answer_count": 0, "creation_date": 1349692657, "reputation": 6}, "body": "<p>I am crawling the URLs with Nutch2.0 in deploy mode:</p>\n\n<ol>\n<li>I build the Nutch with Ant </li>\n<li>I used Nutch-2.0.job with Hadoop. </li>\n</ol>\n\n<p>I ran the following commands:</p>\n\n<ol>\n<li><p><code>sudo -u hdfs hadoop jar /home/bluesky/nutch/runtime/deploy/apache-nutch-2.0.job org.apache.nutch.crawl.InjectorJob /user/bluesky/nutch/urls</code></p></li>\n<li><p><code>sudo -u hdfs hadoop jar /home/bluesky/nutch/runtime/deploy/apache-nutch-2.0.job org.apache.nutch.crawl.GeneratorJob</code> </p></li>\n<li><p><code>sudo -u hdfs hadoop jar /home/bluesky/nutch/runtime/deploy/apache-nutch-2.0.job org.apache.nutch.fetcher.FetcherJob (batch -id)</code></p></li>\n</ol>\n\n<p>But when I ran the last command, it is giving me error in logs: </p>\n\n<pre><code>2012-10-09 15:35:04,292 ERROR org.apache.nutch.fetcher.FetcherJob: Unexpected error for http://www.imdb.com/title/tt0327056/\njava.lang.AbstractMethodError: org.apache.gora.persistency.ListGenericArray.size()I\n        at org.apache.avro.generic.GenericDatumWriter.getArraySize(GenericDatumWriter.java:142)\n        at org.apache.avro.generic.GenericDatumWriter.writeArray(GenericDatumWriter.java:128)\n        at org.apache.avro.generic.GenericDatumWriter.write(GenericDatumWriter.java:66)\n        at org.apache.gora.avro.PersistentDatumWriter.write(PersistentDatumWriter.java:61)\n        at org.apache.avro.generic.GenericDatumWriter.writeRecord(GenericDatumWriter.java:102)\n        at org.apache.gora.avro.PersistentDatumWriter.writeRecord(PersistentDatumWriter.java:74)\n        at org.apache.avro.generic.GenericDatumWriter.write(GenericDatumWriter.java:64)\n        at org.apache.gora.avro.PersistentDatumWriter.write(PersistentDatumWriter.java:61)\n        at org.apache.gora.util.IOUtils.serialize(IOUtils.java:170)\n        at org.apache.gora.sql.store.SqlStore.setField(SqlStore.java:718)\n        at org.apache.gora.sql.store.SqlStore.setObject(SqlStore.java:669)\n        at org.apache.gora.sql.statement.MySqlInsertUpdateStatement.toStatement(MySqlInsertUpdateStatement.java:96)\n        at org.apache.gora.sql.store.SqlStore.put(SqlStore.java:616)\n        at org.apache.gora.mapreduce.GoraRecordWriter.write(GoraRecordWriter.java:60)\n        at org.apache.hadoop.mapred.ReduceTask$NewTrackingRecordWriter.write(ReduceTask.java:514)\n        at org.apache.hadoop.mapreduce.TaskInputOutputContext.write(TaskInputOutputContext.java:80)\n        at org.apache.nutch.fetcher.FetcherReducer$FetcherThread.output(FetcherReducer.java:668)\n        at org.apache.nutch.fetcher.FetcherReducer$FetcherThread.run(FetcherReducer.java:540)\n2012-10-09 15:35:04,293 ERROR org.apache.nutch.fetcher.FetcherJob: fetcher throwable caught\njava.lang.AbstractMethodError: org.apache.gora.persistency.ListGenericArray.size()I\n        at org.apache.avro.generic.GenericDatumWriter.getArraySize(GenericDatumWriter.java:142)\n        at org.apache.avro.generic.GenericDatumWriter.writeArray(GenericDatumWriter.java:128)\n        at org.apache.avro.generic.GenericDatumWriter.write(GenericDatumWriter.java:66)\n        at org.apache.gora.avro.PersistentDatumWriter.write(PersistentDatumWriter.java:61)\n        at org.apache.avro.generic.GenericDatumWriter.writeRecord(GenericDatumWriter.java:102)\n        at org.apache.gora.avro.PersistentDatumWriter.writeRecord(PersistentDatumWriter.java:74)\n        at org.apache.avro.generic.GenericDatumWriter.write(GenericDatumWriter.java:64)\n        at org.apache.gora.avro.PersistentDatumWriter.write(PersistentDatumWriter.java:61)\n        at org.apache.gora.util.IOUtils.serialize(IOUtils.java:170)\n        at org.apache.gora.sql.store.SqlStore.setField(SqlStore.java:718)\n        at org.apache.gora.sql.store.SqlStore.setObject(SqlStore.java:669)\n        at org.apache.gora.sql.statement.MySqlInsertUpdateStatement.toStatement(MySqlInsertUpdateStatement.java:96)\n        at org.apache.gora.sql.store.SqlStore.put(SqlStore.java:616)\n        at org.apache.gora.mapreduce.GoraRecordWriter.write(GoraRecordWriter.java:60)\n</code></pre>\n", "creation_date": 1349779438, "score": 1},
{"title": "Can&#39;t Integrate Solr with Nutch", "view_count": 17, "is_answered": false, "answers": [{"question_id": 33890187, "owner": {"user_id": 432844, "link": "http://stackoverflow.com/users/432844/julien-nioche", "user_type": "registered", "reputation": 939}, "body": "<p>Your problem has nothing to do with SOLR but is due to the fact that the segments have not been parsed.  Use the crawl script, it will run the steps in the right order.</p>\n", "creation_date": 1448446374, "is_accepted": false, "score": 0, "last_activity_date": 1448446374, "answer_id": 33913705}], "question_id": 33890187, "tags": ["apache", "hadoop", "solr", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/33890187/cant-integrate-solr-with-nutch", "last_activity_date": 1448446374, "owner": {"user_id": 5597698, "view_count": 0, "answer_count": 0, "creation_date": 1448333034, "reputation": 1}, "body": "<p>I crawl the data from Nutch to be but have not put the data displayed on Solr.<br>\nHow to integrate Solr with Nutch?<br>\n<a href=\"http://i.stack.imgur.com/sicA4.png\" rel=\"nofollow\">Console Image</a></p>\n", "creation_date": 1448358296, "score": 0},
{"title": "How to index the Nutch Output to Solr?", "view_count": 54, "is_answered": false, "question_id": 33886044, "tags": ["java", "solr", "nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/33886044/how-to-index-the-nutch-output-to-solr", "last_activity_date": 1448369121, "owner": {"user_id": 5427112, "view_count": 0, "answer_count": 0, "creation_date": 1444385538, "reputation": 1}, "body": "<p>I am using Nutch 1.7 and Solr 4.7.0 in my Java application. I am able to crawl a website using the following code:</p>\n\n<pre><code>ToolRunner.run(NutchConfiguration.create(), new Crawl(), tokenize(crawlArg));\n</code></pre>\n\n<p>which I referred from: <a href=\"http://cmusphinx.sourceforge.net/2012/06/building-a-java-application-with-apache-nutch-and-solr/\" rel=\"nofollow\">http://cmusphinx.sourceforge.net/2012/06/building-a-java-application-with-apache-nutch-and-solr/</a>. </p>\n\n<p>Now I would like to 'index' the content in crawldb, segments* to SOLR using the following code:</p>\n\n<pre><code>String indexArg = \"local crawl/crawldb -linkdb crawl/linkdb crawl/segments/*\";\nToolRunner.run(NutchConfiguration.create(), new SolrIndexer(server), tokenize(indexArg));\n</code></pre>\n\n<p>My problem is that Nuch 1.7 does not contain the class - SolrIndexer. The above tutorial uses Nutch 1.5 . Can anyone help me with the proper class to use for indexing to Solr using Nutch 1.7</p>\n\n<p>Thanks &amp; Regards,\nMurali</p>\n", "creation_date": 1448342753, "score": 0},
{"title": "&quot;ant run&quot; failed when I compile nutch", "view_count": 127, "is_answered": false, "question_id": 33851874, "tags": ["ant", "nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/33851874/ant-run-failed-when-i-compile-nutch", "last_activity_date": 1448169482, "owner": {"user_id": 4902900, "answer_count": 0, "creation_date": 1431675815, "view_count": 0, "location": "Shanghai, China", "reputation": 1}, "body": "<p>I followed this book web <strong>crawling and data mining with apache nutch</strong> and want to install nutch.</p>\n\n<blockquote>\n  <p><strong>1. hbase-site.xml</strong> add:</p>\n</blockquote>\n\n<pre><code>&lt;property&gt;\n   &lt;name&gt;hbase.rootdir&lt;/name&gt;\n   &lt;value&gt;/home/wangjiang-fedora/HbaseRootDir&lt;/value&gt;\n&lt;/properity&gt;\n&lt;property&gt;\n   &lt;name&gt;hbase.zookeeper.property.dataDir&lt;/name&gt;\n   &lt;value&gt;/home/wangjiang-fedora/HbaseZookeeper&lt;/value&gt;\n&lt;/properity&gt;\n</code></pre>\n\n<blockquote>\n  <p><strong>2. nutch-site.xml</strong> add:</p>\n</blockquote>\n\n<pre><code>&lt;property&gt;\n   &lt;name&gt;storage.data.store.class&lt;/name&gt;\n   &lt;value&gt;org.apache.gora.hbase.store.HBaseStore&lt;/value&gt;\n   &lt;description&gt;Default class for storing data&lt;/description&gt;\n&lt;/property&gt;\n</code></pre>\n\n<blockquote>\n  <ol start=\"3\">\n  <li><strong>nutch/ivy/ivy.xml</strong>  add </li>\n  </ol>\n</blockquote>\n\n<pre><code>&lt;dependency org=\"org.apache.gora\" name=\"gora-hbase\" rev=\"0.2\" conf=\"*-\n&gt;default\" /&gt;\n</code></pre>\n\n<blockquote>\n  <p>4.Set <strong>gora.properties</strong></p>\n</blockquote>\n\n<pre><code>gora.datastore.default=org.apache.gora.hbase.store.HBaseStore\n</code></pre>\n\n<blockquote>\n  <p>5.run <strong>ant runtime</strong> in my fedora shell.and then show these results:</p>\n</blockquote>\n\n<pre><code>[root@localhost apache-nutch-2.3]# ant runtime\nBuildfile: /opt/apache-nutch-2.3/build.xml\nTrying to override old definition of task javac\n[taskdef] Could not load definitions from resource org/sonar/ant/antlib.xml. It could not be found.\n\nivy-probe-antlib:\n\nivy-download:\n[taskdef] Could not load definitions from  resource org/sonar/ant/antlib.xml. It could not be found.\n\nivy-download-unchecked:\n\nivy-init-antlib:\n\nivy-init:\n\ninit:\n\nclean-lib:\n\nresolve-default:\n[ivy:resolve] :: Apache Ivy 2.3.0 - 20130110142753 ::        http://ant.apache.org/ivy/ ::\n[ivy:resolve] :: loading settings :: file = /opt/apache-nutch-b          2.3/ivy/ivysettings.xml\n......\n</code></pre>\n\n<p>and then,it waited here for a long long time..What is wrong?What can I do next? thanks..(sorry,my English is poor)</p>\n", "creation_date": 1448169482, "score": 0},
{"title": "Hadoop - EMR not submitting job when called through java code", "view_count": 24, "is_answered": false, "question_id": 33839789, "tags": ["hadoop", "amazon-web-services", "nutch", "emr", "amazon-emr"], "answer_count": 0, "link": "http://stackoverflow.com/questions/33839789/hadoop-emr-not-submitting-job-when-called-through-java-code", "last_activity_date": 1448081892, "owner": {"user_id": 3999239, "view_count": 11, "answer_count": 1, "creation_date": 1409639589, "reputation": 21}, "body": "<p>I'm trying to use the program <a href=\"http://nutch.apache.org/\" rel=\"nofollow\">apache-nutch 2.3</a>, which uses hadoop 1.2.0, on hadoop on EMR (Hadoop distribution:Amazon 1.0.3). The program runs fine when built and run normally. But when I spin up the internal server, and try to run the job through its REST api, the program just hangs before the map and reduce stages. </p>\n\n<p>A look at the NameNode logs suggests that the job.jar is getting created in the staging directory, but is not getting copied to the DataNodes. How can I solve this problem?</p>\n\n<h2>More Info</h2>\n\n<p><strong>When run Locally</strong></p>\n\n<p>Command used to run the job locally:</p>\n\n<pre><code>hadoop -jar apache-nutch-2.3.job org.apache.nutch.crawl.InjectorJob &lt;args&gt;\n</code></pre>\n\n<p>Namenode logs when run locally:</p>\n\n<pre><code>2015-11-20 12:27:24,911 INFO org.apache.hadoop.hdfs.StateChange (IPC Server handler 34 on 9000): BLOCK* NameSystem.allocateBlock: /mnt/var/lib/hadoop/tmp/mapred/staging/hadoop/.staging/job_201511091343_0203/job.jar. blk_3454340310393431861_2515\n2015-11-20 12:27:25,841 INFO org.apache.hadoop.hdfs.StateChange (IPC Server handler 25 on 9000): BLOCK* NameSystem.addStoredBlock: blockMap updated: 172.31.55.135:9200 is added to blk_3454340310393431861_2515 size 134217728\n2015-11-20 12:27:25,844 INFO org.apache.hadoop.hdfs.StateChange (IPC Server handler 30 on 9000): BLOCK* NameSystem.allocateBlock: /mnt/var/lib/hadoop/tmp/mapred/staging/hadoop/.staging/job_201511091343_0203/job.jar. blk_7066227555451084927_2515\n2015-11-20 12:27:25,994 INFO org.apache.hadoop.hdfs.StateChange (IPC Server handler 23 on 9000): BLOCK* NameSystem.addStoredBlock: blockMap updated: 172.31.55.135:9200 is added to blk_7066227555451084927_2515 size 24992533\n2015-11-20 12:27:25,995 INFO org.apache.hadoop.hdfs.StateChange (IPC Server handler 48 on 9000): Removing lease on  file /mnt/var/lib/hadoop/tmp/mapred/staging/hadoop/.staging/job_201511091343_0203/job.jar from client DFSClient_-2027758209\n2015-11-20 12:27:25,995 INFO org.apache.hadoop.hdfs.StateChange (IPC Server handler 48 on 9000): DIR* NameSystem.completeFile: file /mnt/var/lib/hadoop/tmp/mapred/staging/hadoop/.staging/job_201511091343_0203/job.jar is closed by DFSClient_-2027758209\n2015-11-20 12:27:25,997 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem (IPC Server handler 52 on 9000): Increasing replication for file /mnt/var/lib/hadoop/tmp/mapred/staging/hadoop/.staging/job_201511091343_0203/job.jar. New replication is 10\n2015-11-20 12:27:26,754 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem (IPC Server handler 59 on 9000): Increasing replication for file /mnt/var/lib/hadoop/tmp/mapred/staging/hadoop/.staging/job_201511091343_0203/job.split. New replication is 10\n2015-11-20 12:27:26,759 INFO org.apache.hadoop.hdfs.StateChange (IPC Server handler 43 on 9000): BLOCK* NameSystem.allocateBlock: /mnt/var/lib/hadoop/tmp/mapred/staging/hadoop/.staging/job_201511091343_0203/job.split. blk_514847027605235509_2516\n2015-11-20 12:27:26,765 INFO org.apache.hadoop.hdfs.StateChange (IPC Server handler 42 on 9000): BLOCK* NameSystem.addStoredBlock: blockMap updated: 172.31.55.135:9200 is added to blk_514847027605235509_2516 size 105\n2015-11-20 12:27:26,766 INFO org.apache.hadoop.hdfs.StateChange (IPC Server handler 57 on 9000): BLOCK* NameSystem.addStoredBlock: blockMap updated: 172.31.55.136:9200 is added to blk_514847027605235509_2516 size 105\n2015-11-20 12:27:26,781 INFO org.apache.hadoop.hdfs.StateChange (IPC Server handler 61 on 9000): Removing lease on  file /mnt/var/lib/hadoop/tmp/mapred/staging/hadoop/.staging/job_201511091343_0203/job.split from client DFSClient_-2027758209\n2015-11-20 12:27:26,781 INFO org.apache.hadoop.hdfs.StateChange (IPC Server handler 61 on 9000): DIR* NameSystem.completeFile: file /mnt/var/lib/hadoop/tmp/mapred/staging/hadoop/.staging/job_201511091343_0203/job.split is closed by DFSClient_-2027758209\n2015-11-20 12:27:26,787 INFO org.apache.hadoop.hdfs.StateChange (IPC Server handler 44 on 9000): BLOCK* NameSystem.allocateBlock: /mnt/var/lib/hadoop/tmp/mapred/staging/hadoop/.staging/job_201511091343_0203/job.splitmetainfo. blk_333372377508863564_2517\n...other files in staging\n</code></pre>\n\n<p>Datanode logs when run locally:</p>\n\n<pre><code>2015-11-20 12:27:25,851 INFO org.apache.hadoop.hdfs.server.datanode.DataNode (org.apache.hadoop.hdfs.server.datanode.DataXceiver@1aea8ec2): Receiving block blk_7066227555451084927_2515 src: /1xx.xx.xx.xx:50456 dest: /1xx.xx.xx.xxx:9200\n2015-11-20 12:27:25,998 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace (PacketResponder 0 for Block blk_7066227555451084927_2515): src: /1xx.xx.xx.xx:50456, dest: /172.31.55.135:9200, bytes: 24992533, op: HDFS_WRITE, cliID: DFSClient_-2027758209, offset: 0, srvID: DS-1140118603-1xx.xx.xx.xxx-9200-1447076616147, blockid: blk_7066227555451084927_2515, duration: 146433082\n2015-11-20 12:27:25,998 INFO org.apache.hadoop.hdfs.server.datanode.DataNode (PacketResponder 0 for Block blk_7066227555451084927_2515): PacketResponder 0 for block blk_7066227555451084927_2515 terminating\n2015-11-20 12:27:26,767 INFO org.apache.hadoop.hdfs.server.datanode.DataNode (org.apache.hadoop.hdfs.server.datanode.DataXceiver@4e49076a): Receiving block blk_514847027605235509_2516 src: /1xx.xx.xx.xxx:36874 dest: /1xx.xx.xx.xxx:9200\n2015-11-20 12:27:26,769 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace (PacketResponder 0 for Block blk_514847027605235509_2516): src: /1xx.xx.xx.xxx:36874, dest: /1xx.xx.xx.xxx:9200, bytes: 105, op: HDFS_WRITE, cliID: DFSClient_-2027758209, offset: 0, srvID: DS-1140118603-1xx.xx.xx.xxx-9200-1447076616147, blockid: blk_514847027605235509_2516, duration: 1261238\n2015-11-20 12:27:26,770 INFO org.apache.hadoop.hdfs.server.datanode.DataNode (PacketResponder 0 for Block blk_514847027605235509_2516): PacketResponder 0 for block blk_514847027605235509_2516 terminating\n2015-11-20 12:27:26,901 INFO org.apache.hadoop.hdfs.server.datanode.DataNode (org.apache.hadoop.hdfs.server.datanode.DataXceiver@4904e883): Receiving block blk_-4059008174962567124_2518 src: /1xx.xx.xx.xx:50459 dest: /1xx.xx.xx.1xx:9200\n...other blocks received\n</code></pre>\n\n<p><strong>When run through the service</strong></p>\n\n<p>Command used to start the server</p>\n\n<pre><code>hadoop jar apache-nutch-2.3.job org.apache.nutch.api.NutchServer\n</code></pre>\n\n<p>Post arguments(which tell the server which main class to call):</p>\n\n<pre><code>POST /job/create\n   {\n      \"crawlId\":\"1\",\n      \"type\":\"INJECT\",\n      \"confId\":\"default\",\n      \"args\":{\"someParam\":\"someValue\"}\n   }\n</code></pre>\n\n<p>Namenode logs when called through the REST service:</p>\n\n<pre><code>2015-11-20 12:39:30,945 INFO org.apache.hadoop.hdfs.StateChange (IPC Server handler 59 on 9000): BLOCK* NameSystem.allocateBlock: /mnt/var/lib/hadoop/tmp/mapred/staging/hadoop/.staging/job_201511091343_0204/job.jar. blk_2155327976899171967_2525\n(nothing else after this)\n</code></pre>\n\n<p>Datanode logs when called through the REST service:</p>\n\n<pre><code>2015-11-20 12:39:29,605 INFO org.apache.hadoop.hdfs.server.datanode.DataNode (org.apache.hadoop.hdfs.server.datanode.DataXceiver@4ee588b9): Receiving block blk_-4889440366294611443_2524 src: /1xx.xx.xx.xx:50903 dest: /1xx.xx.xx.xxx:9200\n2015-11-20 12:39:29,606 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace (PacketResponder 0 for Block blk_-4889440366294611443_2524): src: /1xx.xx.xx.xx:50903, dest: /1xx.xx.xx.xxx:9200, bytes: 243, op: HDFS_WRITE, cliID: DFSClient_hb_m_1xx.xx.xx.xx,60000,1447076659551, offset: 0, srvID: DS-1140118603-1xx.xx.xx.xxx-9200-1447076616147, blockid: blk_-4889440366294611443_2524, duration: 874892\n2015-11-20 12:39:29,606 INFO org.apache.hadoop.hdfs.server.datanode.DataNode (PacketResponder 0 for Block blk_-4889440366294611443_2524): PacketResponder 0 for block blk_-4889440366294611443_2524 terminating\n</code></pre>\n\n<p>The user is hadoop in both the cases and <code>hadoop.security.authorization</code> is set to false. The job.jars created by call to REST service remain in the staging directory while those created by direct command are deleted.</p>\n", "creation_date": 1448080768, "score": 0},
{"title": "[Nutch]cause:org.apache.hadoop.mapred.InvalidInputException: Input path does not exist:", "view_count": 132, "is_answered": false, "question_id": 33839811, "tags": ["nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/33839811/nutchcauseorg-apache-hadoop-mapred-invalidinputexception-input-path-does-not", "last_activity_date": 1448081068, "owner": {"user_id": 5588159, "view_count": 2, "answer_count": 0, "creation_date": 1448078030, "reputation": 1}, "body": "<p>I am trying to crawl a website using apache nutch.</p>\n\n<pre><code>./bin/crawl bin/urls/ testcrawl localhost:8080/solr/ 1\n</code></pre>\n\n<p>I am getting the following error log : </p>\n\n<pre><code>    2015-11-20 23:04:00,352 WARN  util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n    2015-11-20 23:04:00,405 ERROR security.UserGroupInformation - PriviledgedActionException as:archana cause:org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/home/archana/apache-nutch-1.6/testcrawl/20151120230312/crawl_fetch\n    Input path does not exist: file:/home/archana/apache-nutch-1.6/testcrawl/20151120230312/crawl_parse\n    Input path does not exist: file:/home/archana/apache-nutch-1.6/testcrawl/20151120230312/parse_data\n    Input path does not exist: file:/home/archana/apache-nutch-1.6/testcrawl/20151120230312/parse_text\n    2015-11-20 23:04:00,405 ERROR solr.SolrIndexer - org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/home/archana/apache-nutch-1.6/testcrawl/20151120230312/crawl_fetch\n    Input path does not exist: file:/home/archana/apache-nutch-1.6/testcrawl/20151120230312/crawl_parse\n    Input path does not exist: file:/home/archana/apache-nutch-1.6/testcrawl/20151120230312/parse_data\n    Input path does not exist: file:/home/archana/apache-nutch-1.6/testcrawl/20151120230312/parse_text\n    2015-11-20 23:04:01,155 INFO  solr.SolrDeleteDuplicates - SolrDeleteDuplicates: starting at 2015-11-20 23:04:01\n    2015-11-20 23:04:01,155 INFO  solr.SolrDeleteDuplicates - SolrDeleteDuplicates: Solr url: localhost:8080/solr/collection1\n    2015-11-20 23:04:01,569 WARN  util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n    2015-11-20 23:04:01,633 ERROR security.UserGroupInformation - PriviledgedActionException as:archana cause:java.net.MalformedURLException: unknown protocol: localhost\n\n~\n</code></pre>\n\n<p>it is failed to find the input paths:</p>\n\n<pre><code>/home/archana/apache-nutch-1.6/testcrawl/20151120230312/crawl_parse\n/home/archana/apache-nutch-1.6/testcrawl/20151120230312/parse_data\n/home/archana/apache-nutch-1.6/testcrawl/20151120230312/parse_text\n</code></pre>\n\n<p>But all the three folders are present in </p>\n\n<pre><code>/home/archana/apache-nutch-1.6/testcrawl/segments/20151120230312/\n</code></pre>\n\n<p>where can i configure the input path in nutch as the files it is trying to index into solr exists but it is searching in a current directory?</p>\n\n<p>I have also given all permission to these folders so dont knoe why is it giving me permission issue?</p>\n", "creation_date": 1448081068, "score": 0},
{"title": "Is there a nutch schema.xml that works with solr 5.3.1 in basic config (single core)?", "view_count": 654, "is_answered": false, "answers": [{"question_id": 33138170, "owner": {"user_id": 1699191, "link": "http://stackoverflow.com/users/1699191/ameertawfik", "user_type": "registered", "reputation": 618}, "body": "<p>I believe you could use schema-4.xml found under nutch's conf folder. </p>\n", "creation_date": 1444938246, "is_accepted": false, "score": 0, "last_activity_date": 1444938246, "answer_id": 33157172}, {"question_id": 33138170, "owner": {"user_id": 5447458, "link": "http://stackoverflow.com/users/5447458/jch", "user_type": "registered", "reputation": 6}, "body": "<p>Thanks. I got it to work with the following ...</p>\n\n<ol>\n<li>I copied schema-solr4.xml from notch conf folder to the solar 5.3.1 conf folder. </li>\n<li>Copied the sole schema.xml to schema (copy).xml</li>\n<li>Renamed schema-solr4.xml to schema.xml (in the Solr core folder)</li>\n<li>Ran the  command in a  terminal window\n\n<ul>\n<li>then ran  -d sample_techproducts_configs \nReceived an error message \"ERROR: Error CREATEing SolrCore : Unable to create core [core] Caused by: enablePositionIncrements is not a valid option as of Lucene 5.0\"</li>\n</ul></li>\n<li>Revised schema.xml to delete all \" enablePositionIncrements=\"true\"</li>\n<li>Reran  -d sample_techproducts_configs \nReceive an ERROR: Error CREATEing SolrCore 'core': Unable to create core [core] Caused by: copyField dest :'location' is not an explicit field and doesn't match a dynamicField.</li>\n<li>Revised schema.xml to delete \"\"</li>\n<li><p>Reran  -d sample_techproducts_configs</p>\n\n<p>It worked - create the core.</p></li>\n<li><p>Was able to index from nutch 1.10 to this core in solr 5.3.1 (using bin/nutch solrindex ...)</p></li>\n</ol>\n\n<p>*It may be necessary to restart solr before creating core. </p>\n", "creation_date": 1445037606, "is_accepted": false, "score": 0, "last_activity_date": 1445037606, "answer_id": 33180943}], "question_id": 33138170, "tags": ["solr", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/33138170/is-there-a-nutch-schema-xml-that-works-with-solr-5-3-1-in-basic-config-single-c", "last_activity_date": 1447942513, "owner": {"user_id": 5447458, "view_count": 1, "answer_count": 1, "creation_date": 1444870933, "reputation": 6}, "body": "<p>Trying to integrate Nutch 4.10.1 with Solr 5.3.1 as in the Nutch Tutorial to index a crawl. </p>\n\n<ul>\n<li>A similar questions asked on 4/9, but no answer.</li>\n</ul>\n", "creation_date": 1444871377, "score": 0},
{"title": "Nutch Rest not working on EMR in distributed mode", "view_count": 90, "is_answered": false, "question_id": 33801845, "tags": ["hadoop", "amazon-web-services", "nutch", "emr", "amazon-emr"], "answer_count": 0, "link": "http://stackoverflow.com/questions/33801845/nutch-rest-not-working-on-emr-in-distributed-mode", "last_activity_date": 1447930412, "owner": {"user_id": 3999239, "view_count": 11, "answer_count": 1, "creation_date": 1409639589, "reputation": 21}, "body": "<p>I'm running Nutch 2.3 on EMR (AMI version 2.4.2). The crawl steps are working fine in local and distributed mode (<code>hadoop -jar apache-nutch-2.3.job &lt;MainClass&gt; &lt;args&gt;</code>), and am able to call the steps by spinning up the rest service in local mode. But, when I try to run the rest in distributed mode (<code>hadoop -jar apache-nutch-2.3.job org.apache.nutch.api.NutchServer</code>), the rest is receiving the calls, but is not getting the job done. What is the correct way to run nutch in distributed mode?</p>\n\n<h2>Info</h2>\n\n<p>When the InjectorJob is run offline in a distributed mode, the output is as follows:</p>\n\n<pre>\n<code>\nCOMMAND:\nhadoop jar ./apache-nutch-2.3.job org.apache.nutch.crawl.InjectorJob s3://myemrbucket/urls -crawlId 2\n</code>\n</pre>\n\n<pre><code>15/11/19 09:55:06 INFO crawl.InjectorJob: InjectorJob: starting at 2015-11-19 09:55:06\n15/11/19 09:55:06 INFO crawl.InjectorJob: InjectorJob: Injecting urlDir: s3://myemrbucket/urls\n15/11/19 09:55:06 INFO s3native.NativeS3FileSystem: Created AmazonS3 with InstanceProfileCredentialsProvider\n15/11/19 09:55:08 WARN store.HBaseStore: Mismatching schema's names. Mappingfile schema: 'webpage'. PersistentClass schema's name: '2_webpage'Assuming they are the same.\n15/11/19 09:55:08 INFO crawl.InjectorJob: InjectorJob: Using class org.apache.gora.hbase.store.HBaseStore as the Gora storage class.\n15/11/19 09:55:08 INFO mapred.JobClient: Default number of map tasks: null\n15/11/19 09:55:08 INFO mapred.JobClient: Setting default number of map tasks based on cluster size to : 4\n15/11/19 09:55:08 INFO mapred.JobClient: Default number of reduce tasks: 0\n15/11/19 09:55:10 INFO security.ShellBasedUnixGroupsMapping: add hadoop to shell userGroupsCache\n15/11/19 09:55:10 INFO mapred.JobClient: Setting group to hadoop\n15/11/19 09:55:10 INFO input.FileInputFormat: Total input paths to process : 1\n15/11/19 09:55:10 INFO lzo.GPLNativeCodeLoader: Loaded native gpl library\n15/11/19 09:55:10 WARN lzo.LzoCodec: Could not find build properties file with revision hash\n15/11/19 09:55:10 INFO lzo.LzoCodec: Successfully loaded &amp; initialized native-lzo library [hadoop-lzo rev UNKNOWN]\n15/11/19 09:55:10 WARN snappy.LoadSnappy: Snappy native library is available\n15/11/19 09:55:10 INFO snappy.LoadSnappy: Snappy native library loaded\n15/11/19 09:55:10 INFO mapred.JobClient: Running job: job_201511182052_0037\n15/11/19 09:55:11 INFO mapred.JobClient:  map 0% reduce 0%\n15/11/19 09:55:38 INFO mapred.JobClient:  map 100% reduce 0%\n15/11/19 09:55:43 INFO mapred.JobClient: Job complete: job_201511182052_0037\n15/11/19 09:55:43 INFO mapred.JobClient: Counters: 20\n15/11/19 09:55:43 INFO mapred.JobClient:   Job Counters \n15/11/19 09:55:43 INFO mapred.JobClient:     SLOTS_MILLIS_MAPS=16424\n15/11/19 09:55:43 INFO mapred.JobClient:     Total time spent by all reduces waiting after reserving slots (ms)=0\n15/11/19 09:55:43 INFO mapred.JobClient:     Total time spent by all maps waiting after reserving slots (ms)=0\n15/11/19 09:55:43 INFO mapred.JobClient:     Rack-local map tasks=1\n15/11/19 09:55:43 INFO mapred.JobClient:     Launched map tasks=1\n15/11/19 09:55:43 INFO mapred.JobClient:     SLOTS_MILLIS_REDUCES=0\n15/11/19 09:55:43 INFO mapred.JobClient:   File Output Format Counters \n15/11/19 09:55:43 INFO mapred.JobClient:     Bytes Written=0\n15/11/19 09:55:43 INFO mapred.JobClient:   injector\n15/11/19 09:55:43 INFO mapred.JobClient:     urls_injected=1\n15/11/19 09:55:43 INFO mapred.JobClient:   FileSystemCounters\n15/11/19 09:55:43 INFO mapred.JobClient:     HDFS_BYTES_READ=98\n15/11/19 09:55:43 INFO mapred.JobClient:     S3_BYTES_READ=61\n15/11/19 09:55:43 INFO mapred.JobClient:     FILE_BYTES_WRITTEN=36254\n15/11/19 09:55:43 INFO mapred.JobClient:   File Input Format Counters \n15/11/19 09:55:43 INFO mapred.JobClient:     Bytes Read=61\n15/11/19 09:55:43 INFO mapred.JobClient:   Map-Reduce Framework\n15/11/19 09:55:43 INFO mapred.JobClient:     Map input records=1\n15/11/19 09:55:43 INFO mapred.JobClient:     Physical memory (bytes) snapshot=193712128\n15/11/19 09:55:43 INFO mapred.JobClient:     Spilled Records=0\n15/11/19 09:55:43 INFO mapred.JobClient:     CPU time spent (ms)=3960\n15/11/19 09:55:43 INFO mapred.JobClient:     Total committed heap usage (bytes)=298319872\n15/11/19 09:55:43 INFO mapred.JobClient:     Virtual memory (bytes) snapshot=1525059584\n15/11/19 09:55:43 INFO mapred.JobClient:     Map output records=1\n15/11/19 09:55:43 INFO mapred.JobClient:     SPLIT_RAW_BYTES=98\n15/11/19 09:55:44 INFO crawl.InjectorJob: InjectorJob: total number of urls rejected by filters: 0\n15/11/19 09:55:44 INFO crawl.InjectorJob: InjectorJob: total number of urls injected after normalization and filtering: 1\n15/11/19 09:55:44 INFO crawl.InjectorJob: Injector: finished at 2015-11-19 09:55:44, elapsed: 00:00:38\n</code></pre>\n\n<p>By calling it through the REST, the job gets stuck after giving out the following output:</p>\n\n<pre>\n<code>\nPOST ARGS:\n\n    {\n      \"crawlId\":\"11\",\n      \"confId\":\"default\",\n      \"type\":\"INJECT\",\n      \"args\":{\"seedDir\":\"s3://myemrbucket/urls\"}\n    }\n</code>\n</pre>\n\n<pre><code>15/11/19 09:46:14 INFO api.NutchServer: Starting NutchServer on port: 8081 with logging level: INFO ...\nNov 19, 2015 9:46:14 AM org.restlet.engine.connector.NetServerHelper start\nINFO: Starting the internal [HTTP/1.1] server on port 8081\n15/11/19 09:46:14 INFO api.NutchServer: Started NutchServer on port 8081\nNov 19, 2015 9:46:25 AM org.restlet.engine.log.LogFilter afterHandle\nINFO: 2015-11-19    09:46:25    1xx.xx.x.xx -   -   8081    POST    /job/create -   200 28  110 498 http://ec2-xx-xxx-xxx-xx.compute-1.amazonaws.com:8081   Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/46.0.2490.80 Safari/537.36-\n15/11/19 09:46:25 INFO s3native.NativeS3FileSystem: Created AmazonS3 with InstanceProfileCredentialsProvider\n15/11/19 09:46:27 WARN store.HBaseStore: Mismatching schema's names. Mappingfile schema: 'webpage'. PersistentClass schema's name: '11_webpage'Assuming they are the same.\n15/11/19 09:46:28 INFO crawl.InjectorJob: InjectorJob: Using class org.apache.gora.hbase.store.HBaseStore as the Gora storage class.\n15/11/19 09:46:28 INFO mapred.JobClient: Default number of map tasks: null\n15/11/19 09:46:28 INFO mapred.JobClient: Setting default number of map tasks based on cluster size to : 4\n15/11/19 09:46:28 INFO mapred.JobClient: Default number of reduce tasks: 0\n15/11/19 09:46:28 WARN mapred.JobClient: Use GenericOptionsParser for parsing the arguments. Applications should implement Tool for the same.\n</code></pre>\n\n<p>and does not move ahead.</p>\n", "creation_date": 1447930412, "score": 1},
{"title": "Optimize map reduce operations in apache nutch", "view_count": 39, "is_answered": false, "question_id": 33794151, "tags": ["hadoop", "nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/33794151/optimize-map-reduce-operations-in-apache-nutch", "last_activity_date": 1447900101, "owner": {"user_id": 3367701, "answer_count": 2, "creation_date": 1393661808, "accept_rate": 60, "view_count": 29, "reputation": 161}, "body": "<p>In Nutch 1.8:</p>\n\n<p>At the start of the crawl, the map-reduce task detailed below seem to run much faster. However, after crawling only 40,000 pages, the map-reduce task seems to take much longer. Looking at logs below, it takes a second to accomplish 1% of the map-reduce task. Any ideas on what is making the map-reduce task take so long to finish? I am just dumping the crawl results via <code>-dir</code> parameter in a directory, with no added configuration in hadoop nor am I using additional storage libraries. Thanks.</p>\n\n<p><strong>Additional question:</strong></p>\n\n<p>Upon further investigation of the problem, still can't understand what this map-reduce task does. Why does Nutch perform this?</p>\n\n<pre><code>2015-11-19 10:07:16,707 INFO  regex.RegexURLNormalizer (RegexURLNormalizer.java:regexNormalize(174)) - can't find rules for scope 'crawldb', using default\n2015-11-19 10:07:17,172 INFO  mapred.JobClient (JobClient.java:monitorAndPrintJob(1393)) -  map 57% reduce 0%\n2015-11-19 10:07:22,664 INFO  mapred.LocalJobRunner (LocalJobRunner.java:statusUpdate(466)) - file:/C:/Users/user/workspace/trunk/crawl/crawldb/current/part-00000/data:100663296+33554432\n2015-11-19 10:07:23,172 INFO  mapred.JobClient (JobClient.java:monitorAndPrintJob(1393)) -  map 59% reduce 0%\n2015-11-19 10:07:25,664 INFO  mapred.LocalJobRunner (LocalJobRunner.java:statusUpdate(466)) - file:/C:/Users/user/workspace/trunk/crawl/crawldb/current/part-00000/data:100663296+33554432\n2015-11-19 10:07:26,172 INFO  mapred.JobClient (JobClient.java:monitorAndPrintJob(1393)) -  map 60% reduce 0%\n</code></pre>\n", "creation_date": 1447899649, "score": 2},
{"title": "nutch crawler is crawling &#39; as &#226;\u20ac", "view_count": 268, "is_answered": true, "answers": [{"question_id": 4865386, "owner": {"user_id": 18157, "accept_rate": 100, "link": "http://stackoverflow.com/users/18157/jim-garrison", "user_type": "registered", "reputation": 57444}, "body": "<p><code>\u00e2\u20ac\u2122</code> is the UTF-8 encoding of the single closing quote (not the apostrophe), and you're interpreting it as Windows-1252.  You need to use the right encoding (UTF-8).  <a href=\"http://wiki.apache.org/nutch/GettingNutchRunningWithUtf8\" rel=\"nofollow\">This link</a> may help.</p>\n", "creation_date": 1296579151, "is_accepted": false, "score": 1, "last_activity_date": 1296579151, "answer_id": 4865446}, {"last_edit_date": 1447875650, "owner": {"user_id": 22656, "accept_rate": 86, "link": "http://stackoverflow.com/users/22656/jon-skeet", "user_type": "registered", "reputation": 904775}, "body": "<p>I haven't used Nutch myself, but <a href=\"http://wiki.apache.org/nutch/GettingNutchRunningWithUtf8\" rel=\"nofollow\">this page</a> looks like it's relevant:</p>\n\n<blockquote>\n  <p>To enable passing of UTF-8 characters, edit $TOMCAT/conf/server.xml. Locate the &lt;Connector> tag for the web (look for \"8080\") and insert this parameter assignment: URIEncoding=\"UTF-8\" as explained in Tomcat 5 FAQ at <a href=\"http://tomcat.apache.org/faq/connectors.html#utf8\" rel=\"nofollow\">http://tomcat.apache.org/faq/connectors.html#utf8</a></p>\n</blockquote>\n", "question_id": 4865386, "creation_date": 1296579191, "is_accepted": false, "score": 1, "last_activity_date": 1447875650, "answer_id": 4865457}], "question_id": 4865386, "tags": ["java", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/4865386/nutch-crawler-is-crawling-as-%c3%a2%e2%82%ac", "last_activity_date": 1447875650, "owner": {"user_id": 593710, "answer_count": 0, "creation_date": 1296213398, "accept_rate": 0, "view_count": 3, "reputation": 31}, "body": "<p>nutch crawler is crawling <code>let's</code> as <code>Let\u00e2\u20ac\u2122s</code> y??? is there is any setting to change the this charset..</p>\n", "creation_date": 1296578830, "score": 0},
{"title": "Using Nutch how to crawl the dynamic content of web page that are uisng ajax?", "view_count": 875, "is_answered": true, "answers": [{"question_id": 32966642, "owner": {"user_id": 495520, "accept_rate": 100, "link": "http://stackoverflow.com/users/495520/sjdirect", "user_type": "registered", "reputation": 858}, "body": "<p>Most web crawler libraries do not offer javascript rendering out of the box. You usually have to plugin another library or product that offers js rendering like Selenium or PhantomJS.</p>\n\n<p>Here is a <a href=\"http://soryy.com/blog/2014/ajax-javascript-enabled-parsing-apache-nutch-selenium/\" rel=\"nofollow\">tutorial using nutch and Selenium</a>.</p>\n", "creation_date": 1444147777, "is_accepted": false, "score": 1, "last_activity_date": 1444147777, "answer_id": 32974644}, {"question_id": 32966642, "owner": {"user_id": 2702341, "link": "http://stackoverflow.com/users/2702341/sujen-shah", "user_type": "registered", "reputation": 183}, "body": "<p>Checkout the latest Nutch 1.11 trunk which includes a new plugin protocol-interactive selenium. (<a href=\"https://github.com/apache/nutch/tree/trunk/src/plugin/protocol-interactiveselenium\" rel=\"nofollow\">https://github.com/apache/nutch/tree/trunk/src/plugin/protocol-interactiveselenium</a>)</p>\n\n<p>This plugin allows you to write your own handler and execute javascript to get dynamic content. </p>\n", "creation_date": 1444502994, "is_accepted": false, "score": 0, "last_activity_date": 1444502994, "answer_id": 33057701}], "question_id": 32966642, "tags": ["java", "ajax", "plugins", "web-crawler", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/32966642/using-nutch-how-to-crawl-the-dynamic-content-of-web-page-that-are-uisng-ajax", "last_activity_date": 1447770208, "owner": {"user_id": 3364049, "answer_count": 3, "creation_date": 1393577655, "accept_rate": 22, "view_count": 14, "reputation": 94}, "body": "<p>I am using apache Nutch 1.10 to crawl the web pages and to extract the contents in the page. Some of the links contains dynamic contents which are loaded on the call of ajax. Nutch cannot able to crawl and extract the dynamic contents of ajax. How can I solve this? Is there any solution? if yes please help me with your answers. </p>\n\n<p>Thanks in advance.</p>\n", "creation_date": 1444124364, "score": 1},
{"title": "Nutch+Hbase on EMR CLASSPATH issue", "view_count": 123, "is_answered": false, "question_id": 33735368, "tags": ["hadoop", "amazon-web-services", "nutch", "amazon-emr"], "answer_count": 0, "link": "http://stackoverflow.com/questions/33735368/nutchhbase-on-emr-classpath-issue", "last_activity_date": 1447682681, "owner": {"user_id": 3999239, "view_count": 11, "answer_count": 1, "creation_date": 1409639589, "reputation": 21}, "body": "<p>I'm a hadoop newbie and trying to run Nutch 2.3, with Hbase as backend, on EMR. Since Nutch uses hadoop-1.2.0, we chose the AMI version:2.4.2 which comes with Hadoop 1.0.3 and HBase 0.92.0.</p>\n\n<p>When I build Nutch, it is crawling without problem on local mode. But when run in distributed mode, the job stops at injector step with the following exception:</p>\n\n<pre><code>Injecting seed URLs\n/home/hadoop/.../runtime/deploy/bin/nutch inject s3://myemrbucket/urls/ -crawlId 8\n15/11/16 11:23:21 INFO crawl.InjectorJob: InjectorJob: starting at 2015-11-16 11:23:21\n15/11/16 11:23:21 INFO crawl.InjectorJob: InjectorJob: Injecting urlDir: s3://myemrbucket/urls\n15/11/16 11:23:21 INFO s3native.NativeS3FileSystem: Created AmazonS3 with InstanceProfileCredentialsProvider\n15/11/16 11:23:23 WARN store.HBaseStore: Mismatching schema's names. Mappingfile schema: 'webpage'. PersistentClass schema's name: '8_webpage'Assuming they are the same.\n15/11/16 11:23:23 INFO crawl.InjectorJob: InjectorJob: Using class org.apache.gora.hbase.store.HBaseStore as the Gora storage class.\n15/11/16 11:23:23 INFO mapred.JobClient: Default number of map tasks: null\n15/11/16 11:23:23 INFO mapred.JobClient: Setting default number of map tasks based on cluster size to : 2\n15/11/16 11:23:23 INFO mapred.JobClient: Default number of reduce tasks: 0\n15/11/16 11:23:24 INFO security.ShellBasedUnixGroupsMapping: add hadoop to shell userGroupsCache\n15/11/16 11:23:24 INFO mapred.JobClient: Setting group to hadoop\n15/11/16 11:23:25 INFO input.FileInputFormat: Total input paths to process : 1\n15/11/16 11:23:25 INFO lzo.GPLNativeCodeLoader: Loaded native gpl library\n15/11/16 11:23:25 WARN lzo.LzoCodec: Could not find build properties file with revision hash\n15/11/16 11:23:25 INFO lzo.LzoCodec: Successfully loaded &amp; initialized native-lzo library [hadoop-lzo rev UNKNOWN]\n15/11/16 11:23:25 WARN snappy.LoadSnappy: Snappy native library is available\n15/11/16 11:23:25 INFO snappy.LoadSnappy: Snappy native library loaded\n15/11/16 11:23:25 INFO mapred.JobClient: Running job: job_201511101059_0054\n15/11/16 11:23:26 INFO mapred.JobClient:  map 0% reduce 0%\n15/11/16 11:23:46 INFO mapred.JobClient: Task Id : attempt_201511101059_0054_m_000000_0, Status : FAILED\nError: org.apache.hadoop.hbase.HColumnDescriptor.setMaxVersions(I)Lorg/apache/hadoop/hbase/HColumnDescriptor;\n15/11/16 11:23:55 INFO mapred.JobClient: Task Id : attempt_201511101059_0054_m_000000_1, Status : FAILED\nError: org.apache.hadoop.hbase.HColumnDescriptor.setMaxVersions(I)Lorg/apache/hadoop/hbase/HColumnDescriptor;\n15/11/16 11:24:04 INFO mapred.JobClient: Task Id : attempt_201511101059_0054_m_000000_2, Status : FAILED\nError: org.apache.hadoop.hbase.HColumnDescriptor.setMaxVersions(I)Lorg/apache/hadoop/hbase/HColumnDescriptor;\n15/11/16 11:24:19 INFO mapred.JobClient: Job complete: job_201511101059_0054\n15/11/16 11:24:19 INFO mapred.JobClient: Counters: 7\n15/11/16 11:24:19 INFO mapred.JobClient:   Job Counters \n15/11/16 11:24:19 INFO mapred.JobClient:     SLOTS_MILLIS_MAPS=27542\n15/11/16 11:24:19 INFO mapred.JobClient:     Total time spent by all reduces waiting after reserving slots (ms)=0\n15/11/16 11:24:19 INFO mapred.JobClient:     Total time spent by all maps waiting after reserving slots (ms)=0\n15/11/16 11:24:19 INFO mapred.JobClient:     Rack-local map tasks=4\n15/11/16 11:24:19 INFO mapred.JobClient:     Launched map tasks=4\n15/11/16 11:24:19 INFO mapred.JobClient:     SLOTS_MILLIS_REDUCES=0\n15/11/16 11:24:19 INFO mapred.JobClient:     Failed map tasks=1\n15/11/16 11:24:19 ERROR crawl.InjectorJob: InjectorJob: java.lang.RuntimeException: job failed: name=[8]inject s3://myemrbucket/urls, jobid=job_201511101059_0054\n    at org.apache.nutch.util.NutchJob.waitForCompletion(NutchJob.java:54)\n    at org.apache.nutch.crawl.InjectorJob.run(InjectorJob.java:231)\n    at org.apache.nutch.crawl.InjectorJob.inject(InjectorJob.java:252)\n    at org.apache.nutch.crawl.InjectorJob.run(InjectorJob.java:275)\n    at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n    at org.apache.nutch.crawl.InjectorJob.main(InjectorJob.java:284)\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n    at java.lang.reflect.Method.invoke(Method.java:606)\n    at org.apache.hadoop.util.RunJar.main(RunJar.java:187)\n\nError running:\n  /home/hadoop/.../runtime/deploy/bin/nutch inject s3://myemrbucket/urls/ -crawlId 8\n</code></pre>\n\n<p>This is happening because of the version mismatch between the different hbase jars used by Nutch(hbase-0.94.14.jar) and EMR(hbase-0.92.0.jar).</p>\n\n<h2>Things I have tried</h2>\n\n<ul>\n<li><p><a href=\"http://docs.aws.amazon.com/ElasticMapReduce/latest/DeveloperGuide/emr-hadoop-config_hadoop-user-env.sh.html\" rel=\"nofollow\">Set hadoop specific env variables</a></p>\n\n<pre><code>export HADOOP_USER_CLASSPATH_FIRST=true\nexport HADOOP_CLASSPATH=/home/hadoop/pathtomy/hbase-0.94.14.jar:$HADOOP_CLASSPATH\n</code></pre>\n\n<p>in hadoop-user-env.sh</p></li>\n<li><p><a href=\"http://grepalex.com/2013/02/25/hadoop-libjars/\" rel=\"nofollow\">Added hbase-0.94 in the TaskTracker's classpath using libjars</a></p>\n\n<pre><code>hadoop jar apache-nutch-2.3.job org.apache.nutch.crawl.InjectorJob -libjars s3://myemrbucket/hbase-0.94.14.jar s3://myemrbucket/urls/ -crawlId 8\n</code></pre></li>\n<li><p><a href=\"http://stackoverflow.com/a/11697223/3999239\">Added</a></p>\n\n<pre><code>-D mapreduce.user.classpath.first=true\n</code></pre></li>\n</ul>\n\n<p>to my crawl script (I've also tried adding the three options specified in the\n       question)</p>\n\n<ul>\n<li><p><a href=\"http://stackoverflow.com/questions/682852/apache-ant-manifest-class-path\">Added Classpath in manifest file</a></p>\n\n<pre><code>&lt;manifest&gt;\n    &lt;attribute name=\"Class-Path\" value=\"./lib/\"/&gt;\n&lt;/manifest&gt;\n</code></pre>\n\n<p>in the build.xml</p></li>\n<li><p><a href=\"https://groups.google.com/a/cloudera.org/forum/#!topic/scm-users/rAsh09voJ8A\" rel=\"nofollow\">Tried adding HADOOP_TASKTRACKER_OPTS env variable</a> </p>\n\n<pre><code>HADOOP_TASKTRACKER_OPTS=\"-classpath /home/hadoop/pathtomy/hbase-0.94.14.jar ${HADOOP_TASKTRACKER_OPTS}\"\n</code></pre></li>\n</ul>\n\n<p>in hadoop-user-env.sh (putting this causes the command <code>hadoop tasktracker restart</code> to fail)</p>\n\n<ul>\n<li>Replaced EMR's hbase-0.92.0.jar and hbase.jar with my hbase-0.94.14.jar (This works! but I don't want to risk my cluster for my application)</li>\n</ul>\n\n<p>How do I get out of this frustrating issue?</p>\n", "creation_date": 1447676767, "score": 0},
{"title": "Nutch not crawling anchor urls that contains tags (like H1, H2 etc..)", "view_count": 88, "is_answered": false, "question_id": 33698715, "tags": ["web-crawler", "anchor", "href", "nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/33698715/nutch-not-crawling-anchor-urls-that-contains-tags-like-h1-h2-etc", "last_activity_date": 1447436306, "owner": {"user_id": 955140, "answer_count": 34, "creation_date": 1316533639, "accept_rate": 71, "view_count": 141, "reputation": 968}, "body": "<p>In one of webpage the href anchor text contains H2 tag and nutch is not crawling that particular url.</p>\n\n<p>Once I remove the H2 tag, nutch crawls that URL without any issues. Any idea how to fix this issue? </p>\n\n<p>Note: I don't have rights to remove H2 tags in all the pages that nutch is crawling.</p>\n\n<pre><code>&lt;a href=\"test.html#!/intid/id/int12345?make=Apple&amp;amp;model=Apple6s\" id=\"int12345\" class=\"ng-scope\"&gt;&lt;h2 &gt;Record a voice memo&lt;/h2&gt;&lt;/a&gt;\n</code></pre>\n\n<p>Thanks</p>\n", "creation_date": 1447436306, "score": 0},
{"title": "Do I need to use Nutch to crawl local files if I want to index them?", "view_count": 543, "is_answered": true, "answers": [{"question_id": 19434120, "owner": {"user_id": 1333610, "accept_rate": 90, "link": "http://stackoverflow.com/users/1333610/arun", "user_type": "registered", "reputation": 4680}, "body": "<p>You can use data import handler. See \n<a href=\"https://wiki.apache.org/solr/DataImportHandler#FileListEntityProcessor\" rel=\"nofollow\">https://wiki.apache.org/solr/DataImportHandler#FileListEntityProcessor</a></p>\n", "creation_date": 1382034458, "is_accepted": false, "score": 1, "last_activity_date": 1382034458, "answer_id": 19434429}, {"last_edit_date": 1447332077, "owner": {"user_id": 432844, "link": "http://stackoverflow.com/users/432844/julien-nioche", "user_type": "registered", "reputation": 939}, "body": "<p>You can get Nutch to crawl the filesystem if the files are already there. You'll need to activate the <code>protocol-file</code> plugin. </p>\n\n<p>Alternatively <a href=\"https://github.com/DigitalPebble/behemoth\" rel=\"nofollow\">Behemoth</a> is also a good option. Look at its Apache Tika module for extracting the text and metadata from the original docs, as well as the solr module.</p>\n", "question_id": 19434120, "creation_date": 1384784578, "is_accepted": false, "score": 3, "last_activity_date": 1447332077, "answer_id": 20050081}], "question_id": 19434120, "tags": ["solr", "elasticsearch", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/19434120/do-i-need-to-use-nutch-to-crawl-local-files-if-i-want-to-index-them", "last_activity_date": 1447332077, "owner": {"user_id": 125470, "answer_count": 35, "creation_date": 1245368692, "accept_rate": 44, "view_count": 440, "reputation": 2518}, "body": "<p>My understanding of using Nutch is to download the web page to local filesystem so ElasticSearch / Solr can index it.</p>\n\n<p>If I want to index local filesystem, since all the files are in the local filesystem already, do I still need to use Nutch for that?</p>\n\n<p>Thanks.</p>\n", "creation_date": 1382033453, "score": 1},
{"title": "On Apache Nutch-1.10 CrawlDb update: java.io.IOException: Job failed", "view_count": 65, "is_answered": false, "question_id": 33612264, "tags": ["hadoop", "nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/33612264/on-apache-nutch-1-10-crawldb-update-java-io-ioexception-job-failed", "last_activity_date": 1447260592, "owner": {"user_id": 5012237, "view_count": 7, "answer_count": 0, "creation_date": 1434386822, "reputation": 1}, "body": "<p>I run Apache Nutch-1.10 at depth 2 &amp;  Only one seed url.<br>\nIn depth 1 generate,fetch,parse do well.<br>\nBut when I update my crawdb by using this command</p>\n\n<pre><code>#bin/nutch  updatedb  crawl/crawldb crawl/segments/20151109202030\n</code></pre>\n\n<p>it fails.</p>\n\n<h2>logs(partial):</h2>\n\n<pre><code>2015-11-09 20:20:40,759 INFO  crawl.CrawlDb - CrawlDb update: Merging segment data into db.\n2015-11-09 20:20:41,509 INFO  crawl.FetchScheduleFactory - Using FetchSchedule impl: org.apache.nutch.crawl.DefaultFetchSchedule\n2015-11-09 20:20:41,510 INFO  crawl.AbstractFetchSchedule - defaultInterval=2592000\n2015-11-09 20:20:41,510 INFO  crawl.AbstractFetchSchedule - maxInterval=7776000\n2015-11-09 20:20:41,528 WARN  mapred.LocalJobRunner - job_local387984556_0001\njava.lang.ArrayIndexOutOfBoundsException: 0\n    at org.apache.hadoop.util.PriorityQueue.clear(PriorityQueue.java:115)\n    at org.apache.nutch.crawl.CrawlDbReducer.reduce(CrawlDbReducer.java:75)\n    at org.apache.nutch.crawl.CrawlDbReducer.reduce(CrawlDbReducer.java:38)\n    at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:522)\n    at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:421)\n    at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:398)\n2015-11-09 20:20:42,024 ERROR crawl.CrawlDb - CrawlDb update: java.io.IOException: Job failed!\n    at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:1357)\n    at org.apache.nutch.crawl.CrawlDb.update(CrawlDb.java:109)\n    at org.apache.nutch.crawl.CrawlDb.run(CrawlDb.java:226)\n    at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n    at org.apache.nutch.crawl.CrawlDb.main(CrawlDb.java:176)\n\n2015-11-09 20:20:42,874 WARN  util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n</code></pre>\n", "creation_date": 1447082195, "score": 0},
{"title": "Parse microdata using apache tika plugin on apache nutch", "view_count": 254, "is_answered": true, "answers": [{"question_id": 33651824, "owner": {"user_id": 432844, "link": "http://stackoverflow.com/users/432844/julien-nioche", "user_type": "registered", "reputation": 939}, "body": "<p>You need to implement your own <a href=\"https://github.com/apache/nutch/blob/2.x/src/java/org/apache/nutch/parse/ParseFilter.java\" rel=\"nofollow\">ParseFilter</a> and implement the extraction logic there. You will get a DocumentFragment generated by the Tika parser and could use e.g. XPath to get the micro data.</p>\n\n<p>Note that the DOM generated by Tika are heavily normalised / modified so your Xpath expressions could possibly not match. Maybe better to rely on the old HTML parser instead.</p>\n\n<p>One generic way of doing would be to use <a href=\"https://any23.apache.org/\" rel=\"nofollow\">Apache Any23</a> as done for instance in <a href=\"https://github.com/PopSugar/storm-crawler-extensions/tree/master/microdata-parser\" rel=\"nofollow\">this storm-crawler module</a>. </p>\n\n<p>BTW There is an <a href=\"https://issues.apache.org/jira/browse/TIKA-980\" rel=\"nofollow\">open JIRA</a> for a MicroDataHandler in Tika which hasn't been committed yet.</p>\n\n<p>HTH</p>\n", "creation_date": 1447259685, "is_accepted": false, "score": 1, "last_activity_date": 1447259685, "answer_id": 33655275}], "question_id": 33651824, "tags": ["solr", "hbase", "nutch", "microdata", "apache-tika"], "answer_count": 1, "link": "http://stackoverflow.com/questions/33651824/parse-microdata-using-apache-tika-plugin-on-apache-nutch", "last_activity_date": 1447259685, "owner": {"age": 24, "answer_count": 5, "creation_date": 1394724680, "user_id": 3416153, "accept_rate": 67, "view_count": 7, "location": "Philippines", "reputation": 101}, "body": "<p>my objective is to \n- crawl on urls and \n- extract micro data and \n- save to solr</p>\n\n<p>I used this <a href=\"https://github.com/renepickhardt/metalcon/wiki/simpleNutchSolrSetup\" rel=\"nofollow\">guide</a> to setup nutch, hbase, and solr</p>\n\n<p>Im using <a href=\"http://nutch.apache.org\" rel=\"nofollow\">nutch</a> to crawl on urls and <a href=\"http://hbase.apache.org\" rel=\"nofollow\">hbase</a>, im using tika pluggin for nutch to parse pages, but it only gets meta data.</p>\n\n<p>Did I miss something to config? please guide me or suggest alternatives</p>\n", "creation_date": 1447249332, "score": 0},
{"title": "nutch 1.10 input path does not exist /linkdb/current", "view_count": 224, "owner": {"user_id": 2233559, "answer_count": 14, "creation_date": 1364847802, "accept_rate": 82, "view_count": 32, "reputation": 392}, "is_answered": true, "answers": [{"question_id": 33508720, "owner": {"user_id": 2233559, "accept_rate": 82, "link": "http://stackoverflow.com/users/2233559/anonymous-man", "user_type": "registered", "reputation": 392}, "body": "<p>Ok, it seems as though I have run into a version of this problem:</p>\n\n<p><a href=\"https://issues.apache.org/jira/browse/NUTCH-2041\" rel=\"nofollow\">https://issues.apache.org/jira/browse/NUTCH-2041</a></p>\n\n<p>Which is a result of the crawl script not being aware of changes to ignore_external_links  my nutch-site.xml file. </p>\n\n<p>I am trying to crawl several sites and was hoping to keep my life simple by ignoring external links and leaving regex-urlfilter.txt alone (just using +.)</p>\n\n<p>Now it looks like I'll have to change ignore_external_links back to false and add a regex filter for each of my urls. Hopefully I can get a nutch 1.11 release soon. It looks like this is fixed there. </p>\n", "creation_date": 1447103443, "is_accepted": true, "score": 3, "last_activity_date": 1447103443, "answer_id": 33618341}], "question_id": 33508720, "tags": ["hadoop", "solr", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/33508720/nutch-1-10-input-path-does-not-exist-linkdb-current", "last_activity_date": 1447103443, "accepted_answer_id": 33618341, "body": "<p>When I run <code>nutch 1.10</code> with the following command, assuming that <code>TestCrawl2</code> did not previously exist and needs to be created,...</p>\n\n<pre><code>sudo -E bin/crawl -i -D solr.server.url=http://localhost:8983/solr/TestCrawlCore2 urls/ TestCrawl2/ 20\n</code></pre>\n\n<p>I receive an error on indexing that claims:</p>\n\n<pre><code>Indexer: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/opt/apache-nutch-1.10/TestCrawl2/linkdb/current\n</code></pre>\n\n<p>The linkdb directory exists, but does not contain the 'current' directory. The directory is owned by root so there should be no permissions issues. Because the process exited from an error, the linkdb directory contains <code>.locked</code> and ..<code>locked.crc</code> files. If I run the command again, these lock files cause it to exit in the same place. Delete <code>TestCrawl2</code> directory, rinse, repeat. </p>\n\n<p>Note that the nutch and solr installaions themselves have run previously without problems in a <code>TestCrawl</code> instance. It's just now that I'm trying a new one that I'm having problems. Any suggestions on troubleshooting this issue?</p>\n", "creation_date": 1446583485, "score": 6},
{"title": "Nutch or other framework to crawl webservices", "view_count": 52, "is_answered": true, "answers": [{"question_id": 33398885, "owner": {"user_id": 432844, "link": "http://stackoverflow.com/users/432844/julien-nioche", "user_type": "registered", "reputation": 939}, "body": "<p>You could use Nutch, it is not limited to HTML. If something can be accessed via a URL then Nutch will fetch it, however you might need to implement some custom parsers and indexers to deal with your content.</p>\n\n<p>Alternatively <a href=\"https://github.com/DigitalPebble/storm-crawler\" rel=\"nofollow\">storm-crawler</a> would be both scalable and customisable. You might find it easier to learn than Nutch and more flexible. In your use case you could have one or more queues (e.g. RabbitMQ, AWS SQS, etc...) in front of SC. The seed URLs would be the ones to use on the first service and you could have custom parse filters to generate the URLs for the second one. Finally you'd have a bespoke indexing bolt sending the data to persist to the DB. There's loads of resources available for Storm you could piggy back.</p>\n\n<p>HTH</p>\n", "creation_date": 1446560680, "is_accepted": false, "score": 2, "last_activity_date": 1446560680, "answer_id": 33501526}], "question_id": 33398885, "tags": ["web-services", "mapreduce", "web-scraping", "nutch", "yarn"], "answer_count": 1, "link": "http://stackoverflow.com/questions/33398885/nutch-or-other-framework-to-crawl-webservices", "last_activity_date": 1446560680, "owner": {"user_id": 1216225, "answer_count": 18, "creation_date": 1329481871, "accept_rate": 74, "view_count": 140, "reputation": 517}, "body": "<p>I'm looking for a framework what I can use for the following scenario: I have 2 web-services. I call the first service which has json response. In the json response I have some Ids, which I use to call other services and then I merge the services responses and store it in db. I want to call these services every day to update my db.</p>\n\n<p>What I found is Nutch, but it looks like it is a webcrawler for mostly html pages. Is there any framework that I can use for the scenario above? I'm looking for a fault tolerant salable java framework.</p>\n\n<p>Thanks!</p>\n", "creation_date": 1446056999, "score": 0},
{"title": "nutch 1.10 job failed, bad request error indexing to solr 5.3.1", "view_count": 529, "owner": {"user_id": 2233559, "answer_count": 14, "creation_date": 1364847802, "accept_rate": 82, "view_count": 32, "reputation": 392}, "is_answered": true, "answers": [{"question_id": 33444515, "owner": {"user_id": 2233559, "accept_rate": 82, "link": "http://stackoverflow.com/users/2233559/anonymous-man", "user_type": "registered", "reputation": 392}, "body": "<p>This turned out to be a mismatch between the nutch and solr schemas. </p>\n\n<p>Thanks to TMBT (see comments above) I found an additional error in the Solr logs claiming \"unidentified field: \"anchor\".</p>\n\n<p>All I had to do was copy the anchor field declaration from the nutch schema into the Solr schema and restart the solr service. Now running fine. </p>\n", "creation_date": 1446496948, "is_accepted": true, "score": 0, "last_activity_date": 1446496948, "answer_id": 33486501}], "question_id": 33444515, "tags": ["hadoop", "solr", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/33444515/nutch-1-10-job-failed-bad-request-error-indexing-to-solr-5-3-1", "last_activity_date": 1446496948, "accepted_answer_id": 33486501, "body": "<p>I have put together a crawler in a test environment that was running just fine with 2 small sites, including successfully indexing to solr. So, the integration between nutch and solr seem to be fine.</p>\n\n<p>The only change I have made is adding another site to seed.txt and another line in regex-urlfilters.txt, using the exact same syntax as the other sites.</p>\n\n<p>Now when I run the crawler it runs fine for a while then crashes with a 'Job failed!' error and little helpful information. </p>\n\n<p>This is the output to console. It is useful to note that this is the 3rd segment created in the crawl so it has already successfully indexed 2 segments before the error. Could there be something in the new site that is causing corruption?</p>\n\n<pre><code>Indexing 20151030150906 to index\n/opt/apache-nutch-1.10/bin/nutch index -Dsolr.server.url=http://localhost:8983/solr/TestCrawlCore TestCrawl//crawldb -linkdb TestCrawl//linkdb TestCrawl//segments/20151030150906\nIndexer: starting at 2015-10-30 15:14:00\nIndexer: deleting gone documents: false\nIndexer: URL filtering: false\nIndexer: URL normalizing: false\nActive IndexWriters :\nSOLRIndexWriter\n    solr.server.url : URL of the SOLR instance (mandatory)\n    solr.commit.size : buffer size when sending to SOLR (default 1000)\n    solr.mapping.file : name of the mapping file for fields (default solrindex-mapping.xml)\n    solr.auth : use authentication (default false)\n    solr.auth.username : username for authentication\n    solr.auth.password : password for authentication\n\n\nIndexer: java.io.IOException: Job failed!\n    at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:1357)\n    at org.apache.nutch.indexer.IndexingJob.index(IndexingJob.java:113)\n    at org.apache.nutch.indexer.IndexingJob.run(IndexingJob.java:177)\n    at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n    at org.apache.nutch.indexer.IndexingJob.main(IndexingJob.java:187)\n\nError running:\n  /opt/apache-nutch-1.10/bin/nutch index -Dsolr.server.url=http://localhost:8983/solr/TestCrawlCore TestCrawl//crawldb -linkdb TestCrawl//linkdb TestCrawl//segments/20151030150906\nFailed with exit value 255.\n</code></pre>\n\n<p>This is the relevant data from hadoop.log</p>\n\n<pre><code>2015-10-30 15:14:00,854 INFO  indexer.IndexingJob - Indexer: starting at 2015-10-30 15:14:00\n2015-10-30 15:14:00,909 INFO  indexer.IndexingJob - Indexer: deleting gone documents: false\n2015-10-30 15:14:00,909 INFO  indexer.IndexingJob - Indexer: URL filtering: false\n2015-10-30 15:14:00,910 INFO  indexer.IndexingJob - Indexer: URL normalizing: false\n2015-10-30 15:14:01,113 INFO  indexer.IndexWriters - Adding org.apache.nutch.indexwriter.solr.SolrIndexWriter\n2015-10-30 15:14:01,113 INFO  indexer.IndexingJob - Active IndexWriters :\nSOLRIndexWriter\n        solr.server.url : URL of the SOLR instance (mandatory)\n        solr.commit.size : buffer size when sending to SOLR (default 1000)\n        solr.mapping.file : name of the mapping file for fields (default solrindex-mapping.xml)\n        solr.auth : use authentication (default false)\n        solr.auth.username : username for authentication\n        solr.auth.password : password for authentication\n\n\n2015-10-30 15:14:01,118 INFO  indexer.IndexerMapReduce - IndexerMapReduce: crawldb: TestCrawl/crawldb\n2015-10-30 15:14:01,118 INFO  indexer.IndexerMapReduce - IndexerMapReduce: linkdb: TestCrawl/linkdb\n2015-10-30 15:14:01,119 INFO  indexer.IndexerMapReduce - IndexerMapReduces: adding segment: TestCrawl/segments/20151030150906\n2015-10-30 15:14:01,264 WARN  util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n2015-10-30 15:14:01,722 INFO  anchor.AnchorIndexingFilter - Anchor deduplication is: off\n2015-10-30 15:14:02,253 INFO  indexer.IndexWriters - Adding org.apache.nutch.indexwriter.solr.SolrIndexWriter\n2015-10-30 15:14:02,271 INFO  solr.SolrMappingReader - source: content dest: content\n2015-10-30 15:14:02,271 INFO  solr.SolrMappingReader - source: title dest: title\n2015-10-30 15:14:02,271 INFO  solr.SolrMappingReader - source: host dest: host\n2015-10-30 15:14:02,271 INFO  solr.SolrMappingReader - source: segment dest: segment\n2015-10-30 15:14:02,271 INFO  solr.SolrMappingReader - source: boost dest: boost\n2015-10-30 15:14:02,271 INFO  solr.SolrMappingReader - source: digest dest: digest\n2015-10-30 15:14:02,271 INFO  solr.SolrMappingReader - source: tstamp dest: tstamp\n2015-10-30 15:14:02,370 INFO  solr.SolrIndexWriter - Indexing 38 documents\n2015-10-30 15:14:02,487 INFO  solr.SolrIndexWriter - Indexing 38 documents\n2015-10-30 15:14:02,524 WARN  mapred.LocalJobRunner - job_local593696138_0001\norg.apache.solr.common.SolrException: Bad Request\n\nBad Request\n\nrequest: http://localhost:8983/solr/TestCrawlCore/update?wt=javabin&amp;version=2\n        at org.apache.solr.client.solrj.impl.CommonsHttpSolrServer.request(CommonsHttpSolrServer.java:430)\n        at org.apache.solr.client.solrj.impl.CommonsHttpSolrServer.request(CommonsHttpSolrServer.java:244)\n        at org.apache.solr.client.solrj.request.AbstractUpdateRequest.process(AbstractUpdateRequest.java:105)\n        at org.apache.nutch.indexwriter.solr.SolrIndexWriter.close(SolrIndexWriter.java:153)\n        at org.apache.nutch.indexer.IndexWriters.close(IndexWriters.java:115)\n        at org.apache.nutch.indexer.IndexerOutputFormat$1.close(IndexerOutputFormat.java:44)\n        at org.apache.hadoop.mapred.ReduceTask$OldTrackingRecordWriter.close(ReduceTask.java:467)\n        at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:535)\n        at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:421)\n        at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:398)\n2015-10-30 15:14:03,508 ERROR indexer.IndexingJob - Indexer: java.io.IOException: Job failed!\n        at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:1357)\n        at org.apache.nutch.indexer.IndexingJob.index(IndexingJob.java:113)\n        at org.apache.nutch.indexer.IndexingJob.run(IndexingJob.java:177)\n        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n        at org.apache.nutch.indexer.IndexingJob.main(IndexingJob.java:187)\n</code></pre>\n\n<p>I'm just figuring this stuff out so I don't know the next step in troubleshooting this problem. Any help would be appreciated. I'm happy to include more information if there is something specific that would be helpful.</p>\n", "creation_date": 1446237467, "score": 1},
{"title": "how nutch plugins work?", "view_count": 1243, "is_answered": true, "answers": [{"last_edit_date": 1446382144, "owner": {"user_id": 259705, "accept_rate": 62, "link": "http://stackoverflow.com/users/259705/mlathe", "user_type": "registered", "reputation": 1917}, "body": "<p>All Lucene does is provide a way for \"<a href=\"http://lucene.apache.org/java/2_3_2/api/org/apache/lucene/document/Document.html\" rel=\"nofollow\">Documents</a>\" to be added into a structured index and for queries to be executed against that index.</p>\n\n<p>The Nutch crawler (I assume that's what you mean by nutch) just provides an easy way to get unstructured data (ie a website) to get pushed into the index. Just like you can use Solr to easily push xml data into a lucene index.</p>\n\n<p>Nutch plugins simply provide a hook were you can put customer logic. For instance the \"<a href=\"http://nutch.apache.org/apidocs-1.1/org/apache/nutch/parse/pdf/PdfParser.html\" rel=\"nofollow\">parse-pdf</a>\" can convert a binary PDF file into one of these \"lucene Documents\". Basically all it does is use an API that can read PDF documents (<a href=\"http://pdfbox.apache.org/\" rel=\"nofollow\">pdfbox</a>) to extract the text (this is similar to what \"parse-html\" does since html has a lot of parts that isn't text, for example all html tags).</p>\n\n<p>So regarding your concern about binary formats, its not difficult to parse, just difficult to get something useful. For instance we can write a \"parse-image\" plugin that could extract a lot of info about the image (ie name, format, size), it's just that parsing the \"face\" or the \"dog\" in the picture is difficult.</p>\n", "question_id": 1448329, "creation_date": 1289437409, "is_accepted": false, "score": 1, "last_activity_date": 1446382144, "answer_id": 4150585}], "question_id": 1448329, "tags": ["nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/1448329/how-nutch-plugins-work", "last_activity_date": 1446382144, "owner": {"user_id": 162767, "answer_count": 3, "creation_date": 1251208205, "accept_rate": 57, "view_count": 62, "location": "Bangalore, India", "reputation": 658}, "body": "<p>I am new to nutch, but i know nutch uses Lucene for indexing,which only understands text format.</p>\n\n<p>Nutch have many plug-ins that can  is used for crawling the particular format that plug-in meant for.\n my doubt is how actually the nutch plug-in works?.</p>\n\n<p>I seen the Team wiki page for <a href=\"http://wiki.apache.org/nutch/PluginCentral\" rel=\"nofollow\">nutch</a></p>\n\n<p>i want some information like how actually nutch works with lucene.</p>\n\n<p>Thanks you.</p>\n", "creation_date": 1253357805, "score": 0},
{"title": "Nutch 1.10 won&#39;t crawl subdirectories on my site", "view_count": 134, "owner": {"user_id": 2233559, "answer_count": 14, "creation_date": 1364847802, "accept_rate": 82, "view_count": 32, "reputation": 392}, "is_answered": true, "answers": [{"question_id": 33418166, "owner": {"user_id": 1489043, "accept_rate": 82, "link": "http://stackoverflow.com/users/1489043/everreadyeddy", "user_type": "registered", "reputation": 298}, "body": "<p>Does your site have a robots.txt file? It may be restricted on the links it can crawl due to that. Change the Nutch logging to Debug and it might give you a better idea.</p>\n", "creation_date": 1446201530, "is_accepted": true, "score": 1, "last_activity_date": 1446201530, "answer_id": 33433998}], "question_id": 33418166, "tags": ["regex", "solr", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/33418166/nutch-1-10-wont-crawl-subdirectories-on-my-site", "last_activity_date": 1446201530, "accepted_answer_id": 33433998, "body": "<p>I'm new to Nutch and Solr so I've probably just got something configured incorrectly, but I can't find a setting for this in any conf files. </p>\n\n<p>I'm testing Nutch on a relatively small site and it will crawl any page in the root of the site, but nothing in a subdir. So when I look at the core in Solr (5.3.1) and search I can find a page <code>www.somesite.com/somepage.php</code> but none of the pages with urls like <code>www.somesite.com/somedir/somepage.php</code> are there. </p>\n\n<p>I am using the following command to run the crawl script:</p>\n\n<pre><code>sudo -E bin/crawl -i -D solr.server.url=http://localhost:8983/solr/TestCrawlCore urls/ TestCrawl/ 5\n</code></pre>\n\n<p>This should take it through 5 iterations, but it only runs one and reports that there are no more URLs to fetch and exits. There are no errors in the console or hadoop log. </p>\n\n<p>Result:</p>\n\n<pre><code>Injecting seed URLs\n/opt/apache-nutch-1.10/bin/nutch inject TestCrawl//crawldb urls/\nInjector: starting at 2015-10-29 09:51:55\nInjector: crawlDb: TestCrawl/crawldb\nInjector: urlDir: urls\nInjector: Converting injected urls to crawl db entries.\nInjector: Total number of urls rejected by filters: 0\nInjector: Total number of urls after normalization: 1\nInjector: Merging injected urls into crawl db.\nInjector: overwrite: false\nInjector: update: false\nInjector: URLs merged: 1\nInjector: Total new urls injected: 0\nInjector: finished at 2015-10-29 09:51:58, elapsed: 00:00:02\nThu Oct 29 09:51:58 CDT 2015 : Iteration 1 of 5\nGenerating a new segment\n/opt/apache-nutch-1.10/bin/nutch generate -D mapred.reduce.tasks=2 -D mapred.child.java.opts=-Xmx1000m -D mapred.reduce.tasks.speculative.execution=false -D mapred.map.tasks.speculative.execution=false -D mapred.compress.map.output=true TestCrawl//crawldb TestCrawl//segments -topN 50000 -numFetchers 1 -noFilter\nGenerator: starting at 2015-10-29 09:51:58\nGenerator: Selecting best-scoring urls due for fetch.\nGenerator: filtering: false\nGenerator: normalizing: true\nGenerator: topN: 50000\nGenerator: 0 records selected for fetching, exiting ...\nGenerate returned 1 (no new segments created)\nEscaping loop: no more URLs to fetch now\n</code></pre>\n\n<p>seed.txt</p>\n\n<pre><code>http://www.somesite.com\n</code></pre>\n\n<p>(I have also tried adding a trailing '/' but that didn't change anything.)</p>\n\n<p>I have tried all of the following in regex-urlfilter.txt and none seem to work any differently than the others. I have a poor understanding of these filters though.</p>\n\n<pre><code>+^http://([a-z0-9\\]*\\.)*www.somesite.com/\n+^http://([a-z0-9\\-A-Z]*\\.)*www.somesite.com/\n+^http://([a-z0-9\\-A-Z]*\\.)*www.somesite.com/([a-z0-9\\-A-Z]*\\/)*\n+^http://([a-z0-9\\]*\\.)*www.somesite.com/([a-z0-9\\]*\\/)*\n</code></pre>\n\n<p>I've gone through the hadoop log extensively just to be sure they didn't get crawled in an earlier run, thinking this may be a problem with indexing in solr, but it looks like they have just never been crawled and are being ignored. </p>\n\n<p>Can someone point me in the right direction here to troubleshoot this thing? I'm out of ideas and googles. </p>\n", "creation_date": 1446132005, "score": 0},
{"title": "Nutch - clone website", "view_count": 39, "is_answered": false, "question_id": 33354460, "tags": ["web-scraping", "web-crawler", "nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/33354460/nutch-clone-website", "last_activity_date": 1445889294, "owner": {"user_id": 4052054, "answer_count": 2, "creation_date": 1410986671, "accept_rate": 43, "view_count": 6, "reputation": 59}, "body": "<p>I am playing with Apache Nutch and I am crawling a website successfully. I want to make a clone of a website with Nutch so that I can access the crawled webpages offline. Is there a way to do that? I'm looking for something like an endpoint that receives a url and returns the content of the webpage as if I were GETting the url with curl.</p>\n\n<p>I know there are more specialized solutions like <a href=\"http://www.httrack.com/\" rel=\"nofollow\">HTTrack</a>, but I want to know if it is possible to use Nutch to do this.</p>\n", "creation_date": 1445889294, "score": 1},
{"title": "Nutch: input url gets modified by nutch parsechecker", "view_count": 62, "owner": {"user_id": 1377135, "answer_count": 2, "creation_date": 1336238734, "accept_rate": 57, "view_count": 22, "reputation": 71}, "is_answered": true, "answers": [{"last_edit_date": 1445875669, "owner": {"user_id": 2897669, "accept_rate": 75, "link": "http://stackoverflow.com/users/2897669/luke", "user_type": "registered", "reputation": 199}, "body": "<p><strike>This appears to be an internal issue with the particular site. The same thing happens when trying to run  <code>wget http://www.doctorslounge.com/forums/viewtopic.php?f=7&amp;t=40267</code>.</strike></p>\n\n<p>Try this:</p>\n\n<p><code>bin/nutch parsechecker -dumpText \"http://www.doctorslounge.com/forums/viewtopic.php?f=7&amp;t=40267\"</code></p>\n\n<p>That is, you need to quote (or escape) the &amp;.</p>\n\n<p>The other problem you'll have with parsing this page with nutch is that it's prohibited by the site's robots.txt file:</p>\n\n<pre><code>User-agent: *\n...\nDisallow: /forums/viewtopic.php\n</code></pre>\n", "question_id": 33281235, "creation_date": 1445784588, "is_accepted": true, "score": 1, "last_activity_date": 1445875669, "answer_id": 33331011}], "question_id": 33281235, "tags": ["nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/33281235/nutch-input-url-gets-modified-by-nutch-parsechecker", "last_activity_date": 1445875669, "accepted_answer_id": 33331011, "body": "<p>I am using v 1.0 Nutch  <strong>parsechecker</strong> command to parse the following URL <br>\n<a href=\"http://www.doctorslounge.com/forums/viewtopic.php?f=7&amp;t=40267\" rel=\"nofollow\">http://www.doctorslounge.com/forums/viewtopic.php?f=7&amp;t=40267</a><br></p>\n\n<p>But on running parsechecker i get the below result\n<code>\n\"bin/nutch parsechecker -dumpText http://www.doctorslounge.com/forums/viewtopic.php?f=7&amp;t=40267\"\n[1] 8956\n$ fetching: http://www.doctorslounge.com/forums/viewtopic.php?f=7 \nFetch failed with protocol status: notfound(14), lastModified=0:http://www.doctorslounge.com/forums/viewtopic.php?f=7\n</code><br>\nSomehow nutch is automatically modifying my input url <br>\n<a href=\"http://www.doctorslounge.com/forums/viewtopic.php?f=7&amp;t=40267\" rel=\"nofollow\">http://www.doctorslounge.com/forums/viewtopic.php?f=7&amp;t=40267</a> <br>\n<strong>to</strong> <br>\n<a href=\"http://www.doctorslounge.com/forums/viewtopic.php?f=7\" rel=\"nofollow\">http://www.doctorslounge.com/forums/viewtopic.php?f=7</a> <br>\nCan anyone help me circumvent this problem. Thanks</p>\n\n<p>P.S - it fetches other urls of the same domain<br>\ninput- <a href=\"http://www.doctorslounge.com/index.php/articles/page/51032\" rel=\"nofollow\">http://www.doctorslounge.com/index.php/articles/page/51032</a> works perfectly </p>\n", "creation_date": 1445517451, "score": 0},
{"title": "Problems to activate nutch headings plugin", "view_count": 288, "is_answered": false, "answers": [{"last_edit_date": 1405897322, "owner": {"user_id": 3858789, "link": "http://stackoverflow.com/users/3858789/kay-uwe", "user_type": "unregistered", "reputation": 1}, "body": "<p>Within </p>\n\n<pre><code>&lt;name&gt;index.parse.md&lt;/name&gt;\n</code></pre>\n\n<p>check for metatag.h1 and metatag.h2</p>\n\n<pre><code>&lt;property&gt;\n  &lt;name&gt;index.parse.md&lt;/name&gt;\n  &lt;value&gt;metatag.h1,metatag.h2/value&gt;\n  ...\n</code></pre>\n\n<p>btw. Headings is no parse-... filter.\nYou have to use</p>\n\n<pre><code> &lt;name&gt;plugin.includes&lt;/name&gt;\n &lt;value&gt;headings|parse-(html|tika|metatags)|...\n</code></pre>\n\n<p>Now it should work...</p>\n", "question_id": 24686062, "creation_date": 1405893699, "is_accepted": false, "score": 0, "last_activity_date": 1405897322, "answer_id": 24855343}, {"question_id": 24686062, "owner": {"user_id": 5482257, "link": "http://stackoverflow.com/users/5482257/seanb", "user_type": "registered", "reputation": 1}, "body": "<p>After struggeling with this myself I've found that the following should work (Apache Nutch 1.9):</p>\n\n<pre><code>  &lt;property&gt;\n    &lt;name&gt;plugin.includes&lt;/name&gt;\n    &lt;value&gt;protocol-http|headings|parse-(html|tika|metatags)|...&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;index.parse.md&lt;/name&gt;\n    &lt;value&gt;h1,h2,h3&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;headings&lt;/name&gt;\n    &lt;value&gt;h1,h2,h3&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;headings.multivalued&lt;/name&gt;\n    &lt;value&gt;true&lt;/value&gt;\n  &lt;/property&gt;\n</code></pre>\n\n<p>The following should be added to your schema.xml file (when using Apache Solr):</p>\n\n<pre><code>&lt;!-- fields for the headings plugin --&gt;\n&lt;field name=\"h1\" type=\"text\" stored=\"true\" indexed=\"true\" multiValued=\"true\"/&gt;\n&lt;field name=\"h2\" type=\"text\" stored=\"true\" indexed=\"true\" multiValued=\"true\"/&gt;\n&lt;field name=\"h3\" type=\"text\" stored=\"true\" indexed=\"true\" multiValued=\"true\"/&gt;\n</code></pre>\n", "creation_date": 1445650846, "is_accepted": false, "score": 0, "last_activity_date": 1445650846, "answer_id": 33313660}], "question_id": 24686062, "tags": ["plugins", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/24686062/problems-to-activate-nutch-headings-plugin", "last_activity_date": 1445650846, "owner": {"user_id": 3827084, "view_count": 0, "answer_count": 1, "creation_date": 1405024532, "reputation": 16}, "body": "<p>I try to activate the headings plugin in nutch 1.8, but somehow it does not work. Here are the parts of my nutch-site.xml:</p>\n\n<pre><code>&lt;property&gt;\n    &lt;name&gt;plugin.includes&lt;/name&gt;\n    &lt;value&gt;protocol-http|urlfilter-regex|parse-(html|tika|metatags|headings)|index-(basic|anchor|metadata)|scoring-opic|urlnormalizer-(pass|regex|basic)&lt;/value&gt;\n    &lt;description&gt;activates metatag parsing &lt;/description&gt;\n&lt;/property&gt;\n\n&lt;property&gt;\n  &lt;name&gt;headings&lt;/name&gt;\n  &lt;value&gt;h1;h2&lt;/value&gt;\n  &lt;description&gt;Comma separated list of headings to retrieve from the document&lt;/description&gt;\n&lt;/property&gt;\n\n&lt;property&gt;\n  &lt;name&gt;headings.multivalued&lt;/name&gt;\n  &lt;value&gt;false&lt;/value&gt;\n  &lt;description&gt;Whether to support multivalued headings.&lt;/description&gt;\n&lt;/property&gt;\n\n&lt;property&gt;\n &lt;name&gt;index.parse.md&lt;/name&gt;\n &lt;value&gt;metatag.description,metatag.title, metatag.keywords, metatag.author, \nmetatag.author, headings.h1, headings.h2&lt;/value&gt;\n&lt;description&gt; Comma-separated list of keys to be taken from the parse metadata to generate fields. Can be used e.g. for 'description' or 'keywords' provided that these values are generated by a parser (see parse-metatags plugin)\n&lt;/description&gt;\n&lt;/property&gt;\n</code></pre>\n\n<p>can someone help?</p>\n\n<p>Thanks Chris </p>\n", "creation_date": 1405025474, "score": 0},
{"title": "Hadoop 1.03 and Nutch 1.5 issue", "view_count": 410, "is_answered": true, "answers": [{"question_id": 12357448, "owner": {"user_id": 1217274, "accept_rate": 60, "link": "http://stackoverflow.com/users/1217274/badal-singh", "user_type": "registered", "reputation": 190}, "body": "<p>If you want to use hadoop 1.0.3 then use nutch1.5.1 instead of 1.5</p>\n\n<p>Check out the release note of nutch1.5.1\n<a href=\"https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=10680&amp;version=12321850\" rel=\"nofollow\">https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=10680&amp;version=12321850</a></p>\n\n<p>It doesn't say if NUTCH-1084 got fixed in this version but following patch was included in this release\n<a href=\"https://issues.apache.org/jira/browse/NUTCH-1398\" rel=\"nofollow\">https://issues.apache.org/jira/browse/NUTCH-1398</a></p>\n", "creation_date": 1347424661, "is_accepted": false, "score": 1, "last_activity_date": 1347424661, "answer_id": 12381292}], "question_id": 12357448, "tags": ["hadoop", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/12357448/hadoop-1-03-and-nutch-1-5-issue", "last_activity_date": 1445487312, "owner": {"user_id": 1101845, "view_count": 164, "answer_count": 62, "creation_date": 1324035445, "reputation": 3535}, "body": "<p>I get the following error when I try to run nutch-1.5 on hadoop 1.03. </p>\n\n<p>hadoop jar nutch-1.5.job org.apache.nutch.crawl.Crawl urls -dir urls -depth 1 -topN 5</p>\n\n<pre><code>**Caused by: java.io.IOException: can't find class: org.apache.nutch.protocol.ProtocolStatus because org.apache.nutch.protocol.ProtocolStatus**\n</code></pre>\n\n<p>I see the bug report <a href=\"https://issues.apache.org/jira/browse/NUTCH-1084\" rel=\"nofollow\">https://issues.apache.org/jira/browse/NUTCH-1084</a>  on nutch-1.3 but it seems that is not yet resolved.\nAny help is appreciated.</p>\n\n<p>I follow this tutorials:</p>\n\n<p><a href=\"http://wiki.apache.org/nutch/NutchHadoopTutorial\" rel=\"nofollow\">http://wiki.apache.org/nutch/NutchHadoopTutorial</a></p>\n\n<p><a href=\"http://wiki.apache.org/nutch/NutchTutorial\" rel=\"nofollow\">http://wiki.apache.org/nutch/NutchTutorial</a></p>\n\n<p><a href=\"http://wiki.apache.org/hadoop/HowToConfigure\" rel=\"nofollow\">http://wiki.apache.org/hadoop/HowToConfigure</a></p>\n\n<p><strong>EDIT</strong></p>\n\n<p>I follow this tutorial <a href=\"http://www.rui-yang.com/develop/build-nutch-1-4-cluster-with-hadoop/\" rel=\"nofollow\">http://www.rui-yang.com/develop/build-nutch-1-4-cluster-with-hadoop/</a> and it works for me. I don't know what exactly fix the problem. I run hadoop in a single node. \nI make this changes: </p>\n\n<p>1.copy the hadoop-env.sh, core-site.xml, hdfs-site.xml, mapred-site.xml, master, slaves from hadoop/conf to nutch/conf and rebuild nutch</p>\n\n<p>2.export CLASSPATH=:$NUTCH_HOME/runtime/local/lib</p>\n\n<p>I create the following tutorial <a href=\"http://dataspider.blogspot.com.es/2012/09/instalacion-de-hadoop.html\" rel=\"nofollow\">http://dataspider.blogspot.com.es/2012/09/instalacion-de-hadoop.html</a> </p>\n", "creation_date": 1347301749, "score": 0},
{"title": "error building nutch 2.1 on windows 7 Could not load definitions from resource org/sonar/ant/antlib.xml", "view_count": 1667, "is_answered": false, "answers": [{"question_id": 14536531, "owner": {"user_id": 4187633, "link": "http://stackoverflow.com/users/4187633/randy-childers", "user_type": "registered", "reputation": 174}, "body": "<p>For those desperate Googlers who find this:</p>\n\n<p>I had a similar issue which I was able to troubleshoot by adding -v (verbose) to my ant command.  With that, I could see that it was a cygwin/windows path issue that was actually causing the problem.  Reluctantly, I opened a cmd.exe prompt, set the appropriate environment variables, and it worked fine.</p>\n", "creation_date": 1445441514, "is_accepted": false, "score": 0, "last_activity_date": 1445441514, "answer_id": 33263391}], "question_id": 14536531, "tags": ["ant", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/14536531/error-building-nutch-2-1-on-windows-7-could-not-load-definitions-from-resource-o", "last_activity_date": 1445441514, "owner": {"user_id": 328836, "answer_count": 4, "creation_date": 1272541802, "accept_rate": 62, "view_count": 86, "location": "Goa India", "reputation": 500}, "body": "<p>When I try to build nutch 2.1 on my windows 7 machine i get the below error:</p>\n\n<pre><code>Buildfile: C:\\apache-nutch-2.1\\build.xml\n  [taskdef] Could not load definitions from resource org/sonar/ant/antlib.xml. It could not be found.\nivy-probe-antlib:\nivy-download:\n  [taskdef] Could not load definitions from resource org/sonar/ant/antlib.xml. It could not be found.\nivy-download-unchecked:\nivy-init-antlib:\nivy-init:\ninit:\n\nresolve-default:\n[ivy:resolve] :: Ivy 2.2.0 - 20100923230623 :: http://ant.apache.org/ivy/ ::\n[ivy:resolve] :: loading settings :: file = C:\\apache-nutch-2.1\\ivy\\ivysettings.xml\n  [taskdef] Could not load definitions from resource org/sonar/ant/antlib.xml. It could not be found.\n\ncopy-libs:\n\ncompile-core:\n     C:\\apache-nutch-2.1\\build.xml:97: warning: 'includeantruntime' was not set, defaulting to build.sysclasspath=last; set to false for repeatable builds\n     Compiling 181 source files to C:\\apache-nutch-2.1\\build\\classes\n     warning: [path] bad path element \"C:\\apache-nutch-2.1\\build\\lib\\activation.jar\": no such file or directory\n     warning: [options] bootstrap class path not set in conjunction with -source 1.6\n     C:\\apache-nutch-2.1\\src\\java\\org\\apache\\nutch\\api\\APIInfoResource.java:23: error: package org.restlet.resource does not exist\n     import org.restlet.resource.Get;\n                                ^\n     C:\\apache-nutch-2.1\\src\\java\\org\\apache\\nutch\\api\\APIInfoResource.java:24: error: package org.restlet.resource does not exist\n     import org.restlet.resource.ServerResource;\n                                ^\n     C:\\apache-nutch-2.1\\src\\java\\org\\apache\\nutch\\api\\APIInfoResource.java:26: error: cannot find symbol\n     public class APIInfoResource extends ServerResource {\n                                          ^\n       symbol: class ServerResource\n     C:\\apache-nutch-2.1\\src\\java\\org\\apache\\nutch\\api\\AdminResource.java:23: error: package org.restlet.resource does not exist\n     import org.restlet.resource.Get;\n                                ^\n     C:\\apache-nutch-2.1\\src\\java\\org\\apache\\nutch\\api\\AdminResource.java:24: error: package org.restlet.resource does not exist\n     import org.restlet.resource.ServerResource;\n                                ^\n     C:\\apache-nutch-2.1\\src\\java\\org\\apache\\nutch\\api\\AdminResource.java:28: error: cannot find symbol\n     public class AdminResource extends ServerResource {\n                                        ^\n       symbol: class ServerResource\n     C:\\apache-nutch-2.1\\src\\java\\org\\apache\\nutch\\api\\ConfResource.java:22: error: package org.restlet.data does not exist\n     import org.restlet.data.Form;\n                            ^\n     C:\\apache-nutch-2.1\\src\\java\\org\\apache\\nutch\\api\\ConfResource.java:23: error: package org.restlet.resource does not exist\n     import org.restlet.resource.Delete;\n                                ^\n     C:\\apache-nutch-2.1\\src\\java\\org\\apache\\nutch\\api\\ConfResource.java:24: error: package org.restlet.resource does not exist\n     import org.restlet.resource.Get;\n                                ^\n     C:\\apache-nutch-2.1\\src\\java\\org\\apache\\nutch\\api\\ConfResource.java:25: error: package org.restlet.resource does not exist\n     import org.restlet.resource.Post;\n                                ^\n     C:\\apache-nutch-2.1\\src\\java\\org\\apache\\nutch\\api\\ConfResource.java:26: error: package org.restlet.resource does not exist\n     import org.restlet.resource.Put;\n                                ^\n     C:\\apache-nutch-2.1\\src\\java\\org\\apache\\nutch\\api\\ConfResource.java:27: error: package org.restlet.resource does not exist\n     import org.restlet.resource.ServerResource;\n                                ^\n     C:\\apache-nutch-2.1\\src\\java\\org\\apache\\nutch\\api\\ConfResource.java:29: error: cannot find symbol\n     public class ConfResource extends ServerResource {\n                                       ^\n       symbol: class ServerResource\n     C:\\apache-nutch-2.1\\src\\java\\org\\apache\\nutch\\api\\DbReader.java:29: error: package org.apache.avro.util does not exist\n     import org.apache.avro.util.Utf8;\n                                ^\n     C:\\apache-nutch-2.1\\src\\java\\org\\apache\\nutch\\api\\DbReader.java:30: error: package org.apache.gora.query does not exist\n     import org.apache.gora.query.Query;\n                                 ^\n     C:\\apache-nutch-2.1\\src\\java\\org\\apache\\nutch\\api\\DbReader.java:31: error: package org.apache.gora.query does not exist\n     import org.apache.gora.query.Result;\n                                 ^\n     C:\\apache-nutch-2.1\\src\\java\\org\\apache\\nutch\\api\\DbReader.java:32: error: package org.apache.gora.store does not exist\n     import org.apache.gora.store.DataStore;\n                                 ^\n     C:\\apache-nutch-2.1\\src\\java\\org\\apache\\nutch\\api\\DbReader.java:52: error: cannot find symbol\n       DataStore&lt;String,WebPage&gt; store;\n       ^\n       symbol:   class DataStore\n       location: class DbReader\n     C:\\apache-nutch-2.1\\src\\java\\org\\apache\\nutch\\storage\\WebPage.java:22: error: package org.apache.avro does not exist\n     import org.apache.avro.Protocol;\n                           ^\n     C:\\apache-nutch-2.1\\src\\java\\org\\apache\\nutch\\storage\\WebPage.java:23: error: package org.apache.avro does not exist\n     import org.apache.avro.Schema;\n                           ^\n     C:\\apache-nutch-2.1\\src\\java\\org\\apache\\nutch\\storage\\WebPage.java:24: error: package org.apache.avro does not exist\n     import org.apache.avro.AvroRuntimeException;\n                           ^\n     C:\\apache-nutch-2.1\\src\\java\\org\\apache\\nutch\\storage\\WebPage.java:25: error: package org.apache.avro does not exist\n     import org.apache.avro.Protocol;\n                           ^\n     C:\\apache-nutch-2.1\\src\\java\\org\\apache\\nutch\\storage\\WebPage.java:26: error: package org.apache.avro.util does not exist\n     import org.apache.avro.util.Utf8;\n                                ^\n     C:\\apache-nutch-2.1\\src\\java\\org\\apache\\nutch\\storage\\WebPage.java:27: error: package org.apache.avro.ipc does not exist\n     import org.apache.avro.ipc.AvroRemoteException;\n                               ^\n     C:\\apache-nutch-2.1\\src\\java\\org\\apache\\nutch\\storage\\WebPage.java:28: error: package org.apache.avro.generic does not exist\n     import org.apache.avro.generic.GenericArray;\n                                   ^\n     C:\\apache-nutch-2.1\\src\\java\\org\\apache\\nutch\\storage\\WebPage.java:29: error: package org.apache.avro.specific does not exist\n     import org.apache.avro.specific.SpecificExceptionBase;\n                                    ^\n     C:\\apache-nutch-2.1\\src\\java\\org\\apache\\nutch\\storage\\WebPage.java:30: error: package org.apache.avro.specific does not exist\n     import org.apache.avro.specific.SpecificRecordBase;\n                                    ^\n     C:\\apache-nutch-2.1\\src\\java\\org\\apache\\nutch\\storage\\WebPage.java:31: error: package org.apache.avro.specific does not exist\n     import org.apache.avro.specific.SpecificRecord;\n                                    ^\n     C:\\apache-nutch-2.1\\src\\java\\org\\apache\\nutch\\storage\\WebPage.java:32: error: package org.apache.avro.specific does not exist\n     import org.apache.avro.specific.SpecificFixed;\n                                    ^\n     C:\\apache-nutch-2.1\\src\\java\\org\\apache\\nutch\\storage\\WebPage.java:33: error: package org.apache.gora.persistency does not exist\n     import org.apache.gora.persistency.StateManager;\n                                       ^\n     C:\\apache-nutch-2.1\\src\\java\\org\\apache\\nutch\\storage\\WebPage.java:34: error: package org.apache.gora.persistency.impl does not exist\n     import org.apache.gora.persistency.impl.PersistentBase;\n                                            ^\n     C:\\apache-nutch-2.1\\src\\java\\org\\apache\\nutch\\storage\\WebPage.java:35: error: package org.apache.gora.persistency.impl does not exist\n     import org.apache.gora.persistency.impl.StateManagerImpl;\n                                            ^\n     C:\\apache-nutch-2.1\\src\\java\\org\\apache\\nutch\\storage\\WebPage.java:36: error: package org.apache.gora.persistency does not exist\n     import org.apache.gora.persistency.StatefulHashMap;\n                                       ^\n     C:\\apache-nutch-2.1\\src\\java\\org\\apache\\nutch\\storage\\WebPage.java:37: error: package org.apache.gora.persistency does not exist\n     import org.apache.gora.persistency.ListGenericArray;\n                                       ^\n     C:\\apache-nutch-2.1\\src\\java\\org\\apache\\nutch\\storage\\WebPage.java:40: error: cannot find symbol\n     public class WebPage extends PersistentBase {\n                                  ^\n       symbol: class PersistentBase\n     C:\\apache-nutch-2.1\\src\\java\\org\\apache\\nutch\\storage\\WebPage.java:41: error: cannot find symbol\n       public static final Schema _SCHEMA = Schema.parse(\"{\\\"type\\\":\\\"record\\\",\\\"name\\\":\\\"WebPage\\\",\\\"namespace\\\":\\\"org.apache.nutch.storage\\\",\\\"fields\\\":[{\\\"name\\\":\\\"baseUrl\\\",\\\"type\\\":\\\"string\\\"},{\\\"name\\\":\\\"status\\\",\\\"type\\\":\\\"int\\\"},{\\\"name\\\":\\\"fetchTime\\\",\\\"type\\\":\\\"long\\\"},{\\\"name\\\":\\\"prevFetchTime\\\",\\\"type\\\":\\\"long\\\"},{\\\"name\\\":\\\"fetchInterval\\\",\\\"type\\\":\\\"int\\\"},{\\\"name\\\":\\\"retriesSinceFetch\\\",\\\"type\\\":\\\"int\\\"},{\\\"name\\\":\\\"modifiedTime\\\",\\\"type\\\":\\\"long\\\"},{\\\"name\\\":\\\"protocolStatus\\\",\\\"type\\\":{\\\"type\\\":\\\"record\\\",\\\"name\\\":\\\"ProtocolStatus\\\",\\\"fields\\\":[{\\\"name\\\":\\\"code\\\",\\\"type\\\":\\\"int\\\"},{\\\"name\\\":\\\"args\\\",\\\"type\\\":{\\\"type\\\":\\\"array\\\",\\\"items\\\":\\\"string\\\"}},{\\\"name\\\":\\\"lastModified\\\",\\\"type\\\":\\\"long\\\"}]}},{\\\"name\\\":\\\"content\\\",\\\"type\\\":\\\"bytes\\\"},{\\\"name\\\":\\\"contentType\\\",\\\"type\\\":\\\"string\\\"},{\\\"name\\\":\\\"prevSignature\\\",\\\"type\\\":\\\"bytes\\\"},{\\\"name\\\":\\\"signature\\\",\\\"type\\\":\\\"bytes\\\"},{\\\"name\\\":\\\"title\\\",\\\"type\\\":\\\"string\\\"},{\\\"name\\\":\\\"text\\\",\\\"type\\\":\\\"string\\\"},{\\\"name\\\":\\\"parseStatus\\\",\\\"type\\\":{\\\"type\\\":\\\"record\\\",\\\"name\\\":\\\"ParseStatus\\\",\\\"fields\\\":[{\\\"name\\\":\\\"majorCode\\\",\\\"type\\\":\\\"int\\\"},{\\\"name\\\":\\\"minorCode\\\",\\\"type\\\":\\\"int\\\"},{\\\"name\\\":\\\"args\\\",\\\"type\\\":{\\\"type\\\":\\\"array\\\",\\\"items\\\":\\\"string\\\"}}]}},{\\\"name\\\":\\\"score\\\",\\\"type\\\":\\\"float\\\"},{\\\"name\\\":\\\"reprUrl\\\",\\\"type\\\":\\\"string\\\"},{\\\"name\\\":\\\"headers\\\",\\\"type\\\":{\\\"type\\\":\\\"map\\\",\\\"values\\\":\\\"string\\\"}},{\\\"name\\\":\\\"outlinks\\\",\\\"type\\\":{\\\"type\\\":\\\"map\\\",\\\"values\\\":\\\"string\\\"}},{\\\"name\\\":\\\"inlinks\\\",\\\"type\\\":{\\\"type\\\":\\\"map\\\",\\\"values\\\":\\\"string\\\"}},{\\\"name\\\":\\\"markers\\\",\\\"type\\\":{\\\"type\\\":\\\"map\\\",\\\"values\\\":\\\"string\\\"}},{\\\"name\\\":\\\"metadata\\\",\\\"type\\\":{\\\"type\\\":\\\"map\\\",\\\"values\\\":\\\"bytes\\\"}}]}\");\n                           ^\n       symbol:   class Schema\n       location: class WebPage\n     C:\\apache-nutch-2.1\\src\\java\\org\\apache\\nutch\\storage\\WebPage.java:77: error: cannot find symbol\n       private Utf8 baseUrl;\n               ^\n       symbol:   class Utf8\n       location: class WebPage\n     C:\\apache-nutch-2.1\\src\\java\\org\\apache\\nutch\\storage\\ProtocolStatus.java:22: error: package org.apache.avro does not exist\n     import org.apache.avro.Protocol;\n                           ^\n     C:\\apache-nutch-2.1\\src\\java\\org\\apache\\nutch\\storage\\ProtocolStatus.java:23: error: package org.apache.avro does not exist\n     import org.apache.avro.Schema;\n                           ^\n     C:\\apache-nutch-2.1\\src\\java\\org\\apache\\nutch\\storage\\ProtocolStatus.java:24: error: package org.apache.avro does not exist\n     import org.apache.avro.AvroRuntimeException;\n                           ^\n     C:\\apache-nutch-2.1\\src\\java\\org\\apache\\nutch\\storage\\ProtocolStatus.java:25: error: package org.apache.avro does not exist\n     import org.apache.avro.Protocol;\n                           ^\n     C:\\apache-nutch-2.1\\src\\java\\org\\apache\\nutch\\storage\\ProtocolStatus.java:26: error: package org.apache.avro.util does not exist\n     import org.apache.avro.util.Utf8;\n                                ^\n     C:\\apache-nutch-2.1\\src\\java\\org\\apache\\nutch\\storage\\ProtocolStatus.java:27: error: package org.apache.avro.ipc does not exist\n     import org.apache.avro.ipc.AvroRemoteException;\n                               ^\n     C:\\apache-nutch-2.1\\src\\java\\org\\apache\\nutch\\storage\\ProtocolStatus.java:28: error: package org.apache.avro.generic does not exist\n     import org.apache.avro.generic.GenericArray;\n                                   ^\n     C:\\apache-nutch-2.1\\src\\java\\org\\apache\\nutch\\storage\\ProtocolStatus.java:29: error: package org.apache.avro.specific does not exist\n     import org.apache.avro.specific.SpecificExceptionBase;\n                                    ^\n     C:\\apache-nutch-2.1\\src\\java\\org\\apache\\nutch\\storage\\ProtocolStatus.java:30: error: package org.apache.avro.specific does not exist\n     import org.apache.avro.specific.SpecificRecordBase;\n                                    ^\n     C:\\apache-nutch-2.1\\src\\java\\org\\apache\\nutch\\storage\\ProtocolStatus.java:31: error: package org.apache.avro.specific does not exist\n     import org.apache.avro.specific.SpecificRecord;\n                                    ^\n     C:\\apache-nutch-2.1\\src\\java\\org\\apache\\nutch\\storage\\ProtocolStatus.java:32: error: package org.apache.avro.specific does not exist\n     import org.apache.avro.specific.SpecificFixed;\n                                    ^\n     C:\\apache-nutch-2.1\\src\\java\\org\\apache\\nutch\\storage\\ProtocolStatus.java:33: error: package org.apache.gora.persistency does not exist\n     import org.apache.gora.persistency.StateManager;\n                                       ^\n     C:\\apache-nutch-2.1\\src\\java\\org\\apache\\nutch\\storage\\ProtocolStatus.java:34: error: package org.apache.gora.persistency.impl does not exist\n     import org.apache.gora.persistency.impl.PersistentBase;\n                                            ^\n     C:\\apache-nutch-2.1\\src\\java\\org\\apache\\nutch\\storage\\ProtocolStatus.java:35: error: package org.apache.gora.persistency.impl does not exist\n     import org.apache.gora.persistency.impl.StateManagerImpl;\n                                            ^\n     C:\\apache-nutch-2.1\\src\\java\\org\\apache\\nutch\\storage\\ProtocolStatus.java:36: error: package org.apache.gora.persistency does not exist\n     import org.apache.gora.persistency.StatefulHashMap;\n                                       ^\n     C:\\apache-nutch-2.1\\src\\java\\org\\apache\\nutch\\storage\\ProtocolStatus.java:37: error: package org.apache.gora.persistency does not exist\n     import org.apache.gora.persistency.ListGenericArray;\n                                       ^\n     C:\\apache-nutch-2.1\\src\\java\\org\\apache\\nutch\\storage\\ProtocolStatus.java:40: error: cannot find symbol\n     public class ProtocolStatus extends PersistentBase {\n                                         ^\n       symbol: class PersistentBase\n     C:\\apache-nutch-2.1\\src\\java\\org\\apache\\nutch\\storage\\WebPage.java:86: error: cannot find symbol\n       private Utf8 contentType;\n               ^\n       symbol:   class Utf8\n       location: class WebPage\n     C:\\apache-nutch-2.1\\src\\java\\org\\apache\\nutch\\storage\\WebPage.java:89: error: cannot find symbol\n       private Utf8 title;\n               ^\n       symbol:   class Utf8\n       location: class WebPage\n     C:\\apache-nutch-2.1\\src\\java\\org\\apache\\nutch\\storage\\WebPage.java:90: error: cannot find symbol\n       private Utf8 text;\n               ^\n       symbol:   class Utf8\n       location: class WebPage\n     C:\\apache-nutch-2.1\\src\\java\\org\\apache\\nutch\\storage\\ParseStatus.java:22: error: package org.apache.avro does not exist\n     import org.apache.avro.Protocol;\n                           ^\n     C:\\apache-nutch-2.1\\src\\java\\org\\apache\\nutch\\storage\\ParseStatus.java:23: error: package org.apache.avro does not exist\n     import org.apache.avro.Schema;\n                           ^\n     C:\\apache-nutch-2.1\\src\\java\\org\\apache\\nutch\\storage\\ParseStatus.java:24: error: package org.apache.avro does not exist\n     import org.apache.avro.AvroRuntimeException;\n                           ^\n     C:\\apache-nutch-2.1\\src\\java\\org\\apache\\nutch\\storage\\ParseStatus.java:25: error: package org.apache.avro does not exist\n     import org.apache.avro.Protocol;\n                           ^\n     C:\\apache-nutch-2.1\\src\\java\\org\\apache\\nutch\\storage\\ParseStatus.java:26: error: package org.apache.avro.util does not exist\n     import org.apache.avro.util.Utf8;\n                                ^\n     C:\\apache-nutch-2.1\\src\\java\\org\\apache\\nutch\\storage\\ParseStatus.java:27: error: package org.apache.avro.ipc does not exist\n     import org.apache.avro.ipc.AvroRemoteException;\n                               ^\n     C:\\apache-nutch-2.1\\src\\java\\org\\apache\\nutch\\storage\\ParseStatus.java:28: error: package org.apache.avro.generic does not exist\n     import org.apache.avro.generic.GenericArray;\n                                   ^\n     C:\\apache-nutch-2.1\\src\\java\\org\\apache\\nutch\\storage\\ParseStatus.java:29: error: package org.apache.avro.specific does not exist\n     import org.apache.avro.specific.SpecificExceptionBase;\n                                    ^\n     C:\\apache-nutch-2.1\\src\\java\\org\\apache\\nutch\\storage\\ParseStatus.java:30: error: package org.apache.avro.specific does not exist\n     import org.apache.avro.specific.SpecificRecordBase;\n                                    ^\n     C:\\apache-nutch-2.1\\src\\java\\org\\apache\\nutch\\storage\\ParseStatus.java:31: error: package org.apache.avro.specific does not exist\n     import org.apache.avro.specific.SpecificRecord;\n                                    ^\n     C:\\apache-nutch-2.1\\src\\java\\org\\apache\\nutch\\storage\\ParseStatus.java:32: error: package org.apache.avro.specific does not exist\n     import org.apache.avro.specific.SpecificFixed;\n                                    ^\n     C:\\apache-nutch-2.1\\src\\java\\org\\apache\\nutch\\storage\\ParseStatus.java:33: error: package org.apache.gora.persistency does not exist\n     import org.apache.gora.persistency.StateManager;\n                                       ^\n     C:\\apache-nutch-2.1\\src\\java\\org\\apache\\nutch\\storage\\ParseStatus.java:34: error: package org.apache.gora.persistency.impl does not exist\n     import org.apache.gora.persistency.impl.PersistentBase;\n                                            ^\n     C:\\apache-nutch-2.1\\src\\java\\org\\apache\\nutch\\storage\\ParseStatus.java:35: error: package org.apache.gora.persistency.impl does not exist\n     import org.apache.gora.persistency.impl.StateManagerImpl;\n                                            ^\n     C:\\apache-nutch-2.1\\src\\java\\org\\apache\\nutch\\storage\\ParseStatus.java:36: error: package org.apache.gora.persistency does not exist\n     import org.apache.gora.persistency.StatefulHashMap;\n                                       ^\n     C:\\apache-nutch-2.1\\src\\java\\org\\apache\\nutch\\storage\\ParseStatus.java:37: error: package org.apache.gora.persistency does not exist\n     import org.apache.gora.persistency.ListGenericArray;\n                                       ^\n     C:\\apache-nutch-2.1\\src\\java\\org\\apache\\nutch\\storage\\ParseStatus.java:40: error: cannot find symbol\n     public class ParseStatus extends PersistentBase {\n                                      ^\n       symbol: class PersistentBase\n     C:\\apache-nutch-2.1\\src\\java\\org\\apache\\nutch\\storage\\WebPage.java:93: error: cannot find symbol\n       private Utf8 reprUrl;\n               ^\n       symbol:   class Utf8\n       location: class WebPage\n     C:\\apache-nutch-2.1\\src\\java\\org\\apache\\nutch\\storage\\WebPage.java:94: error: cannot find symbol\n       private Map&lt;Utf8,Utf8&gt; headers;\n                   ^\n       symbol:   class Utf8\n       location: class WebPage\n     C:\\apache-nutch-2.1\\src\\java\\org\\apache\\nutch\\storage\\WebPage.java:94: error: cannot find symbol\n       private Map&lt;Utf8,Utf8&gt; headers;\n                        ^\n       symbol:   class Utf8\n       location: class WebPage\n     C:\\apache-nutch-2.1\\src\\java\\org\\apache\\nutch\\storage\\WebPage.java:95: error: cannot find symbol\n       private Map&lt;Utf8,Utf8&gt; outlinks;\n                   ^\n       symbol:   class Utf8\n       location: class WebPage\n     C:\\apache-nutch-2.1\\src\\java\\org\\apache\\nutch\\storage\\WebPage.java:95: error: cannot find symbol\n       private Map&lt;Utf8,Utf8&gt; outlinks;\n                        ^\n       symbol:   class Utf8\n       location: class WebPage\n     C:\\apache-nutch-2.1\\src\\java\\org\\apache\\nutch\\storage\\WebPage.java:96: error: cannot find symbol\n       private Map&lt;Utf8,Utf8&gt; inlinks;\n                   ^\n       symbol:   class Utf8\n       location: class WebPage\n     C:\\apache-nutch-2.1\\src\\java\\org\\apache\\nutch\\storage\\WebPage.java:96: error: cannot find symbol\n       private Map&lt;Utf8,Utf8&gt; inlinks;\n                        ^\n       symbol:   class Utf8\n       location: class WebPage\n     C:\\apache-nutch-2.1\\src\\java\\org\\apache\\nutch\\storage\\WebPage.java:97: error: cannot find symbol\n       private Map&lt;Utf8,Utf8&gt; markers;\n                   ^\n       symbol:   class Utf8\n       location: class WebPage\n     C:\\apache-nutch-2.1\\src\\java\\org\\apache\\nutch\\storage\\WebPage.java:97: error: cannot find symbol\n       private Map&lt;Utf8,Utf8&gt; markers;\n                        ^\n       symbol:   class Utf8\n       location: class WebPage\n     C:\\apache-nutch-2.1\\src\\java\\org\\apache\\nutch\\storage\\WebPage.java:98: error: cannot find symbol\n       private Map&lt;Utf8,ByteBuffer&gt; metadata;\n                   ^\n       symbol:   class Utf8\n       location: class WebPage\n     C:\\apache-nutch-2.1\\src\\java\\org\\apache\\nutch\\storage\\WebPage.java:102: error: cannot find symbol\n       public WebPage(StateManager stateManager) {\n                      ^\n       symbol:   class StateManager\n       location: class WebPage\n     C:\\apache-nutch-2.1\\src\\java\\org\\apache\\nutch\\storage\\WebPage.java:110: error: cannot find symbol\n       public WebPage newInstance(StateManager stateManager) {\n                                  ^\n       symbol:   class StateManager\n       location: class WebPage\n     C:\\apache-nutch-2.1\\src\\java\\org\\apache\\nutch\\storage\\WebPage.java:113: error: cannot find symbol\n       public Schema getSchema() { return _SCHEMA; }\n              ^\n       symbol:   class Schema\n       location: class WebPage\n     C:\\apache-nutch-2.1\\src\\java\\org\\apache\\nutch\\storage\\WebPage.java:171: error: cannot find symbol\n       public Utf8 getBaseUrl() {\n              ^\n       symbol:   class Utf8\n       location: class WebPage\n     C:\\apache-nutch-2.1\\src\\java\\org\\apache\\nutch\\storage\\WebPage.java:174: error: cannot find symbol\n       public void setBaseUrl(Utf8 value) {\n                              ^\n       symbol:   class Utf8\n       location: class WebPage\n     C:\\apache-nutch-2.1\\src\\java\\org\\apache\\nutch\\storage\\WebPage.java:225: error: cannot find symbol\n       public Utf8 getContentType() {\n              ^\n       symbol:   class Utf8\n       location: class WebPage\n     C:\\apache-nutch-2.1\\src\\java\\org\\apache\\nutch\\storage\\WebPage.java:228: error: cannot find symbol\n       public void setContentType(Utf8 value) {\n                                  ^\n       symbol:   class Utf8\n       location: class WebPage\n     C:\\apache-nutch-2.1\\src\\java\\org\\apache\\nutch\\storage\\WebPage.java:243: error: cannot find symbol\n       public Utf8 getTitle() {\n              ^\n       symbol:   class Utf8\n       location: class WebPage\n     C:\\apache-nutch-2.1\\src\\java\\org\\apache\\nutch\\storage\\WebPage.java:246: error: cannot find symbol\n       public void setTitle(Utf8 value) {\n                            ^\n       symbol:   class Utf8\n       location: class WebPage\n     C:\\apache-nutch-2.1\\src\\java\\org\\apache\\nutch\\storage\\WebPage.java:249: error: cannot find symbol\n       public Utf8 getText() {\n              ^\n       symbol:   class Utf8\n       location: class WebPage\n     C:\\apache-nutch-2.1\\src\\java\\org\\apache\\nutch\\storage\\WebPage.java:252: error: cannot find symbol\n       public void setText(Utf8 value) {\n                           ^\n       symbol:   class Utf8\n       location: class WebPage\n     C:\\apache-nutch-2.1\\src\\java\\org\\apache\\nutch\\storage\\WebPage.java:267: error: cannot find symbol\n       public Utf8 getReprUrl() {\n              ^\n       symbol:   class Utf8\n       location: class WebPage\n     C:\\apache-nutch-2.1\\src\\java\\org\\apache\\nutch\\storage\\WebPage.java:270: error: cannot find symbol\n       public void setReprUrl(Utf8 value) {\n                              ^\n       symbol:   class Utf8\n       location: class WebPage\n     C:\\apache-nutch-2.1\\src\\java\\org\\apache\\nutch\\storage\\WebPage.java:273: error: cannot find symbol\n       public Map&lt;Utf8, Utf8&gt; getHeaders() {\n                  ^\n       symbol:   class Utf8\n       location: class WebPage\n     C:\\apache-nutch-2.1\\src\\java\\org\\apache\\nutch\\storage\\WebPage.java:273: error: cannot find symbol\n       public Map&lt;Utf8, Utf8&gt; getHeaders() {\n                        ^\n       symbol:   class Utf8\n       location: class WebPage\n     C:\\apache-nutch-2.1\\src\\java\\org\\apache\\nutch\\storage\\WebPage.java:276: error: cannot find symbol\n       public Utf8 getFromHeaders(Utf8 key) {\n                                  ^\n       symbol:   class Utf8\n       location: class WebPage\n     C:\\apache-nutch-2.1\\src\\java\\org\\apache\\nutch\\indexer\\mongodb\\MongodbIndexer.java:18: warning: [deprecation] JobConf in org.apache.hadoop.mapred has been deprecated\n     import org.apache.hadoop.mapred.JobConf;\n                                    ^\n     C:\\apache-nutch-2.1\\src\\java\\org\\apache\\nutch\\indexer\\mongodb\\MongodbWriter.java:7: warning: [deprecation] JobConf in org.apache.hadoop.mapred has been deprecated\n     import org.apache.hadoop.mapred.JobConf;\n                                    ^\n     C:\\apache-nutch-2.1\\src\\java\\org\\apache\\nutch\\tools\\arc\\ArcInputFormat.java:23: warning: [deprecation] FileInputFormat in org.apache.hadoop.mapred has been deprecated\n     import org.apache.hadoop.mapred.FileInputFormat;\n                                    ^\n     C:\\apache-nutch-2.1\\src\\java\\org\\apache\\nutch\\tools\\arc\\ArcInputFormat.java:24: warning: [deprecation] FileSplit in org.apache.hadoop.mapred has been deprecated\n     import org.apache.hadoop.mapred.FileSplit;\n                                    ^\n     ..\n     ..\n</code></pre>\n\n<p>more errors are there ..........</p>\n\n<pre><code> C:\\apache-nutch-2.1\\src\\java\\org\\apache\\nutch\\util\\SuffixStringMatcher.java:46: warning: [rawtypes] found raw type: Iterator\n     Iterator iter= suffixes.iterator();\n     ^\n   missing type arguments for generic class Iterator&lt;E&gt;\n   where E is a type-variable:\n     E extends Object declared in interface Iterator\n C:\\apache-nutch-2.1\\src\\java\\org\\apache\\nutch\\util\\ToolUtil.java:48: warning: [unchecked] unchecked cast\n     Map&lt;String,Object&gt; jobs = (Map&lt;String,Object&gt;)results.get(Nutch.STAT_JOBS);\n                                                              ^\n   required: Map&lt;String,Object&gt;\n   found:    Object\n 100 errors\n 69 warnings\n</code></pre>\n\n<p>How can I fix this and get Nutch running?</p>\n", "creation_date": 1359200126, "score": 1},
{"title": "index all content from all segments to Solr gives error", "view_count": 20, "is_answered": false, "question_id": 33211435, "tags": ["java", "solr", "nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/33211435/index-all-content-from-all-segments-to-solr-gives-error", "last_activity_date": 1445252540, "owner": {"user_id": 1793324, "view_count": 1, "answer_count": 1, "creation_date": 1351833373, "reputation": 16}, "body": "<p>I have working with Apache Solr 4.7.0 and Nutch 1.7.\nusing Nutch I am able to run crawler to fetch data but when I am connecting to solr by below query it give me error.</p>\n\n<blockquote>\n  <p>bin/nutch solrindex <a href=\"http://127.0.0.1:8983/solr/\" rel=\"nofollow\">http://127.0.0.1:8983/solr/</a> crawl/crawldb crawl/linkdb crawl/segments/*</p>\n</blockquote>\n\n<p>Error :</p>\n\n<pre><code>SOLRIndexWriter\n        solr.server.url : URL of the SOLR instance (mandatory)\n        solr.commit.size : buffer size when sending to SOLR (default 1000)\n        solr.mapping.file : name of the mapping file for fields (default solrindex-mapping.xml)\n        solr.auth : use authentication (default false)\n        solr.auth.username : use authentication (default false)\n        solr.auth : username for authentication\n        solr.auth.password : password for authentication\n\njava.lang.Exception: No content found\n</code></pre>\n", "creation_date": 1445248267, "score": 1},
{"title": "Nutch1.X add COOKIE to request", "view_count": 17, "is_answered": false, "question_id": 33206901, "tags": ["java", "cookies", "social-networking", "nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/33206901/nutch1-x-add-cookie-to-request", "last_activity_date": 1445230782, "owner": {"user_id": 5260902, "view_count": 0, "answer_count": 0, "creation_date": 1440428398, "reputation": 1}, "body": "<p>I am new to Nutch.I have an idea of collecting user's data of qzone( a social network in China). </p>\n\n<p>I know how to use Apache HttpClient to send request with cookie ,but I want to enlarge my program to catch data with more than one PC.I am using Java and find Nutch is a good choice.I need add cookie to my request and don't know how to do.\nAfter searching the Internet ,I found no such demos or references.</p>\n\n<p>Can anyone help me?</p>\n", "creation_date": 1445230782, "score": 0},
{"title": "Apache Solr: I have run this command in Cygwin Terminal. ./nutch crawl urls -dir newCrawl -solr http://localhost:8939/solr/ -depth 10 -topN 10", "view_count": 119, "is_answered": true, "answers": [{"question_id": 33140593, "owner": {"user_id": 137650, "link": "http://stackoverflow.com/users/137650/matslindh", "user_type": "registered", "reputation": 13901}, "body": "<p>Solr doesn't seem to be running - Connection refused indicates that something is unable to contact whatever it should contact. Your configuration is probably broken, or you haven't started Solr or you're running on a different port.</p>\n", "creation_date": 1444922456, "is_accepted": false, "score": 0, "last_activity_date": 1444922456, "answer_id": 33152403}, {"question_id": 33140593, "owner": {"user_id": 1699191, "link": "http://stackoverflow.com/users/1699191/ameertawfik", "user_type": "registered", "reputation": 618}, "body": "<p>Your solr url is not valid. You need to write the core as well. By default the core is collection1. Therefore, in your case the url is <a href=\"http://localhost:8983/solr/collection1\" rel=\"nofollow\">http://localhost:8983/solr/collection1</a></p>\n", "creation_date": 1444937983, "is_accepted": false, "score": 1, "last_activity_date": 1444937983, "answer_id": 33157099}], "question_id": 33140593, "tags": ["java", "indexing", "solr", "cygwin", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/33140593/apache-solr-i-have-run-this-command-in-cygwin-terminal-nutch-crawl-urls-dir", "last_activity_date": 1444937983, "owner": {"user_id": 5448021, "view_count": 0, "answer_count": 0, "creation_date": 1444886103, "reputation": 1}, "body": "<p>SolrIndexer: starting at 2015-10-15 10:13:00<br>\nAdding 90 documents:</p>\n\n<pre><code>java.io.IOException: Job failed!\nSolrDeleteDuplicates: starting at 2015-10-15 10:13:11\nSolrDeleteDuplicates: Solr url: http://localhost:8939/solr/\nException in thread \"main\" java.io.IOException: org.apache.solr.client.solrj.SolrServerException: java.net.ConnectException: Connection refused: connect\n        at org.apache.nutch.indexer.solr.SolrDeleteDuplicates$SolrInputFormat.getSplits(SolrDeleteDuplicates.java:200)\n        at org.apache.hadoop.mapred.JobClient.writeOldSplits(JobClient.java:810)\n        at org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:781)\n        at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:730)\n        at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:1249)\n        at org.apache.nutch.indexer.solr.SolrDeleteDuplicates.dedup(SolrDeleteDuplicates.java:373)\n        at org.apache.nutch.indexer.solr.SolrDeleteDuplicates.dedup(SolrDeleteDuplicates.java:353)\n        at org.apache.nutch.crawl.Crawl.run(Crawl.java:153)\n        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n        at org.apache.nutch.crawl.Crawl.main(Crawl.java:55)\nCaused by: org.apache.solr.client.solrj.SolrServerException: java.net.ConnectException: Connection refused: connect\n        at org.apache.solr.client.solrj.impl.CommonsHttpSolrServer.request(CommonsHttpSolrServer.java:478)\n        at org.apache.solr.client.solrj.impl.CommonsHttpSolrServer.request(CommonsHttpSolrServer.java:244)\n        at org.apache.solr.client.solrj.request.QueryRequest.process(QueryRequest.java:89)\n        at org.apache.solr.client.solrj.SolrServer.query(SolrServer.java:118)\n        at org.apache.nutch.indexer.solr.SolrDeleteDuplicates$SolrInputFormat.getSplits(SolrDeleteDuplicates.java:198)\n        ... 9 more\nCaused by: java.net.ConnectException: Connection refused: connect\n        at java.net.DualStackPlainSocketImpl.connect0(Native Method)\n        at java.net.DualStackPlainSocketImpl.socketConnect(DualStackPlainSocketImpl.java:79)\n        at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:345)\n        at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)\n        at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)\n        at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)\n        at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)\n        at java.net.Socket.connect(Socket.java:589)\n        at java.net.Socket.connect(Socket.java:538)\n        at java.net.Socket.&lt;init&gt;(Socket.java:434)\n        at java.net.Socket.&lt;init&gt;(Socket.java:286)\n        at org.apache.commons.httpclient.protocol.DefaultProtocolSocketFactory.createSocket(DefaultProtocolSocketFactory.java:80)\n        at org.apache.commons.httpclient.protocol.DefaultProtocolSocketFactory.createSocket(DefaultProtocolSocketFactory.java:122)\n        at org.apache.commons.httpclient.HttpConnection.open(HttpConnection.java:707)\n        at org.apache.commons.httpclient.HttpMethodDirector.executeWithRetry(HttpMethodDirector.java:387)\n        at org.apache.commons.httpclient.HttpMethodDirector.executeMethod(HttpMethodDirector.java:171)\n        at org.apache.commons.httpclient.HttpClient.executeMethod(HttpClient.java:397)\n        at org.apache.commons.httpclient.HttpClient.executeMethod(HttpClient.java:323)\n        at org.apache.solr.client.solrj.impl.CommonsHttpSolrServer.request(CommonsHttpSolrServer.java:422)\n        ... 13 more\n</code></pre>\n\n<p>How to resolve this issue?</p>\n", "creation_date": 1444887166, "score": -3},
{"title": "Nutch readlinkdb does not output anything", "view_count": 416, "is_answered": true, "answers": [{"last_edit_date": 1444874310, "owner": {"user_id": 1013444, "accept_rate": 85, "link": "http://stackoverflow.com/users/1013444/kitwalker", "user_type": "registered", "reputation": 517}, "body": "<p>Maybe you\u2019re indexing just a particular site. In that case, if <code>db.ignore.internal.links</code> in <code>nutch-default.xml</code> is true, nutch won\u2019t store internal links. Set it to false in <code>nutch-site.xml</code> and your linkdb will start growing.</p>\n\n<pre><code>&lt;property&gt;\n  &lt;name&gt;db.ignore.internal.links&lt;/name&gt;\n  &lt;value&gt;false&lt;/value&gt;\n  &lt;description&gt;If true, when adding new links to a page, links from\n  the same host are ignored.  This is an effective way to limit the\n  size of the link database, keeping only the highest quality\n  links.\n  &lt;/description&gt;\n&lt;/property&gt;\n</code></pre>\n", "question_id": 12781027, "creation_date": 1365268084, "is_accepted": false, "score": 3, "last_activity_date": 1444874310, "answer_id": 15853691}], "question_id": 12781027, "tags": ["nutch", "web-crawler"], "answer_count": 1, "link": "http://stackoverflow.com/questions/12781027/nutch-readlinkdb-does-not-output-anything", "last_activity_date": 1444874310, "owner": {"user_id": 937918, "answer_count": 15, "creation_date": 1315635668, "accept_rate": 23, "view_count": 44, "location": "Bangalore", "reputation": 180}, "body": "<p>I used Nutch 1.5 to crawl (used the crawl command), post this the readlinkdb dump contains nothing. Also, in an Indexing filter the inlinks is null. What causes the inlinks to be null?</p>\n", "creation_date": 1349696526, "score": 3},
{"title": "how to get a facebook friend location using web crawlers", "view_count": 65, "is_answered": true, "answers": [{"question_id": 33101556, "owner": {"user_id": 757508, "link": "http://stackoverflow.com/users/757508/luschn", "user_type": "registered", "reputation": 40516}, "body": "<ul>\n<li>You can\u00b4t get any data of friends, friend permissions have been removed a long time ago and especially the location would be a major privacy issue.</li>\n<li>Crawling/Scraping is not allowed on Facebook: <a href=\"https://www.facebook.com/apps/site_scraping_tos_terms.php\" rel=\"nofollow\">https://www.facebook.com/apps/site_scraping_tos_terms.php</a></li>\n</ul>\n", "creation_date": 1444736433, "is_accepted": false, "score": 1, "last_activity_date": 1444736433, "answer_id": 33101740}], "question_id": 33101556, "tags": ["android", "facebook", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/33101556/how-to-get-a-facebook-friend-location-using-web-crawlers", "last_activity_date": 1444736433, "owner": {"user_id": 5440508, "view_count": 0, "answer_count": 0, "creation_date": 1444734799, "reputation": 1}, "body": "<p>I want to ask if there is any way I can get mine Facebook friends location using Nutch crawler or other open source web crawlers. </p>\n", "creation_date": 1444735839, "score": -4},
{"title": "Failure in URL fetching using Nutch", "view_count": 109, "is_answered": false, "question_id": 33071187, "tags": ["java", "nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/33071187/failure-in-url-fetching-using-nutch", "last_activity_date": 1444667545, "owner": {"user_id": 5280153, "answer_count": 2, "creation_date": 1440865126, "accept_rate": 27, "view_count": 18, "reputation": 55}, "body": "<p>I have been using nutch to build my own web crawler to search for images through a given seed list, and  I am getting an error, I reinstalled nutch to try crawling again but the error keeps occuring again. What can be the problem?</p>\n\n<p>Some replies on the Nutch list indicate not enough space in the /tmp folder but I am not sure.</p>\n\n<p>Here is my log:</p>\n\n<pre><code>Fetcher: java.io.IOException: Job failed!\nat org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:836)\nat org.apache.nutch.fetcher.Fetcher.fetch(Fetcher.java:496)\nat org.apache.nutch.fetcher.Fetcher.run(Fetcher.java:532)\nat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)\nat org.apache.nutch.fetcher.Fetcher.main(Fetcher.java:505)\nError running:\n/home/vasan/nutch/runtime/local/bin/nutch fetch -D mapred.reduce.tasks=2 -D mapred.child.java.opts=-Xmx1000m -D mapred.reduce.tasks.speculative.execution=false -D mapred.map.tasks.speculative.execution=false -D mapred.compress.map.output=true -D fetcher.timelimit.mins=180 /media/sf_vasans_repository/WrkSpcFinal/accurateshooter_com//segments/20151011160429 -noParsing -threads 50\nFailed with exit value 255.\n</code></pre>\n\n<p>Any advice would be appreciated. I don't think any problems are present in the configuration files, as they are working fine.</p>\n", "creation_date": 1444605864, "score": 0},
{"title": "Nutch Crawl Script", "view_count": 152, "is_answered": true, "answers": [{"question_id": 32487710, "owner": {"user_id": 2702341, "link": "http://stackoverflow.com/users/2702341/sujen-shah", "user_type": "registered", "reputation": 183}, "body": "<p>-i : Is a program like Solr/ElasticSearch etc. So when you specify the -i option, the crawl script runs the index job or else it skips it. </p>\n\n<p>Crawl Dir : is the directory where the crawl data is stored. This includes the crawldb, segments and linkdb. So basically all the data relating to the crawl goes in here. </p>\n\n<p>The results of a crawl go into the crawlDir you specify. It is stored as a sequence file and there are commands to view the data. </p>\n\n<p>You can find them at - <a href=\"https://wiki.apache.org/nutch/CommandLineOptions\" rel=\"nofollow\">https://wiki.apache.org/nutch/CommandLineOptions</a>.</p>\n", "creation_date": 1444369346, "is_accepted": false, "score": 1, "last_activity_date": 1444369346, "answer_id": 33030690}], "question_id": 32487710, "tags": ["solr", "cygwin", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/32487710/nutch-crawl-script", "last_activity_date": 1444369346, "owner": {"user_id": 2539093, "answer_count": 1, "creation_date": 1372684065, "accept_rate": 60, "view_count": 4, "location": "Watertown, NY", "reputation": 8}, "body": "<p>Running Nutch 1.10 and I'm having trouble using the Crawl Script provided by the Nutch developers:</p>\n\n<pre><code>Usage: crawl [-i|--index] [-D \"key=value\"] &lt;Seed Dir&gt; &lt;Crawl Dir&gt; &lt;Num     Rounds&gt;\n    -i|--index      Indexes crawl results into a configured indexer\n    -D              A Java property to pass to Nutch calls\n    Seed Dir        Directory in which to look for a seeds file\n    Crawl Dir       Directory where the crawl/link/segments dirs are saved\n    Num Rounds      The number of rounds to run this crawl for\n Example: bin/crawl -i -D solr.server.url=http://localhost:8983/solr/ urls/ TestCrawl/  2\n</code></pre>\n\n<p>I was wondering if anyone can give me some insight into reading this. For instance:</p>\n\n<pre><code>    -i|--index      **What is the configured indexer? Is this part of Nutch? Or is it an another program like Solr? When I put in -i, what am I doing?**\n    -D              **Not sure how these get used in the crawl but the instruction is pretty self-explanatory.**\n    Seed Dir        **Self-explanatory but where do I put the directory within Nutch? I created a urls directory (per the instructions) in the apache-nutch-1.10 directory. I've also tried putting it in the apache-nutch-1.10/bin file because that is were the crawl starts from.**\n    Crawl Dir       **Is this where the results of the crawl go or is there where the data for the injection to the crawldb goes? If its the latter where do I get said data? The directory starts out empty and never gets filled. Confusing!**\n    Num Rounds      **Self-explanatory**\n</code></pre>\n\n<p>Other questions:\nWhere do the results of the crawl go? Do they have to go to a Solr core (or some other peice of software)? Can they just go to a directory so I can look at them?\nWhat format do they come out?</p>\n\n<p>Thanks!</p>\n", "creation_date": 1441826861, "score": -2},
{"title": "Nutch agent rotate", "view_count": 65, "is_answered": false, "answers": [{"question_id": 32901332, "owner": {"user_id": 2702341, "link": "http://stackoverflow.com/users/2702341/sujen-shah", "user_type": "registered", "reputation": 183}, "body": "<p>You could try setting up a local server and crawl on your localhost and check the server logs to see if the agent actually changes or not. </p>\n\n<p>I had come across a similar issue, but when I checked the server logs, the agent was actually rotating. </p>\n", "creation_date": 1444368668, "is_accepted": false, "score": 0, "last_activity_date": 1444368668, "answer_id": 33030585}], "question_id": 32901332, "tags": ["ant", "lucene", "web-crawler", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/32901332/nutch-agent-rotate", "last_activity_date": 1444368668, "owner": {"user_id": 5400131, "view_count": 1, "answer_count": 0, "creation_date": 1443764683, "reputation": 1}, "body": "<p>I am trying to use rotate agent feature in nutch-1.10, down below is my configuration in nutch-site.xml</p>\n\n<pre><code>&lt;property&gt;\n    &lt;name&gt;http.agent.rotate&lt;/name&gt;\n    &lt;value&gt;true&lt;/value&gt;\n    &lt;description&gt;\n        If true, instead of http.agent.name, alternating agent names are\n        chosen from a list provided via http.agent.rotate.file.\n    &lt;/description&gt;\n&lt;/property&gt;\n\n&lt;property&gt;\n    &lt;name&gt;http.agent.rotate.file&lt;/name&gt;\n    &lt;value&gt;agents.txt&lt;/value&gt;\n    &lt;description&gt;\n        File containing alternative user agent names to be used instead of\n        http.agent.name on a rotating basis if http.agent.rotate is true.\n        Each line of the file should contain exactly one agent\n        specification including name, version, description, URL, etc.\n    &lt;/description&gt;\n&lt;/property&gt;\n</code></pre>\n\n<p>Also, here is the content of my agents.txt file</p>\n\n<p>\"NutchCVS/0.7 Nutch <a href=\"http://lucene.apache.org/nutch/bot.html\" rel=\"nofollow\">http://lucene.apache.org/nutch/bot.html</a> nutch-agent@lucene.apache.org\"</p>\n\n<p>I've tried all kinds of ways to set agents.txt, when I tried to grep the 'agent' in hadoop.log, the agent remains to be the one I set in http.agent.name. I also ran 'ant runtime' to recompile the project after making changes. Please help me figure out what should be wrong. (I think it's the agents.txt file, but I don't know what is the right format of agents)</p>\n", "creation_date": 1443765428, "score": 0},
{"title": "Crawling with Apache Nutch 1.9 using Java code", "view_count": 151, "is_answered": false, "answers": [{"question_id": 32949695, "owner": {"user_id": 2702341, "link": "http://stackoverflow.com/users/2702341/sujen-shah", "user_type": "registered", "reputation": 183}, "body": "<p>Currently there is a REST API being developed for Nutch 1.X - <a href=\"https://wiki.apache.org/nutch/Nutch_1.X_RESTAPI\" rel=\"nofollow\">https://wiki.apache.org/nutch/Nutch_1.X_RESTAPI</a></p>\n\n<p>There is no direct crawl api like the one you are talking about, but you could try invoking the jobs using the REST endpoints to see if they fit your use case. </p>\n", "creation_date": 1444368451, "is_accepted": false, "score": 0, "last_activity_date": 1444368451, "answer_id": 33030542}], "question_id": 32949695, "tags": ["java", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/32949695/crawling-with-apache-nutch-1-9-using-java-code", "last_activity_date": 1444368451, "owner": {"user_id": 71505, "answer_count": 6, "creation_date": 1235671113, "accept_rate": 32, "view_count": 105, "reputation": 1251}, "body": "<p>We have developed a data processing pipeline which crawls web data given a set of configured URLs using Apache Nutch 1.4. The pipeline subsequently applies a series of mapreduce tasks to process the web data and finally it is indexed into Solr. We use both pre-configured hadoop cluster as well as Amazon EMR.</p>\n\n<p>This application was developed 3 years ago and have not been used for almost a year. When we tried running it now with the latest EMR release (4.x) it failed. I suppose, Apache Nutch 1.4 is not supported anymore as it uses older hadoop version (1.x). So we decided to upgrade to Nutch 1.9. However, we found that Nutch 1.9 does not have the Crawl class (org.apache.nutch.crawl.Crawl) anymore that we were using to in 1.4 version to crawl from Java code. Checking the docs I found that the suggested approach is to use the script <strong>bin/crawl</strong>. </p>\n\n<p>However, per me calling a script from java code does not seem too good an approach as we are invoking an external process that gives us much less control.\nSo how do I proceed? Write my own version of org.apache.nutch.crawl.Crawl or is there some other class which I am missing here?</p>\n\n<p>My requirement is simple. I want to invoke the Nutch API 1.9 or above from Java code.</p>\n", "creation_date": 1444051187, "score": 0},
{"title": "Getting status of a Nutch crawl?", "view_count": 278, "owner": {"user_id": 847338, "answer_count": 4, "creation_date": 1310774492, "accept_rate": 75, "view_count": 40, "reputation": 144}, "is_answered": true, "answers": [{"question_id": 33021010, "owner": {"user_id": 2702341, "link": "http://stackoverflow.com/users/2702341/sujen-shah", "user_type": "registered", "reputation": 183}, "body": "<p>As of now, there is no way in which you could see the status of a crawl while it is being fetched apart from the log. You can query a crawldb only after it fetch-parse-updatedb jobs are over. </p>\n\n<p>And I think you are missing the bin/nutch updatedb   job before running bin/nutch solrindex. </p>\n\n<p>As you have mentioned, it seems like you are not using the ./bin/crawl script but calling each job individually. </p>\n\n<p>For crawls as large as yours, one way I could think of is by using the ./bin/crawl script which, by default, generates 50k urls for fetching per iteration. And after every iteration you could use the: </p>\n\n<pre><code>./bin/nutch readdb &lt;crawl_db&gt; -stats\n</code></pre>\n\n<p>command given at <a href=\"https://wiki.apache.org/nutch/CommandLineOptions\" rel=\"nofollow\">https://wiki.apache.org/nutch/CommandLineOptions</a> to check the crawldb status. </p>\n\n<p>If you want to check updates more frequently then change(lower) the '-topN' parameter(which is passed to the generate job) in the ./bin/crawl script. And now by varying the number of iterations you whould be able to crawl your entire seedlist. </p>\n\n<p>Hope this helps :)</p>\n", "creation_date": 1444367618, "is_accepted": true, "score": 3, "last_activity_date": 1444367618, "answer_id": 33030354}], "question_id": 33021010, "tags": ["solr", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/33021010/getting-status-of-a-nutch-crawl", "last_activity_date": 1444367618, "accepted_answer_id": 33030354, "body": "<p>I've set up Nutch and gave it a seedlist of URLs to crawl. I configured it such that it will not crawl anything outside of my seed list. The seed list contains ~1.5 million urls. I followed the guide and kicked off nutch like so:</p>\n\n<pre><code>bin/nutch inject crawl/crawldb urls\nbin/nutch generate crawl/crawldb crawl/segments\ns1=`ls -d crawl/segments/2* | tail -1`\nbin/nutch fetch $s1\nbin/nutch parse $s1\nbin/nutch invertlinks crawl/linkdb -dir crawl/segments\nbin/nutch solrindex http://127.0.0.1:8983/solr/ crawl/crawldb -linkdb crawl/linkdb $s1 -addBinaryContent -base64\n</code></pre>\n\n<blockquote>\n  <p>Aside: I really wish I knew how to crawl and index\n  at the same time (e.g., crawl a page -> index it, crawl next page),\n  because I currently have to wait for this entire crawl to finish\n  before anything is indexed at all.</p>\n</blockquote>\n\n<p>Anyway, right now, from checking the hadoop.log, I believe I've crawled about 40k links in 48 hours. However, I'd like to make sure that it's grabbing all the content correctly. I'd also like to see which links have been crawled, and which links are left. I've read all the documentation and I can't seem to figure out how to get the status of a Nutch crawl unless it was started as a job.</p>\n\n<p>I'm running Nutch 1.10 with Solr 4.10.</p>\n", "creation_date": 1444321005, "score": 0},
{"title": "Crawling issue in Apache Nutch", "view_count": 276, "is_answered": true, "answers": [{"last_edit_date": 1444298963, "owner": {"user_id": 3568831, "accept_rate": 71, "link": "http://stackoverflow.com/users/3568831/nwawel-a-iroume", "user_type": "registered", "reputation": 137}, "body": "<p><code>bin/nutch crawl urls -dir crawl -depth 3</code> is deprecated use this instead <code>bin/crawl &lt;seedDir&gt; &lt;crawlDir&gt; &lt;solrURL&gt; &lt;numberOfRounds&gt;</code> </p>\n\n<p>for example <code>bin/crawl urls/ TestCrawl/ http://localhost:8983/solr/your_core</code></p>\n\n<p>As i read your log you havn't provided url to solr (nutch need it to send crawled documents ). So you have to download and launch <strong>Solr</strong> before using these commands.  <a href=\"http://wiki.apache.org/solr/\" rel=\"nofollow\">follow this apache solr link to read how to setup and install solr</a></p>\n", "question_id": 23296109, "creation_date": 1417704653, "is_accepted": false, "score": 1, "last_activity_date": 1444298963, "answer_id": 27296967}], "question_id": 23296109, "tags": ["nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/23296109/crawling-issue-in-apache-nutch", "last_activity_date": 1444301851, "owner": {"user_id": 2975853, "answer_count": 1, "creation_date": 1384075793, "accept_rate": 38, "view_count": 11, "reputation": 56}, "body": "<p>I am working on a web project. My purpose is to take a web page and decide that it is blog or not. In order to perform this i have to use crawler and i use apache nutch. I execute all of orders in <a href=\"http://wiki.apache.org/nutch/NutchTutorial\" rel=\"nofollow\">Apache Nutch Tutorial Page</a> but i have failed. For the  <code>bin/nutch crawl urls -dir crawl -depth 3 -topN 50</code> command my result is:</p>\n\n<pre><code>solrUrl is not set, indexing will be skipped...\n2014-04-25 15:29:11.324 java[4405:1003] Unable to load realm info from SCDynamicStore\ncrawl started in: crawl\nrootUrlDir = urls\nthreads = 10\ndepth = 3\nsolrUrl=null\ntopN = 50\nInjector: starting at 2014-04-25 15:29:11\nInjector: crawlDb: crawl/crawldb\nInjector: urlDir: urls\nInjector: Converting injected urls to crawl db entries.\nInjector: total number of urls rejected by filters: 0\nInjector: total number of urls injected after normalization and filtering: 1\nInjector: Merging injected urls into crawl db.\nInjector: finished at 2014-04-25 15:29:13, elapsed: 00:00:02\nGenerator: starting at 2014-04-25 15:29:13\nGenerator: Selecting best-scoring urls due for fetch.\nGenerator: filtering: true\nGenerator: normalizing: true\nGenerator: topN: 50\nGenerator: jobtracker is 'local', generating exactly one partition.\nGenerator: 0 records selected for fetching, exiting ...\nStopping at depth=0 - no more URLs to fetch.\nNo URLs to fetch - check your seed list and URL filters.\ncrawl finished: crawl\n</code></pre>\n\n<p>My urls/seed.txt file is:</p>\n\n<pre><code>http://yahoo.com/\n</code></pre>\n\n<p>My regex-urlfilter.txt file is:</p>\n\n<pre><code># skip file: ftp: and mailto: urls\n-^(file|ftp|mailto):\n\n# skip image and other suffixes we can't yet parse\n# for a more extensive coverage use the urlfilter-suffix plugin\n-\\.(gif|GIF|jpg|JPG|png|PNG|ico|ICO|css|CSS|sit|SIT|eps|EPS|wmf|WMF|zip|ZIP|ppt|PPT|mpg|MPG|xls|XLS|gz|GZ|rpm|RPM|tgz|TGZ|mov|MOV|exe|EXE|jpeg|JPEG|bmp|BMP|js|JS)$\n\n# skip URLs containing certain characters as probable queries, etc.\n-[?*!@=]\n\n# skip URLs with slash-delimited segment that repeats 3+ times, to break loops\n-.*(/[^/]+)/[^/]+\\1/[^/]+\\1/\n\n# accept anything else\n+^http://([a-z0-9]*\\.)*yahoo.com/\n</code></pre>\n\n<p>My nutch-site.xml file is:</p>\n\n<pre><code>&lt;?xml version=\"1.0\"?&gt;\n&lt;?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?&gt;\n\n&lt;!-- Put site-specific property overrides in this file. --&gt;\n&lt;configuration&gt;\n\n    &lt;property&gt;\n        &lt;name&gt;http.agent.name&lt;/name&gt;\n        &lt;value&gt;Baris Spider&lt;/value&gt;\n        &lt;description&gt;This crawler is used to fetch text documents from\n            web pages that will be used as a corpus for Part-of-speech-tagging\n        &lt;/description&gt;\n    &lt;/property&gt;\n\n\n\n&lt;/configuration&gt;\n</code></pre>\n\n<p>What is wrong?</p>\n", "creation_date": 1398436536, "score": 0},
{"title": "ERROR util.Shell - Failed to locate the winutils binary in the hadoop binary path in Windows 10", "view_count": 436, "is_answered": false, "answers": [{"question_id": 32930980, "owner": {"user_id": 420544, "accept_rate": 62, "link": "http://stackoverflow.com/users/420544/harman", "user_type": "registered", "reputation": 412}, "body": "<p>As per <a href=\"http://wiki.apache.org/hadoop/Hadoop2OnWindows\" rel=\"nofollow\">Hadoop wiki page to setup on Winows</a>, Apache Hadoop has been tested and used on Windows Server 2008 and Windows Server 2008 R2, which will also likely work on Windows Vista and Windows 7 due to the Win32 API similarities.</p>\n\n<p>This said, you have 3 options:</p>\n\n<ol>\n<li>You can go in for a Hadoop distribution which supports Windows 10.</li>\n<li>You can opt for a different Windows version which is supported by the distribution you choose.</li>\n<li>The one I would recommend, go in for a Linux (preferably CentOS,RedHat,etc.) based machine if possible. And then you have a whole lot of distributions to choose from.</li>\n</ol>\n\n<p>In case, you find a solution to the issue you're facing I would encourage you to answer your question yourself :)</p>\n", "creation_date": 1444280008, "is_accepted": false, "score": 0, "last_activity_date": 1444280008, "answer_id": 33006922}], "question_id": 32930980, "tags": ["apache", "hadoop", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/32930980/error-util-shell-failed-to-locate-the-winutils-binary-in-the-hadoop-binary-pat", "last_activity_date": 1444280008, "owner": {"user_id": 5406109, "view_count": 0, "answer_count": 0, "creation_date": 1443941785, "reputation": 13}, "body": "<p>Can anyone please help me how to configure nutch in eclipse. I tried all the tutorial available in the wiki .(<a href=\"http://wiki.apache.org/nutch/Nutch2Tutorial\" rel=\"nofollow\">http://wiki.apache.org/nutch/Nutch2Tutorial</a>, and many others as I am not able to paste all the links here). But everytime I am getting a NullPointerException. Is there any other tutorial available for the same.</p>\n\n<p>But in My hadoop.log I found this message\n<strong>ERROR util.Shell - Failed to locate the winutils binary in the hadoop binary path.</strong></p>\n\n<p>I am using hadoop-2.5.2 but this package does not contain any winutils.exe .  I also tried other tutorials available to build Winutils.exe using VisualStudio but unable to create it.</p>\n\n<p>Please help me how to create winutils.exe using Visual Studio 2015, I think without winutils.exe I cant run Nutch.</p>\n\n<p>Note: I am using Windows 10 , Visual Studio 2015, hadoop-2.5.2, Apache Nutch 2.x</p>\n\n<p>After putting the winutils.exe available in another link, Now I am getting a new Error\n<strong>java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z</strong></p>\n", "creation_date": 1443942868, "score": 1},
{"title": "error with crawling with nutch", "view_count": 842, "owner": {"user_id": 2310870, "view_count": 1, "answer_count": 0, "creation_date": 1366713020, "reputation": 8}, "is_answered": true, "answers": [{"question_id": 16167620, "owner": {"user_id": 1736002, "accept_rate": 60, "link": "http://stackoverflow.com/users/1736002/cguzel", "user_type": "registered", "reputation": 576}, "body": "<p>Check your seed list. This error occurred when running injector job. May be due to your seed list. Your seed urls should be as follows: <strong><a href=\"http://www.example.com\" rel=\"nofollow\">http://www.example.com</a></strong> . You must add protocols as \"http//\" .</p>\n", "creation_date": 1367106994, "is_accepted": true, "score": 0, "last_activity_date": 1367106994, "answer_id": 16258124}], "question_id": 16167620, "tags": ["web-crawler", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/16167620/error-with-crawling-with-nutch", "last_activity_date": 1444066684, "accepted_answer_id": 16258124, "body": "<p>I was trying to crawl website with nutch and got this error:</p>\n\n<pre><code>java.net.MalformedURLException: no protocol:\n    Exception in thread \"main\" java.io.IOException: Job failed!\n            at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:1265)\n            at org.apache.nutch.crawl.Injector.inject(Injector.java:296)\n            at org.apache.nutch.crawl.Crawl.run(Crawl.java:127)\n            at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n            at org.apache.nutch.crawl.Crawl.main(Crawl.java:55)\n</code></pre>\n", "creation_date": 1366714829, "score": 1},
{"title": "Apache Nutch support authentication with certificate?", "view_count": 14, "is_answered": false, "question_id": 32909486, "tags": ["apache", "nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/32909486/apache-nutch-support-authentication-with-certificate", "last_activity_date": 1443796137, "owner": {"age": 22, "answer_count": 9, "creation_date": 1389373618, "user_id": 3182764, "view_count": 10, "location": "Curitiba, Brazil", "reputation": 125}, "body": "<p>I only found that <a href=\"https://wiki.apache.org/nutch/HttpAuthenticationSchemes\" rel=\"nofollow\">https://wiki.apache.org/nutch/HttpAuthenticationSchemes</a></p>\n\n<p>I need to authenticate on site with p12 and pem cetificates and extract the data.</p>\n\n<p>Apache Nutch does support's it?</p>\n", "creation_date": 1443796137, "score": 0},
{"title": "Integrating Solr with Nutch issue", "view_count": 2016, "owner": {"user_id": 3250183, "answer_count": 12, "creation_date": 1391019858, "accept_rate": 67, "view_count": 40, "reputation": 141}, "is_answered": true, "answers": [{"question_id": 25137426, "owner": {"user_id": 3250183, "accept_rate": 67, "link": "http://stackoverflow.com/users/3250183/user3250183", "user_type": "registered", "reputation": 141}, "body": "<p>may be because of some versioning differences the tutorial suggested to copy the conf/schema.xml whereas in this particular version of solr, the file schema-solr4.xml was supposed to be copied followed by addition of : <code>&lt;field name=\"_version_\" type=\"long\" indexed=\"true\" stored=\"true\"/&gt;</code> in line no 351. Restart the solr by <code>java -jar start.jar</code> and it works all normal! Hope this helps someone!</p>\n", "creation_date": 1407767647, "is_accepted": true, "score": 0, "last_activity_date": 1407767647, "answer_id": 25245819}], "question_id": 25137426, "tags": ["solr", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/25137426/integrating-solr-with-nutch-issue", "last_activity_date": 1443641179, "accepted_answer_id": 25245819, "body": "<p>I am following a tutorial from <a href=\"http://wiki.apache.org/nutch/NutchTutorial\" rel=\"nofollow\">here</a>. i have got solr and nutch installed separately and they are both working all fine. The problem comes when i have to integrate them. From the earlier posts on this site i learned that there could some issue with the schema files. As mentioned in the tut i copied the schema.xml of nutch to the schema.xml of solr and restarted the solr. solr stoped because of configuration issues. So i simply copied the  contents of each file into the other along with the existing content. Now (and previously as well) i get this error:</p>\n\n<pre><code>Indexer: starting at 2014-08-05 11:10:21\nIndexer: deleting gone documents: false\nIndexer: URL filtering: false\nIndexer: URL normalizing: false\nActive IndexWriters :\nSOLRIndexWriter\n        solr.server.url : URL of the SOLR instance (mandatory)\n        solr.commit.size : buffer size when sending to SOLR (default 1000)\n        solr.mapping.file : name of the mapping file for fields (default solrindex-mapping.xml)\n        solr.auth : use authentication (default false)\n        solr.auth.username : use authentication (default false)\n        solr.auth : username for authentication\n        solr.auth.password : password for authentication\n\n\nIndexer: java.io.IOException: Job failed!\n        at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:1357)\n        at org.apache.nutch.indexer.IndexingJob.index(IndexingJob.java:114)\n        at org.apache.nutch.indexer.IndexingJob.run(IndexingJob.java:176)\n        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n        at org.apache.nutch.indexer.IndexingJob.main(IndexingJob.java:186)\n</code></pre>\n\n<p>Can someone suggest what should be done?\nI am using apache-nutch-1.8 and solr-4.9.0\n Here is how my hadoop.log file looks like:</p>\n\n<pre><code>2014-08-05 12:50:05,032 INFO  crawl.Injector - Injector: starting at 2014-08-05 12:50:05\n2014-08-05 12:50:05,033 INFO  crawl.Injector - Injector: crawlDb: -dir/crawldb\n2014-08-05 12:50:05,033 INFO  crawl.Injector - Injector: urlDir: urls\n.\n.\n.\n.\n.\n2014-08-05 13:04:21,255 INFO  solr.SolrIndexWriter - Indexing 1 documents\n2014-08-05 13:04:21,286 WARN  mapred.LocalJobRunner - job_local1310160376_0001\norg.apache.solr.common.SolrException: Bad Request\n\nBad Request\n\nrequest: http://my-solr-url:8983/solr/update?wt=javabin&amp;version=2\n    at org.apache.solr.client.solrj.impl.CommonsHttpSolrServer.request(CommonsHttpSolrServer.java:430)\n    at org.apache.solr.client.solrj.impl.CommonsHttpSolrServer.request(CommonsHttpSolrServer.java:244)\n    at org.apache.solr.client.solrj.request.AbstractUpdateRequest.process(AbstractUpdateRequest.java:105)\n    at org.apache.nutch.indexwriter.solr.SolrIndexWriter.close(SolrIndexWriter.java:155)\n    at org.apache.nutch.indexer.IndexWriters.close(IndexWriters.java:118)\n    at org.apache.nutch.indexer.IndexerOutputFormat$1.close(IndexerOutputFormat.java:44)\n    at org.apache.hadoop.mapred.ReduceTask$OldTrackingRecordWriter.close(ReduceTask.java:467)\n    at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:535)\n    at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:421)\n    at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:398)\n2014-08-05 13:04:21,544 ERROR indexer.IndexingJob - Indexer: java.io.IOException: Job failed!\n    at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:1357)\n    at org.apache.nutch.indexer.IndexingJob.index(IndexingJob.java:114)\n    at org.apache.nutch.indexer.IndexingJob.run(IndexingJob.java:176)\n    at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n    at org.apache.nutch.indexer.IndexingJob.main(IndexingJob.java:186)\n\n2014-08-05 13:10:37,855 INFO  crawl.Injector - Injector: starting at 2014-08-05 13:10:37\n.\n.\n.\n</code></pre>\n", "creation_date": 1407237211, "score": 0},
{"title": "Solr indexing following a Nutch crawl fails, reports &quot;Job Failed&quot;", "view_count": 5867, "is_answered": true, "answers": [{"question_id": 21617141, "owner": {"user_id": 973840, "accept_rate": 85, "link": "http://stackoverflow.com/users/973840/diaa", "user_type": "registered", "reputation": 385}, "body": "<p>This happens when not all required fields from nutch are in the schema.xml of solr.\nDid you add the fields from nutch's schema.xml?\nIf you add in the section \"fields\" the following, things should work:</p>\n\n<pre><code>     &lt;field name=\"id\" type=\"string\" stored=\"true\" indexed=\"true\"/&gt;\n    &lt;!-- core fields --&gt;\n    &lt;field name=\"segment\" type=\"string\" stored=\"true\" indexed=\"false\"/&gt;\n    &lt;field name=\"digest\" type=\"string\" stored=\"true\" indexed=\"false\"/&gt;\n    &lt;field name=\"boost\" type=\"float\" stored=\"true\" indexed=\"false\"/&gt;\n\n    &lt;!-- fields for index-basic plugin --&gt;\n    &lt;field name=\"host\" type=\"string\" stored=\"false\" indexed=\"true\"/&gt;\n    &lt;field name=\"url\" type=\"url\" stored=\"true\" indexed=\"true\"\n        required=\"true\"/&gt;\n    &lt;field name=\"content\" type=\"text_general\" stored=\"false\" indexed=\"true\"/&gt;\n    &lt;field name=\"title\" type=\"text_general\" stored=\"true\" indexed=\"true\"/&gt;\n    &lt;field name=\"cache\" type=\"string\" stored=\"true\" indexed=\"false\"/&gt;\n    &lt;field name=\"tstamp\" type=\"date\" stored=\"true\" indexed=\"false\"/&gt;\n\n    &lt;!-- fields for index-anchor plugin --&gt;\n    &lt;field name=\"anchor\" type=\"string\" stored=\"true\" indexed=\"true\"\n        multiValued=\"true\"/&gt;\n\n    &lt;!-- fields for index-more plugin --&gt;\n    &lt;field name=\"type\" type=\"string\" stored=\"true\" indexed=\"true\"\n        multiValued=\"true\"/&gt;\n    &lt;field name=\"contentLength\" type=\"long\" stored=\"true\"\n        indexed=\"false\"/&gt;\n    &lt;field name=\"lastModified\" type=\"date\" stored=\"true\"\n        indexed=\"false\"/&gt;\n    &lt;field name=\"date\" type=\"date\" stored=\"true\" indexed=\"true\"/&gt;\n\n    &lt;!-- fields for languageidentifier plugin --&gt;\n    &lt;field name=\"lang\" type=\"string\" stored=\"true\" indexed=\"true\"/&gt;\n\n    &lt;!-- fields for subcollection plugin --&gt;\n    &lt;field name=\"subcollection\" type=\"string\" stored=\"true\"\n        indexed=\"true\" multiValued=\"true\"/&gt;\n\n    &lt;!-- fields for feed plugin (tag is also used by microformats-reltag)--&gt;\n    &lt;field name=\"author\" type=\"string\" stored=\"true\" indexed=\"true\"/&gt;\n    &lt;field name=\"tag\" type=\"string\" stored=\"true\" indexed=\"true\" multiValued=\"true\"/&gt;\n    &lt;field name=\"feed\" type=\"string\" stored=\"true\" indexed=\"true\"/&gt;\n    &lt;field name=\"publishedDate\" type=\"date\" stored=\"true\"\n        indexed=\"true\"/&gt;\n    &lt;field name=\"updatedDate\" type=\"date\" stored=\"true\"\n        indexed=\"true\"/&gt;\n\n    &lt;!-- fields for creativecommons plugin --&gt;\n    &lt;field name=\"cc\" type=\"string\" stored=\"true\" indexed=\"true\"\n        multiValued=\"true\"/&gt;\n\n    &lt;!-- fields for tld plugin --&gt;    \n    &lt;field name=\"tld\" type=\"string\" stored=\"false\" indexed=\"false\"/&gt;\n</code></pre>\n", "creation_date": 1392378011, "is_accepted": false, "score": 2, "last_activity_date": 1392378011, "answer_id": 21778415}, {"question_id": 21617141, "owner": {"user_id": 2482678, "link": "http://stackoverflow.com/users/2482678/lucobada", "user_type": "registered", "reputation": 23}, "body": "<p>I had a similar problem with Nutch 1.8 and Solr 4.8.0. In fact Diaa's answer helped me solve the problem. After having removed some intersections of schema.xml with Diaa's field list and after having changed two entries marked as \"added by wb\" and \"changed by wb\" I ended up with the following field list which worked for me. As opposed to earlier versions of nutch and solr there is no tag for \"fields\" any more. Entries tagged as \"field\" are simply within \"schema\". This is the complete field list:</p>\n\n<pre><code>   &lt;field name=\"_root_\" type=\"string\" indexed=\"true\" stored=\"false\"/&gt;\n\n   &lt;!-- Only remove the \"id\" field if you have a very good reason to. While not strictly\n     required, it is highly recommended. A &lt;uniqueKey&gt; is present in almost all Solr \n     installations. See the &lt;uniqueKey&gt; declaration below where &lt;uniqueKey&gt; is set to \"id\".\n   --&gt;   \n   &lt;field name=\"id\" type=\"string\" indexed=\"true\" stored=\"true\" required=\"true\" multiValued=\"false\" /&gt; \n\n   &lt;field name=\"sku\" type=\"text_en_splitting_tight\" indexed=\"true\" stored=\"true\" omitNorms=\"true\"/&gt;\n   &lt;field name=\"name\" type=\"text_general\" indexed=\"true\" stored=\"true\"/&gt;\n   &lt;field name=\"manu\" type=\"text_general\" indexed=\"true\" stored=\"true\" omitNorms=\"true\"/&gt;\n   &lt;field name=\"cat\" type=\"string\" indexed=\"true\" stored=\"true\" multiValued=\"true\"/&gt;\n   &lt;field name=\"features\" type=\"text_general\" indexed=\"true\" stored=\"true\" multiValued=\"true\"/&gt;\n   &lt;field name=\"includes\" type=\"text_general\" indexed=\"true\" stored=\"true\" termVectors=\"true\" termPositions=\"true\" termOffsets=\"true\" /&gt;\n\n   &lt;field name=\"weight\" type=\"float\" indexed=\"true\" stored=\"true\"/&gt;\n   &lt;field name=\"price\"  type=\"float\" indexed=\"true\" stored=\"true\"/&gt;\n   &lt;field name=\"popularity\" type=\"int\" indexed=\"true\" stored=\"true\" /&gt;\n   &lt;field name=\"inStock\" type=\"boolean\" indexed=\"true\" stored=\"true\" /&gt;\n\n   &lt;field name=\"store\" type=\"location\" indexed=\"true\" stored=\"true\"/&gt;\n\n   &lt;!-- Common metadata fields, named specifically to match up with\n     SolrCell metadata when parsing rich documents such as Word, PDF.\n     Some fields are multiValued only because Tika currently may return\n     multiple values for them. Some metadata is parsed from the documents,\n     but there are some which come from the client context:\n       \"content_type\": From the HTTP headers of incoming stream\n       \"resourcename\": From SolrCell request param resource.name\n   --&gt;\n   &lt;field name=\"title\" type=\"text_general\" indexed=\"true\" stored=\"true\" multiValued=\"true\"/&gt;\n   &lt;field name=\"subject\" type=\"text_general\" indexed=\"true\" stored=\"true\"/&gt;\n   &lt;field name=\"description\" type=\"text_general\" indexed=\"true\" stored=\"true\"/&gt;\n   &lt;field name=\"comments\" type=\"text_general\" indexed=\"true\" stored=\"true\"/&gt;\n   &lt;field name=\"author\" type=\"text_general\" indexed=\"true\" stored=\"true\"/&gt;\n   &lt;field name=\"keywords\" type=\"text_general\" indexed=\"true\" stored=\"true\"/&gt;\n   &lt;field name=\"category\" type=\"text_general\" indexed=\"true\" stored=\"true\"/&gt;\n   &lt;field name=\"resourcename\" type=\"text_general\" indexed=\"true\" stored=\"true\"/&gt;\n\n   &lt;!-- added by wb: required=\"true\" --&gt;\n   &lt;field name=\"url\" type=\"text_general\" indexed=\"true\" stored=\"true\" required=\"true\"/&gt; \n\n   &lt;field name=\"content_type\" type=\"string\" indexed=\"true\" stored=\"true\" multiValued=\"true\"/&gt;\n   &lt;field name=\"last_modified\" type=\"date\" indexed=\"true\" stored=\"true\"/&gt;\n   &lt;field name=\"links\" type=\"string\" indexed=\"true\" stored=\"true\" multiValued=\"true\"/&gt;\n\n   &lt;!-- Main body of document extracted by SolrCell.\n        NOTE: This field is not indexed by default, since it is also copied to \"text\"\n        using copyField below. This is to save space. Use this field for returning and\n        highlighting document content. Use the \"text\" field to search the content. --&gt;\n\n   &lt;!-- changedby wb: indexed=\"true\" --&gt;\n   &lt;field name=\"content\" type=\"text_general\" indexed=\"true\" stored=\"true\" multiValued=\"true\"/&gt; \n\n\n   &lt;!-- catchall field, containing all other searchable text fields (implemented\n        via copyField further on in this schema  --&gt;\n   &lt;field name=\"text\" type=\"text_general\" indexed=\"true\" stored=\"false\" multiValued=\"true\"/&gt;\n\n   &lt;!-- catchall text field that indexes tokens both normally and in reverse for efficient\n        leading wildcard queries. --&gt;\n   &lt;field name=\"text_rev\" type=\"text_general_rev\" indexed=\"true\" stored=\"false\" multiValued=\"true\"/&gt;\n\n   &lt;!-- non-tokenized version of manufacturer to make it easier to sort or group\n        results by manufacturer.  copied from \"manu\" via copyField --&gt;\n   &lt;field name=\"manu_exact\" type=\"string\" indexed=\"true\" stored=\"false\"/&gt;\n\n   &lt;field name=\"payloads\" type=\"payloads\" indexed=\"true\" stored=\"true\"/&gt;\n\n   &lt;!-- Fields needed for Nutch 1.8 integration: --&gt;\n\n    &lt;field name=\"segment\" type=\"string\" stored=\"true\" indexed=\"false\"/&gt;\n    &lt;field name=\"digest\" type=\"string\" stored=\"true\" indexed=\"false\"/&gt;\n    &lt;field name=\"boost\" type=\"float\" stored=\"true\" indexed=\"false\"/&gt;\n\n    &lt;!-- fields for index-basic plugin --&gt;\n    &lt;field name=\"host\" type=\"string\" stored=\"false\" indexed=\"true\"/&gt;\n    &lt;field name=\"cache\" type=\"string\" stored=\"true\" indexed=\"false\"/&gt;\n    &lt;field name=\"tstamp\" type=\"date\" stored=\"true\" indexed=\"false\"/&gt;\n\n    &lt;!-- fields for index-anchor plugin --&gt;\n    &lt;field name=\"anchor\" type=\"string\" stored=\"true\" indexed=\"true\" multiValued=\"true\"/&gt;\n\n    &lt;!-- fields for index-more plugin --&gt;\n    &lt;field name=\"type\" type=\"string\" stored=\"true\" indexed=\"true\" multiValued=\"true\"/&gt;\n    &lt;field name=\"contentLength\" type=\"long\" stored=\"true\" indexed=\"false\"/&gt;\n    &lt;field name=\"lastModified\" type=\"date\" stored=\"true\" indexed=\"false\"/&gt;\n    &lt;field name=\"date\" type=\"date\" stored=\"true\" indexed=\"true\"/&gt;\n\n    &lt;!-- fields for languageidentifier plugin --&gt;\n    &lt;field name=\"lang\" type=\"string\" stored=\"true\" indexed=\"true\"/&gt;\n\n    &lt;!-- fields for subcollection plugin --&gt;\n    &lt;field name=\"subcollection\" type=\"string\" stored=\"true\" indexed=\"true\" multiValued=\"true\"/&gt;\n\n    &lt;!-- fields for feed plugin (tag is also used by microformats-reltag)--&gt;\n    &lt;field name=\"tag\" type=\"string\" stored=\"true\" indexed=\"true\" multiValued=\"true\"/&gt;\n    &lt;field name=\"feed\" type=\"string\" stored=\"true\" indexed=\"true\"/&gt;\n    &lt;field name=\"publishedDate\" type=\"date\" stored=\"true\" indexed=\"true\"/&gt;\n    &lt;field name=\"updatedDate\" type=\"date\" stored=\"true\" indexed=\"true\"/&gt;\n\n    &lt;!-- fields for creativecommons plugin --&gt;\n    &lt;field name=\"cc\" type=\"string\" stored=\"true\" indexed=\"true\" multiValued=\"true\"/&gt;\n\n    &lt;!-- fields for tld plugin --&gt;    \n    &lt;field name=\"tld\" type=\"string\" stored=\"false\" indexed=\"false\"/&gt;\n\n   &lt;!-- End of fields needed for Nutch 1.8 integration: --&gt;\n</code></pre>\n", "creation_date": 1398951884, "is_accepted": false, "score": 1, "last_activity_date": 1398951884, "answer_id": 23408959}], "question_id": 21617141, "tags": ["solr", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/21617141/solr-indexing-following-a-nutch-crawl-fails-reports-job-failed", "last_activity_date": 1443640692, "owner": {"user_id": 3281894, "view_count": 0, "answer_count": 0, "creation_date": 1391732198, "reputation": 21}, "body": "<p>I have a site hosted on my local machine that I am attempting to crawl with Nutch and index in Solr (both also on my local machine). I installed Solr 4.6.1 and Nutch 1.7 per the instructions given on the Nutch site (<a href=\"http://wiki.apache.org/nutch/NutchTutorial\" rel=\"nofollow\">http://wiki.apache.org/nutch/NutchTutorial</a>), and I have Solr running in my browser without issue. </p>\n\n<p>I am running the following command:</p>\n\n<pre><code>bin/nutch crawl urls -solr http://localhost:8983/solr/ -depth 1 -topN 2\n</code></pre>\n\n<p>The crawl is working fine, but when it attemps to put the data into Solr, it fails with the following output:</p>\n\n<pre><code>Indexer: starting at 2014-02-06 16:29:28\nIndexer: deleting gone documents: false\nIndexer: URL filtering: false\nIndexer: URL normalizing: false\nActive IndexWriters :\nSOLRIndexWriter\n    solr.server.url : URL of the SOLR instance (mandatory)\n    solr.commit.size : buffer size when sending to SOLR (default 1000)\n    solr.mapping.file : name of the mapping file for fields (default solrindex-mapping.xml)\n    solr.auth : use authentication (default false)\n    solr.auth.username : use authentication (default false)\n    solr.auth : username for authentication\n    solr.auth.password : password for authentication\n\n\nException in thread \"main\" java.io.IOException: Job failed!\n    at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:1357)\n    at org.apache.nutch.indexer.IndexingJob.index(IndexingJob.java:123)\n    at org.apache.nutch.indexer.IndexingJob.index(IndexingJob.java:81)\n    at org.apache.nutch.indexer.IndexingJob.index(IndexingJob.java:65)\n    at org.apache.nutch.crawl.Crawl.run(Crawl.java:155)\n    at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n    at org.apache.nutch.crawl.Crawl.main(Crawl.java:55)\n</code></pre>\n\n<p>I went to the Nutch logs directory and tailed the hadoop.log file, it shows this:</p>\n\n<pre><code>2014-02-06 16:29:28,920 INFO  solr.SolrIndexWriter - Indexing 1 documents\n2014-02-06 16:29:28,921 INFO  httpclient.HttpMethodDirector - I/O exception (org.apache.commons.httpclient.NoHttpResponseException) caught when processing request: The server localhost failed to respond\n2014-02-06 16:29:28,921 INFO  httpclient.HttpMethodDirector - Retrying request\n2014-02-06 16:29:28,924 WARN  mapred.LocalJobRunner - job_local331896790_0009\njava.io.IOException\n    at org.apache.nutch.indexwriter.solr.SolrIndexWriter.makeIOException(SolrIndexWriter.java:173)\n    at org.apache.nutch.indexwriter.solr.SolrIndexWriter.close(SolrIndexWriter.java:159)\n    at org.apache.nutch.indexer.IndexWriters.close(IndexWriters.java:118)\n    at org.apache.nutch.indexer.IndexerOutputFormat$1.close(IndexerOutputFormat.java:44)\n    at org.apache.hadoop.mapred.ReduceTask$OldTrackingRecordWriter.close(ReduceTask.java:467)\n    at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:535)\n    at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:421)\n    at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:398)\nCaused by: org.apache.solr.client.solrj.SolrServerException: java.net.SocketException: Connection reset\n    at org.apache.solr.client.solrj.impl.CommonsHttpSolrServer.request(CommonsHttpSolrServer.java:478)\n    at org.apache.solr.client.solrj.impl.CommonsHttpSolrServer.request(CommonsHttpSolrServer.java:244)\n    at org.apache.solr.client.solrj.request.AbstractUpdateRequest.process(AbstractUpdateRequest.java:105)\n    at org.apache.nutch.indexwriter.solr.SolrIndexWriter.close(SolrIndexWriter.java:155)\n    ... 6 more\nCaused by: java.net.SocketException: Connection reset\n    at java.net.SocketInputStream.read(SocketInputStream.java:168)\n    at java.io.BufferedInputStream.fill(BufferedInputStream.java:218)\n    at java.io.BufferedInputStream.read(BufferedInputStream.java:237)\n    at org.apache.commons.httpclient.HttpParser.readRawLine(HttpParser.java:78)\n    at org.apache.commons.httpclient.HttpParser.readLine(HttpParser.java:106)\n    at org.apache.commons.httpclient.HttpConnection.readLine(HttpConnection.java:1116)\n    at org.apache.commons.httpclient.HttpMethodBase.readStatusLine(HttpMethodBase.java:1973)\n    at org.apache.commons.httpclient.HttpMethodBase.readResponse(HttpMethodBase.java:1735)\n    at org.apache.commons.httpclient.HttpMethodBase.execute(HttpMethodBase.java:1098)\n    at org.apache.commons.httpclient.HttpMethodDirector.executeWithRetry(HttpMethodDirector.java:398)\n    at org.apache.commons.httpclient.HttpMethodDirector.executeMethod(HttpMethodDirector.java:171)\n    at org.apache.commons.httpclient.HttpClient.executeMethod(HttpClient.java:397)\n    at org.apache.commons.httpclient.HttpClient.executeMethod(HttpClient.java:323)\n    at org.apache.solr.client.solrj.impl.CommonsHttpSolrServer.request(CommonsHttpSolrServer.java:422)\n</code></pre>\n\n<p>Yet, I am still able to access Solr in my browser just fine. This is my first try at Solr/Nutch - any help from those with more knowledge would be much appreciated. Thanks. </p>\n", "creation_date": 1391733638, "score": 4},
{"title": "Exception in thread &quot;main&quot; java.io.IOException: Job failed! on Nutch 1.7", "view_count": 4687, "is_answered": false, "answers": [{"question_id": 19551949, "owner": {"user_id": 4662134, "accept_rate": 0, "link": "http://stackoverflow.com/users/4662134/najmi", "user_type": "registered", "reputation": 21}, "body": "<p>Mr Mrkreyes do you got an answer for your problem's nutch</p>\n", "creation_date": 1426678236, "is_accepted": false, "score": 0, "last_activity_date": 1426678236, "answer_id": 29120884}, {"question_id": 19551949, "owner": {"user_id": 5394648, "link": "http://stackoverflow.com/users/5394648/madebykai", "user_type": "registered", "reputation": 1}, "body": "<p>I had the same issue, I resolved the problem by including the core in the command</p>\n\n<ol>\n<li><p>Find your core name</p>\n\n<p>1a. go to <a href=\"http://localhost:8983/solr\" rel=\"nofollow\">http://localhost:8983/solr</a></p>\n\n<p>1b. on the left-hand navigation, there is a pull down menu titled \"Core Selector\", click on the menu and see a list of Solr core.</p>\n\n<p>1c. write down the core name. (ex: collection1)</p></li>\n<li><p>Put the core name in the command</p>\n\n<p>2a. $ bin/nutch solrindex <a href=\"http://localhost:8983/solr/\" rel=\"nofollow\">http://localhost:8983/solr/</a><strong>collection1</strong> crawl/crawldb -linkdb crawl/linkdb crawl/segments/*</p></li>\n</ol>\n", "creation_date": 1443640435, "is_accepted": false, "score": 0, "last_activity_date": 1443640435, "answer_id": 32873929}], "question_id": 19551949, "tags": ["apache", "search", "solr", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/19551949/exception-in-thread-main-java-io-ioexception-job-failed-on-nutch-1-7", "last_activity_date": 1443640435, "owner": {"user_id": 1880601, "answer_count": 2, "creation_date": 1354744967, "accept_rate": 18, "view_count": 78, "reputation": 186}, "body": "<p>Solr and Nutch are already setup locally (on separate directories) and I wish to crawl a URL, index it, then integrate that index into Solr.</p>\n\n<p>Running this crawl on terminal:</p>\n\n<pre><code>                $ bin/nutch crawl urls -solr http://localhost:8983/solr/ -depth 3 -topN 5\n</code></pre>\n\n<p>Reports this error on the command line:</p>\n\n<pre><code>                Exception in thread \"main\" java.io.IOException: Job failed!\n                        at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:1357)\n                        at org.apache.nutch.indexer.IndexingJob.index(IndexingJob.java:123)\n                        at org.apache.nutch.indexer.IndexingJob.index(IndexingJob.java:81)\n                        at org.apache.nutch.indexer.IndexingJob.index(IndexingJob.java:65)\n                        at org.apache.nutch.crawl.Crawl.run(Crawl.java:155)\n                        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n                        at org.apache.nutch.crawl.Crawl.main(Crawl.java:55)\n</code></pre>\n\n<p>That said, in my attempt to then integrate I run this command:</p>\n\n<pre><code>                $ bin/nutch solrindex http://localhost:8983/solr/ crawl/crawldb -linkdb crawl/linkdb crawl/segments/*\n</code></pre>\n\n<p>Which reports this error on the command line:</p>\n\n<pre><code>                2013-10-23 13:23:38.347 java[15444:1203] Unable to load realm info from SCDynamicStore\n                Indexer: java.io.IOException: Job failed!\n                        at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:1357)\n                        at org.apache.nutch.indexer.IndexingJob.index(IndexingJob.java:123)\n                        at org.apache.nutch.indexer.IndexingJob.run(IndexingJob.java:185)\n                        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n                        at org.apache.nutch.indexer.IndexingJob.main(IndexingJob.java:195)\n</code></pre>\n\n<p>My environment and app versions are as follows:</p>\n\n<ul>\n    <li>Nutch 1.7</li>\n    <li>Solr 4.5</li>\n    <li>MAC OSX (10.8.5)</li>\n    <li>java version \"1.6.0_51\"</li>\n</ul>\n\n<p>Suggestions would be appreciated.</p>\n", "creation_date": 1382561184, "score": 1},
{"title": "How to extract all the address information from text?", "view_count": 134, "is_answered": true, "answers": [{"question_id": 32838245, "owner": {"user_id": 432844, "link": "http://stackoverflow.com/users/432844/julien-nioche", "user_type": "registered", "reputation": 939}, "body": "<p>Use an Information Extraction library or framework for the detection of addresses. There has been loads of work on this and this will always be better than writing regular expressions on text.</p>\n\n<p>You could for instance leverage <a href=\"http://gate.ac.uk\" rel=\"nofollow\">GATE</a> which comes with ANNIE, a simple IE pipeline which can extract addresses. One way of doing would be to use <a href=\"https://github.com/DigitalPebble/behemoth\" rel=\"nofollow\">Behemoth</a> and either run GATE within it or export to the GATE format so that you can run GATE separately. You could also piggyback the code in the GATE module for Behemoth and write a custom parser for Nutch so that the extraction gets done within Nutch. </p>\n\n<p>There are other NLP resources that can do this, check UIMA etc... Again it's a well-known sport in the Natural Language Processing field and you don't need to reinvent the wheel.</p>\n\n<p>You should also have a look at <a href=\"http://schema.org\" rel=\"nofollow\">schema.org</a> and write a custom ParseFilter for Nutch to handle pages annotated with microdata straight from Nutch.</p>\n\n<p>Hope this helps. </p>\n", "creation_date": 1443536287, "is_accepted": false, "score": 1, "last_activity_date": 1443536287, "answer_id": 32846379}], "question_id": 32838245, "tags": ["java", "algorithm", "parsing", "nutch", "text-mining"], "answer_count": 1, "link": "http://stackoverflow.com/questions/32838245/how-to-extract-all-the-address-information-from-text", "last_activity_date": 1443536287, "owner": {"user_id": 3364049, "answer_count": 3, "creation_date": 1393577655, "accept_rate": 22, "view_count": 14, "reputation": 94}, "body": "<p>Using Nutch I have crawled the URL, scraped data and dumped the output as text. Now I have text data, out of which I want to extract/strip only the address information. How can I do this ? </p>\n\n<p>If I am not wrong only reg-ex wont help me in this case, I should write a reg-ex followed by some code logic. </p>\n\n<p>Can anyone help me to solve this problem ?</p>\n\n<p>Thanks in advance.</p>\n\n<p><strong>Pastebin url for Sample text:</strong> <a href=\"http://pastebin.com/n8Eftp2K\" rel=\"nofollow\">http://pastebin.com/n8Eftp2K</a></p>\n\n<p><strong>Sample text :</strong> </p>\n\n<pre><code>Recno:: 0\nURL:: http://hiltongardeninn3.hilton.com/en/hotels/alabama/hilton-garden-inn-auburn-opelika-AUOAPGI/offers/index.htm\n\nParseText::\nHotels Auburn, AL - Hilton Garden Inn Auburn Opelika Deals Skip to Content My Reservations My Reservations View Promotions Sign In Join \u00a0 \u00a0 \u00a0 / \u00a0 My Account Sign Out Show Sign In Form \u00a0 \u00a0 View/change a specific reservation: Your Confirmation # Last Name Find \u00a0 Find - OR - Sign in to view all reservations in your account Existing Travel Reservations: Hotel + Air + Car Reservation Air Itinerary Car Rental Details Close Your Next Stay: See all \u00a0 \u00a0 Digital Key Offered View/Edit Go To My Account Page Update your password regularly to keep your account safe. Your Last Stay: See all \u00a0 \u00a0 View Receipt Book Again Join Hilton HHonors\u2122 Upgrade your account and earn points at over 3,600 hotels in 82 countries around the world. Join HHonors \u00a0 \u00a0\u00a0 \u00a0|\u00a0 \u00a0|\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Close Search GI Skip brand navigation Find a Hotel Offers Meetings and Events About Hilton Garden Inn skip form Where are you going? City, airport, address, attraction, or hotel Arrival You are now focused on a datepicker field. Press the down arrow to enter the calendar table. Once focused on the table, press left or right to navigate days. Press up or down to navigate between weeks. Enter to select. Escape to close datepicker. Your arrival date must be within the next year. \u00a0 Departure You are now focused on a datepicker field. Press the down arrow to enter the calendar table. Once focused on the table, press left or right to navigate days. Press up or down to navigate between weeks. Enter to select. Escape to close datepicker. Your departure date must be within 4 months after your arrival date. \u00a0 Use flexible dates Use HHonors Points Rooms Adults (18+) Children Rooms 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26+ Adults in Room 1 1 2 3 4 Children in Room 1 0 1 2 3 4 Room 2 : Adults in Room 2 1 2 3 4 Children in Room 2 0 1 2 3 4 Room 3 : Adults in Room 3 1 2 3 4 Children in Room 3 0 1 2 3 4 Room 4 : Adults in Room 4 1 2 3 4 Children in Room 4 0 1 2 3 4 Room 5 : Adults in Room 5 1 2 3 4 Children in Room 5 0 1 2 3 4 Room 6 : Adults in Room 6 1 2 3 4 Children in Room 6 0 1 2 3 4 Room 7 : Adults in Room 7 1 2 3 4 Children in Room 7 0 1 2 3 4 Room 8 : Adults in Room 8 1 2 3 4 Children in Room 8 0 1 2 3 4 Room 9 : Adults in Room 9 1 2 3 4 Children in Room 9 0 1 2 3 4 *Best Price Guarantee | More Options Less Options Add special rate codes (AAA, AARP, etc) Find it \u00a0 Find it Check Rooms &amp; Rates Promotion/Offer code: Group code: Corporate account: Travel agent AAA rate * AARP rate * Senior rate * Government / Military rates * * ID required at check-in Close Close close tab panel Offers Discover award-winning service, thoughtful amenities and great deals at HGI hotels. View all Offers Earn 2X Points Book early and save Bed N Breakfast Deal End of tab panel close tab panel Meetings &amp; Events Host your next meeting or special event in one of our newly renovated hotels. Meetings &amp; Events Meetings Weddings Planning Tools Small Meeting Packages End of tab panel close tab panel About Hilton Garden Inn We\u2019re here to help you be successful with great service and complimentary amenities. Learn More about HGI Locations Search more than 640 Hilton Garden Inn hotels worldwide to find the right one for your next trip. See our locations New Hotels See our new hotels and find out where we\u2019re scheduled to open soon. See all new hotels End of tab panel \u00a0 menu_item_property_offers AUOAPGI Hilton Garden Inn Auburn/Opelika 2555 Hilton Garden Drive , Auburn , Alabama , 36830 , USA TEL: +1-334-502-3500 FAX: +1-334-502-3572 Skip secondary navigation Hotel Home Hotel Details Amenities &amp; Services Maps &amp; Directions Rooms &amp; Suites Plan an Event Special Offers Dining Things To Do \u00a0 Not what you're looking for? Find Nearby Hotels Our Promise We promise to do whatever it takes to ensure you're satisfied, or you don't pay. You can count on us. GUARANTEED. The Hilton Garden Inn Promise reflects our focus on hospitality and integrity. We are committed to providing an excellent hotel experience for every guest, every time. Not what you're looking for? Find Nearby Hotels HHonors Reward Category: 7 \u00a0 Find a Special Offer Arrival \u00a0 Departure \u00a0 Find Offers Find Offers Hotel Information Check-in: 3:00 pm Check-out: 12:00 pm Smoking: Non-Smoking A fee of up to 250 USD will be assessed for smoking in a non-smoking room. Please ask the Front Desk for locations of designated outdoor smoking areas. Parking: Self parking: (Complimentary) Valet: Not Available Pets: Service animals allowed: Yes Pets allowed: No Hotel Policies Where we are Find where we are located View the Maps &amp; Directions Page Share Print Special Offers Sort by: \u00a0 Brand Book Date Compare offer Premium Wi-Fi Compare this offer , You can compare up to 4 offers Premium Wi-Fi Boost your speed with Premium Internet access &lt;\n</code></pre>\n\n<p><strong>Code:</strong></p>\n\n<pre><code>private static Pattern regexPreciseData = Pattern.compile(\"(?:(no|NO|No|(?:(p|P)?\\\\.?\\\\s?(o|O)?\\\\.?\\\\s?((B|b)(o|O)(x|X))))?(?:\\\\s?\\\\.?\\\\#?\\\\/?\\\\:?\\\\-?\\\\s?)[0-9]\\\\/?\\\\:?)+(?:\\\\,?\\\\s?\\\\w+\\\\s?){2,5}(?:\\\\s?\\\\w+\\\\s?)\\\\-?\\\\s?(\\\\d{2,5})\",Pattern.CASE_INSENSITIVE);    \nutils.getText(sb, root); // extract text\ntext = sb.toString();\nsb.setLength(0);\nMatcher addressPattern = regexPreciseData.matcher(text);\nwhile (addressPattern.find()) {\n      int start = addressPattern.start();\n      int end = addressPattern.end();\n      sb.append(text.substring(start, end));\n         }\ntext = sb.toString();\n</code></pre>\n\n<p>From the above code you can see I am trying to match my text with the regex pattern and strip only the address information out of it. But my regex is matching some irrelevant data also. Many suggested that only reg-ex can't be accurate enough to strip the address info from the text.</p>\n", "creation_date": 1443512030, "score": 2},
{"title": "How to crawl and parse only precise data using Nutch?", "view_count": 338, "is_answered": false, "answers": [{"last_edit_date": 1443533298, "owner": {"user_id": 1977773, "link": "http://stackoverflow.com/users/1977773/jorge-luis", "user_type": "registered", "reputation": 510}, "body": "<p>Checkout <a href=\"https://issues.apache.org/jira/browse/NUTCH-1870\" rel=\"nofollow\">NUTCH-1870</a> a work in progress on a generic XPath plugin for Nutch, the alternative is to write a custom HtmlParseFilter that scrap the data that you want. A good (and simple) example is the <a href=\"http://svn.apache.org/repos/asf/nutch/trunk/src/plugin/headings/\" rel=\"nofollow\">headings</a> plugin. Keep in mind that both of this links are for the 1.x branch of Nutch, and you're working with the 2.x although things are different in some degree the logic should be portable, the other alternative is using the 1.x branch.</p>\n\n<p>Based on your comment:</p>\n\n<p>Since you don't know the structure of the webpage, the problem is somehow different: Essentially you'll need to \"teach\" Nutch how to detect the text you want, based on some regexp or using some library that does address extraction out of plain text like jgeocoder library, you'll need to parse (iterate on every node of the webpage) trying to find something that resembles an address, phone number, fax number, etc. This is kind of similar to what the headings plugin does, but instead of looking for addresses or phone numbers it just finds the title nodes in the HTML structure. This could be a starting point to write some plugin that does what you want, but I don't think there is anything out of the box for this do. </p>\n", "question_id": 32758204, "creation_date": 1443248283, "is_accepted": false, "score": 0, "last_activity_date": 1443533298, "answer_id": 32794268}], "question_id": 32758204, "tags": ["java", "parsing", "solr", "web-crawler", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/32758204/how-to-crawl-and-parse-only-precise-data-using-nutch", "last_activity_date": 1443533298, "owner": {"user_id": 3364049, "answer_count": 3, "creation_date": 1393577655, "accept_rate": 22, "view_count": 14, "reputation": 94}, "body": "<p>I'm new to Nutch and crawling. I have installed Nutch 2.0, crawled and indexed the data using Solr 4.5 by following some basic tutorials. Now I don't want to parse all the text content of a page, I want to customize it like Nutch should crawl the page and scrape/fetch only the data related to address because my use case is to crawl URLs and parse only address info as text.</p>\n\n<p>For example, I need to crawl and parse only the text content which has address information, email id, phone number and fax number.</p>\n\n<ol>\n<li>How should I do this? Is there any plugin already available for this? </li>\n<li>If I want to write a customized parser for this can anyone help me in this regards?</li>\n</ol>\n", "creation_date": 1443087844, "score": 2},
{"title": "Dump all segments from nutch", "view_count": 1285, "owner": {"user_id": 791547, "answer_count": 1, "creation_date": 1307645071, "accept_rate": 92, "view_count": 20, "reputation": 335}, "is_answered": true, "answers": [{"question_id": 7968534, "owner": {"user_id": 608167, "accept_rate": 50, "link": "http://stackoverflow.com/users/608167/varshith", "user_type": "registered", "reputation": 252}, "body": "<p>You should give the path of segment till the segments dir(the one with the timestamp). If you want to read all the segments in the segments/ dir, you could have a wrapper class where you can list contents in the segments dir and call readseg from there.</p>\n", "creation_date": 1321336940, "is_accepted": true, "score": 0, "last_activity_date": 1321336940, "answer_id": 8132144}, {"question_id": 7968534, "owner": {"user_id": 2982904, "link": "http://stackoverflow.com/users/2982904/punit", "user_type": "registered", "reputation": 23}, "body": "<p>Alternately, here is what you can try</p>\n\n<p>Merge all segments first:</p>\n\n<pre><code>bin/nutch mergesegs crawl/merged crawl/segments/*\n</code></pre>\n\n<p>Then dump merged segment</p>\n\n<pre><code>bin/nutch readseg -dump crawl/merged/* dumpedContent\n</code></pre>\n", "creation_date": 1404482255, "is_accepted": false, "score": 0, "last_activity_date": 1404482255, "answer_id": 24575963}, {"question_id": 7968534, "owner": {"user_id": 1506477, "accept_rate": 100, "link": "http://stackoverflow.com/users/1506477/thamme-gowda-n", "user_type": "registered", "reputation": 2475}, "body": "<p>To read segments content from sequence files and create individual files on files:</p>\n\n<h3>1. Merge segments</h3>\n\n<p>this command creates <code>mergedseg</code> by combining all the segments in <code>segments/*</code></p>\n\n<pre><code>nutch mergesegs mergedseg -dir segments/\n</code></pre>\n\n<h3>2. Dump the merged segment</h3>\n\n<p>this command should be creating files under <code>content_dump</code></p>\n\n<pre><code>nutch dump -segment mergedseg -outputDir content_dump\n</code></pre>\n\n<blockquote>\n  <h2>Notes</h2>\n  \n  <ul>\n  <li>Tested in version 1.10</li>\n  <li>The <code>nutch dump</code> seems to be bit tricky. It didn't dump when I gave path of segment. In the above example <code>mergedseg</code> is a parent directory of segment directory.</li>\n  <li>You can also dump specific mimeTypes. Check help of <code>nutch dump</code></li>\n  </ul>\n</blockquote>\n", "creation_date": 1443404994, "is_accepted": false, "score": 0, "last_activity_date": 1443404994, "answer_id": 32814714}], "question_id": 7968534, "tags": ["apache", "nutch"], "answer_count": 3, "link": "http://stackoverflow.com/questions/7968534/dump-all-segments-from-nutch", "last_activity_date": 1443404994, "accepted_answer_id": 8132144, "body": "<p>I am simply trying to dump my segments from a crawl using readseg.  If I only have one folder the command bin/nutch readseg -dump crawl/segments/* dumpFolder works yet if I have multiple segment folders it fails.  Any ideas??</p>\n", "creation_date": 1320160247, "score": 1},
{"title": "Web crawling specific data using Solr Nutch", "view_count": 192, "is_answered": true, "answers": [{"question_id": 32265384, "owner": {"user_id": 432844, "link": "http://stackoverflow.com/users/432844/julien-nioche", "user_type": "registered", "reputation": 939}, "body": "<p>There is no such thing as 'SOLR Nutch'. They are separate projects.</p>\n\n<p>You can write HTMLParseFilters in Nutch and implement the extraction logic then configure the indexing filters so that the data gets sent to SOLR.</p>\n\n<p>BTW you might find StormCrawler easier to use and extend. It has a <a href=\"https://github.com/DigitalPebble/storm-crawler/blob/master/core/src/main/java/com/digitalpebble/storm/crawler/parse/filter/XPathFilter.java\" rel=\"nofollow\">ParseFilter implementation</a> which takes XPath expressions for extracting information from webpages.</p>\n\n<p>Of course you can do the same within a HTMLParseFilter in Nutch but it would require writing some code and put it in a custom plugin.</p>\n", "creation_date": 1442929304, "is_accepted": false, "score": 1, "last_activity_date": 1442929304, "answer_id": 32718524}], "question_id": 32265384, "tags": ["solr", "web-scraping", "web-crawler", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/32265384/web-crawling-specific-data-using-solr-nutch", "last_activity_date": 1442929304, "owner": {"user_id": 5275795, "view_count": 3, "answer_count": 0, "creation_date": 1440744932, "reputation": 1}, "body": "<p>I saw some search websites like <a href=\"http://homes.mitula.ph/homes/makati\" rel=\"nofollow\">http://homes.mitula.ph/homes/makati</a> and I wonder how they crawl data in other websites like <code>price</code>, <code>image</code>, and <code>description</code> and display it to their site. </p>\n\n<p>I'm thinking of using Solr to index data and Nutch to crawl it. I'm new to web crawling and indexing and so far I can only crawl the content of a web page. </p>\n\n<p>Can Solr Nutch do that kind of crawling? and how?</p>\n", "creation_date": 1440746296, "score": 0},
{"title": "Nutch numSlaves parameter in crawl script", "view_count": 53, "is_answered": true, "answers": [{"question_id": 32686968, "owner": {"user_id": 432844, "link": "http://stackoverflow.com/users/432844/julien-nioche", "user_type": "registered", "reputation": 939}, "body": "<p>Nope. You'll need to modify the crawl script and restart it. No big deal though, just SSH to the master node and create a file .STOP in runtime/deploy/bin. This will stop the crawl loop when the current iteration is complete. You can then restart the script after setting the value to 10.</p>\n\n<p>BTW you'd get quicker answers by asking on the Nutch mailing lists</p>\n", "creation_date": 1442928600, "is_accepted": false, "score": 1, "last_activity_date": 1442928600, "answer_id": 32718243}], "question_id": 32686968, "tags": ["nutch", "emr"], "answer_count": 1, "link": "http://stackoverflow.com/questions/32686968/nutch-numslaves-parameter-in-crawl-script", "last_activity_date": 1442928600, "owner": {"user_id": 4771719, "answer_count": 0, "creation_date": 1428628867, "view_count": 0, "location": "Melbourne", "reputation": 1}, "body": "<p>I am using Nutch 1.9 to crawl a set of 500 websites. I am running nutch in Amazon EMR cluster and indexing the data to Solr. </p>\n\n<p>While starting an EMR cluster I have started with 5 slave nodes. I have specified the numSlaves parameter to 5 in crawl script. I would like to increase my slaves to 10 to fasten the process. I am able to increase the number of slave nodes in the AWS console to 10. Will the nutch utilize all the 10 slave nodes without restarting my crawl or modifying the crawl script.  </p>\n\n<p>Thanks</p>\n", "creation_date": 1442806632, "score": 0},
{"title": "Running multiple Apache Nutch fetch map tasks on a Hadoop Cluster", "view_count": 239, "is_answered": false, "answers": [{"question_id": 25926729, "owner": {"user_id": 4585667, "link": "http://stackoverflow.com/users/4585667/keith-dsouza", "user_type": "registered", "reputation": 13}, "body": "<p>Are you using Nutch 1.xx for this? In this case, the Generator class looks for a flag called \"mapred.job.tracker\" and tries to see if it is local. This property has been deprecated in Hadoop2 and the default value is set to local. You will have to overwrite the value of the property to something other than local and the Generator will generate multiple partitions for the segments.</p>\n", "creation_date": 1424378037, "is_accepted": false, "score": 0, "last_activity_date": 1424378037, "answer_id": 28616336}, {"question_id": 25926729, "owner": {"user_id": 3130388, "link": "http://stackoverflow.com/users/3130388/agmangas", "user_type": "registered", "reputation": 421}, "body": "<p>I've recently faced this problem and thought it'd be a good idea to build upon Keith's answer to provide a more thorough explanation about how to solve this issue.</p>\n\n<p>I've tested this with Nutch 1.10 and Hadoop 2.4.0.</p>\n\n<p>As Keith said the <em>if</em> block on line 542 in <strong>Generator.java</strong> reads the <em>mapred.job.tracker</em> property and sets the value of variable <code>numLists</code> to <code>1</code> if the property is <code>local</code>. This variable seems to control the number of reduce tasks and has influence in the number of map tasks.</p>\n\n<p>Overwriting the value of said property in <code>mapred-site.xml</code> fixes this:</p>\n\n<pre><code>&lt;property&gt;\n\u00a0\u00a0\u00a0\u00a0&lt;name&gt;mapred.job.tracker&lt;/name&gt;\n\u00a0\u00a0\u00a0\u00a0&lt;value&gt;distributed&lt;/value&gt;\n&lt;/property&gt;\n</code></pre>\n\n<p>(Or any other value you like except <code>local</code>).</p>\n\n<p>The problem is this wasn't enough in my case to generate more than one <em>fetch</em> map task. I also had to update the value of the <code>numSlaves</code> parameter in the <strong>runtime/deploy/bin/crawl</strong> script. I didn't find any mentions of this parameter in the Nutch 1.x docs so I stumbled upon it after a bit of trial and error.</p>\n\n<pre><code>#############################################\n# MODIFY THE PARAMETERS BELOW TO YOUR NEEDS #\n#############################################\n\n# set the number of slaves nodes\nnumSlaves=3\n\n# and the total number of available tasks\n# sets Hadoop parameter \"mapred.reduce.tasks\"\nnumTasks=`expr $numSlaves \\* 2`\n\n...\n</code></pre>\n", "creation_date": 1442907850, "is_accepted": false, "score": 0, "last_activity_date": 1442907850, "answer_id": 32711183}], "question_id": 25926729, "tags": ["nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/25926729/running-multiple-apache-nutch-fetch-map-tasks-on-a-hadoop-cluster", "last_activity_date": 1442907850, "owner": {"user_id": 1965449, "answer_count": 6, "creation_date": 1357790106, "accept_rate": 55, "view_count": 143, "reputation": 783}, "body": "<p>I am  unable to run multiple fetch Map taks for Nutch 1.7 on Hadoop YARN.</p>\n\n<p>I am   using the bin/crawl script and did the following tweaks to trigger a fetch with multiple map tasks , however I am unable to do so.</p>\n\n<ol>\n<li><p>Added maxNumSegments and numFetchers parameters to the generate phase.\n$bin/nutch generate $commonOptions $CRAWL_PATH/crawldb $CRAWL_PATH/segments -maxNumSegments $numFetchers -numFetchers $numFetchers -noFilter</p></li>\n<li><p>Removed the topN paramter and removed the noParsing parameter because I want the parsing to happen at the time of fetch.\n$bin/nutch fetch $commonOptions -D fetcher.timelimit.mins=$timeLimitFetch $CRAWL_PATH/segments/$SEGMENT -threads $numThreads #-noParsing#</p></li>\n</ol>\n\n<p>The generate phase is not generating more than one segment.</p>\n\n<p>And as a result the fetch phase is not creating multiple map tasks, also I belive the script is written it does not allow the fecth to fecth multiple segemnts even if the generate were to generate multiple segments.</p>\n\n<p>Can someone please let me know , how they go the script to run in a distributed Hadoop cluster ? Or if there is a different version of script that should be used?</p>\n\n<p>Thanks.</p>\n", "creation_date": 1411103014, "score": 0},
{"title": "Can I use Nutch 2.x and Gora with a Solr backend", "view_count": 286, "is_answered": false, "question_id": 32673602, "tags": ["solr", "nutch", "gora"], "answer_count": 0, "link": "http://stackoverflow.com/questions/32673602/can-i-use-nutch-2-x-and-gora-with-a-solr-backend", "last_activity_date": 1442726145, "owner": {"age": 30, "answer_count": 23, "creation_date": 1363403535, "user_id": 2176123, "accept_rate": 50, "view_count": 32, "location": "Reston, VA", "reputation": 191}, "body": "<p>The Nutch 2.x branch <code>gora.properties</code> file lists Solr as a possible backend for Nutch, but I cannot find any documentation online. So, two questions:</p>\n\n<ol>\n<li><p>Can I do this: <code>Nutch -&gt; Gora -&gt; Solr</code>, and use SOLR for both storage and indexing? I can't find any documentation on this particular setup.</p></li>\n<li><p>Is there any benefit to doing this: <code>Nutch -&gt; Gora -&gt; HBase -&gt; Solr</code>, where Solr is only used to index HBase. This seems to be the most common approach, but HBase seems unnecessary if the main goal is to consume the results with Solr.</p></li>\n</ol>\n\n<p>Here is the 2.x <a href=\"https://github.com/apache/nutch/blob/2.x/conf/gora.properties\" rel=\"nofollow\">gora.properties</a>:</p>\n\n<pre><code>############################\n# SolrStore properties     #\n############################\n#gora.datastore.default=org.apache.gora.solr.store.SolrStore\n#gora.solrstore.solr.url=http://localhost:9876/solr\n#gora.solrstore.solr.config=solrconfig.xml\n#gora.solrstore.solr.schema=gora-solr-schema.xml\n#gora.solrstore.solr.batchSize=100\n#gora.solrstore.solr.solrjserver=http\n#gora.solrstore.solr.commitWithin=1000\n#gora.solrstore.solr.resultsSize=100\n</code></pre>\n", "creation_date": 1442699574, "score": 0},
{"title": "Solr new Core from UI", "view_count": 110, "owner": {"user_id": 2539093, "answer_count": 1, "creation_date": 1372684065, "accept_rate": 60, "view_count": 4, "location": "Watertown, NY", "reputation": 8}, "is_answered": true, "answers": [{"last_edit_date": 1441899164, "owner": {"user_id": 2539093, "accept_rate": 60, "link": "http://stackoverflow.com/users/2539093/gates3353", "user_type": "registered", "reputation": 8}, "body": "<p>I found a tutorial here: <a href=\"http://examples.javacodegeeks.com/enterprise-java/apache-solr/apache-solr-tutorial-beginners/\" rel=\"nofollow\">apache-solr-tutorial-beginners</a></p>\n\n<p>I followed the exact instructions the author gives for creating a new core via the command line from <code>solar-5.3.0/bin</code>:</p>\n\n<pre><code>solr create -c jcg -d basic_configs\n</code></pre>\n\n<p><code>jcg</code> then appeared in my Solr UI. </p>\n\n<p>I went back and tried this same thing with my Project specs and it worked! I still have no idea how to do this from the UI but at least I can move forward an inch!</p>\n", "question_id": 32485448, "creation_date": 1441823322, "is_accepted": false, "score": 0, "last_activity_date": 1441899164, "answer_id": 32486765}, {"question_id": 32485448, "owner": {"user_id": 2254048, "accept_rate": 100, "link": "http://stackoverflow.com/users/2254048/younghobbit", "user_type": "registered", "reputation": 7661}, "body": "<p>I followed the following steps for adding a <code>core</code> using solr admin UI.</p>\n\n<ol>\n<li>Start the solr server using <code>~/solr-5.2.0/bin/solr start</code>. This will start the solr on 8983 port.</li>\n<li>Now go to <code>solr</code> directory. <code>cd ~/solr-5.2.0/server/solr</code>.</li>\n<li>Create a new folder, which will contain the solr core configuration. <code>mkdir newCore</code>.</li>\n<li>Now create a <code>conf</code> directory in side the <code>newCore</code> and copy your <code>schema.xml</code> and <code>solrconfig.xml</code> along with other necessary files.</li>\n<li>Go to <strong>Solr Admin UI</strong>, <code>Core Admims</code> section. Specify the <strong>core name</strong>, as per your requirement and <code>newCore</code> (name of the directory which we have created) in the <code>instanceDir</code> field. Click the <code>Add Core</code> button.</li>\n</ol>\n", "creation_date": 1441875318, "is_accepted": true, "score": 0, "last_activity_date": 1441875318, "answer_id": 32497311}], "question_id": 32485448, "tags": ["windows", "solr", "cygwin", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/32485448/solr-new-core-from-ui", "last_activity_date": 1441899164, "accepted_answer_id": 32497311, "body": "<p>I'm trying to create a new Core with Solr 5.3. I have no experience working with Solr until a few days ago. I think I need this broken down Barney style. I've been through the system doc, wiki's, YouTube, and random discussion boards. The information I've found is either not current or not what I'm seeing from my UI. I've now wasted five hours trying to get this to work. I'm out of options. I'm about ready to drop this project and start from scratch. I'm completely exasperated and throwing myself to the mercy of my betters. Can anyone just show me how to do it?</p>\n", "creation_date": 1441818137, "score": 0},
{"title": "metadata from images on nutch 2x", "view_count": 45, "is_answered": false, "question_id": 32492936, "tags": ["hadoop", "solr", "nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/32492936/metadata-from-images-on-nutch-2x", "last_activity_date": 1441858871, "owner": {"age": 25, "answer_count": 1, "creation_date": 1432178548, "user_id": 4922925, "view_count": 4, "location": "Indonesia", "reputation": 1}, "body": "<p>I have recently using working on nutch, some far I go its crawl some site and images, I had crawl metadata too but just some site. I want metadata from images. \nis some way to get metadata from images? I using nutch 2x and solr 4.8.1</p>\n\n<p>this my nutch-site.xml, suffix-urlfilter.xml and regex-urlfilter.xml\n<a href=\"http://picturenutch.blogspot.com/2015/09/nutchsite.html\" rel=\"nofollow\">http://picturenutch.blogspot.com/2015/09/nutchsite.html</a></p>\n", "creation_date": 1441857764, "score": 0},
{"title": "how to get the images in Nutch results?", "view_count": 1158, "is_answered": false, "answers": [{"question_id": 3247589, "owner": {"user_id": 4922925, "link": "http://stackoverflow.com/users/4922925/indah-setiarini", "user_type": "registered", "reputation": 1}, "body": "<p>change your regex-urlfilter.txt in conf </p>\n\n<blockquote>\n  <p>-.(ico|ICO|css|CSS|sit|SIT|eps|EPS|wmf|WMF|zip|ZIP|ppt|PPT|xls|XLS|gz|GZ|rpm|RPM|tgz|TGZ|exe|EXE|js|JS|gif|GIF|png|PNG||jpg|JPG|jpeg|JPEG|bmp|BMP|mpg|MPG|mov|MOV)$ </p>\n</blockquote>\n\n<p>delete jpeg, jpg, gif or type picture that you want to grep</p>\n\n<p>and then change suffix-urlfilter.txt in conf</p>\n\n<p>add # to jpeg or gif or png </p>\n\n<p>thats work for me   </p>\n", "creation_date": 1441858490, "is_accepted": false, "score": 0, "last_activity_date": 1441858490, "answer_id": 32493036}], "question_id": 3247589, "tags": ["solr", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/3247589/how-to-get-the-images-in-nutch-results", "last_activity_date": 1441858490, "owner": {"age": 30, "answer_count": 3, "creation_date": 1258953861, "user_id": 216772, "accept_rate": 4, "view_count": 249, "location": "Bangalore, India", "reputation": 158}, "body": "<pre><code> how to get the images in Nutch results?\n\n           can you please explain it is possible with images? or there is any other open search engine which is producing the results with images? \n</code></pre>\n\n<p>Thanks,\nMurali</p>\n", "creation_date": 1279120353, "score": 1},
{"title": "error while building Nutch 1.10", "view_count": 25, "is_answered": false, "question_id": 32455030, "tags": ["ant", "ivy", "nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/32455030/error-while-building-nutch-1-10", "last_activity_date": 1441707119, "owner": {"user_id": 1377135, "answer_count": 2, "creation_date": 1336238734, "accept_rate": 57, "view_count": 22, "reputation": 71}, "body": "<p>I am  trying  to build nutch 1.0 using ant on win7 platform. However, I am getting the below error. </p>\n\n<pre><code>[ivy:resolve]           ::::::::::::::::::::::::::::::::::::::::::::::\n[ivy:resolve]           ::          UNRESOLVED DEPENDENCIES         ::\n[ivy:resolve]           ::::::::::::::::::::::::::::::::::::::::::::::\n[ivy:resolve]           :: org.slf4j#slf4j-api;1.6.1: not found\n[ivy:resolve]           :: org.slf4j#slf4j-log4j12;1.6.1: not found\n[ivy:resolve]           :: log4j#log4j;1.2.15: not found\n[ivy:resolve]           :: commons-lang#commons-lang;2.6: not found\n[ivy:resolve]           :: commons-collections#commons-collections;3.1: not foun\nd\n[ivy:resolve]           :: commons-httpclient#commons-httpclient;3.1: not found\n[ivy:resolve]           :: commons-codec#commons-codec;1.3: not found\n[ivy:resolve]           :: org.apache.commons#commons-compress;1.9: not found\n[ivy:resolve]           :: org.apache.hadoop#hadoop-core;1.2.0: not found\n[ivy:resolve]           :: org.apache.tika#tika-core;1.8: not found\n[ivy:resolve]           :: com.ibm.icu#icu4j;55.1: not found\n[ivy:resolve]           :: xerces#xercesImpl;2.9.1: not found\n[ivy:resolve]           :: xerces#xmlParserAPIs;2.6.2: not found\n[ivy:resolve]           :: oro#oro;2.0.8: not found\n[ivy:resolve]           :: com.google.guava#guava;11.0.2: not found\n[ivy:resolve]           :: com.google.code.crawler-commons#crawler-commons;0.5:\nnot found\n[ivy:resolve]           :: org.apache.cxf#cxf;3.0.4: not found\n[ivy:resolve]           :: org.apache.cxf#cxf-rt-frontend-jaxws;3.0.4: not found\n\n[ivy:resolve]           :: org.apache.cxf#cxf-rt-frontend-jaxrs;3.0.4: not found\n\n[ivy:resolve]           :: org.apache.cxf#cxf-rt-transports-http;3.0.4: not foun\nd\n[ivy:resolve]           :: org.apache.cxf#cxf-rt-transports-http-jetty;3.0.4: no\nt found\n[ivy:resolve]           :: com.fasterxml.jackson.core#jackson-databind;2.5.1: no\nt found\n[ivy:resolve]           :: com.fasterxml.jackson.dataformat#jackson-dataformat-c\nbor;2.5.1: not found\n[ivy:resolve]           :: com.fasterxml.jackson.jaxrs#jackson-jaxrs-json-provid\ner;2.5.1: not found\n[ivy:resolve]           ::::::::::::::::::::::::::::::::::::::::::::::\n[ivy:resolve]\n[ivy:resolve]\n[ivy:resolve] :: USE VERBOSE OR DEBUG MESSAGE LEVEL FOR MORE DETAILS\n\nBUILD FAILED\nC:\\nutch\\build.xml:476: impossible to resolve dependencies:\n        resolve failed - see output for details\n\nTotal time: 6 seconds\n</code></pre>\n\n<p>I tried many forums but to no avail. Not sure what's causing this or how to fix it. Any help would be appreciated.</p>\n", "creation_date": 1441707119, "score": 0},
{"title": "Nutch is not indexing content to solr but does not log any kind of error or waring", "view_count": 57, "is_answered": false, "answers": [{"question_id": 32348850, "owner": {"user_id": 5291727, "link": "http://stackoverflow.com/users/5291727/patrick-wilmes", "user_type": "registered", "reputation": 1}, "body": "<p>I'll found the problem. There was a typo in the solrindex-mappings.xml. Eather Nutch nor Solr logged some kind of error even in debug mode, but after changing the solrindex-mappings everything worked just fine.</p>\n", "creation_date": 1441276833, "is_accepted": false, "score": 0, "last_activity_date": 1441276833, "answer_id": 32373366}], "question_id": 32348850, "tags": ["tomcat", "logging", "solr", "nutch", "solrj"], "answer_count": 1, "link": "http://stackoverflow.com/questions/32348850/nutch-is-not-indexing-content-to-solr-but-does-not-log-any-kind-of-error-or-wari", "last_activity_date": 1441276833, "owner": {"user_id": 5291727, "view_count": 1, "answer_count": 1, "creation_date": 1441183962, "reputation": 1}, "body": "<p>I updated my nutch instance (on centos) from 1.4 to 1.8. For the frist 3-4 days it works just fine and was indexing anything to solr (3.5). But now nutch runs without any issue and also logs that the solr update is successful but there are no documents in the solr index.</p>\n\n<p>Solr logs are also without any error or warning.</p>\n\n<p>Does anyone has an idea what i'am missing? I'll tried allmost anything and searched hours for a solution.</p>\n\n<p>regards</p>\n", "creation_date": 1441184486, "score": -1},
{"title": "How to Build a Search Engine? (2013 Update)", "view_count": 404, "is_answered": true, "answers": [{"question_id": 17777440, "owner": {"user_id": 2692415, "link": "http://stackoverflow.com/users/2692415/jantar", "user_type": "registered", "reputation": 158}, "body": "<p>Udacity's got very good course on learning Python via creating web crawler, try it here: \n<a href=\"https://www.udacity.com/course/cs101\" rel=\"nofollow\">https://www.udacity.com/course/cs101</a></p>\n", "creation_date": 1377343039, "is_accepted": false, "score": 1, "last_activity_date": 1377343039, "answer_id": 18417987}, {"last_edit_date": 1441253197, "owner": {"user_id": 579026, "accept_rate": 85, "link": "http://stackoverflow.com/users/579026/ren", "user_type": "registered", "reputation": 2040}, "body": "<p>I'll use this question to share some experience at writing a small search engine from scratch (no search-specific libraries were used) for a fairly small dataset (it actually searches stackoverflow as it was neither too small nor too large to work on a single server). <a href=\"http://stackse.com\" rel=\"nofollow\">Check it out</a>. Below are my findings on the subject. </p>\n\n<p><strong>Crawler</strong></p>\n\n<p>First, the crawler is a tough thing to do. The real problem is writing data to the disk as fast as you get the web pages. The main data structure is an inverted index and so when you get the word \"banana\" you need to pull from disk the \"banana\" index (list of documents where it occurs - together with positions in the doc) append it with the new record and write it back. As the list grows, pulling and writing it back is getting slower. So one trick would be to slice inverted indexes (and the documents) into partitions, say 1-1000 documents in the first partition and so on. The other \"trick\" is while crawling a partition  to keep indexes in memory and flush them to disk only when the partition is done.</p>\n\n<p>Important bit: what to use to store the data? There are many options and after many experiments I've found leveldb to be the best choice as of today. And don't forget SSD disks!</p>\n\n<p>So, all in all, crawling most of stackoverflow (~13 000 000 pages) in this manner using one machine (4 Gb ram) takes about 2 months. And the resulting data (the inverted index, the raw stripped text, etc) - about 80 GB of disk space.</p>\n\n<p><strong>Search</strong></p>\n\n<p>The goal is to do it fast and with high quality. One thing to realize is that if you want it to be fast, you can't search the entire dataset. Luckily, I had everything partitioned so the search takes the first 100 partitions where the keywords appear (separate index for that) and if it finds \"good enough\" results - stops, if not - takes another 100 and so on.</p>\n\n<p>The slowest part is reading indexes from disk and deserialising it. Leveldb supports fast sequential reading, so the data needs to be stored in a way that most of it could be read sequentially. Once in memory set intersection is pretty fast.</p>\n\n<p>Now the quality. That's the toughest and never good enough. My initial attempt was to keep inverted indexes not only for the text, but also for the titles, link text and urls. Every hit in these adds some points to the document. Another thing is to rephrase the query using synonyms and somehow check which query worked best. That would probably deserves a post in it's own.</p>\n\n<p>Anyway, I hope it'll be useful reading!</p>\n", "question_id": 17777440, "creation_date": 1441227388, "is_accepted": false, "score": 1, "last_activity_date": 1441253197, "answer_id": 32362967}], "question_id": 17777440, "tags": ["search", "search-engine", "nutch", "common-crawl"], "answer_count": 2, "link": "http://stackoverflow.com/questions/17777440/how-to-build-a-search-engine-2013-update", "last_activity_date": 1441253197, "owner": {"age": 32, "answer_count": 34, "creation_date": 1258057205, "user_id": 209920, "accept_rate": 88, "view_count": 523, "location": "Morristown, NJ", "reputation": 1856}, "body": "<p>This isn't the first time this question has been <a href=\"http://stackoverflow.com/questions/384276/how-to-create-search-engines-like-google\">asked</a> here at Stackoverflow - but it is nearly five years later - and the times and technologies have changed a bit. I'm wondering what folks are thinking these days about building a search engine?</p>\n\n<p>For example, I know <a href=\"http://nutch.apache.org/\">Nutch</a> is continuing to be developed - but is it still the most robust solution available? Are there alternative mature solutions available for other languages - e.g. C#, PHP, VB.NET?</p>\n\n<p>I also know that there is now a publicly available mass index that can be utilized, reducing the need to perform one's own spidering from <a href=\"http://www.commoncrawl.org/\">Common Crawl</a>.</p>\n\n<p>There are of course, still a few custom search engine solutions out there, most well-known being <a href=\"http://www.google.com/cse/\">Google's CSE</a>...but I'm not aware of any other major/stable/reputable ones that I'd trust to build an engine upon?</p>\n\n<p>What resources are available now to learn programming search engines that weren't available a few years ago, or even last year?</p>\n", "creation_date": 1374444422, "score": 8},
{"title": "Can&#39;t run nutch2.3-snapshot on hadoop2.4.0 using gora0.5 and mongodb as backend datastore", "view_count": 181, "is_answered": false, "question_id": 27085342, "tags": ["hadoop", "nutch", "gora"], "answer_count": 0, "link": "http://stackoverflow.com/questions/27085342/cant-run-nutch2-3-snapshot-on-hadoop2-4-0-using-gora0-5-and-mongodb-as-backend", "last_activity_date": 1441228568, "owner": {"user_id": 4283420, "view_count": 3, "answer_count": 0, "creation_date": 1416713154, "reputation": 16}, "body": "<p>I'm running into this problem for a few days. \nWhen I use hadoop1.2, it works all right. \nWhile I turn to hadoop2.x(hadoop2.4 or hadoop2.5.2), I get this problem:</p>\n\n<pre><code>java.lang.Exception: java.lang.IncompatibleClassChangeError: Found interface org.apache.hadoop.mapreduce.TaskAttemptContext, but class was expected\n    at org.apache.hadoop.mapred.LocalJobRunner$Job.runTasks(LocalJobRunner.java:462)\n    at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:522)\nCaused by: java.lang.IncompatibleClassChangeError: Found interface org.apache.hadoop.mapreduce.TaskAttemptContext, but class was expected\n    at org.apache.gora.mapreduce.GoraOutputFormat.getRecordWriter(GoraOutputFormat.java:83)\n    at org.apache.hadoop.mapred.MapTask$NewDirectOutputCollector.&lt;init&gt;(MapTask.java:624)\n    at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:744)\n    at org.apache.hadoop.mapred.MapTask.run(MapTask.java:340)\n    at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:243)\n    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)\n    at java.util.concurrent.FutureTask.run(FutureTask.java:262)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n    at java.lang.Thread.run(Thread.java:744)\n</code></pre>\n\n<p>I found that when I use hadoop2.x in the <code>ivy.xml</code>, it will produce a <code>hadoop-core-1.0.1.jar</code> automatically, it seems influenced by gora's dependency. After excluded hadoop-core-*, this problem will occur!\nI also update the jar file <code>avro-mapre-1.7.6.jar</code> to <code>avro-mapred-1.7.6-hadoop2.jar</code> by hand, while unfortunately nothing changes!\nAny ideas would be appreciated, thx!</p>\n", "creation_date": 1416713932, "score": 3},
{"title": "How to write a plugin for Nutch 2.3", "view_count": 128, "is_answered": false, "question_id": 32202665, "tags": ["java", "apache", "plugins", "nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/32202665/how-to-write-a-plugin-for-nutch-2-3", "last_activity_date": 1440501537, "owner": {"user_id": 3454410, "answer_count": 20, "creation_date": 1395646154, "accept_rate": 48, "view_count": 174, "reputation": 832}, "body": "<p>I am using Nutch 2.3 version to crawl data. I have to add a plugin in Nutch. I have search from web. I have found some guide from web e.g. <a href=\"http://wiki.apache.org/nutch/WritingPluginExample\" rel=\"nofollow\">wiki.apache.org</a>. But it is for older version of Nutch 1.x.</p>\n\n<p>How I'll do it please elaborate it as I am new in this field ?</p>\n", "creation_date": 1440501537, "score": 0},
{"title": "Java CSS Crawler", "view_count": 259, "owner": {"user_id": 451461, "answer_count": 6, "creation_date": 1284824297, "accept_rate": 86, "view_count": 78, "reputation": 517}, "is_answered": true, "answers": [{"question_id": 4707598, "owner": {"user_id": 577771, "link": "http://stackoverflow.com/users/577771/anil", "user_type": "unregistered", "reputation": 1}, "body": "<p>I recommend using plain HTTPClient and simple regex. You can store the responses in file, database or archive of your own (See Heritrix).</p>\n\n<p>It keeps things simple, instead of using a heavy weighted crawler. Since the there are few CSS per domain, you can safely ignore complex url following within domain.</p>\n\n<p>Cheers !</p>\n", "creation_date": 1295208985, "is_accepted": false, "score": 0, "last_activity_date": 1295208985, "answer_id": 4707781}, {"question_id": 4707598, "owner": {"user_id": 1216112, "accept_rate": 85, "link": "http://stackoverflow.com/users/1216112/chaiavi", "user_type": "registered", "reputation": 364}, "body": "<p>You are right, don't use those, they are way too heavy.</p>\n\n<p>Use: <a href=\"https://github.com/yasserg/crawler4j\" rel=\"nofollow\">Crawler4j</a></p>\n\n<p>Follow the onsite tutorial for a simple crawler.</p>\n\n<p>The only change you need is in MyCrawler.java:\nRemove \"css\" from the FILTERS pattern\nIn the visit() method, put a simple condition as follows:</p>\n\n<pre><code>if (url.contains(\".css\")) {\n    // do what you need with it\n}\n</code></pre>\n\n<p>That's it - you are good!</p>\n", "creation_date": 1440418179, "is_accepted": true, "score": 0, "last_activity_date": 1440418179, "answer_id": 32181986}], "question_id": 4707598, "tags": ["java", "lucene", "web-crawler", "nutch", "xapian"], "answer_count": 2, "link": "http://stackoverflow.com/questions/4707598/java-css-crawler", "last_activity_date": 1440418179, "accepted_answer_id": 32181986, "body": "<p>I'm looking for a web crawler with the ability to grab the page's CSS. I don't need any other fancy crawling abilities.</p>\n\n<p>I'm trying to make my way through Xapian, Nutch and Heritrix. They all seem to be a bit complex. If anyone has any experience or recommendation I would love to hear. An accessible tutorial to any of the above platforms, is also welcomed.</p>\n\n<p>David</p>\n", "creation_date": 1295206699, "score": 1},
{"title": "Mapping data into Elasticsearch from Nutch 1.x", "view_count": 180, "owner": {"user_id": 3125823, "answer_count": 11, "creation_date": 1387649781, "accept_rate": 78, "view_count": 68, "reputation": 286}, "is_answered": true, "answers": [{"question_id": 31925371, "owner": {"user_id": 1577405, "link": "http://stackoverflow.com/users/1577405/aperfectpoint", "user_type": "registered", "reputation": 41}, "body": "<p>There are two things to consider here:</p>\n\n<ol>\n<li>What is the data that is Indexed?</li>\n<li>How to index it correctly to es</li>\n</ol>\n\n<p>Regarding the indexed data, the index-plugins you use affect this. For example, the basic-index will add <em>content</em>, <em>host</em>, <em>url</em>, <em>etc.</em> for every <strong><em>doc</em></strong>. You can either check the plugins' documentation or to simply see what is the output (like you did).</p>\n\n<p>After you know the indexed data and how to you want to approach it in the es cluster, you can create a new index in es with the correct/ optimized mappings, and make sure Nutch will index to that index. </p>\n\n<p>Of course you can also re-index what you already crawled (see <a href=\"https://www.elastic.co/guide/en/elasticsearch/guide/current/reindex.html\" rel=\"nofollow\">this es article</a>).</p>\n", "creation_date": 1440399274, "is_accepted": true, "score": 0, "last_activity_date": 1440399274, "answer_id": 32176079}], "question_id": 31925371, "tags": ["indexing", "elasticsearch", "mapping", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/31925371/mapping-data-into-elasticsearch-from-nutch-1-x", "last_activity_date": 1440399274, "accepted_answer_id": 32176079, "body": "<p>I've been working with Nutch 1.10 to make some small web crawls and indexing the crawl data using Elasticsearch 1.4.1 - it seems that the only way to optimize the index mapping is to crawl first, review the mapping that ES did on its own and then change it accordingly (if necessary) with the mapping API.</p>\n\n<p>Does anyone know of a more effective solution to optimize the mappings within an ES index for web crawling?</p>\n\n<p><strong>UPDATE:</strong>\nIs it even possible to update an ES mapping from a Nutch web crawl?</p>\n", "creation_date": 1439226648, "score": 0},
{"title": "Nutch, Solr/Lucene and hibernate integration", "view_count": 40, "is_answered": false, "question_id": 32128125, "tags": ["hibernate", "solr", "lucene", "nutch", "hibernate-search"], "answer_count": 0, "link": "http://stackoverflow.com/questions/32128125/nutch-solr-lucene-and-hibernate-integration", "last_activity_date": 1440103778, "owner": {"user_id": 1890873, "answer_count": 1, "creation_date": 1355121579, "accept_rate": 86, "view_count": 29, "reputation": 48}, "body": "<p>Sorry for my English. I use Hibernate for work with the database. The database contains addresses of sites for indexing Nutch. </p>\n\n<p>1) Need to Nutch received from the database the addresses of sites for indexing.</p>\n\n<p>2) Then add the address of the indexed pages in the database and search text with <code>id</code> into Solr/Lucene.</p>\n\n<p>Please tell me how to do it. </p>\n", "creation_date": 1440103778, "score": 0},
{"title": "How to inform Nutch to avoid crawling list of websites", "view_count": 172, "owner": {"user_id": 3454410, "answer_count": 20, "creation_date": 1395646154, "accept_rate": 48, "view_count": 174, "reputation": 832}, "is_answered": true, "answers": [{"question_id": 32088782, "owner": {"user_id": 5238184, "link": "http://stackoverflow.com/users/5238184/christian-caesar", "user_type": "registered", "reputation": 11}, "body": "<p>Have you tried entering specific \"seed URLs\" into the configuration, e.g. <a href=\"http://my.site.to/crawl\" rel=\"nofollow\">http://my.site.to/crawl</a> (replace by something more useful)</p>\n\n<p>There should be a config file called \"seed.txt\" in the folder /conf/urls.</p>\n", "creation_date": 1439969637, "is_accepted": false, "score": 1, "last_activity_date": 1439969637, "answer_id": 32089219}, {"question_id": 32088782, "owner": {"user_id": 2265190, "accept_rate": 50, "link": "http://stackoverflow.com/users/2265190/hms", "user_type": "registered", "reputation": 293}, "body": "<p>I think in Apache nutch configuration directory, there is a file name regex-urlfilter.txt. That will do the job what you are looking for. For example, if you have to block a website <a href=\"http://wiki.thm.com\" rel=\"nofollow\">http://wiki.thm.com</a> then you write following in above file</p>\n\n<pre><code>-^(http|https)://http://wiki.thm.com.*$\n</code></pre>\n\n<p>For further please study <a href=\"https://wiki.apache.org/nutch/FrontPage\" rel=\"nofollow\">Nutch wiki</a>. </p>\n", "creation_date": 1440041595, "is_accepted": true, "score": 1, "last_activity_date": 1440041595, "answer_id": 32109123}], "question_id": 32088782, "tags": ["web-crawler", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/32088782/how-to-inform-nutch-to-avoid-crawling-list-of-websites", "last_activity_date": 1440041595, "accepted_answer_id": 32109123, "body": "<p>I am using Apache nutch 2.2.1. My crawler is crawling whole web i.e. no filter is applied. I have few websites that I want not to be crawled by nutch forever.</p>\n\n<p>How it can be done?</p>\n", "creation_date": 1439968367, "score": 0},
{"title": "How to parse and fetch XML sitemap nutch", "view_count": 602, "owner": {"user_id": 1841456, "answer_count": 35, "creation_date": 1353490674, "accept_rate": 64, "view_count": 130, "location": "San Jose, CA, United States", "reputation": 570}, "is_answered": true, "answers": [{"question_id": 21740273, "owner": {"user_id": 2674407, "link": "http://stackoverflow.com/users/2674407/user2674407", "user_type": "registered", "reputation": 38}, "body": "<p>I found this link on the Nutch Wiki<br>\n<a href=\"https://wiki.apache.org/nutch/SitemapFeature\" rel=\"nofollow\">https://wiki.apache.org/nutch/SitemapFeature</a>                                                                               </p>\n", "creation_date": 1392330910, "is_accepted": true, "score": 1, "last_activity_date": 1392330910, "answer_id": 21767030}, {"question_id": 21740273, "owner": {"user_id": 1736002, "accept_rate": 60, "link": "http://stackoverflow.com/users/1736002/cguzel", "user_type": "registered", "reputation": 576}, "body": "<p>You can see the <a href=\"https://issues.apache.org/jira/browse/NUTCH-1741\" rel=\"nofollow\">nutch issue</a>.</p>\n\n<p>There is a working for nutch sitemap parser <a href=\"https://github.com/cguzel/nutch-sitemapCrawler\" rel=\"nofollow\">here</a> </p>\n", "creation_date": 1440006869, "is_accepted": false, "score": 0, "last_activity_date": 1440006869, "answer_id": 32102402}], "question_id": 21740273, "tags": ["xml", "web-crawler", "sitemap", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/21740273/how-to-parse-and-fetch-xml-sitemap-nutch", "last_activity_date": 1440006869, "accepted_answer_id": 21767030, "body": "<p>When Nutch fetches a sitemap, it does not go ahead fetch all the links in the  tag in the sitemap.</p>\n\n<p>What configuration option do I set to make nutch crawl and fetch all the links mentioned in the sitemap.</p>\n", "creation_date": 1392241180, "score": 0},
{"title": "Nutch on Hadoop | Input path does not exist:", "view_count": 264, "is_answered": false, "answers": [{"question_id": 32040216, "owner": {"user_id": 1910545, "accept_rate": 100, "link": "http://stackoverflow.com/users/1910545/ahmed-abobakr", "user_type": "registered", "reputation": 665}, "body": "<p>I am not sure about nutch, but regarding Hadoop try loading the configuration files using the configuration object before starting the MapReduce job.</p>\n\n<p>This solution works with me:</p>\n\n<pre><code>Configuration conf = new Configuration();        \nconf.addResource(new Path(\"path to hadoop/conf/core-site.xml\"));\nconf.addResource(new Path(\"path to hadoop/conf/hdfs-site.xml\"));\nFileSystem fs = FileSystem.get(conf);\n</code></pre>\n\n<p>You may also give it a try with the full path of the input directory  </p>\n\n<pre><code>hdfs://localhost:54310/user/hdravi\n</code></pre>\n", "creation_date": 1439771644, "is_accepted": false, "score": 0, "last_activity_date": 1439771644, "answer_id": 32041304}], "question_id": 32040216, "tags": ["hadoop", "mapreduce", "nutch", "hadoop2"], "answer_count": 1, "link": "http://stackoverflow.com/questions/32040216/nutch-on-hadoop-input-path-does-not-exist", "last_activity_date": 1439814943, "owner": {"user_id": 4495320, "view_count": 2, "answer_count": 0, "creation_date": 1422286755, "reputation": 3}, "body": "<p>I am getting the error Input path does not exist when I run the command</p>\n\n<pre><code>nutch inject crawldb urls\n</code></pre>\n\n<p>In nutch/logs I got this error in hadoop.log</p>\n\n<pre><code>2015-08-16 16:08:12,834 INFO  crawl.Injector - Injector: starting at 2015-08-16 16:08:12\n2015-08-16 16:08:12,834 INFO  crawl.Injector - Injector: crawlDb: crawldb\n2015-08-16 16:08:12,835 INFO  crawl.Injector - Injector: urlDir: urls\n2015-08-16 16:08:12,835 INFO  crawl.Injector - Injector: Converting injected urls to crawl db entries.\n2015-08-16 16:08:13,296 WARN  util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n2015-08-16 16:08:13,417 WARN  snappy.LoadSnappy - Snappy native library not loaded\n2015-08-16 16:08:13,430 ERROR security.UserGroupInformation - PriviledgedActionException as:hdravi cause:org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/home/hdravi/urls\n2015-08-16 16:08:13,432 ERROR crawl.Injector - Injector: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/home/hdravi/urls\n    at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:197)\n    at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:208)\n    at org.apache.hadoop.mapred.JobClient.writeOldSplits(JobClient.java:1081)\n    at org.apache.hadoop.mapred.JobClient.writeSplits(JobClient.java:1073)\n    at org.apache.hadoop.mapred.JobClient.access$700(JobClient.java:179)\n    at org.apache.hadoop.mapred.JobClient$2.run(JobClient.java:983)\n    at org.apache.hadoop.mapred.JobClient$2.run(JobClient.java:936)\n    at java.security.AccessController.doPrivileged(Native Method)\n    at javax.security.auth.Subject.doAs(Subject.java:415)\n    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)\n    at org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:936)\n    at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:910)\n    at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:1353)\n    at org.apache.nutch.crawl.Injector.inject(Injector.java:323)\n    at org.apache.nutch.crawl.Injector.run(Injector.java:379)\n    at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n    at org.apache.nutch.crawl.Injector.main(Injector.java:369)\n</code></pre>\n\n<p>It some how searches in local file system.</p>\n\n<p>This is the content of hadoop's core-site.xml</p>\n\n<pre><code>&lt;configuration&gt;\n&lt;property&gt;\n  &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;\n  &lt;value&gt;/app/hadoop/tmp&lt;/value&gt;\n  &lt;description&gt;A base for other temporary directories.&lt;/description&gt;\n&lt;/property&gt;\n\n&lt;property&gt;\n  &lt;name&gt;fs.default.name&lt;/name&gt;\n  &lt;value&gt;hdfs://localhost:54310&lt;/value&gt;\n  &lt;description&gt;The name of the default file system.  A URI whose\n  scheme and authority determine the FileSystem implementation.  The\n  uri's scheme determines the config property (fs.SCHEME.impl) naming\n  the FileSystem implementation class.  The uri's authority is used to\n  determine the host, port, etc. for a filesystem.&lt;/description&gt;\n&lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre>\n\n<p>This is the content hadoop's hdfs-site.xml</p>\n\n<pre><code>&lt;configuration&gt;\n&lt;property&gt;\n  &lt;name&gt;dfs.replication&lt;/name&gt;\n  &lt;value&gt;1&lt;/value&gt;\n  &lt;description&gt;Default block replication.\n  The actual number of replications can be specified when the file is created.\n  The default is used if replication is not specified in create time.\n  &lt;/description&gt;\n&lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre>\n\n<p>When I type <code>hadoop fs -ls -R /</code> , this is the output I get </p>\n\n<pre><code>drwxrwxrwx   - hdravi supergroup          0 2015-08-16 16:06 /user\ndrwxrwxrwx   - hdravi supergroup          0 2015-08-16 16:06 /user/hdravi\ndrwxr-xr-x   - hdravi supergroup          0 2015-08-16 16:06 /user/hdravi/urls\n-rw-r--r--   1 hdravi supergroup        240 2015-08-16 16:06 /user/hdravi/urls/seed.txt\n</code></pre>\n\n<p>Am I missing any configuration in hadoop/nutch?</p>\n\n<p><strong>UPDATE</strong></p>\n\n<p>I get the following error when I use the complete HDFS path</p>\n\n<pre><code>2015-08-16 23:33:22,876 INFO  crawl.Injector - Injector: starting at 2015-08-16 23:33:22\n2015-08-16 23:33:22,877 INFO  crawl.Injector - Injector: crawlDb: crawldb\n2015-08-16 23:33:22,877 INFO  crawl.Injector - Injector: urlDir: hdfs://localhost:54310/user/hdravi/user/hdravi/urls\n2015-08-16 23:33:22,878 INFO  crawl.Injector - Injector: Converting injected urls to crawl db entries.\n2015-08-16 23:33:23,317 WARN  util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n2015-08-16 23:33:23,410 WARN  snappy.LoadSnappy - Snappy native library not loaded\n2015-08-16 23:33:23,762 ERROR security.UserGroupInformation - PriviledgedActionException as:hdravi cause:org.apache.hadoop.ipc.RemoteException: Server IPC version 9 cannot communicate with client version 4\n2015-08-16 23:33:23,764 ERROR crawl.Injector - Injector: org.apache.hadoop.ipc.RemoteException: Server IPC version 9 cannot communicate with client version 4\n    at org.apache.hadoop.ipc.Client.call(Client.java:1107)\n    at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)\n    at com.sun.proxy.$Proxy1.getProtocolVersion(Unknown Source)\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n    at java.lang.reflect.Method.invoke(Method.java:606)\n    at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:85)\n    at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:62)\n    at com.sun.proxy.$Proxy1.getProtocolVersion(Unknown Source)\n    at org.apache.hadoop.ipc.RPC.checkVersion(RPC.java:422)\n    at org.apache.hadoop.hdfs.DFSClient.createNamenode(DFSClient.java:183)\n    at org.apache.hadoop.hdfs.DFSClient.&lt;init&gt;(DFSClient.java:281)\n    at org.apache.hadoop.hdfs.DFSClient.&lt;init&gt;(DFSClient.java:245)\n    at org.apache.hadoop.hdfs.DistributedFileSystem.initialize(DistributedFileSystem.java:100)\n    at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:1437)\n    at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:66)\n    at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:1455)\n    at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:254)\n    at org.apache.hadoop.fs.Path.getFileSystem(Path.java:187)\n    at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:176)\n    at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:208)\n    at org.apache.hadoop.mapred.JobClient.writeOldSplits(JobClient.java:1081)\n    at org.apache.hadoop.mapred.JobClient.writeSplits(JobClient.java:1073)\n    at org.apache.hadoop.mapred.JobClient.access$700(JobClient.java:179)\n    at org.apache.hadoop.mapred.JobClient$2.run(JobClient.java:983)\n    at org.apache.hadoop.mapred.JobClient$2.run(JobClient.java:936)\n    at java.security.AccessController.doPrivileged(Native Method)\n    at javax.security.auth.Subject.doAs(Subject.java:415)\n    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)\n    at org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:936)\n    at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:910)\n    at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:1353)\n    at org.apache.nutch.crawl.Injector.inject(Injector.java:323)\n    at org.apache.nutch.crawl.Injector.run(Injector.java:379)\n    at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n    at org.apache.nutch.crawl.Injector.main(Injector.java:369)\n</code></pre>\n", "creation_date": 1439761312, "score": 0},
{"title": "Nutch possibilities", "view_count": 103, "is_answered": false, "answers": [{"question_id": 30047204, "owner": {"user_id": 2318281, "accept_rate": 78, "link": "http://stackoverflow.com/users/2318281/ganapat", "user_type": "registered", "reputation": 119}, "body": "<p>You will need to use plugin to extract specific data &amp; add that data to nutch document while indexing.\nThis plugin can be used to extract data\nwww.atlantbh.com/precise-data-extraction-with-apache-nutch/</p>\n", "creation_date": 1439634850, "is_accepted": false, "score": 0, "last_activity_date": 1439634850, "answer_id": 32023867}], "question_id": 30047204, "tags": ["web-scraping", "web-crawler", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/30047204/nutch-possibilities", "last_activity_date": 1439634850, "owner": {"user_id": 4865347, "view_count": 0, "answer_count": 0, "creation_date": 1430811604, "reputation": 1}, "body": "<p>i am new to nutch and am using nutch 1.9. right now am doing some POC on a sample site(shaadi.com). I have few questions, can somebody help me out on this?</p>\n\n<ol>\n<li><p>i cant access the urls that requires login authentication(<strong>form based</strong>), though i setup the configuration in httpclient-auth.xml, nutch-site.xml and all.</p></li>\n<li><p>i know nutch fetches us only the whole content of the website. but is it possible to get only a piece of information like first name, address etc.. from the website page using nutch? (i think its more like scraping.. this is what pythons <strong>scrapy</strong> does)</p></li>\n</ol>\n\n<p>Thanks in advance.</p>\n", "creation_date": 1430812620, "score": 0},
{"title": "Error when creating HTable", "view_count": 399, "owner": {"age": 24, "answer_count": 4, "creation_date": 1426319117, "user_id": 4670035, "accept_rate": 67, "view_count": 8, "location": "internet", "reputation": 35}, "is_answered": true, "answers": [{"question_id": 31937110, "owner": {"user_id": 4670035, "accept_rate": 67, "link": "http://stackoverflow.com/users/4670035/eddga", "user_type": "registered", "reputation": 35}, "body": "<p>I will post my solution to this problem maybe it will help someone in the future.\nI use Hadoop 2.4.0, HBase 0.98.0, Apache Nutch 1.8 and Solr 4.2.1.\nProblems were caused by lack of some libraries. I'll post a list with all libraries, which I inserted in nutch /lib folder (this way isn't best one because nutch isn't correctly installed on the cluster, yet).</p>\n\n<ul>\n<li>activation-1.1.jar</li>\n<li>apache-nutch-1.8.jar</li>\n<li>asm-3.1.jar</li>\n<li>avro-1.7.4.jar</li>\n<li>commons-beanutils-1.7.0.jar</li>\n<li>commons-beanutils-core-1.8.0.jar</li>\n<li>commons-cli-1.2.jar</li>\n<li>commons-codec-1.4.jar</li>\n<li>commons-collections-3.2.1.jar</li>\n<li>commons-configuration-1.6.jar</li>\n<li>commons-digester-1.8.jar</li>\n<li>commons-el-1.0.jar</li>\n<li>commons-httpclient-3.1.jar</li>\n<li>commons-io-2.4.jar</li>\n<li>commons-lang-2.6.jar</li>\n<li>commons-logging-1.1.1.jar</li>\n<li>commons-math-2.1.jar</li>\n<li>commons-net-1.4.1.jar</li>\n<li>crawler-commons-0.3.jar</li>\n<li>elasticsearch-0.90.1.jar</li>\n<li>guava-11.0.2.jar</li>\n<li>hadoop-auth-2.2.0.jar</li>\n<li>hadoop-common-2.2.0.jar</li>\n<li>hadoop-yarn-api-2.2.0.jar</li>\n<li>hadoop-yarn-common-2.2.0.jar</li>\n<li>hadoop-mapreduce-client-common-2.2.0.jar</li>\n<li>hadoop-mapreduce-client-core-2.2.0.jar</li>\n<li>hadoop-mapreduce-client-jobclient-2.2.0.jar</li>\n<li>hadoop-mapreduce-client-shuffle-2.2.0.jar</li>\n<li>hbase-client-0.98.0-hadoop2.jar</li>\n<li>hbase-common-0.98.0-hadoop2.jar</li>\n<li>hbase-protocol-0.98.0-hadoop2.jar</li>\n<li>htrace-core-2.04.jar</li>\n<li>httpclient-4.1.1.jar</li>\n<li>httpcore-4.1.jar</li>\n<li>icu4j-4.0.1.jar</li>\n<li>jackson-core-asl-1.8.8.jar</li>\n<li>jackson-jaxrs-1.7.1.jar</li>\n<li>jackson-mapper-asl-1.8.8.jar</li>\n<li>jackson-xc-1.7.1.jar</li>\n<li>jasper-compiler-5.5.12.jar</li>\n<li>jasper-runtime-5.5.12.jar</li>\n<li>jaxb-api-2.2.2.jar</li>\n<li>jaxb-impl-2.2.3-1.jar</li>\n<li>jersey-core-1.8.jar</li>\n<li>jersey-json-1.8.jar</li>\n<li>jersey-server-1.8.jar</li>\n<li>jettison-1.1.jar</li>\n<li>jetty-6.1.26.jar</li>\n<li>jetty-client-6.1.22.jar</li>\n<li>jetty-sslengine-6.1.22.jar</li>\n<li>jetty-util-6.1.26.jar</li>\n<li>jsp-2.1-6.1.14.jar</li>\n<li>jsp-api-2.1-6.1.14.jar</li>\n<li>jsr305-1.3.9.jar</li>\n<li>junit-3.8.1.jar</li>\n<li>log4j-1.2.15.jar</li>\n<li>lucene-analyzers-common-4.3.0.jar</li>\n<li>lucene-codecs-4.3.0.jar</li>\n<li>lucene-core-4.3.0.jar</li>\n<li>lucene-grouping-4.3.0.jar</li>\n<li>lucene-highlighter-4.3.0.jar</li>\n<li>lucene-join-4.3.0.jar</li>\n<li>lucene-memory-4.3.0.jar</li>\n<li>lucene-queries-4.3.0.jar</li>\n<li>lucene-queryparser-4.3.0.jar</li>\n<li>lucene-sandbox-4.3.0.jar</li>\n<li>lucene-spatial-4.3.0.jar</li>\n<li>lucene-suggest-4.3.0.jar</li>\n<li>netty-3.6.6.Final.jar</li>\n<li>oro-2.0.8.jar</li>\n<li>protobuf-java-2.5.0.jar</li>\n<li>servlet-api-2.5-6.1.14.jar</li>\n<li>slf4j-api-1.6.6.jar</li>\n<li>slf4j-log4j12-1.6.1.jar</li>\n<li>spatial4j-0.3.jar</li>\n<li>stax-api-1.0-2.jar</li>\n<li>tika-core-1.5.jar</li>\n<li>xercesImpl-2.9.1.jar</li>\n<li>xml-apis-1.3.04.jar</li>\n<li>xmlenc-0.52.jar</li>\n<li>xmlParserAPIs-2.6.2.jar</li>\n<li>zookeeper-3.4.6.jar</li>\n</ul>\n", "creation_date": 1439374126, "is_accepted": true, "score": 1, "last_activity_date": 1439374126, "answer_id": 31962131}], "question_id": 31937110, "tags": ["java", "parsing", "hadoop", "hbase", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/31937110/error-when-creating-htable", "last_activity_date": 1439374126, "accepted_answer_id": 31962131, "body": "<p>I'm using Nutch 1.8 to crawl data from website. I am writing now a custom plugin for Nutch to parse HTML and save data to HBase.\nBy tutorials I create configuration:</p>\n\n<pre><code>Configuration conf = HBaseConfiguration.create();\n</code></pre>\n\n<p>Then I call openz() method to set configuration things etc.</p>\n\n<pre><code>public static void openz() throws IOException {\n    LOG.info(\"openz()\");\n    System.out.println(\"openz()\");\n    System.out.println(\"Establishing connection with database..\");\n    conf = HBaseConfiguration.create();\n    conf.set(\"hbase.master\", SERVER_IP);\n    conf.set(\"hbase.zookeeper.quorum\", MASTER_PC);\n    conf.set(\"zookeeper.znode.parent\", ZOOKEEPER_PARENT_NODE);\n    conf.set(\"hbase.zookeeper.property.clientPort\",\"2181\");\n    System.out.println(\"Conf here? :\" + conf);\n    System.out.println(\"Creating table variable..\");\n    table = new HTable(conf, \"bstore\");\n}\n</code></pre>\n\n<p>At this point, on line, where I create HTable, I get</p>\n\n<blockquote>\n  <p>java.io.IOException: java.lang.reflect.InvocationTargetException\n          at org.apache.hadoop.hbase.client.ConnectionManager.createConnection\n  java.lang.NoSuchMethodError: org.apache.hadoop.hbase.protobuf.generated.ClientProtos$Result$Builder.setStale(Z)Lorg/apache/hadoop/hbase/protobuf/generated/ClientProtos$Result$Builder;</p>\n</blockquote>\n\n<p>Previously I got a lot of trouble with libraries. \nWhen I run my plugins code on netbeans with remote connection, it works fine. Saves website data to hbase without having any trouble.\nBut these errors I get when I launch Nutch crawler on cluster.</p>\n", "creation_date": 1439282618, "score": 0},
{"title": "Nutch - regex to include only urls which end in a numeric sequence", "view_count": 40, "owner": {"age": 39, "answer_count": 2, "creation_date": 1352107431, "user_id": 1799691, "accept_rate": 75, "view_count": 10, "location": "Belfast, United Kingdom", "reputation": 36}, "is_answered": true, "answers": [{"last_edit_date": 1439322870, "owner": {"user_id": 5114938, "link": "http://stackoverflow.com/users/5114938/sleafar", "user_type": "registered", "reputation": 1041}, "body": "<p>The correct regex for your excludes would be rather something like this:</p>\n\n<pre><code>-^http://eprints.ulster.ac.uk(/[a-z]+)+/?\n</code></pre>\n\n<ul>\n<li><code>[a-z]+</code>: one or more occurences of the letters a-z</li>\n<li><code>(...)+</code>: one or more occurences of the part in the parentheses</li>\n<li><code>/?</code>: optional slash at the end</li>\n</ul>\n\n<p><strong>Edit:</strong></p>\n\n<p>I took a look at the site you linked. The structure of it looks like this:</p>\n\n<pre><code>1. http://eprints.ulster.ac.uk\n    |\n    V\n2. http://eprints.ulster.ac.uk/view/\n    |\n    V\n3. http://eprints.ulster.ac.uk/view/year/\n    |\n    V\n4. http://eprints.ulster.ac.uk/view/year/2015.html\n    |\n    V\n5. http://eprints.ulster.ac.uk/31307/\n</code></pre>\n\n<p>So 1. is your starting point and 5. is the file you want. If the crawler has to find the last file in the list, it must also fetch the files 2., 3. and 4.</p>\n\n<p>My guess is, that this is exactly your problem. You want to exclude the files 2. and 3., but the crawler needs them. Therefore you can't exclude them from being fetched.</p>\n", "question_id": 31946569, "creation_date": 1439310394, "is_accepted": true, "score": 2, "last_activity_date": 1439322870, "answer_id": 31947136}], "question_id": 31946569, "tags": ["regex", "web-crawler", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/31946569/nutch-regex-to-include-only-urls-which-end-in-a-numeric-sequence", "last_activity_date": 1439322870, "accepted_answer_id": 31947136, "body": "<p>In nutch 1.9, I'm trying to use regex-urlfilter.txt to limit a crawl on the academic research repository at <a href=\"http://eprints.ulster.ac.uk\" rel=\"nofollow\">http://eprints.ulster.ac.uk</a> to only return urls for the actual articles, which end in a numeric sequence such as:</p>\n\n<p><a href=\"http://eprints.ulster.ac.uk/143/\" rel=\"nofollow\">http://eprints.ulster.ac.uk/143/</a>\n<a href=\"http://eprints.ulster.ac.uk/24122/\" rel=\"nofollow\">http://eprints.ulster.ac.uk/24122/</a></p>\n\n<p>and to <em>exclude</em> urls such as <a href=\"http://eprints.ulster.ac.uk/view\" rel=\"nofollow\">http://eprints.ulster.ac.uk/view</a> and <a href=\"http://eprints.ulster.ac.uk/cgi/latest/\" rel=\"nofollow\">http://eprints.ulster.ac.uk/cgi/latest/</a></p>\n\n<p>I have tried various combinations similar to:</p>\n\n<p>-^<a href=\"http://eprints.ulster.ac.uk/[a-z]*/(*)/(*)\" rel=\"nofollow\">http://eprints.ulster.ac.uk/[a-z]*/(*)/(*)</a><br>\n+^<a href=\"http://eprints.ulster.ac.uk/[0-9]\" rel=\"nofollow\">http://eprints.ulster.ac.uk/[0-9]</a>{1,}/  </p>\n\n<p>but nothing seems to work, with the crawl invariably returning no results.</p>\n\n<p>Any help would be much appreciated</p>\n", "creation_date": 1439308652, "score": 0},
{"title": "Nutch 2.X Plugin Development Tutorial", "view_count": 251, "is_answered": true, "answers": [{"question_id": 27904075, "owner": {"user_id": 1654519, "accept_rate": 50, "link": "http://stackoverflow.com/users/1654519/seldon", "user_type": "registered", "reputation": 57}, "body": "<p>I also tried to find such info, but nothing, everywhere just nothing.  No official documentation, i think guys from Apache nutch project don't know words like \"documentation\"</p>\n", "creation_date": 1422810282, "is_accepted": false, "score": 1, "last_activity_date": 1422810282, "answer_id": 28265297}], "question_id": 27904075, "tags": ["plugins", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/27904075/nutch-2-x-plugin-development-tutorial", "last_activity_date": 1439276606, "owner": {"user_id": 4022888, "view_count": 2, "answer_count": 1, "creation_date": 1410266118, "reputation": 3}, "body": "<p>I am looking for tutorial that helps me learn the development of plugin for Apache nutch 2.X. </p>\n\n<p>I have searched online but all of them are tutorials about nutch 1.X plugin development. Is the development of plugin for nutch 2.X same as the one for nutch 1.X?</p>\n", "creation_date": 1421072348, "score": 0},
{"title": "Nutch plugin development", "view_count": 1865, "owner": {"age": 55, "answer_count": 1209, "creation_date": 1219883520, "user_id": 3333, "accept_rate": 83, "view_count": 12303, "location": "Rochester, NY", "reputation": 113417}, "is_answered": true, "answers": [{"question_id": 1213343, "owner": {"user_id": 123582, "accept_rate": 56, "link": "http://stackoverflow.com/users/123582/rich-seller", "user_type": "registered", "reputation": 61397}, "body": "<p>I know nothing of Nutch, but from looking at the wiki, it doesn't seem that your plugin will actually depend upon any parts of the nutch source code. The plugin seems to be pretty much standalone. I'm guessing that they recommend you put it there so it will be bundled along with the rest of the project when built.</p>\n\n<p>Have you tried to create a plugin from the <strong>Required Files</strong> step onwards and then taking your built plugin and putting it wherever Nutch expects? (you might have to create a hello world plugin to see where that is)</p>\n", "creation_date": 1249064286, "is_accepted": true, "score": 1, "last_activity_date": 1249064286, "answer_id": 1214057}], "question_id": 1213343, "tags": ["eclipse", "ant", "plugins", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/1213343/nutch-plugin-development", "last_activity_date": 1439269831, "accepted_answer_id": 1214057, "body": "<p>The nutch wiki has instructions on <a href=\"http://wiki.apache.org/nutch/WritingPluginExample-0%2e9\" rel=\"nofollow\">how to build nutch plugins</a>, but only if you download the entire nutch source tree and put it in there, below $NUTCH_HOME/src/plugin.  I don't want my source code mixed in their subversion tree, I want it in my src/com/xcski git repository.  And I shouldn't have to download the source code for nutch just to build a plugin, I should just be able to stick a jar in my Eclipse build path and go.</p>\n\n<p>So basically, I'm looking for what to put in my ant build.xml file and what to put into Eclipse to write and build my plugin.  And keep in mind that I'm a total ant neophyte.</p>\n", "creation_date": 1249055471, "score": 2},
{"title": "Integrating Nutch on Hortownworks OR YARN", "view_count": 244, "is_answered": false, "answers": [{"question_id": 29007040, "owner": {"user_id": 821478, "accept_rate": 100, "link": "http://stackoverflow.com/users/821478/cjackson", "user_type": "registered", "reputation": 874}, "body": "<p>HDP 2.3 doesn't support Nutch out of the box (There is a chart on the HDP website showing supported services: <a href=\"http://hortonworks.com/hdp/whats-new/\" rel=\"nofollow\">HDP2.3 What's New</a>). However it does support the services that Nutch depends on. A custom Ambari Service could be defined and added to the HDP 2.3 stack definition to enable support for Nutch. </p>\n", "creation_date": 1439266344, "is_accepted": false, "score": 0, "last_activity_date": 1439266344, "answer_id": 31933075}], "question_id": 29007040, "tags": ["hadoop", "web-crawler", "nutch", "hortonworks-data-platform"], "answer_count": 1, "link": "http://stackoverflow.com/questions/29007040/integrating-nutch-on-hortownworks-or-yarn", "last_activity_date": 1439266344, "owner": {"user_id": 679916, "answer_count": 27, "creation_date": 1301302173, "accept_rate": 65, "view_count": 324, "location": "London, United Kingdom", "reputation": 892}, "body": "<p>I am trying to crawl the web. Preferably with Nutch.\nDid not find the references if Hortownworks out of the box supports Nutch.</p>\n\n<p>Has any one integrated Nutch on YARN specially with Hortonworks HDP ?\nOr someone has tried integrating Nutch on the Hadoop 2.x (YARN) ?</p>\n\n<p>Thanks in advance.</p>\n", "creation_date": 1426155492, "score": 3},
{"title": "Would Parse be executed after Fetch finished if fetcher.parse is set to true, using Nutch?", "view_count": 50, "owner": {"user_id": 4787211, "view_count": 0, "answer_count": 0, "creation_date": 1429013382, "reputation": 5}, "is_answered": true, "answers": [{"last_edit_date": 1438975091, "owner": {"user_id": 1965370, "link": "http://stackoverflow.com/users/1965370/do-do", "user_type": "registered", "reputation": 389}, "body": "<p>Parse phase will be executed after Fetch phase anyway, it doesn't matter if you set <code>fetcher.parse</code> to False or True. Only thing you should notice is that when you set it to True, the crawling will be safer, parsing issues don't lead to a loss of the fetched content.</p>\n\n<p>Please have a look on this:</p>\n\n<p><a href=\"https://issues.apache.org/jira/browse/NUTCH-872\" rel=\"nofollow\">https://issues.apache.org/jira/browse/NUTCH-872</a></p>\n", "question_id": 31644437, "creation_date": 1438970565, "is_accepted": true, "score": 0, "last_activity_date": 1438975091, "answer_id": 31884026}], "question_id": 31644437, "tags": ["web-crawler", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/31644437/would-parse-be-executed-after-fetch-finished-if-fetcher-parse-is-set-to-true-us", "last_activity_date": 1438979547, "accepted_answer_id": 31884026, "body": "<p>I want to parse web page content roughly in phase <strong>Fetch</strong> and subtly in phase <strong>Parse</strong>. What should I do? Would Parse be executed after Fetch finished if <code>fetcher.parse</code> is set to <code>true</code>?</p>\n", "creation_date": 1437967105, "score": 0},
{"title": "Injecting urls taking too long", "view_count": 117, "is_answered": false, "answers": [{"question_id": 31583348, "owner": {"user_id": 1577405, "link": "http://stackoverflow.com/users/1577405/aperfectpoint", "user_type": "registered", "reputation": 41}, "body": "<p>Can you post the last part of the log? (under $NUTCH_ROOT/runtime/local/logs/hadoop.log)</p>\n", "creation_date": 1437920784, "is_accepted": false, "score": 0, "last_activity_date": 1437920784, "answer_id": 31638082}, {"question_id": 31583348, "owner": {"user_id": 1965370, "link": "http://stackoverflow.com/users/1965370/do-do", "user_type": "registered", "reputation": 389}, "body": "<p>Your Nutch version is 2.3. You should not run the command line in <code>$NUTCH_ROOT/runtime/local/bin/nutch</code>, you should run the command in <code>$NUTCH_ROOT/runtime/deploy/bin/nutch</code> instead.</p>\n\n<p>Hope this helps,</p>\n\n<p>Le Quoc Do</p>\n", "creation_date": 1438970898, "is_accepted": false, "score": -1, "last_activity_date": 1438970898, "answer_id": 31884106}], "question_id": 31583348, "tags": ["nutch", "inject"], "answer_count": 2, "link": "http://stackoverflow.com/questions/31583348/injecting-urls-taking-too-long", "last_activity_date": 1438970898, "owner": {"user_id": 2503045, "view_count": 1, "answer_count": 0, "creation_date": 1371678678, "reputation": 1}, "body": "<p>I have configured Hbase 0.94.14 and Nutch 2.3 through <a href=\"https://gist.github.com/xrstf/b48a970098a8e76943b9\" rel=\"nofollow\">this</a> tutorial and made a seed directory which contains a text file with the urls. When I want to inject these urls using this command: </p>\n\n<pre><code>$NUTCH_ROOT/runtime/local/bin/nutch inject /seed\n</code></pre>\n\n<p>I get the following output:</p>\n\n<pre><code>InjectorJob: starting at 2015-07-23 14:00:24\n\nInjectorJob: Injecting urlDir: /seed\n</code></pre>\n\n<p>and stays in this state forever. </p>\n\n<p>Can anybody help me with this problem?</p>\n", "creation_date": 1437643699, "score": 0},
{"title": "Nutch failed to crawl particular site", "view_count": 775, "owner": {"user_id": 1211452, "answer_count": 13, "creation_date": 1329313121, "accept_rate": 100, "view_count": 60, "reputation": 180}, "is_answered": true, "answers": [{"last_edit_date": 1438959438, "owner": {"user_id": 1211452, "accept_rate": 100, "link": "http://stackoverflow.com/users/1211452/lina-clark", "user_type": "registered", "reputation": 180}, "body": "<p>Finally, able to solve this problem after breaking my head for long. So sharing it here :)\nYou have to adjust the parameters defined in <code>nutch-default.xml</code> in conf directory</p>\n\n<p>So check the <code>max.content.length</code>, value defined for this will be around 60K but actually the page content was much more so it was not able to crawl whole page and that's why the links were not able to show up in crawled page.</p>\n\n<p>So before crawling any site do check these parameters :)\nEnjoy crawling :)</p>\n\n<p>PS: I am sorry i case some1 feels that I post question here and then post solution. Before posting question i actually tried a lot..</p>\n", "question_id": 9866585, "creation_date": 1332830109, "is_accepted": true, "score": 2, "last_activity_date": 1438959438, "answer_id": 9884461}], "question_id": 9866585, "tags": ["web-crawler", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/9866585/nutch-failed-to-crawl-particular-site", "last_activity_date": 1438959438, "accepted_answer_id": 9884461, "body": "<p>I am using nutch 1.4 to crawl websites. For demo purpose, I started crawling with jabong.com but i observed that nutch could not fetch all the links in the site.</p>\n\n<p>After visiting <a href=\"http://www.jabong.com/women/clothing/womens-suits-sets/\" rel=\"nofollow\">http://www.jabong.com/women/clothing/womens-suits-sets/</a>\nIt is not fetching links present in this site which are mapped on images.</p>\n\n<p>I have configured nutch as:-\nconf/nuth-default.xml ---> added the agent name\nconf/regex-urlfilter.txt ---> Instead of +. , I wrote +^http://([a-z0-9]*.)*jabong.com/\nseed.txt contains <a href=\"http://www.jabong.com/\" rel=\"nofollow\">http://www.jabong.com/</a></p>\n\n<p>Can someone tell me what could be the problem it is not fetching all the links ?</p>\n", "creation_date": 1332735872, "score": 1},
{"title": "Document index time boosting in solr via Apache Nutch", "view_count": 60, "is_answered": false, "question_id": 31831740, "tags": ["solr", "nutch", "solr-boost"], "answer_count": 0, "link": "http://stackoverflow.com/questions/31831740/document-index-time-boosting-in-solr-via-apache-nutch", "last_activity_date": 1438812088, "owner": {"user_id": 3454410, "answer_count": 20, "creation_date": 1395646154, "accept_rate": 48, "view_count": 174, "reputation": 832}, "body": "<p>I am crawling some websites using apache Nutch. I have to give boost to one website out of all. suppose out of 100 urls, there is a wiki url in seed. I want to give all data from wiki some boot, so that they should be displayed at top. I am using solr 4.10.3.</p>\n\n<p>I recrawl these websites after few days. So I think, index boot via solr will not work, it will be Nutch that should do it. Any idea ?</p>\n", "creation_date": 1438775529, "score": 0},
{"title": "Is my nutch work flow correct?", "view_count": 32, "is_answered": false, "question_id": 31821528, "tags": ["web-crawler", "nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/31821528/is-my-nutch-work-flow-correct", "last_activity_date": 1438734586, "owner": {"age": 26, "answer_count": 501, "creation_date": 1294499143, "user_id": 568109, "accept_rate": 64, "view_count": 1841, "location": "New Delhi, India", "reputation": 29972}, "body": "<p>I am using the following commands to fetch data and want to check the url crawled. But I get empty file in output. Please tell what is wrong.</p>\n\n<pre><code>nutch inject dmoz -crawlId 1\nnutch readdb -crawlId 1 -stats\nnutch generate -crawlId 1\nnutch fetch 1438535695-540986939 -crawlId 1 -threads 4\nnutch updatedb 1438535695-540986939 -crawlId 1 \nnutch readdb -crawlId 1 -dump output\n</code></pre>\n\n<p>I am using nutch v2.3 with Cassandra v2.2.</p>\n", "creation_date": 1438734586, "score": 0},
{"title": "Nutch job failing when sending data to Solr", "view_count": 790, "is_answered": true, "answers": [{"last_edit_date": 1389232125, "owner": {"user_id": 2146864, "accept_rate": 60, "link": "http://stackoverflow.com/users/2146864/billni", "user_type": "registered", "reputation": 82}, "body": "<p>Did you see the log in solr that revealed the error reason. I had ever same problem in nutch, and the solr's log showed a message \"unknown field 'host'\". After I modified the schema.xml for solr, the problem vanished.</p>\n", "question_id": 18767537, "creation_date": 1389230779, "is_accepted": false, "score": 0, "last_activity_date": 1389232125, "answer_id": 21010043}, {"question_id": 18767537, "owner": {"user_id": 546232, "accept_rate": 57, "link": "http://stackoverflow.com/users/546232/alec", "user_type": "registered", "reputation": 98}, "body": "<p>Was expecting the same error on fresh Solr 5.2.1 and Nutch 1.10:</p>\n\n<p>2015-07-30 20:56:23,015 WARN  mapred.LocalJobRunner - job_local_0001\norg.apache.solr.common.SolrException: Not Found</p>\n\n<p>Not Found</p>\n\n<p>request: <a href=\"http://127.0.0.1:8983/solr/update?wt=javabin&amp;version=2\" rel=\"nofollow\">http://127.0.0.1:8983/solr/update?wt=javabin&amp;version=2</a></p>\n\n<p>So i have created a collection (or core, i am not an expert in SOLR):</p>\n\n<blockquote>\n  <p>bin/solr create -c demo</p>\n</blockquote>\n\n<p>And changed URL in Nutch indexing script:</p>\n\n<blockquote>\n  <p>bin/nutch solrindex <a href=\"http://127.0.0.1:8983/solr/demo\" rel=\"nofollow\">http://127.0.0.1:8983/solr/demo</a> crawl/crawldb -linkdb crawl/linkdb crawl/segments/*</p>\n</blockquote>\n\n<p>I know that the question is rather old, but maybe i will help somebody with it...</p>\n", "creation_date": 1438280895, "is_accepted": false, "score": 1, "last_activity_date": 1438280895, "answer_id": 31731378}], "question_id": 18767537, "tags": ["search", "solr", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/18767537/nutch-job-failing-when-sending-data-to-solr", "last_activity_date": 1438280895, "owner": {"user_id": 2147097, "answer_count": 36, "creation_date": 1362718907, "accept_rate": 80, "view_count": 73, "reputation": 969}, "body": "<p>I've been trying various things with no avail. My configuration of Nutch/Solr is based on this:</p>\n\n<p><a href=\"http://ubuntuforums.org/showthread.php?t=1532230\" rel=\"nofollow\">http://ubuntuforums.org/showthread.php?t=1532230</a></p>\n\n<p>Now that I have Nutch and Solr up and running, I would like to use Solr to index the crawl data. Nutch successfully crawls the domain I specified but fails when I run the command to communicate that data to Solr. Here's the command:</p>\n\n<pre><code>bin/nutch solrindex http://solr:8181/solr/ crawl/crawldb crawl/linkdb crawl/segments/*\n</code></pre>\n\n<p>Here's the output:</p>\n\n<pre><code>Indexer: starting at 2013-09-12 10:34:43\nIndexer: deleting gone documents: false\nIndexer: URL filtering: false\nIndexer: URL normalizing: false\nActive IndexWriters :\nSOLRIndexWriter\nsolr.server.url : URL of the SOLR instance (mandatory)\nsolr.commit.size : buffer size when sending to SOLR (default 1000)\nsolr.mapping.file : name of the mapping file for fields (default solrindex-mapping.xml)\nsolr.auth : use authentication (default false)\nsolr.auth.username : use authentication (default false)\nsolr.auth : username for authentication\nsolr.auth.password : password for authentication\n\n\nIndexer: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist:             file:/usr/share/apache-nutch-1.7/crawl/linkdb/crawl_fetch\nInput path does not exist: file:/usr/share/apache-nutch-1.7/crawl/linkdb/crawl_parse\nInput path does not exist: file:/usr/share/apache-nutch-1.7/crawl/linkdb/parse_data\nInput path does not exist: file:/usr/share/apache-nutch-1.7/crawl/linkdb/parse_text\nat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:197)\nat org.apache.hadoop.mapred.SequenceFileInputFormat.listStatus(SequenceFileInputFormat.java:40)\nat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:208)\nat org.apache.hadoop.mapred.JobClient.writeOldSplits(JobClient.java:1081)\nat org.apache.hadoop.mapred.JobClient.writeSplits(JobClient.java:1073)\nat org.apache.hadoop.mapred.JobClient.access$700(JobClient.java:179)\nat org.apache.hadoop.mapred.JobClient$2.run(JobClient.java:983)\nat org.apache.hadoop.mapred.JobClient$2.run(JobClient.java:936)\nat java.security.AccessController.doPrivileged(Native Method)\nat javax.security.auth.Subject.doAs(Subject.java:415)\nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)\nat org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:936)\nat org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:910)\nat org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:1353)\nat org.apache.nutch.indexer.IndexingJob.index(IndexingJob.java:123)\nat org.apache.nutch.indexer.IndexingJob.run(IndexingJob.java:185)\nat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\nat org.apache.nutch.indexer.IndexingJob.main(IndexingJob.java:195)\n</code></pre>\n\n<p>I've also tried another command after much Googling:</p>\n\n<pre><code>bin/nutch solrindex http://solr:8181/solr/ crawl/crawldb -linkdb crawl/linkdb crawl/segments/*\n</code></pre>\n\n<p>With this output: </p>\n\n<pre><code>Indexer: starting at 2013-09-12 10:45:51\nIndexer: deleting gone documents: false\nIndexer: URL filtering: false\nIndexer: URL normalizing: false\nActive IndexWriters :\nSOLRIndexWriter\nsolr.server.url : URL of the SOLR instance (mandatory)\nsolr.commit.size : buffer size when sending to SOLR (default 1000)\nsolr.mapping.file : name of the mapping file for fields (default solrindex-mapping.xml)\nsolr.auth : use authentication (default false)\nsolr.auth.username : use authentication (default false)\nsolr.auth : username for authentication\nsolr.auth.password : password for authentication\n\n\nIndexer: java.io.IOException: Job failed!\nat org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:1357)\nat org.apache.nutch.indexer.IndexingJob.index(IndexingJob.java:123)\nat org.apache.nutch.indexer.IndexingJob.run(IndexingJob.java:185)\nat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\nat org.apache.nutch.indexer.IndexingJob.main(IndexingJob.java:195)\n</code></pre>\n\n<p>Does anyone have any ideas of how to overcome these errors?</p>\n", "creation_date": 1378997385, "score": 2},
{"title": "What exactly is Hadoop", "view_count": 48, "owner": {"user_id": 3125823, "answer_count": 11, "creation_date": 1387649781, "accept_rate": 78, "view_count": 68, "reputation": 286}, "is_answered": true, "answers": [{"question_id": 31704865, "owner": {"user_id": 2406926, "link": "http://stackoverflow.com/users/2406926/nameless912", "user_type": "registered", "reputation": 361}, "body": "<p>Hadoop is what's called a distributed map-reduce algorithm. Basically, imagine you and 3 of your friends want to take a randomly sorted deck of cards into the 4 different suits; the easy way to do this is for each of you to grab a smaller stack of cards, sort the smaller piles, and then come together to collate the small piles into bigger piles. This way, the work is divided and the final result is much faster to compute.</p>\n\n<p>My question to you is, given this kind of idea (where you <strong>map</strong> a function over a small piece of a data set and <strong>reduce</strong> the mappings into a final result), how does that apply to Nutch and web crawling?</p>\n\n<p>There's no context here for anyone to be able to give you an answer. What do you want to <em>do</em> with the websites that makes you think hadoop might be a good idea?</p>\n", "creation_date": 1438184575, "is_accepted": true, "score": 1, "last_activity_date": 1438184575, "answer_id": 31705140}], "question_id": 31704865, "tags": ["hadoop", "elasticsearch", "web-crawler", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/31704865/what-exactly-is-hadoop", "last_activity_date": 1438184575, "accepted_answer_id": 31705140, "body": "<p>If I want to crawl (using Nutch 1.x) about 500 websites and use Elasticsearch to index the data and make it searchable, what would I use Hadoop for? Do I need Hadoop to use Nutch 1.x?</p>\n\n<p>What versions of Nutch and Elasticsearch work well together (both locally and on AWS?) Any recommended versions?</p>\n", "creation_date": 1438183843, "score": 0},
{"title": "Writing data to HBase", "view_count": 34, "owner": {"age": 24, "answer_count": 4, "creation_date": 1426319117, "user_id": 4670035, "accept_rate": 67, "view_count": 8, "location": "internet", "reputation": 35}, "is_answered": true, "answers": [{"question_id": 31668782, "owner": {"user_id": 4670035, "accept_rate": 67, "link": "http://stackoverflow.com/users/4670035/eddga", "user_type": "registered", "reputation": 35}, "body": "<p>I got idea. If someone is interested: Use any unique value of data as \"key\". For example, I have to work with books, so my row \"number\" will be unique ISBN code.</p>\n", "creation_date": 1438168368, "is_accepted": true, "score": 0, "last_activity_date": 1438168368, "answer_id": 31698840}], "question_id": 31668782, "tags": ["java", "web-crawler", "row", "hbase", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/31668782/writing-data-to-hbase", "last_activity_date": 1438168368, "accepted_answer_id": 31698840, "body": "<p>I'm using Nutch to crawl site data and then writing data to HBase. Problem is that, in tutorials is mentioned, how to update the specific row (put), but not how to create with Java code. How to create row at crawling time, that \"row number\" will increment?</p>\n", "creation_date": 1438064682, "score": 0},
{"title": "Nutch 2.2.1, Hbase 0.90.4 and Hadoop 1.2.1 session timeout", "view_count": 57, "is_answered": false, "question_id": 31652891, "tags": ["hadoop", "ant", "hbase", "zookeeper", "nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/31652891/nutch-2-2-1-hbase-0-90-4-and-hadoop-1-2-1-session-timeout", "last_activity_date": 1437999420, "owner": {"age": 21, "answer_count": 6, "creation_date": 1415100811, "user_id": 4214037, "accept_rate": 89, "view_count": 16, "location": "Slovakia", "reputation": 109}, "body": "<p>I followed this <a href=\"http://www.blogjava.net/paulwong/archive/2013/08/31/403513.html\" rel=\"nofollow\">tutorial</a> and I get stuck when I tried to inject urls to nutch from hadoop. I configured nutch files like this <a href=\"http://nutchhadoop.blogspot.sk/\" rel=\"nofollow\">tutorial</a> by copying hadoop conf files to nutch conf directory. When I tried to run ant runtime with configured files according to the first tutorial, it did not work.</p>\n\n<pre><code>ubuntu@ip-172-31-35-238:~/apache-nutch-2.2.1/runtime/deploy$ bin/nutch inject urls\nWarning: $HADOOP_HOME is deprecated.\n\n15/07/27 12:01:07 INFO crawl.InjectorJob: InjectorJob: starting at 2015-07-27 12:01:07\n15/07/27 12:01:07 INFO crawl.InjectorJob: InjectorJob: Injecting urlDir: urls\n15/07/27 12:01:10 INFO zookeeper.ZooKeeper: Client environment:zookeeper.version=3.3.2-1031432, built on 11/05/2010 05:32 GMT\n15/07/27 12:01:10 INFO zookeeper.ZooKeeper: Client environment:host.name=master\n15/07/27 12:01:10 INFO zookeeper.ZooKeeper: Client environment:java.version=1.6.0_45\n15/07/27 12:01:10 INFO zookeeper.ZooKeeper: Client environment:java.vendor=Sun Microsystems Inc.\n15/07/27 12:01:10 INFO zookeeper.ZooKeeper: Client environment:java.home=/usr/lib/jvm/java-6-oracle/jre\n15/07/27 12:01:10 INFO zookeeper.ZooKeeper: Client environment:java.class.path=/home/ubuntu/hadoop-1.2.1/libexec/../conf:/usr/lib/jvm/java-6-oracle/lib/tools.jar:/home/ubuntu/hadoop-1.2.1/libexec/..:/home/ubuntu/hadoop-1.2.1/libexec/../hadoop-core-1.2.1.jar:/home/ubuntu/hadoop-1.2.1/libexec/../lib/asm-3.2.jar:/home/ubuntu/hadoop-1.2.1/libexec/../lib/aspectjrt-1.6.11.jar:/home/ubuntu/hadoop-1.2.1/libexec/../lib/aspectjtools-1.6.11.jar:/home/ubuntu/hadoop-1.2.1/libexec/../lib/commons-beanutils-1.7.0.jar:/home/ubuntu/hadoop-1.2.1/libexec/../lib/commons-beanutils-core-1.8.0.jar:/home/ubuntu/hadoop-1.2.1/libexec/../lib/commons-cli-1.2.jar:/home/ubuntu/hadoop-1.2.1/libexec/../lib/commons-codec-1.4.jar:/home/ubuntu/hadoop-1.2.1/libexec/../lib/commons-collections-3.2.1.jar:/home/ubuntu/hadoop-1.2.1/libexec/../lib/commons-configuration-1.6.jar:/home/ubuntu/hadoop-1.2.1/libexec/../lib/commons-daemon-1.0.1.jar:/home/ubuntu/hadoop-1.2.1/libexec/../lib/commons-digester-1.8.jar:/home/ubuntu/hadoop-1.2.1/libexec/../lib/commons-el-1.0.jar:/home/ubuntu/hadoop-1.2.1/libexec/../lib/commons-httpclient-3.0.1.jar:/home/ubuntu/hadoop-1.2.1/libexec/../lib/commons-io-2.1.jar:/home/ubuntu/hadoop-1.2.1/libexec/../lib/commons-lang-2.4.jar:/home/ubuntu/hadoop-1.2.1/libexec/../lib/commons-logging-1.1.1.jar:/home/ubuntu/hadoop-1.2.1/libexec/../lib/commons-logging-api-1.0.4.jar:/home/ubuntu/hadoop-1.2.1/libexec/../lib/commons-math-2.1.jar:/home/ubuntu/hadoop-1.2.1/libexec/../lib/commons-net-3.1.jar:/home/ubuntu/hadoop-1.2.1/libexec/../lib/core-3.1.1.jar:/home/ubuntu/hadoop-1.2.1/libexec/../lib/hadoop-capacity-scheduler-1.2.1.jar:/home/ubuntu/hadoop-1.2.1/libexec/../lib/hadoop-fairscheduler-1.2.1.jar:/home/ubuntu/hadoop-1.2.1/libexec/../lib/hadoop-thriftfs-1.2.1.jar:/home/ubuntu/hadoop-1.2.1/libexec/../lib/hsqldb-1.8.0.10.jar:/home/ubuntu/hadoop-1.2.1/libexec/../lib/jackson-core-asl-1.8.8.jar:/home/ubuntu/hadoop-1.2.1/libexec/../lib/jackson-mapper-asl-1.8.8.jar:/home/ubuntu/hadoop-1.2.1/libexec/../lib/jasper-compiler-5.5.12.jar:/home/ubuntu/hadoop-1.2.1/libexec/../lib/jasper-runtime-5.5.12.jar:/home/ubuntu/hadoop-1.2.1/libexec/../lib/jdeb-0.8.jar:/home/ubuntu/hadoop-1.2.1/libexec/../lib/jersey-core-1.8.jar:/home/ubuntu/hadoop-1.2.1/libexec/../lib/jersey-json-1.8.jar:/home/ubuntu/hadoop-1.2.1/libexec/../lib/jersey-server-1.8.jar:/home/ubuntu/hadoop-1.2.1/libexec/../lib/jets3t-0.6.1.jar:/home/ubuntu/hadoop-1.2.1/libexec/../lib/jetty-6.1.26.jar:/home/ubuntu/hadoop-1.2.1/libexec/../lib/jetty-util-6.1.26.jar:/home/ubuntu/hadoop-1.2.1/libexec/../lib/jsch-0.1.42.jar:/home/ubuntu/hadoop-1.2.1/libexec/../lib/junit-4.5.jar:/home/ubuntu/hadoop-1.2.1/libexec/../lib/kfs-0.2.2.jar:/home/ubuntu/hadoop-1.2.1/libexec/../lib/log4j-1.2.15.jar:/home/ubuntu/hadoop-1.2.1/libexec/../lib/mockito-all-1.8.5.jar:/home/ubuntu/hadoop-1.2.1/libexec/../lib/oro-2.0.8.jar:/home/ubuntu/hadoop-1.2.1/libexec/../lib/servlet-api-2.5-20081211.jar:/home/ubuntu/hadoop-1.2.1/libexec/../lib/slf4j-api-1.4.3.jar:/home/ubuntu/hadoop-1.2.1/libexec/../lib/slf4j-log4j12-1.4.3.jar:/home/ubuntu/hadoop-1.2.1/libexec/../lib/xmlenc-0.52.jar:/home/ubuntu/hadoop-1.2.1/libexec/../lib/jsp-2.1/jsp-2.1.jar:/home/ubuntu/hadoop-1.2.1/libexec/../lib/jsp-2.1/jsp-api-2.1.jar\n15/07/27 12:01:10 INFO zookeeper.ZooKeeper: Client environment:java.library.path=/home/ubuntu/hadoop-1.2.1/libexec/../lib/native/Linux-amd64-64\n15/07/27 12:01:10 INFO zookeeper.ZooKeeper: Client environment:java.io.tmpdir=/tmp\n15/07/27 12:01:10 INFO zookeeper.ZooKeeper: Client environment:java.compiler=&lt;NA&gt;\n15/07/27 12:01:10 INFO zookeeper.ZooKeeper: Client environment:os.name=Linux\n15/07/27 12:01:10 INFO zookeeper.ZooKeeper: Client environment:os.arch=amd64\n15/07/27 12:01:10 INFO zookeeper.ZooKeeper: Client environment:os.version=3.2.0-75-virtual\n15/07/27 12:01:10 INFO zookeeper.ZooKeeper: Client environment:user.name=ubuntu\n15/07/27 12:01:10 INFO zookeeper.ZooKeeper: Client environment:user.home=/home/ubuntu\n15/07/27 12:01:10 INFO zookeeper.ZooKeeper: Client environment:user.dir=/home/ubuntu/apache-nutch-2.2.1/runtime/deploy\n15/07/27 12:01:10 INFO zookeeper.ZooKeeper: Initiating client connection, connectString=localhost:2181 sessionTimeout=180000 watcher=hconnection\n15/07/27 12:01:10 INFO zookeeper.ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181\n15/07/27 12:01:10 INFO zookeeper.ClientCnxn: Socket connection established to localhost/127.0.0.1:2181, initiating session\n15/07/27 12:01:10 INFO zookeeper.ClientCnxn: Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x14ecf53dd5f0007, negotiated timeout = 180000\n</code></pre>\n\n<p>Can somebody help me?</p>\n", "creation_date": 1437999420, "score": 0},
{"title": "Nuch 2.2.1 Hbase 0.90.4 ClassNotFoundException bug", "view_count": 349, "is_answered": false, "answers": [{"question_id": 26178919, "owner": {"user_id": 4214037, "accept_rate": 89, "link": "http://stackoverflow.com/users/4214037/32cupo", "user_type": "registered", "reputation": 109}, "body": "<p>I tried to crawl websites with Hadoop &amp; Nutch and I got the same error. When I checked my ivy/ivy.xml I saw missing dependency </p>\n\n<p><code>&lt;dependency org=\"org.apache.gora\" name=\"gora-hbase\" rev=\"0.2\" conf=\"*-&gt;default\" /&gt;</code></p>\n\n<p>When I uncomment it, run ant clean and ant runtime, bug disapper. I am using Ubuntu 12 server too.</p>\n", "creation_date": 1437936878, "is_accepted": false, "score": 0, "last_activity_date": 1437936878, "answer_id": 31640718}], "question_id": 26178919, "tags": ["java", "hbase", "web-crawler", "classnotfoundexception", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/26178919/nuch-2-2-1-hbase-0-90-4-classnotfoundexception-bug", "last_activity_date": 1437936878, "owner": {"age": 28, "answer_count": 16, "creation_date": 1398342846, "user_id": 3568831, "accept_rate": 71, "view_count": 51, "location": "Cameroon", "reputation": 137}, "body": "<p>Hi all Iam facing a big problem to set my first crawling using Nutch 2.2.1 and Hbase 0.90.4 It seems that nutch is buggy. nothing is working at all. please help me.</p>\n\n<p>I downloaded Nutch 2.2.1 and Hbase 0.90.4 from apache and made changes on configuration files before compile. i uncommented the gora-core and gora-hbase in ivy.xml to this one</p>\n\n<pre><code>&lt;dependency org=\"org.apache.gora\" name=\"gora-hbase\" rev=\"0.2\" conf=\"*-&gt;default\" /&gt;\n&lt;dependency org=\"org.apache.gora\" name=\"gora-core\" rev=\"0.2\" conf=\"*-&gt;default\"/&gt;\n</code></pre>\n\n<p>after doing all stuff i run:</p>\n\n<pre><code>bin/nutch inject urls/seed.txt\n</code></pre>\n\n<p>and i got Exception trace </p>\n\n<pre><code>InjectorJob: starting at 2014-10-03 12:58:16\nInjectorJob: Injecting urlDir: urls/seed.txt\nInjectorJob: java.lang.ClassNotFoundException:  org.apache.gora.hbase.store.HBaseStore \nat java.net.URLClassLoader$1.run(URLClassLoader.java:366)\nat java.net.URLClassLoader$1.run(URLClassLoader.java:355)\nat java.security.AccessController.doPrivileged(Native Method)\nat java.net.URLClassLoader.findClass(URLClassLoader.java:354)\nat java.lang.ClassLoader.loadClass(ClassLoader.java:425)\nat sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)\nat java.lang.ClassLoader.loadClass(ClassLoader.java:358)\nat java.lang.Class.forName0(Native Method)\nat java.lang.Class.forName(Class.java:190)\nat org.apache.nutch.storage.StorageUtils.getDataStoreClass(StorageUtils.java:89)\nat org.apache.nutch.storage.StorageUtils.createWebStore(StorageUtils.java:73)\nat org.apache.nutch.crawl.InjectorJob.run(InjectorJob.java:221)\nat org.apache.nutch.crawl.InjectorJob.inject(InjectorJob.java:251)\nat org.apache.nutch.crawl.InjectorJob.run(InjectorJob.java:273)\nat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\nat org.apache.nutch.crawl.InjectorJob.main(InjectorJob.java:282)\n</code></pre>\n\n<p>I changed the rev version to 0.3 for both gora-core and gora-hbase but i got the same error. As it seem that nutch couldn't find HBaseStore i put the gora-hbase jar into the CLASSPATH but still got the same error. </p>\n\n<p>Iam using ubuntu 12</p>\n", "creation_date": 1412339216, "score": 0},
{"title": "apache nutch skip &#39;parse&#39; stage", "view_count": 63, "is_answered": false, "answers": [{"question_id": 31448873, "owner": {"user_id": 1577405, "link": "http://stackoverflow.com/users/1577405/aperfectpoint", "user_type": "registered", "reputation": 41}, "body": "<p>You're using the bin/crawl script which will go over and over (<em>number of rounds</em> times) on the generate-fetch-parse-... steps. Check out the <a href=\"https://wiki.apache.org/nutch/NutchTutorial\" rel=\"nofollow\">nutch  tutorial</a>, you can issue any command by yourself (and building your own script) using bin/nutch.</p>\n\n<p>However, if I understand what you are doing correctly, meaning indexing the html/css/js to a local filesystem, instead of changing the sources, you can <a href=\"https://wiki.apache.org/nutch/WritingPluginExample\" rel=\"nofollow\">create your own plugins</a> (you'll need a parse-plugin and an index-plugin I think), and apply them on the standard nutch process.</p>\n", "creation_date": 1437921614, "is_accepted": false, "score": 0, "last_activity_date": 1437921614, "answer_id": 31638238}], "question_id": 31448873, "tags": ["web-crawler", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/31448873/apache-nutch-skip-parse-stage", "last_activity_date": 1437921614, "owner": {"user_id": 2517384, "answer_count": 4, "creation_date": 1372097860, "accept_rate": 77, "view_count": 57, "reputation": 359}, "body": "<p>I'm using apache nutch 1.10 version, and I'm changed sources to save raw htmls,css,js files to directory on local disk , all works fine but after fetching step comes a slow parse stage, how I can skip parsing?\nI run crawling using  that command:</p>\n\n<pre><code>$ bin/crawl  urls/  data/ 10\n</code></pre>\n", "creation_date": 1437034674, "score": 0},
{"title": "Hbase Standalone vs Pseudo distributed mode for nutch crawler", "view_count": 184, "is_answered": false, "question_id": 31516042, "tags": ["hadoop", "hbase", "hdfs", "nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/31516042/hbase-standalone-vs-pseudo-distributed-mode-for-nutch-crawler", "last_activity_date": 1437393881, "owner": {"user_id": 4793563, "view_count": 3, "answer_count": 7, "creation_date": 1429125664, "reputation": 21}, "body": "<p>I'm running Nutch crawler for three smaller sites. Every day I crawl &lt;500k URLs, on a single machine. Per customer wishes, there will not be a cluster of 2+ machines for hadoop cluster. Only one server instance. </p>\n\n<p>I understand that neither Standalone and Pseudo distributed mode is recommended for production, but...</p>\n\n<p><strong>Q1:</strong> which of these modes is 'Lesser evil' - Nutch is written so it can be run on single machine. Which of these modes were intended for this scenario?</p>\n\n<p><strong>Q2:</strong> Currently we are running Hbase in Standalone mode and from time to time hbase just crashes without anything in logs. HMaster stops and cannot be restarted without fixing meta and repairing inconsistencies (Connection refused). Could such error be caused by the standalone mode?</p>\n\n<p><strong>note 1:</strong> We do not need hadoop data replication, because number of urls is small.</p>\n\n<p><strong>note 2:</strong> We do not need parallelism for hadoop jobs, because number of urls is small</p>\n\n<p>Because of note 1,2 I think that plain File system should be enough and hdfs in unnecessary. Am I right?</p>\n", "creation_date": 1437393881, "score": 2},
{"title": "Jaunt - check if there is specific element", "view_count": 228, "owner": {"age": 24, "answer_count": 4, "creation_date": 1426319117, "user_id": 4670035, "accept_rate": 67, "view_count": 8, "location": "internet", "reputation": 35}, "is_answered": true, "answers": [{"question_id": 31472501, "owner": {"user_id": 502471, "link": "http://stackoverflow.com/users/502471/javaslugger", "user_type": "registered", "reputation": 139}, "body": "<p>You are correct that the findFirst method throws an Exception if the element is not found..  You can use a try-catch block to catch the NotFound Exception in your code, and take it from there, or if you can write a helper method that does not throw an Exception (if you just need a boolean detector)</p>\n\n<pre><code>public boolean has(Element element, String target){\n  try{\n    element.findFirst(target);\n    return true;\n  }\n  catch(NotFound n){\n    return false;\n  }\n}\n</code></pre>\n\n<p>Alternatively, you can use the findEvery method, which does not throw an Exception, as a boolean detector:</p>\n\n<pre><code>if(body.findEvery(\"&lt;div class=info_books_item&gt;\").size() &gt; 0){\n}\n</code></pre>\n", "creation_date": 1437238855, "is_accepted": true, "score": 1, "last_activity_date": 1437238855, "answer_id": 31493471}], "question_id": 31472501, "tags": ["html", "parsing", "element", "nutch", "jaunt-api"], "answer_count": 1, "link": "http://stackoverflow.com/questions/31472501/jaunt-check-if-there-is-specific-element", "last_activity_date": 1437238855, "accepted_answer_id": 31493471, "body": "<p>I'm using Nutch to crawl website and currently writing a plugin. Jaunt 1.0.0.1 is used to Parse HTML. \nFor example, I have a row </p>\n\n<pre><code>Element infoBooksItem = body.findFirst(\"&lt;div class=info_books_item&gt;\");\n</code></pre>\n\n<p>Which gets and error, when on page is no <code>&lt;div class=info_books_item&gt;</code>.\nCurrently I'm looking at Jaunt JavaDocs, but can't figure out how to check, is there such element or not.</p>\n", "creation_date": 1437125121, "score": 1},
{"title": "Trigger Apache Nutch Crawl Programatically", "view_count": 68, "owner": {"user_id": 4555234, "answer_count": 6, "creation_date": 1423665405, "accept_rate": 64, "view_count": 10, "reputation": 69}, "is_answered": true, "answers": [{"question_id": 30758821, "owner": {"user_id": 4555234, "accept_rate": 64, "link": "http://stackoverflow.com/users/4555234/itsnino91", "user_type": "registered", "reputation": 69}, "body": "<p>I ended up figuring out how to use the NutchApi to answer my question. </p>\n", "creation_date": 1436977096, "is_accepted": true, "score": -1, "last_activity_date": 1436977096, "answer_id": 31435799}], "question_id": 30758821, "tags": ["c#", "asp.net", "apache", "web-api", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/30758821/trigger-apache-nutch-crawl-programatically", "last_activity_date": 1436977096, "accepted_answer_id": 31435799, "body": "<p>I'm trying to create a ASP.NET web api to trigger a crawl event to happen. I can't seem to get cygwin to process any of the commands I give it. The only thing I can really do is get it to open a terminal. Once the terminal is open I'd have to redirect the pwd to another location and then trigger my command I want. </p>\n\n<pre><code>Process p = new Process();\nProcessStartInfo info = new ProcessStartInfo();\ninfo.CreateNoWindow = false;\ninfo.RedirectStandardInput = true;\ninfo.UseShellExecute = false;\ninfo.FileName = \"C:\\\\cygwin64\\\\bin\\\\mintty.exe\";\n\np.StartInfo = info;\np.Start();\nStreamWriter sw = p.StandardInput;\nif (sw.BaseStream.CanWrite)\n{\n    sw.WriteLine(@\"cd C:\\Users\\UName\\Desktop\\apache-nutch-2.3-mongodb\\runtime\\local\\\");\n    sw.WriteLine(\"bin/autoCrawl\");\n}\nsw.Close();\np.WaitForExit();\n</code></pre>\n\n<p>I've tried many approaches, this is the last one I've tried but it just does nothing. Is there a way to launch this crawl from my .NET application? I've looked into the NutchApi about creating a new job with a type of crawl but I'm not sure if that applies here or not. </p>\n", "creation_date": 1433945357, "score": 0},
{"title": "Nutch REST api Results (limited)", "view_count": 65, "owner": {"user_id": 4555234, "answer_count": 6, "creation_date": 1423665405, "accept_rate": 64, "view_count": 10, "reputation": 69}, "is_answered": true, "answers": [{"question_id": 31010040, "owner": {"user_id": 4555234, "accept_rate": 64, "link": "http://stackoverflow.com/users/4555234/itsnino91", "user_type": "registered", "reputation": 69}, "body": "<p>I figured it out. The timestamp used in the GenerateJob step was wrong. It needed to be in a particular format and my code wasn't supporting it. Found a work around.</p>\n", "creation_date": 1436972415, "is_accepted": true, "score": 0, "last_activity_date": 1436972415, "answer_id": 31433996}], "question_id": 31010040, "tags": ["mongodb", "api", "rest", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/31010040/nutch-rest-api-results-limited", "last_activity_date": 1436972415, "accepted_answer_id": 31433996, "body": "<p>I've just figured out how to complete a Nutch crawl via the REST api for the 2.3 version of Nutch. You can see my post <a href=\"http://stackoverflow.com/questions/30919467/apache-nutch-rest-api\">here</a>. So after running the crawl, I go to MongoVue to check out the results and there is no \"status\" or \"baseUrl\" fields, along with others. Now if I do a normal crawl through cygwin, I get all fields. Is there some parameter I'm missing from the POST request to UPDATEDB call?</p>\n\n<p>Here is the last call I make for Updatedb. </p>\n\n<pre><code>{\n  \"args\":{\n    \"crawlId\":\"crawl-01\",\n    \"batch\":\"1428526896161-4430\"\n  },\n  \"confId\":\"default\",\n  \"crawlId\":\"crawl-01\",\n  \"type\":\"UPDATEDB\"\n}\n</code></pre>\n", "creation_date": 1435082156, "score": 0},
{"title": "Nutch is crawling only few links in a given domain", "view_count": 412, "is_answered": false, "answers": [{"question_id": 26709890, "owner": {"user_id": 2915763, "link": "http://stackoverflow.com/users/2915763/ramanan-r", "user_type": "registered", "reputation": 789}, "body": "<p>Set number of url's to crawl in each level using <code>-topN</code>:</p>\n\n<blockquote>\n  <p>bin/nutch crawl $URLS -dir $CRAWL_LOC -depth 3 -topN 1000</p>\n</blockquote>\n", "creation_date": 1415361474, "is_accepted": false, "score": 0, "last_activity_date": 1415361474, "answer_id": 26800767}, {"question_id": 26709890, "owner": {"user_id": 1115008, "link": "http://stackoverflow.com/users/1115008/sew", "user_type": "registered", "reputation": 53}, "body": "<p>In Nutch 1.10, db.ignore.internal.links defaults to true, which means that the link db will not contain internal links, only external. If you haven't changed the default setting, then the link db will not reflect the extent of the crawl. If you want it to contain these links, you can change the value of this property to false in your config file.</p>\n\n<p>If you want to see all the links that were crawled, dump the crawl db. In 1.10 this is done via:</p>\n\n<pre><code>bin/nutch readdb MyCrawl/crawldb/ -dump crawlout\n</code></pre>\n\n<p>I'm not sure what the differences are between 1.9 and 1.10, but I imagine the commands are similar. (The 1.10 release notes don't indicate that the handling of internal links has changed.)</p>\n", "creation_date": 1436904607, "is_accepted": false, "score": 0, "last_activity_date": 1436904607, "answer_id": 31416337}], "question_id": 26709890, "tags": ["web-crawler", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/26709890/nutch-is-crawling-only-few-links-in-a-given-domain", "last_activity_date": 1436904607, "owner": {"user_id": 4205842, "view_count": 1, "answer_count": 0, "creation_date": 1414866468, "reputation": 1}, "body": "<p>Problem occurs in Nutch 1.9 with ubuntu 12.04. I'm trying to crawl the links available in a website. I have given the website url in seed.txt file. I didn't make any changes to the default configuration except the http.agent.name (New) property and db.max.outlinks.per.page (-1). I'm using the following command to crawl <br/>\n<code>crawl urls test -depth 3</code> <br/>\nCrawler is supposed to crawl all the links available within the depth of 3. But only 5 links  are availble when I run the following linkdb command. All the five links are available in the home page<br/></p>\n\n<pre><code>nutch readlinkdb test/linkdb -dump myoutput/out1&lt;br/&gt;\n</code></pre>\n\n<p>Did I miss any configuration changes? Please help me out. </p>\n", "creation_date": 1415000953, "score": 0},
{"title": "Build Failed during nutch-1.5 compiling with ant-1.5.2", "view_count": 29, "is_answered": false, "answers": [{"question_id": 31398012, "owner": {"user_id": 3636071, "accept_rate": 100, "link": "http://stackoverflow.com/users/3636071/abhijit-bashetti", "user_type": "registered", "reputation": 3104}, "body": "<p>You must have missed the IVY library. Download and add in you path and try.</p>\n\n<p>You can Download the same from here <a href=\"http://ant.apache.org/ivy/download.cgi\" rel=\"nofollow\">IVY</a></p>\n", "creation_date": 1436858649, "is_accepted": false, "score": 0, "last_activity_date": 1436858649, "answer_id": 31400231}], "question_id": 31398012, "tags": ["java", "ant", "solr", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/31398012/build-failed-during-nutch-1-5-compiling-with-ant-1-5-2", "last_activity_date": 1436858649, "owner": {"user_id": 5113639, "view_count": 2, "answer_count": 0, "creation_date": 1436849752, "reputation": 1}, "body": "<p>I get this error while executing \"ant\" command from cmd on my Windows for compiling nutch to be used with java.</p>\n\n<p>Screenshot link is: <a href=\"https://www.dropbox.com/s/6toyfdma7bqwmou/Screenshot%20%28193%29.png?dl=0\" rel=\"nofollow\">https://www.dropbox.com/s/6toyfdma7bqwmou/Screenshot%20%28193%29.png?dl=0</a></p>\n\n<p>I followed this tutorial\n<a href=\"http://cmusphinx.sourceforge.net/2012/06/building-a-java-application-with-apache-nutch-and-solr/\" rel=\"nofollow\">http://cmusphinx.sourceforge.net/2012/06/building-a-java-application-with-apache-nutch-and-solr/</a></p>\n", "creation_date": 1436850171, "score": 0},
{"title": "crawler + elasticsearch integration", "view_count": 4777, "owner": {"age": 26, "answer_count": 27, "creation_date": 1335751316, "user_id": 1364923, "accept_rate": 89, "view_count": 322, "location": "Prague, Czech Republic", "reputation": 937}, "is_answered": true, "answers": [{"last_edit_date": 1417090266, "owner": {"user_id": 1552187, "link": "http://stackoverflow.com/users/1552187/konrad-holl", "user_type": "registered", "reputation": 356}, "body": "<p>Did you have a look at the River Web plugin? <a href=\"https://github.com/codelibs/elasticsearch-river-web\" rel=\"nofollow\">https://github.com/codelibs/elasticsearch-river-web</a></p>\n\n<p>It provides a good How To section, including creating the required indexes, scheduling (based on Quartz), authentication (basic and NTLM are supported), meta data extraction, ...</p>\n\n<p>Might be worth having a look at the elasticsearch river plugins overview as well: <a href=\"http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/modules-plugins.html#river\" rel=\"nofollow\">http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/modules-plugins.html#river</a></p>\n", "question_id": 27133235, "creation_date": 1417088950, "is_accepted": true, "score": 4, "last_activity_date": 1417090266, "answer_id": 27170045}, {"last_edit_date": 1436805524, "owner": {"user_id": 4635998, "link": "http://stackoverflow.com/users/4635998/hern%c3%a1n-ignacio-vivani", "user_type": "registered", "reputation": 11}, "body": "<p>You can evaluate indexing <a href=\"https://commoncrawl.org/the-data/\" rel=\"nofollow\">Common Crawl</a> metadata into Elasticsearch using Hadoop: \nWhen working with big volumes of data, Hadoop provides all the power to parallelize the data ingestion.</p>\n\n<p>Here is an example that uses Cascading to index directly into Elasticsearch:\n<a href=\"http://blogs.aws.amazon.com/bigdata/post/TxC0CXZ3RPPK7O/Indexing-Common-Crawl-Metadata-on-Amazon-EMR-Using-Cascading-and-Elasticsearch\" rel=\"nofollow\">http://blogs.aws.amazon.com/bigdata/post/TxC0CXZ3RPPK7O/Indexing-Common-Crawl-Metadata-on-Amazon-EMR-Using-Cascading-and-Elasticsearch</a></p>\n\n<p>The process involves the use of a Hadoop cluster (EMR on this example) running the Cascading application that indexes the JSON metadata directly into Elasticsearch.</p>\n\n<p>Cascading source code is also available to understand how to handle the data ingestion in Elasticsearch.</p>\n", "question_id": 27133235, "creation_date": 1436803793, "is_accepted": false, "score": 0, "last_activity_date": 1436805524, "answer_id": 31388296}], "question_id": 27133235, "tags": ["elasticsearch", "web-crawler", "search-engine", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/27133235/crawler-elasticsearch-integration", "last_activity_date": 1436805524, "accepted_answer_id": 27170045, "body": "<p>I wasn't able to find out, how to crawl website and index data to elasticsearch. I managed to do that in the combination nutch+solr and as nutch should be able from the version 1.8 export data directly to elasticsearch (<a href=\"http://nutch.apache.org/\" rel=\"nofollow\">source</a>), I tried to use nutch again. Nevertheless I didn't succeed. After trying to invoke</p>\n\n<pre><code>$ bin/nutch elasticindex\n</code></pre>\n\n<p>I get:</p>\n\n<pre><code>Error: Could not find or load main class elasticindex\n</code></pre>\n\n<p>I don't insist on using nutch. I just would need the simpliest way to crawl websites and index them to elasticsearch. The problem is, that I wasn't able to find any step-by-step tutorial and I'm quite new to these technologies. </p>\n\n<p>So the question is - what would be the simpliest solution to integrate crawler to elasticsearch and if possible, I would be grateful for any step-by-step solution.</p>\n", "creation_date": 1416937233, "score": 1},
{"title": "Can we crawl and index Google Drive documents using nutch and solr?", "view_count": 313, "is_answered": true, "answers": [{"question_id": 30569353, "owner": {"user_id": 2915763, "link": "http://stackoverflow.com/users/2915763/ramanan-r", "user_type": "registered", "reputation": 789}, "body": "<p>Use Google Drive API to read/manage files </p>\n\n<p><a href=\"https://developers.google.com/drive/web/about-sdk\" rel=\"nofollow\">https://developers.google.com/drive/web/about-sdk</a></p>\n\n<p>Drive Public URL's page won't have direct links to subdirectories, so you will get nothing if you crawl those pages.</p>\n", "creation_date": 1433163725, "is_accepted": false, "score": 1, "last_activity_date": 1433163725, "answer_id": 30574232}], "question_id": 30569353, "tags": ["solr", "google-drive-sdk", "nutch", "moss2007enterprisesearch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/30569353/can-we-crawl-and-index-google-drive-documents-using-nutch-and-solr", "last_activity_date": 1436793448, "owner": {"user_id": 4193280, "answer_count": 114, "creation_date": 1414573314, "accept_rate": 27, "view_count": 114, "location": "India", "reputation": 1229}, "body": "<p>I have tried indexing public url of a google drive document, but it seems that it does not work . Is there any way to crawl google drive documents via nutch and make their index using solr?</p>\n", "creation_date": 1433148867, "score": 2},
{"title": "Apache Nutch NoSuchElementException with bin/nutch inject , readdb, generate options", "view_count": 380, "is_answered": false, "answers": [{"question_id": 31125012, "owner": {"user_id": 713026, "accept_rate": 78, "link": "http://stackoverflow.com/users/713026/blazes", "user_type": "registered", "reputation": 2643}, "body": "<p>I too am new to nutch.  However, I think the problem is that you haven't configured a data store.  I got the same error, and got a bit further.  You need to follow this: <a href=\"https://wiki.apache.org/nutch/Nutch2Tutorial\" rel=\"nofollow\">https://wiki.apache.org/nutch/Nutch2Tutorial</a>, or this: <a href=\"https://wiki.apache.org/nutch/Nutch2Cassandra\" rel=\"nofollow\">https://wiki.apache.org/nutch/Nutch2Cassandra</a>.  Then, rebuild: <code>ant runtime</code></p>\n", "creation_date": 1436703381, "is_accepted": false, "score": 0, "last_activity_date": 1436703381, "answer_id": 31367658}], "question_id": 31125012, "tags": ["web-scraping", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/31125012/apache-nutch-nosuchelementexception-with-bin-nutch-inject-readdb-generate-opt", "last_activity_date": 1436703381, "owner": {"user_id": 3003118, "view_count": 2, "answer_count": 0, "creation_date": 1384741965, "reputation": 6}, "body": "<p>I am new to Apache Nutch 2.3 and Solr. I am trying to get my first crawl working. I installed Apache Nutch and Solr as mentioned in official documentation and both are working fine. However when I did the following steps I get errors - \nbin/nutch inject examples/dmoz/  - Works correctly\n(InjectorJob: total number of urls rejected by filters: 2\nInjectorJob: total number of urls injected after normalization and filtering:130)</p>\n\n<p>Error - $ bin/nutch generate -topN 5\nGeneratorJob: starting at 2015-06-25 17:51:50\nGeneratorJob: Selecting best-scoring urls due for fetch.\nGeneratorJob: starting\nGeneratorJob: filtering: true\nGeneratorJob: normalizing: true\nGeneratorJob: topN: 5</p>\n\n<pre><code>java.util.NoSuchElementException\nat java.util.TreeMap.key(TreeMap.java:1323)\nat java.util.TreeMap.firstKey(TreeMap.java:290)\nat org.apache.gora.memory.store.MemStore.execute(MemStore.java:125)\nat org.apache.gora.query.impl.QueryBase.execute(QueryBase.java:73) ...\nGeneratorJob: generated batch id: 1435279910-1190400607 containing 0 URLs\n</code></pre>\n\n<p>Same errors if i do - $ bin/nutch readdb -stats\nError - java.util.NoSuchElementException ... \nStatistics for WebTable: </p>\n\n<pre><code>jobs:   {db_stats-job_local970586387_0001={jobName=db_stats, jobID=job_local970586387_0001, counters={Map-Reduce Framework={MAP_OUTPUT_MATERIALIZED_BYTES=6, REDUCE_INPUT_RECORDS=0, SPILLED_RECORDS=0, MAP_INPUT_RECORDS=0, SPLIT_RAW_BYTES=653, MAP_OUTPUT_BYTES=0, REDUCE_SHUFFLE_BYTES=0, REDUCE_INPUT_GROUPS=0, COMBINE_OUTPUT_RECORDS=0, REDUCE_OUTPUT_RECORDS=0, MAP_OUTPUT_RECORDS=0, COMBINE_INPUT_RECORDS=0, COMMITTED_HEAP_BYTES=514850816}, File Input Format Counters ={BYTES_READ=0}, File Output Format Counters ={BYTES_WRITTEN=98}, FileSystemCounters={FILE_BYTES_WRITTEN=1389120, FILE_BYTES_READ=1216494}}}}\nTOTAL urls: 0\n</code></pre>\n\n<p>I am also not able to use generate or crawl commands. </p>\n\n<p>Can anyone tell me what am I doing wrong?</p>\n\n<p>Thanks.</p>\n", "creation_date": 1435609886, "score": 1},
{"title": "Nutch and crawling millions of websites", "view_count": 172, "is_answered": false, "answers": [{"question_id": 31303994, "owner": {"user_id": 1577405, "link": "http://stackoverflow.com/users/1577405/aperfectpoint", "user_type": "registered", "reputation": 41}, "body": "<p>Yes, you can. This is essentially the goal of nutch. However, crawling millions of websites takes time and space, and in order to do, you need to setup the environment correctly.</p>\n\n<p>In nutch 1.X the \"crawling database\", e.g. which urls visited, what is the urls frontier (next urls to visit),etc. is persisted to the hadoop filesystem. This is the place where you'll first inject your list of urls.</p>\n\n<p>In addition, in order to view to indexed data, you can use solr (or elasticsearch).</p>\n\n<p>I recommend first going through the <a href=\"http://wiki.apache.org/nutch/NutchTutorial\" rel=\"nofollow\">nutch 1.x tutorial</a> with a <strong>short</strong> list of urls and getting to know how to use nutch and the plugins. </p>\n\n<p>After that, setup an hadoop cluster with tutorials from the <a href=\"http://hadoop.apache.org/docs/stable/\" rel=\"nofollow\">hadoop site</a>, and crawl away!</p>\n", "creation_date": 1436691368, "is_accepted": false, "score": 0, "last_activity_date": 1436691368, "answer_id": 31366072}], "question_id": 31303994, "tags": ["nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/31303994/nutch-and-crawling-millions-of-websites", "last_activity_date": 1436691368, "owner": {"user_id": 5090957, "answer_count": 0, "creation_date": 1436295889, "accept_rate": 20, "view_count": 10, "reputation": 18}, "body": "<p>Can we use nutch 1.10 in order to crawl millions of websites with several number of round?</p>\n\n<p>I don't really understand the database created when I launch nutch 1.10. Is this enough to crawl the important data from the site?</p>\n\n<p>I have a file with a list of url's that take 2 gigabytes. </p>\n", "creation_date": 1436390422, "score": 0},
{"title": "nutch indexing fails with io exception", "view_count": 409, "owner": {"user_id": 5097472, "view_count": 14, "answer_count": 0, "creation_date": 1436428992, "reputation": 3}, "is_answered": true, "answers": [{"question_id": 31316690, "owner": {"user_id": 1699191, "link": "http://stackoverflow.com/users/1699191/ameertawfik", "user_type": "registered", "reputation": 618}, "body": "<p>First of all, if you are crawling and indexing data, then you should use <code>bin/crawl</code> as it is a better tool.</p>\n\n<p>Second, from the stack trace, it is clear that you are not setting solr url correctly. In general, your solr url should be something like <code>http://domainname:port/solr/corename</code></p>\n\n<p>But, i see you have <code>localhost:8983/solr/update</code>. So, your url is missing the core name of solr. By default, it is collection1.</p>\n", "creation_date": 1436495087, "is_accepted": true, "score": 0, "last_activity_date": 1436495087, "answer_id": 31331456}], "question_id": 31316690, "tags": ["indexing", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/31316690/nutch-indexing-fails-with-io-exception", "last_activity_date": 1436495087, "accepted_answer_id": 31331456, "body": "<p>Nutch indexing fails when I run the following command:</p>\n\n<pre><code>root@ubuntu:/home/test-tb/Downloads/apache-nutch-1.10# bin/nutch index mycrl/crawldb/ -dir mycrl/segments/\n</code></pre>\n\n<p>I am using nutch 1.10 on ubuntu 12.04 LTS.</p>\n\n<p>Error log details are:</p>\n\n<pre><code>2015-07-09 17:07:36,940 INFO  indexer.IndexWriters - Adding    org.apache.nutch.indexwriter.solr.SolrIndexWriter\n2015-07-09 17:07:36,970 INFO  solr.SolrMappingReader - source: content dest: content\n2015-07-09 17:07:36,970 INFO  solr.SolrMappingReader - source: title dest: title\n2015-07-09 17:07:36,970 INFO  solr.SolrMappingReader - source: host dest: host\n2015-07-09 17:07:36,970 INFO  solr.SolrMappingReader - source: segment dest: segment\n2015-07-09 17:07:36,970 INFO  solr.SolrMappingReader - source: boost dest: boost\n2015-07-09 17:07:36,970 INFO  solr.SolrMappingReader - source: digest dest: digest\n2015-07-09 17:07:36,970 INFO  solr.SolrMappingReader - source: tstamp dest: tstamp\n2015-07-09 17:07:37,030 INFO  solr.SolrIndexWriter - Indexing 100 documents\n2015-07-09 17:07:37,136 INFO  solr.SolrIndexWriter - Indexing 100 documents\n2015-07-09 17:07:37,166 WARN  mapred.LocalJobRunner - job_local1383488781_0001\norg.apache.solr.common.SolrException: Not Found\n\nNot Found\n\nrequest: http://127.0.0.1:8983/solr/update?wt=javabin&amp;version=2\nat  org.apache.solr.client.solrj.impl.CommonsHttpSolrServer.request(CommonsHttpSolrServer.java:430)\nat org.apache.solr.client.solrj.impl.CommonsHttpSolrServer.request(CommonsHttpSolrServer.java:244)\nat org.apache.solr.client.solrj.request.AbstractUpdateRequest.process(AbstractUpdateRequest.java:105)\nat org.apache.nutch.indexwriter.solr.SolrIndexWriter.close(SolrIndexWriter.java:153)\nat org.apache.nutch.indexer.IndexWriters.close(IndexWriters.java:115)\nat org.apache.nutch.indexer.IndexerOutputFormat$1.close(IndexerOutputFormat.java:44)\nat org.apache.hadoop.mapred.ReduceTask$OldTrackingRecordWriter.close(ReduceTask.java:467)\nat org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:535)\nat org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:421)\nat org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:398)\n\n2015-07-09 17:07:37,957 ERROR indexer.IndexingJob - Indexer: java.io.IOException: Job failed!\nat org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:1357)\nat org.apache.nutch.indexer.IndexingJob.index(IndexingJob.java:113)\nat org.apache.nutch.indexer.IndexingJob.run(IndexingJob.java:177)\nat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\nat org.apache.nutch.indexer.IndexingJob.main(IndexingJob.java:187)\n</code></pre>\n\n<p>Though I am not specifying a solr indexing option to nutch, this error is returned. Am I missing anything here? Your pointers will be very helpful. Thanks in advance.</p>\n", "creation_date": 1436442812, "score": 0},
{"title": "Nutch crawl no error , but result is nothing", "view_count": 2130, "is_answered": true, "answers": [{"question_id": 15995457, "owner": {"user_id": 1150329, "accept_rate": 89, "link": "http://stackoverflow.com/users/1150329/tejas-patil", "user_type": "registered", "reputation": 4793}, "body": "<p>As you are using Nutch 2.X, you need to follow the relevant <a href=\"http://wiki.apache.org/nutch/Nutch2Tutorial\" rel=\"nofollow\">tutorial</a>. The one that you gave was for Nutch 1.x. Nutch 2.X uses external storage backends like HBase, Cassandra so the crawldb, segments etc directories wont be formed.</p>\n\n<p>Also, use <code>bin/crawl</code> script instead of the <code>bin/nutch</code> command.</p>\n", "creation_date": 1366080853, "is_accepted": false, "score": 3, "last_activity_date": 1366080853, "answer_id": 16028007}], "question_id": 15995457, "tags": ["nutch", "web-crawler"], "answer_count": 1, "link": "http://stackoverflow.com/questions/15995457/nutch-crawl-no-error-but-result-is-nothing", "last_activity_date": 1436422932, "owner": {"user_id": 2278790, "view_count": 9, "answer_count": 0, "creation_date": 1365909181, "reputation": 11}, "body": "<p>I try to crawl some urls with nutch 2.1 as follows.</p>\n\n<pre><code>bin/nutch crawl urls -dir crawl -depth 3 -topN 5\n</code></pre>\n\n<p><a href=\"http://wiki.apache.org/nutch/NutchTutorial\" rel=\"nofollow\">http://wiki.apache.org/nutch/NutchTutorial</a></p>\n\n<p>There is no error , but undermentioned\u3000folders don't be made.</p>\n\n<pre><code>crawl/crawldb\ncrawl/linkdb\ncrawl/segments\n</code></pre>\n\n<p>Can anyone help me? \nI have not resolved this trouble for two days.\nThanks a lot!</p>\n\n<p>output is as follows.</p>\n\n<pre><code>FetcherJob: threads: 10\nFetcherJob: parsing: false\nFetcherJob: resuming: false\nFetcherJob : timelimit set for : -1\nUsing queue mode : byHost\nFetcher: threads: 10\nQueueFeeder finished: total 0 records. Hit by time limit :0\n-finishing thread FetcherThread1, activeThreads=0\nFetcher: throughput threshold: -1\nFetcher: throughput threshold sequence: 5\n-finishing thread FetcherThread2, activeThreads=7\n-finishing thread FetcherThread3, activeThreads=6\n-finishing thread FetcherThread4, activeThreads=5\n-finishing thread FetcherThread5, activeThreads=4\n-finishing thread FetcherThread6, activeThreads=3\n-finishing thread FetcherThread7, activeThreads=2\n-finishing thread FetcherThread0, activeThreads=1\n-finishing thread FetcherThread8, activeThreads=0\n-finishing thread FetcherThread9, activeThreads=0\n0/0 spinwaiting/active, 0 pages, 0 errors, 0.0 0.0 pages/s, 0 0 kb/s, 0 URLs in 0 queues\n-activeThreads=0\nParserJob: resuming:    false\nParserJob: forced reparse:  false\nParserJob: parsing all\nFetcherJob: threads: 10\nFetcherJob: parsing: false\nFetcherJob: resuming: false\nFetcherJob : timelimit set for : -1\nUsing queue mode : byHost\nFetcher: threads: 10\nQueueFeeder finished: total 0 records. Hit by time limit :0\n-finishing thread FetcherThread1, activeThreads=0\nFetcher: throughput threshold: -1\nFetcher: throughput threshold sequence: 5\n-finishing thread FetcherThread2, activeThreads=7\n-finishing thread FetcherThread3, activeThreads=6\n-finishing thread FetcherThread4, activeThreads=5\n-finishing thread FetcherThread5, activeThreads=4\n-finishing thread FetcherThread6, activeThreads=3\n-finishing thread FetcherThread7, activeThreads=2\n-finishing thread FetcherThread0, activeThreads=1\n-finishing thread FetcherThread8, activeThreads=0\n-finishing thread FetcherThread9, activeThreads=0\n0/0 spinwaiting/active, 0 pages, 0 errors, 0.0 0.0 pages/s, 0 0 kb/s, 0 URLs in 0 queues\n-activeThreads=0\nParserJob: resuming:    false\nParserJob: forced reparse:  false\nParserJob: parsing all\nFetcherJob: threads: 10\nFetcherJob: parsing: false\nFetcherJob: resuming: false\nFetcherJob : timelimit set for : -1\nUsing queue mode : byHost\nFetcher: threads: 10\nQueueFeeder finished: total 0 records. Hit by time limit :0\nFetcher: throughput threshold: -1\nFetcher: throughput threshold sequence: 5\n-finishing thread FetcherThread9, activeThreads=9\n-finishing thread FetcherThread0, activeThreads=8\n-finishing thread FetcherThread1, activeThreads=7\n-finishing thread FetcherThread2, activeThreads=6\n-finishing thread FetcherThread3, activeThreads=5\n-finishing thread FetcherThread4, activeThreads=4\n-finishing thread FetcherThread5, activeThreads=3\n-finishing thread FetcherThread6, activeThreads=2\n-finishing thread FetcherThread7, activeThreads=1\n-finishing thread FetcherThread8, activeThreads=0\n0/0 spinwaiting/active, 0 pages, 0 errors, 0.0 0.0 pages/s, 0 0 kb/s, 0 URLs in 0 queues\n-activeThreads=0\nParserJob: resuming:    false\nParserJob: forced reparse:  false\nParserJob: parsing all\n</code></pre>\n\n<p>runtime/local/conf/nutch-site.xml</p>\n\n<pre><code>&lt;?xml version=\"1.0\"?&gt;\n&lt;?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?&gt;\n\n&lt;!-- Put site-specific property overrides in this file. --&gt;\n\n\n&lt;configuration&gt;\n&lt;property&gt;\n &lt;name&gt;http.agent.name&lt;/name&gt;\n &lt;value&gt;My Nutch Spider&lt;/value&gt;\n&lt;/property&gt;\n\n&lt;property&gt;\n &lt;name&gt;storage.data.store.class&lt;/name&gt;\n &lt;value&gt;org.apache.gora.hbase.store.HBaseStore&lt;/value&gt;\n &lt;description&gt;Default class for storing data&lt;/description&gt;\n&lt;/property&gt;\n&lt;property&gt;\n  &lt;name&gt;http.robots.agents&lt;/name&gt;\n  &lt;value&gt;My Nutch Spider&lt;/value&gt;\n  &lt;description&gt;The agent strings we'll look for in robots.txt files,\n  comma-separated, in decreasing order of precedence. You should\n  put the value of http.agent.name as the first agent name, and keep the\n  default * at the end of the list. E.g.: BlurflDev,Blurfl,*\n  &lt;/description&gt;\n&lt;/property&gt;\n&lt;property&gt;\n  &lt;name&gt;http.content.limit&lt;/name&gt;\n  &lt;value&gt;262144&lt;/value&gt;\n&lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre>\n\n<p>runtime/local/conf/regex-urlfilter.txt</p>\n\n<pre><code># accept anything else\n+.\n</code></pre>\n\n<p>runtime/local/urls/seed.txt</p>\n\n<pre><code>http://nutch.apache.org/\n</code></pre>\n", "creation_date": 1365910046, "score": 2},
{"title": "Nutch 2.3 and HBase 1.0.0", "view_count": 131, "is_answered": true, "answers": [{"question_id": 31277364, "owner": {"user_id": 1129041, "link": "http://stackoverflow.com/users/1129041/anil-gupta", "user_type": "registered", "reputation": 681}, "body": "<p>HBase0.94 and HBase1.0.0 are not backward compatible. So, if you have code compiled with HBase0.94 then it wont work out of box with HBase1.0.0 cluster. Just recompile your code with HBase1.0.0 and then it should work. You might need to do minor code changes in HBaseClient.<br></p>\n\n<p>Also, HBase0.94 is not supported by HBase now.</p>\n", "creation_date": 1436337048, "is_accepted": false, "score": 2, "last_activity_date": 1436337048, "answer_id": 31285079}], "question_id": 31277364, "tags": ["hbase", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/31277364/nutch-2-3-and-hbase-1-0-0", "last_activity_date": 1436397043, "owner": {"user_id": 5090957, "answer_count": 0, "creation_date": 1436295889, "accept_rate": 20, "view_count": 10, "reputation": 18}, "body": "<p>I try to implement nutch for a projet that will crawl millions urls and actually it seems to work with HBase 0.94 locally.</p>\n\n<p>But the cluster in place is installed with HBase 1.0.0 and we didn't manage to make it works.</p>\n\n<p>Is it actually possible to use nutch 2.3 with HBase 1.0.0 ? </p>\n\n<p>Thanks for the answers.</p>\n", "creation_date": 1436296361, "score": 1},
{"title": "Do not filter outlinks in Nutch?", "view_count": 128, "is_answered": false, "question_id": 19626950, "tags": ["nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/19626950/do-not-filter-outlinks-in-nutch", "last_activity_date": 1436290223, "owner": {"age": 33, "answer_count": 81, "creation_date": 1361670244, "user_id": 2103602, "accept_rate": 62, "view_count": 188, "location": "San Francisco, CA", "reputation": 3167}, "body": "<p>I'm currently trying to perform a deep crawl within a small list of sites. To accomplish this, I updated <code>conf/domain-urlfilter.txt</code> with the domains of the sites I wish to scrape, which worked nicely. However, I found that not only were the links crawled at every step filtered, but the outlinks captured from each page crawled were filtered as well.</p>\n\n<p>Is there a way to avoid filtering captured outlinks while still filtering crawled URLs?</p>\n", "creation_date": 1382932306, "score": 0},
{"title": "Using Nutch 2.3 all my seed urls are being rejected", "view_count": 220, "owner": {"user_id": 2917893, "view_count": 4, "answer_count": 1, "creation_date": 1382654988, "reputation": 10}, "is_answered": true, "answers": [{"question_id": 31200194, "owner": {"user_id": 2917893, "link": "http://stackoverflow.com/users/2917893/hadi-gol", "user_type": "registered", "reputation": 10}, "body": "<p>Well, After spending a lot of time trying to figure things out... since I had changed the conf/regex-urlfilter.txt, i had to rebuild nutch using \"ant runtime\"... and things ended up working, so my conclusion and lesson for the past 2 days is, that always compile nutch after conf changes.</p>\n", "creation_date": 1435973574, "is_accepted": false, "score": 0, "last_activity_date": 1435973574, "answer_id": 31215996}, {"last_edit_date": 1436103252, "owner": {"user_id": 1577405, "link": "http://stackoverflow.com/users/1577405/aperfectpoint", "user_type": "registered", "reputation": 41}, "body": "<p>If you are using the /local runtime environment, you don't need to recompile for every change in a conf/ file.</p>\n\n<p>After you built nutch's runtime (using >ant runtime), the compilation creates the /local environment under <code>$NUTCH_HOME/runtime/local</code>. Under this, there is a conf/ directory, which is essentially a copy of <code>$NUTCH_HOME/conf</code>.\nHowever, you can (and should) edit the files there in order to change the /local configuration.</p>\n\n<p>Thus, if you want to change the name of your crawler, for example, edit <code>$NUTCH_HOME/runtime/local/conf/nutch-site.xml</code> and add/edit the property <code>http.agent.name</code> to whatever name you want.</p>\n", "question_id": 31200194, "creation_date": 1436100012, "is_accepted": true, "score": 1, "last_activity_date": 1436103252, "answer_id": 31230420}], "question_id": 31200194, "tags": ["apache", "web-crawler", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/31200194/using-nutch-2-3-all-my-seed-urls-are-being-rejected", "last_activity_date": 1436103252, "accepted_answer_id": 31230420, "body": "<p>I have 84 urls in my dmoz/urls file \nwhen i execute the command: bin/nutch inject dmoz</p>\n\n<p>i get the following:</p>\n\n<pre><code>[ec2-user@ip-172-31-47-66 local]$ bin/nutch inject dmoz/\nInjectorJob: starting at 2015-07-03 02:33:41\nInjectorJob: Injecting urlDir: dmoz\nInjectorJob: Using class org.apache.gora.hbase.store.HBaseStore as the Gora storage class.\nInjectorJob: total number of urls rejected by filters: 84\nInjectorJob: total number of urls injected after normalization and filtering: 0\nInjector: finished at 2015-07-03 02:33:44, elapsed: 00:00:03\n</code></pre>\n\n<hr>\n\n<p>All URLS are being rejected, here is a snippet of my nutch/conf/regex-url.xml</p>\n\n<pre><code># skip file: ftp: and mailto: urls\n-^(file|ftp|mailto):\n\n# skip image and other suffixes we can't yet parse\n# for a more extensive coverage use the urlfilter-suffix plugin\n\n-\\.(gif|GIF|jpg|JPG|png|PNG|ico|ICO|css|CSS|sit|SIT|eps|EPS|wmf|WMF|zip|ZIP|ppt|PPT|mpg|MPG|xls|XLS|gz|GZ|rpm|RPM|tgz|TGZ|mov|MOV|exe|EXE|jpeg|JPEG|bmp|BMP|js|JS)$\n\n# skip URLs containing certain characters as probable queries, etc.\n-[?*!@=]\n\n# skip URLs with slash-delimited segment that repeats 3+ times, to break loops\n-.*(/[^/]+)/[^/]+\\1/[^/]+\\1/\n\n# accept anything else\n+.\n</code></pre>\n\n<hr>\n\n<p>below is my hadoop.log output for this execution:</p>\n\n<pre><code>2015-07-03 02:33:41,095 INFO  crawl.InjectorJob - InjectorJob: starting at 2015-07-03 02:33:41\n2015-07-03 02:33:41,096 INFO  crawl.InjectorJob - InjectorJob: Injecting urlDir: dmoz\n2015-07-03 02:33:43,301 INFO  crawl.InjectorJob - InjectorJob: Using class org.apache.gora.hbase.store.HBaseStore as the Gora storage class.\n2015-07-03 02:33:43,329 WARN  util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n2015-07-03 02:33:43,389 WARN  snappy.LoadSnappy - Snappy native library not loaded\n2015-07-03 02:33:44,278 INFO  regex.RegexURLNormalizer - can't find rules for scope 'inject', using default\n2015-07-03 02:33:44,430 WARN  mapred.FileOutputCommitter - Output path is null in cleanup\n2015-07-03 02:33:44,768 INFO  crawl.InjectorJob - InjectorJob: total number of urls rejected by filters: 84\n2015-07-03 02:33:44,768 INFO  crawl.InjectorJob - InjectorJob: total number of urls injected after normalization and filtering: 0\n2015-07-03 02:33:44,769 INFO  crawl.InjectorJob - Injector: finished at 2015-07-03 02:33:44, elapsed: 00:00:03\n</code></pre>\n\n<hr>\n\n<p>I Highly appreciate if someone can help me out with this, basically all my urls are being rejected and im not sure why.</p>\n\n<p>Thanks\n-Hadi</p>\n", "creation_date": 1435905614, "score": 0},
{"title": "nutch-2.2.1 generator job failed", "view_count": 163, "is_answered": false, "question_id": 31200856, "tags": ["nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/31200856/nutch-2-2-1-generator-job-failed", "last_activity_date": 1435907883, "owner": {"user_id": 2789026, "view_count": 15, "answer_count": 7, "creation_date": 1379446030, "reputation": 60}, "body": "<p>sabayasachi@sabayasachi-Inspiron-3543:~/apache-nutch-2.2.1/runtime/local$ ./bin/crawl urls/ testCrawl false 1\nInjectorJob: starting at 2015-07-03 12:42:20\nInjectorJob: Injecting urlDir: urls\nInjectorJob: Using class org.apache.gora.sql.store.SqlStore as the Gora storage class.\nInjectorJob: total number of urls rejected by filters: 0\nInjectorJob: total number of urls injected after normalization and filtering: 1\nInjector: finished at 2015-07-03 12:42:22, elapsed: 00:00:02\nFri Jul 3 12:42:22 IST 2015 : Iteration 1 of 1\nGenerating batchId\nGenerating a new fetchlist\nGeneratorJob: starting at 2015-07-03 12:42:23\nGeneratorJob: Selecting best-scoring urls due for fetch.\nGeneratorJob: starting\nGeneratorJob: filtering: false\nGeneratorJob: normalizing: false\nGeneratorJob: topN: 50000\nGeneratorJob: java.lang.RuntimeException: job failed: name=[testCrawl]generate: 1435907542-1867, jobid=job_local1420236121_0001\n    at org.apache.nutch.util.NutchJob.waitForCompletion(NutchJob.java:54)\n    at org.apache.nutch.crawl.GeneratorJob.run(GeneratorJob.java:199)\n    at org.apache.nutch.crawl.GeneratorJob.generate(GeneratorJob.java:223)\n    at org.apache.nutch.crawl.GeneratorJob.run(GeneratorJob.java:279)\n    at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n    at org.apache.nutch.crawl.GeneratorJob.main(GeneratorJob.java:287)</p>\n\n<hr>\n\n<p>I am getting this error while executing the crawl command. I am using mysql for storage and nutch version is 2.2.1</p>\n", "creation_date": 1435907883, "score": 1},
{"title": "Nutch: How to re-try transient errors (and none of the other URLs)?", "view_count": 202, "owner": {"user_id": 5043542, "view_count": 3, "answer_count": 0, "creation_date": 1435132255, "reputation": 3}, "is_answered": true, "answers": [{"last_edit_date": 1435592475, "owner": {"user_id": 4584258, "accept_rate": 83, "link": "http://stackoverflow.com/users/4584258/jgloves", "user_type": "registered", "reputation": 469}, "body": "<p>If there was a temporary problem fetching, Nutch should retry the fetch for you three times by default. After that the page is marked as \"gone\" and Nutch will not try to fetch it again for the maxFetchInterval. \n<a href=\"http://wiki.apache.org/nutch/CrawlDatumStates\" rel=\"nofollow\">http://wiki.apache.org/nutch/CrawlDatumStates</a></p>\n\n<p>You can increase the number of retries by changing the <strong>db.fetch.retry.max</strong> property in nutch-default.xml. </p>\n", "question_id": 31020899, "creation_date": 1435592045, "is_accepted": true, "score": 0, "last_activity_date": 1435592475, "answer_id": 31119784}], "question_id": 31020899, "tags": ["apache", "web-crawler", "screen-scraping", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/31020899/nutch-how-to-re-try-transient-errors-and-none-of-the-other-urls", "last_activity_date": 1435592475, "accepted_answer_id": 31119784, "body": "<p>Nutch sometimes gets a SocketTimeout or ConnectionRefused exception for some URLs. How do I ask Nutch to only retry these URLs? If I re-run the \"crawl\" command, it tells me that there is nothing to re-run. This is understandable since \"db.fetch.interval.default\" is set to 30 days. I do not want to change this since this affects even pages that were successful. What I need is a way to only re-crawl failed crawls.</p>\n\n<p>Is there a way to do this? </p>\n\n<p>Added later: I am using Nutch 1.10</p>\n", "creation_date": 1435132650, "score": 0},
{"title": "Nutch 2.2.1 setup with HBase on hadoop cluster", "view_count": 2880, "is_answered": true, "answers": [{"last_edit_date": 1390343213, "owner": {"user_id": 1337352, "accept_rate": 32, "link": "http://stackoverflow.com/users/1337352/babu", "user_type": "registered", "reputation": 623}, "body": "<p>Most helpful for me was this:</p>\n\n<p><a href=\"http://sujitpal.blogspot.cz/2011/01/exploring-nutch-20-hbase-storage.html\" rel=\"nofollow\">http://sujitpal.blogspot.cz/2011/01/exploring-nutch-20-hbase-storage.html</a></p>\n\n<p>Mapping to hbase is defined here <strong>NUTCH_HOME/conf/gora-hbase-mapping.xml</strong>.\nSo if everything is configured correctly, the crawl script should store it for you.</p>\n\n<p>I have the same configuration and had many many problems to get it work, here are some tips:</p>\n\n<p>Tip 1: be careful about table name</p>\n\n<p>I configure also these properties: </p>\n\n<pre><code>&lt;property&gt;\n  &lt;name&gt;storage.schema.webpage&lt;/name&gt;\n  &lt;value&gt;webpage&lt;/value&gt;\n&lt;/property&gt;\n\n&lt;property&gt;\n  &lt;name&gt;storage.crawl.id&lt;/name&gt;\n  &lt;value&gt;babu&lt;/value&gt;\n&lt;/property&gt;\n</code></pre>\n\n<p>This configuration will crawl data into babu_webpage table in hbase when you give argument -crawlId in script write simple 'babu' -> $CRAWL_ID.</p>\n\n<pre><code>$bin/nutch fetch $commonOptions -D fetcher.timelimit.mins=$timeLimitFetch $batchId -crawlId $CRAWL_ID -threads 50\n</code></pre>\n\n<p>Tip 2: if you have bad table name Nutch still write on console success. </p>\n\n<p>Tip 3: how to simple see if there is crawled something in hbase: </p>\n\n<p>go to  ./bin/hbase shell</p>\n\n<pre><code>list\nscan 'babu_webpage'\n</code></pre>\n", "question_id": 21135495, "creation_date": 1390340402, "is_accepted": false, "score": 2, "last_activity_date": 1390343213, "answer_id": 21269662}, {"question_id": 21135495, "owner": {"user_id": 4214037, "accept_rate": 89, "link": "http://stackoverflow.com/users/4214037/32cupo", "user_type": "registered", "reputation": 109}, "body": "<p>I followed this <a href=\"http://saskia-vola.com/nutch-2-2-elasticsearch-1-x-hbase/\" rel=\"nofollow\">tutorial</a> and I ran Nutch with indexing to Elasticsearch with no problem. The relevant infomation for you are six commands on the end of the article. </p>\n\n<pre><code>bin/nutch inject &lt;seed-url-dir&gt;\nbin/nutch generate -topN &lt;n&gt;\nbin/nutch fetch -all\nbin/nutch parse -all\nbin/nutch updatedb\nbin/nutch elasticindex &lt;clustername&gt; -all\n</code></pre>\n\n<p>I created urls directory in NUTCH_HOME, where I placed file seed.txt. In this file are url adresses to crawl. Next five commands I put to the script file and ran it int infinitive loop. If you will follow only your tutorial, you won't have to use last command bin/nutch elasticindex, of course. The topN number I sat up to 50, because with higher number it sometimes got stuck. But it can be only in my case. </p>\n", "creation_date": 1435527861, "is_accepted": false, "score": 1, "last_activity_date": 1435527861, "answer_id": 31105073}], "question_id": 21135495, "tags": ["apache", "hadoop", "web-crawler", "hbase", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/21135495/nutch-2-2-1-setup-with-hbase-on-hadoop-cluster", "last_activity_date": 1435527861, "owner": {"user_id": 1924891, "answer_count": 1, "creation_date": 1356262140, "accept_rate": 60, "view_count": 14, "reputation": 25}, "body": "<p>I have referred this tutorial (<a href=\"http://wiki.apache.org/nutch/Nutch2Tutorial\" rel=\"nofollow\">http://wiki.apache.org/nutch/Nutch2Tutorial</a>) to setup Nutch 2.2.1.with Hbase. I have completed the setup as given in the tutorial, but how to Crawl and store the data into Hbase tables is not mentioned clearly. </p>\n\n<p>Can you please refer me to some relevant links/books for the same?</p>\n", "creation_date": 1389783301, "score": 2},
{"title": "How to change apache Nutch timestamp value", "view_count": 74, "is_answered": false, "answers": [{"question_id": 29340721, "owner": {"user_id": 4584258, "accept_rate": 83, "link": "http://stackoverflow.com/users/4584258/jgloves", "user_type": "registered", "reputation": 469}, "body": "<p>This was a bug. The next fetch time was being mistakenly assigned as the current fetch time.</p>\n\n<p>There is a patch for it:\n<a href=\"https://issues.apache.org/jira/browse/NUTCH-2045\" rel=\"nofollow\">https://issues.apache.org/jira/browse/NUTCH-2045</a></p>\n", "creation_date": 1435258275, "is_accepted": false, "score": 0, "last_activity_date": 1435258275, "answer_id": 31058484}], "question_id": 29340721, "tags": ["solr", "timestamp", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/29340721/how-to-change-apache-nutch-timestamp-value", "last_activity_date": 1435260293, "owner": {"user_id": 3454410, "answer_count": 20, "creation_date": 1395646154, "accept_rate": 48, "view_count": 174, "reputation": 832}, "body": "<p>I am using apache nutch 2.3. Documents are indexed by nutch to solr are ok. But I have to know when a document was indexed in solr. I need time and date. I am getting following format in timestamp for a documents</p>\n\n<pre><code>\"tstamp\": \"2015-04-06T10:11:16.619Z\"\n</code></pre>\n\n<p>If I suppose that first part is the date then this is third month and not fourth. How I can change this issue. </p>\n\n<p>Any suggestion ?</p>\n", "creation_date": 1427701097, "score": 1},
{"title": "Failed to crawl authenticated page with Nutch 2.3", "view_count": 289, "is_answered": false, "answers": [{"question_id": 30051550, "owner": {"user_id": 4865859, "link": "http://stackoverflow.com/users/4865859/deepak-bisht", "user_type": "registered", "reputation": 1}, "body": "<p>I solved the problem.\nAll you have to do is slight change in httpclient-auth.xml</p>\n\n<pre><code> &lt;credentials authMethod=\"formAuth\"\n            loginUrl=\"http://localhost:44444/Account/Login.aspx\"\n            loginFormId=\"ctl01\"\n            loginRedirect=\"true\"&gt;\n &lt;loginPostData&gt;\n</code></pre>\n\n<p>In loginUrl <strong>enter the url which is responsible for the POST request NOT the actual login URL</strong>. You can check it by analyzing the 'network' tab in chrome browser.(Ctrl+shift+i is the shortcut to open developer mode). Don't forget to tick 'preserve log'.  </p>\n\n<p>Happy Coding. :)</p>\n", "creation_date": 1431423216, "is_accepted": false, "score": 0, "last_activity_date": 1431423216, "answer_id": 30186923}], "question_id": 30051550, "tags": ["java", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/30051550/failed-to-crawl-authenticated-page-with-nutch-2-3", "last_activity_date": 1435239724, "owner": {"user_id": 4865859, "view_count": 1, "answer_count": 1, "creation_date": 1430820266, "reputation": 1}, "body": "<p>I am trying to crawl authenticated web page with Apache Nutch 2.3.\nI did the following code changes in Nutch 2.3 source code to support POST based authentication</p>\n\n<ol>\n<li>Changes in source code in file <code>$NUTCH_HOME/src/plugin/protocol-httpclient/src/java/org/apache/nutch/protocol/httpclient/Http.java</code></li>\n<li>Created two new java files on the same package <code>HttpFormAuthConfigurer.java</code> and <code>HttpFormAuthentication.java</code></li>\n<li><p>Copying <code>\"plugin.includes\"</code> property in <code>$NUTCH_HOME/conf/nutch-site.xml</code> </p>\n\n<pre><code>&lt;property&gt;\n  &lt;name&gt;plugin.includes&lt;/name&gt;\n  &lt;value&gt;protocol-httpclient|urlfilter-regex|parse-(html|tika)|index-(basic|anchor)|indexer-elastic|scoring-opic|urlnormalizer-(pass|regex|basic)&lt;/value&gt;\n  &lt;description&gt;Regular expression naming plugin directory names to\n  include.  Any plugin not matching this expression is excluded.\n  In any case you need at least include the nutch-extensionpoints plugin. By\n  default Nutch includes crawling just HTML and plain text via HTTP,\n  and basic indexing and search plugins. In order to use HTTPS please enable\n  protocol-httpclient, but be aware of possible intermittent problems with the\n  underlying commons-httpclient library.\n  &lt;/description&gt;\n&lt;/property&gt;\n</code></pre>\n\n<ol start=\"4\">\n<li><p>Credential implementation in <code>$NUTCH_HOME/conf/httpclient-auth.xml</code></p>\n\n<p>\n     \n     \n       \n       \n     \n     \n       \n     &lt;/additionalPostHeaders>\n     \n       \n     &lt;/removedFormFields-->\n   \n</p></li>\n</ol></li>\n</ol>\n\n<p>I have commented out tag \"additionalPostHeaders\" and \"removedFormFields\"</p>\n\n<p>Along with Nutch 2.3 I am using Hbase-0.94.14 and ElasticSearch-1.5.1.\nI used the following links and discussions to do configuration.</p>\n\n<ul>\n<li><a href=\"https://issues.apache.org/jira/browse/NUTCH-827\" rel=\"nofollow\">https://issues.apache.org/jira/browse/NUTCH-827</a></li>\n<li><a href=\"http://wiki.apache.org/nutch/Nutch2Tutorial\" rel=\"nofollow\">http://wiki.apache.org/nutch/Nutch2Tutorial</a></li>\n</ul>\n\n<p>When I am trying to crawl the authenticated page. It is htiting the login URL and I am getting only login URL in elasticsearch result. It is not crawling the content of authenticated page.</p>\n\n<p>Hadoop.log</p>\n\n<pre><code>2015-05-05 02:55:26,250 DEBUG httpclient.HttpFormAuthentication -\n</code></pre>\n\n<p>Sending 'POST' request to URL : example.com:8080/abc/login.jsp\n2015-05-05 02:55:26,250 DEBUG httpclient.HttpFormAuthentication - Post parameters : [name=Login, value=Login, name=j_password, value=admin1, name=j_username, value=admin]</p>\n\n<pre><code>2015-05-05 02:55:26,250 DEBUG httpclient.HttpFormAuthentication - Response Code : 200\n2015-05-05 02:55:26,250 DEBUG httpclient.HttpFormAuthentication - Response headers : User-Agent: My Nutch Spider/Nutch-2.3\n\n2015-05-05 02:55:26,250 DEBUG httpclient.HttpFormAuthentication - Response headers : Connection: keep-alive\n\n2015-05-05 02:55:26,250 DEBUG httpclient.HttpFormAuthentication - Response headers : Accept-Language: en-us,en-gb,en;q=0.7,*;q=0.3\n\n2015-05-05 02:55:26,250 DEBUG httpclient.HttpFormAuthentication - Response headers : Accept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\n\n2015-05-05 02:55:26,250 DEBUG httpclient.HttpFormAuthentication - Response headers : Content-Type: application/x-www-form-urlencoded\n\n2015-05-05 02:55:26,251 DEBUG httpclient.HttpFormAuthentication - Response headers : Cookie:\n\n2015-05-05 02:55:26,251 DEBUG httpclient.HttpFormAuthentication - Response headers : User-Agent: My Nutch Spider/Nutch-2.3\n\n2015-05-05 02:55:26,251 DEBUG httpclient.HttpFormAuthentication - Response headers : Accept-Language: en-us,en-gb,en;q=0.7,*;q=0.3\n\n2015-05-05 02:55:26,251 DEBUG httpclient.HttpFormAuthentication - Response headers : Accept-Charset: utf-8,ISO-8859-1;q=0.7,*;q=0.7\n\n2015-05-05 02:55:26,251 DEBUG httpclient.HttpFormAuthentication - Response headers : Accept: text/html,application/xml;q=0.9,application/xhtmlxml,text/xml;q=0.9,text/plain;q=0.8,image/png,*/*;q=0.5\n\n2015-05-05 02:55:26,251 DEBUG httpclient.HttpFormAuthentication - Response headers : Accept-Encoding: x-gzip, gzip, deflate\n\n2015-05-05 02:55:26,251 DEBUG httpclient.HttpFormAuthentication - Response headers : Host: example.com:8080\n\n2015-05-05 02:55:26,251 DEBUG httpclient.HttpFormAuthentication - Response headers : Cookie: $Version=0; JSESSIONID=j0ojgF0cRVImcD8sYco75F60jr7ooESeVotAGYXLsv-4CqP8!-231988951; $Path=/abc\n\n2015-05-05 02:55:26,251 DEBUG httpclient.HttpFormAuthentication - Response headers : Content-Length: 51\n\n2015-05-05 02:55:26,399 DEBUG httpclient.HttpFormAuthentication - login post result: &lt;!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\"\n\"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\"&gt;\n2015-05-05 02:55:26,534 INFO  regex.RegexURLNormalizer - can't find rules for scope 'fetcher', using default\n2015-05-05 02:55:26,540 INFO  fetcher.FetcherJob - -finishing thread FetcherThread0, activeThreads=0\n2015-05-05 02:55:29,449 INFO  fetcher.FetcherJob - 0/0 spinwaiting/active, 1 pages, 0 errors, 0.2 0 pages/s, 0 0 kb/s, 0 URLs in 0 queues\n2015-05-05 02:55:29,450 INFO  fetcher.FetcherJob - -activeThreads=0\n2015-05-05 02:55:29,456 WARN  mapred.FileOutputCommitter - Output path is null in cleanup\n2015-05-05 02:55:30,004 INFO  fetcher.FetcherJob - FetcherJob: finished at 2015-05-05 02:55:30, time elapsed: 00:00:07\n2015-05-05 02:55:31,655 INFO  parse.ParserJob - ParserJob: starting at 2015-05-05 02:55:31\n2015-05-05 02:55:31,658 INFO  parse.ParserJob - ParserJob: resuming:    false\n2015-05-05 02:55:31,658 INFO  parse.ParserJob - ParserJob: forced reparse:      false\n2015-05-05 02:55:31,658 INFO  parse.ParserJob - ParserJob: parsing all\n2015-05-05 02:55:32,501 INFO  crawl.SignatureFactory - Using Signature impl: org.apache.nutch.crawl.MD5Signature\n2015-05-05 02:55:33,268 WARN  util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n2015-05-05 02:55:33,680 INFO  crawl.SignatureFactory - Using Signature impl: org.apache.nutch.crawl.MD5Signature\n2015-05-05 02:55:33,760 INFO  parse.ParserJob - Parsing example.com:8080/abc/AUTHENTICATED_PAGE\n2015-05-05 02:55:33,767 WARN  mapred.FileOutputCommitter - Output path is null in cleanup\n2015-05-05 02:55:34,517 INFO  parse.ParserJob - ParserJob: success\n2015-05-05 02:55:34,518 INFO  parse.ParserJob - ParserJob: finished at 2015-05-05 02:55:34, time elapsed: 00:00:02\n2015-05-05 02:55:36,148 INFO  crawl.DbUpdaterJob - DbUpdaterJob: starting at 2015-05-05 02:55:36\n2015-05-05 02:55:36,148 INFO  crawl.DbUpdaterJob - DbUpdaterJob: updatinging all\n2015-05-05 02:55:37,375 WARN  util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n2015-05-05 02:55:37,995 INFO  crawl.FetchScheduleFactory - Using FetchSchedule impl: org.apache.nutch.crawl.DefaultFetchSchedule\n2015-05-05 02:55:37,995 INFO  crawl.AbstractFetchSchedule - defaultInterval=2592000\n2015-05-05 02:55:37,995 INFO  crawl.AbstractFetchSchedule - maxInterval=7776000\n2015-05-05 02:55:38,016 WARN  mapred.FileOutputCommitter - Output path is null in cleanup\n2015-05-05 02:55:38,599 INFO  crawl.DbUpdaterJob - DbUpdaterJob: finished at 2015-05-05 02:55:38, time elapsed: 00:00:02\n2015-05-05 02:55:40,376 INFO  indexer.IndexingJob - IndexingJob: starting\n2015-05-05 02:55:40,602 INFO  basic.BasicIndexingFilter - Maximum title length for indexing set to: 100\n2015-05-05 02:55:40,602 INFO  indexer.IndexingFilters - Adding org.apache.nutch.indexer.basic.BasicIndexingFilter\n2015-05-05 02:55:40,605 INFO  anchor.AnchorIndexingFilter - Anchor deduplication is: off\n2015-05-05 02:55:40,605 INFO  indexer.IndexingFilters - Adding org.apache.nutch.indexer.anchor.AnchorIndexingFilter\n2015-05-05 02:55:41,475 WARN  util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n2015-05-05 02:55:41,912 INFO  indexer.IndexWriters - Adding org.apache.nutch.indexwriter.elastic.ElasticIndexWriter\n2015-05-05 02:55:42,044 INFO  elasticsearch.plugins - [Sigmar] loaded [], sites []\n2015-05-05 02:55:43,284 INFO  basic.BasicIndexingFilter - Maximum title length for indexing set to: 100\n2015-05-05 02:55:43,284 INFO  indexer.IndexingFilters - Adding org.apache.nutch.indexer.basic.BasicIndexingFilter\n2015-05-05 02:55:43,284 INFO  anchor.AnchorIndexingFilter - Anchor deduplication is: off\n2015-05-05 02:55:43,284 INFO  indexer.IndexingFilters - Adding org.apache.nutch.indexer.anchor.AnchorIndexingFilter\n2015-05-05 02:55:43,354 INFO  elastic.ElasticIndexWriter - Processing remaining requests [docs = 0, length = 0, total docs = 0]\n2015-05-05 02:55:43,354 INFO  elastic.ElasticIndexWriter - Processing to finalize last execute\n2015-05-05 02:55:43,388 WARN  mapred.FileOutputCommitter - Output path is null in cleanup\n2015-05-05 02:55:43,746 INFO  indexer.IndexWriters - Adding org.apache.nutch.indexwriter.elastic.ElasticIndexWriter\n2015-05-05 02:55:43,746 INFO  indexer.IndexingJob - Active IndexWriters :\nElasticIndexWriter\n        elastic.cluster : elastic prefix cluster\n        elastic.host : hostname\n        elastic.port : port  (default 9300)\n        elastic.index : elastic index command\n        elastic.max.bulk.docs : elastic bulk index doc counts. (default 250)\n        elastic.max.bulk.size : elastic bulk index length. (default 2500500 ~2.5MB)\n2015-05-05 02:55:43,752 INFO  elasticsearch.plugins - [Valinor] loaded [], sites []\n2015-05-05 02:55:43,804 INFO  indexer.IndexingJob - IndexingJob: done.\n</code></pre>\n\n<p>How can i crawl the authenticated page? I feel there is some problem with the session creation. \nPlease help. Thanks in advance.</p>\n", "creation_date": 1430824744, "score": 0},
{"title": "Nutch2.x run every url every time", "view_count": 57, "owner": {"user_id": 1959771, "view_count": 1, "answer_count": 2, "creation_date": 1357690948, "reputation": 1}, "is_answered": true, "answers": [{"question_id": 31018843, "owner": {"user_id": 1965370, "link": "http://stackoverflow.com/users/1965370/do-do", "user_type": "registered", "reputation": 389}, "body": "<p>After fetching a website, Nutch marks the URL of the website as a FETCHED URL, and it will not crawl the URL again in the next crawling round. By default, Nutch will re-crawl after 30 days. You can change the default number of seconds between re-fetches of a page by modifying the <strong>db.fetch.interval.default</strong> property.</p>\n\n<p>Hope this helps,</p>\n\n<p>Le Quoc Do</p>\n", "creation_date": 1435140561, "is_accepted": true, "score": 0, "last_activity_date": 1435140561, "answer_id": 31023748}], "question_id": 31018843, "tags": ["nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/31018843/nutch2-x-run-every-url-every-time", "last_activity_date": 1435234643, "accepted_answer_id": 31023748, "body": "<p>IN Nutch2.2.1, when i run nutch every time, it will crawl all urls include i have already crawled. </p>\n\n<p>I want one url only be crawled one time no matter how times nutch run, so, how to configure it ? </p>\n", "creation_date": 1435126124, "score": 0},
{"title": "inject runtime exception nutch 2.3", "view_count": 241, "is_answered": false, "answers": [{"question_id": 30885453, "owner": {"user_id": 1965370, "link": "http://stackoverflow.com/users/1965370/do-do", "user_type": "registered", "reputation": 389}, "body": "<p>It seems that Nutch cannot inject URL to 'webpage' table. First, please check the configuration in gora-hbase. In the case the configuration is correct, you should delete the hbase data directory and start again.</p>\n\n<p>Hope this helps</p>\n", "creation_date": 1435141395, "is_accepted": false, "score": 0, "last_activity_date": 1435141395, "answer_id": 31024067}], "question_id": 30885453, "tags": ["nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/30885453/inject-runtime-exception-nutch-2-3", "last_activity_date": 1435141395, "owner": {"user_id": 4753158, "answer_count": 0, "creation_date": 1428292757, "accept_rate": 0, "view_count": 6, "reputation": 11}, "body": "<p>I get stuck with setup Nutch 2.3 with hbase 0.94:</p>\n\n<pre><code>fx@fx:~$ $NUTCH_HOME/runtime/local/bin/nutch inject file:///home/fx/Abivin/apache-nutch-2.3/seed/urls.txt\nInjectorJob: starting at 2015-06-17 14:46:35\nInjectorJob: Injecting urlDir: file:/home/fx/Abivin/apache-nutch-2.3/seed/urls.txt\nInjectorJob: Using class org.apache.gora.memory.store.MemStore as the Gora storage class.\nInjectorJob: java.lang.RuntimeException: job failed: name=inject file:/home/fx/Abivin/apache-nutch-2.3/seed/urls.txt, jobid=job_local1999341506_0001\n    at org.apache.nutch.util.NutchJob.waitForCompletion(NutchJob.java:54)\n    at org.apache.nutch.crawl.InjectorJob.run(InjectorJob.java:231)\n    at org.apache.nutch.crawl.InjectorJob.inject(InjectorJob.java:252)\n    at org.apache.nutch.crawl.InjectorJob.run(InjectorJob.java:275)\n    at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n    at org.apache.nutch.crawl.InjectorJob.main(InjectorJob.java:284)\n</code></pre>\n\n<p>when seed/urls.txt stores urls. I've searched many similar errors but still get stuck with this. Please give me some ideas to resolve. Thanks</p>\n", "creation_date": 1434527529, "score": 0},
{"title": "Apache Nutch REST api", "view_count": 530, "owner": {"user_id": 4555234, "answer_count": 6, "creation_date": 1423665405, "accept_rate": 64, "view_count": 10, "reputation": 69}, "is_answered": true, "answers": [{"question_id": 30919467, "owner": {"user_id": 4584258, "accept_rate": 83, "link": "http://stackoverflow.com/users/4584258/jgloves", "user_type": "registered", "reputation": 469}, "body": "<p>At the time of this posting, the REST API is not yet complete. A much more detailed document exists, though it's still not comprehensive. It is linked to in the following email from the user mailing list (which you might want to consider joining):</p>\n\n<p><a href=\"http://www.mail-archive.com/user%40nutch.apache.org/msg13652.html\" rel=\"nofollow\">http://www.mail-archive.com/user%40nutch.apache.org/msg13652.html</a></p>\n\n<p>But to answer your question about the seedlist, you can create the seedlist through REST, or you can use the argument \"seedDir\"</p>\n\n<pre><code>{\n    \"args\":{\n        \"seedDir\":\"/path/to/seed/directory\"\n    },\n    \"confId\":\"default\",\n    \"crawlId\":\"sample-crawl-01\",\n    \"type\":\"INJECT\"\n}\n</code></pre>\n", "creation_date": 1434745066, "is_accepted": true, "score": 3, "last_activity_date": 1434745066, "answer_id": 30946603}], "question_id": 30919467, "tags": ["api", "rest", "web-crawler", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/30919467/apache-nutch-rest-api", "last_activity_date": 1435080681, "accepted_answer_id": 30946603, "body": "<p>I'm trying to launch a crawl via the rest api. A crawl starts with injecting urls. Using a chrome developer tool \"Advanced Rest Client\" I'm trying to build this POST payload up but the response I get is a 400 Bad Request.</p>\n\n<p>POST - <code>http://localhost:8081/job/create</code></p>\n\n<p>Payload</p>\n\n<pre><code>{\n  \"crawl-id\":\"crawl-01\",\n  \"type\":\"INJECT\",\n  \"config-id\":\"default\",\n  \"args\":{ \"path/to/seedlist/directory\"}\n}\n</code></pre>\n\n<p>My problem is in the args, I think more is needed but I'm not sure. In the NutchRESTAPI page this is the sample it gives for creating a job.</p>\n\n<pre><code>POST /job/create\n   {\n      \"crawlId\":\"crawl-01\",\n      \"type\":\"FETCH\",\n      \"confId\":\"default\",\n      \"args\":{\"someParam\":\"someValue\"}\n   }\n\nPOST /job/create\n   {\n      \"crawlId\":\"crawl-01\",\n      \"jobClassName\":\"org.apache.nutch.fetcher.FetcherJob\"\n      \"confId\":\"default\",\n      \"args\":{\"someParam\":\"someValue\"}\n   }\n</code></pre>\n\n<p>I'm not sure what param or value to give each of the commands to complete a job. (eg. Inject, Generate, Fetch, Parse, and UpdateDb) Can someone clear this up? How do I tell the api where to look for the seedlist at?</p>\n\n<p><strong>UPDATE</strong></p>\n\n<p>When trying to complete the Generate command I came into a classException error where the value for the topN key is to be of type long but the api reads it as either a string or an int. I found a fix that is supposed to included in the 2.3.1 release (release date: TBA) and applied it and recompiled my code. It can now work.</p>\n", "creation_date": 1434640816, "score": 1},
{"title": "Integrate Solr-5.2.1 with crawled data from Nutch?", "view_count": 639, "is_answered": false, "answers": [{"question_id": 30987830, "owner": {"user_id": 1727204, "accept_rate": 43, "link": "http://stackoverflow.com/users/1727204/ali-n", "user_type": "registered", "reputation": 524}, "body": "<p>AFAIK Solr 5.x uses managed-schema in default which will be created on the fly based on the input documents. However you can copy your schema.xml file content to the <code>${APACHE_SOLR_HOME}/solr/server/solr/$CORE_NAME/conf/managed-schema</code>. But before copy your schema make sure that it is in 5.x schema format. (Some of the old schema components might have changed)</p>\n", "creation_date": 1435071103, "is_accepted": false, "score": 0, "last_activity_date": 1435071103, "answer_id": 31006171}], "question_id": 30987830, "tags": ["solr", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/30987830/integrate-solr-5-2-1-with-crawled-data-from-nutch", "last_activity_date": 1435071103, "owner": {"user_id": 2400582, "answer_count": 0, "creation_date": 1369028906, "accept_rate": 40, "view_count": 5, "reputation": 62}, "body": "<p>Tutorial says:\nNutch already created crawl data from the seed URL(s). Below are the steps to delegate searching to Solr for links to be searchable:</p>\n\n<pre><code>Backup the original Solr example schema.xml:\nmv ${APACHE_SOLR_HOME}/example/solr/collection1/conf/schema.xml ${APACHE_SOLR_HOME}/example/solr/collection1/conf/schema.xml.org\n</code></pre>\n\n<p>But the problem is there is no such directory like /example/solr/collection1/conf, in which directory i will find this schema.xml file? or which schema.xml file to replace?</p>\n", "creation_date": 1434999889, "score": 2},
{"title": "Internal Server error while adding documents Solr", "view_count": 106, "is_answered": false, "question_id": 30951163, "tags": ["java", "apache", "hadoop", "solr", "nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/30951163/internal-server-error-while-adding-documents-solr", "last_activity_date": 1435043445, "owner": {"user_id": 4899457, "answer_count": 0, "creation_date": 1431599621, "view_count": 5, "location": "Mumbai, India", "reputation": 10}, "body": "<p>I use urls to crawl sites and get data from those sites.....I use solr 3.4.0 and nutch 1.9.<br>\n    It was working fine but now suddenly from last week I am getting this error:</p>\n\n<pre><code> 2015-06-18 18:32:49,718 INFO  indexer.IndexingFilters - Adding org.apache.nutch.indexer.anchor.AnchorIndexingFilter\n2015-06-18 18:32:53,531 INFO  solr.SolrMappingReader - source: content dest: content\n2015-06-18 18:32:53,531 INFO  solr.SolrMappingReader - source: site dest: site\n2015-06-18 18:32:53,531 INFO  solr.SolrMappingReader - source: title dest: title\n2015-06-18 18:32:53,531 INFO  solr.SolrMappingReader - source: host dest: host\n2015-06-18 18:32:53,531 INFO  solr.SolrMappingReader - source: segment dest: segment\n2015-06-18 18:32:53,531 INFO  solr.SolrMappingReader - source: boost dest: boost\n2015-06-18 18:32:53,531 INFO  solr.SolrMappingReader - source: digest dest: digest\n2015-06-18 18:32:53,531 INFO  solr.SolrMappingReader - source: tstamp dest: tstamp\n2015-06-18 18:32:53,531 INFO  solr.SolrMappingReader - source: url dest: id\n2015-06-18 18:32:53,531 INFO  solr.SolrMappingReader - source: url dest: url\n2015-06-18 18:32:54,484 INFO  solr.SolrWriter - Adding 1000 documents\n2015-06-18 18:34:34,156 WARN  mapred.LocalJobRunner - job_local_0030\norg.apache.solr.common.SolrException: Internal Server Error\n\nInternal Server Error\n\nrequest: http://host IP:port/solr/news/update?wt=javabin&amp;version=2\n    at org.apache.solr.client.solrj.impl.CommonsHttpSolrServer.request(CommonsHttpSolrServer.java:430)\n    at org.apache.solr.client.solrj.impl.CommonsHttpSolrServer.request(CommonsHttpSolrServer.java:244)\n    at org.apache.solr.client.solrj.request.AbstractUpdateRequest.process(AbstractUpdateRequest.java:105)\n    at org.apache.solr.client.solrj.SolrServer.add(SolrServer.java:49)\n    at org.apache.nutch.indexer.solr.SolrWriter.write(SolrWriter.java:81)\n    at org.apache.nutch.indexer.IndexerOutputFormat$1.write(IndexerOutputFormat.java:54)\n    at org.apache.nutch.indexer.IndexerOutputFormat$1.write(IndexerOutputFormat.java:44)\n    at org.apache.hadoop.mapred.ReduceTask$3.collect(ReduceTask.java:440)\n    at org.apache.nutch.indexer.IndexerMapReduce.reduce(IndexerMapReduce.java:166)\n    at org.apache.nutch.indexer.IndexerMapReduce.reduce(IndexerMapReduce.java:51)\n    at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:463)\n    at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:411)\n    at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:216)\n2015-06-18 18:34:35,062 ERROR solr.SolrIndexer - java.io.IOException: Job failed!\n2015-06-18 18:34:35,140 INFO  solr.SolrDeleteDuplicates - SolrDeleteDuplicates: starting at 2015-06-18 18:34:35\n2015-06-18 18:34:35,140 INFO  solr.SolrDeleteDuplicates - SolrDeleteDuplicates: Solr url: http://Host IP:Port/solr/news\n2015-06-18 18:36:52,718 WARN  mapred.LocalJobRunner - job_local_0031\njava.lang.NullPointerException\n    at org.apache.hadoop.io.Text.encode(Text.java:388)\n    at org.apache.hadoop.io.Text.set(Text.java:178)\n    at org.apache.nutch.indexer.solr.SolrDeleteDuplicates$SolrInputFormat$1.next(SolrDeleteDuplicates.java:270)\n    at org.apache.nutch.indexer.solr.SolrDeleteDuplicates$SolrInputFormat$1.next(SolrDeleteDuplicates.java:241)\n    at org.apache.hadoop.mapred.MapTask$TrackedRecordReader.moveToNext(MapTask.java:192)\n    at org.apache.hadoop.mapred.MapTask$TrackedRecordReader.next(MapTask.java:176)\n    at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:48)\n    at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:358)\n    at org.apache.hadoop.mapred.MapTask.run(MapTask.java:307)\n    at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:177)\n</code></pre>\n\n<p>Anybody who can help me remove this error.Thanks in advance.</p>\n\n<p>The solr log gives me this error:org.apache.solr.update.SolrIndexWriter finalize\nSEVERE: SolrIndexWriter was not closed prior to finalize(), indicates a bug -- POSSIBLE RESOURCE LEAK!!!</p>\n\n<p>org.apache.solr.common.SolrException log\nSEVERE: org.apache.lucene.store.LockObtainFailedException: Lock obtain timed out: NativeFSLock</p>\n", "creation_date": 1434783022, "score": 0},
{"title": "How to config Nutch to crawl only the URLs in seeklist? (no crawl back need)", "view_count": 694, "is_answered": true, "answers": [{"question_id": 14250226, "owner": {"user_id": 950178, "link": "http://stackoverflow.com/users/950178/jayendra", "user_type": "registered", "reputation": 34896}, "body": "<p>If you are using the <a href=\"http://wiki.apache.org/nutch/NutchTutorial#A3.1_Using_the_Crawl_Command\" rel=\"nofollow\">crawl command</a> check for the depth parameter.</p>\n\n<blockquote>\n  <p>-depth depth indicates the link depth from the root page that should be crawled.</p>\n</blockquote>\n\n<p>Using this you can control what level of depth you need Nutch to crawl. Having a value of 1 probably would limit it to the base page only.</p>\n", "creation_date": 1357789551, "is_accepted": false, "score": 0, "last_activity_date": 1357789551, "answer_id": 14250518}, {"question_id": 14250226, "owner": {"user_id": 565296, "link": "http://stackoverflow.com/users/565296/umar", "user_type": "registered", "reputation": 2041}, "body": "<p>In your nutch-site.xml configuration , set the \"db.ignore.external.links\" property to true.</p>\n\n<p>This will ignore any urls to domains outside the injected list.</p>\n", "creation_date": 1357798721, "is_accepted": false, "score": 2, "last_activity_date": 1357798721, "answer_id": 14251912}], "question_id": 14250226, "tags": ["solr", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/14250226/how-to-config-nutch-to-crawl-only-the-urls-in-seeklist-no-crawl-back-need", "last_activity_date": 1434996454, "owner": {"user_id": 1773304, "answer_count": 0, "creation_date": 1351146931, "accept_rate": 0, "view_count": 3, "reputation": 16}, "body": "<p>I have a url seedlist contains more than 100000 urls. I know that nutch will crawl not only the urls in the seedlist but also any url links found inside the websites. However, I would like to know is there any way to stop this behavior ? So that only the urls specified in the seedlist are needed to be crawled.</p>\n", "creation_date": 1357787261, "score": 0},
{"title": "Can&#39;t get Nutch to add new documents below a certain level", "view_count": 263, "is_answered": false, "answers": [{"question_id": 25733425, "owner": {"user_id": 994000, "link": "http://stackoverflow.com/users/994000/betolink", "user_type": "registered", "reputation": 83}, "body": "<p>The documents you are missing are not indexed probably because the \n<code>db.fetch.interval.default</code>\nparameter in nutch-site.xml value is 30 days. Nutch is not going to see if there is anything new in /DC-10 for a while. If you set that to be </p>\n\n<blockquote>\n<pre><code>&lt;property&gt;\n   &lt;name&gt;db.fetch.interval.default&lt;/name&gt;\n   &lt;value&gt;86400&lt;/value&gt;\n   &lt;description&gt;Number of seconds between re-fetches of a page. (86400 = 1 day)\n   &lt;/description&gt;\n&lt;/property&gt;\n</code></pre>\n</blockquote>\n\n<p>Then you will recrawl every day. Also, I think the nutch users mailing list is more active than stackoverflow for Nutch related stuff.</p>\n", "creation_date": 1434835779, "is_accepted": false, "score": 0, "last_activity_date": 1434835779, "answer_id": 30959094}], "question_id": 25733425, "tags": ["nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/25733425/cant-get-nutch-to-add-new-documents-below-a-certain-level", "last_activity_date": 1434835779, "owner": {"user_id": 2177934, "answer_count": 3, "creation_date": 1363465127, "accept_rate": 80, "view_count": 5, "reputation": 43}, "body": "<p>I have a web site serving a series of documents (pdf's) and am using Nutch 1.8 to index them in solr.  The base url is</p>\n\n<pre><code>http://localhost/\n</code></pre>\n\n<p>and the documents are stored in a series of directories in the directory </p>\n\n<pre><code>http://localhost/doccontrol/\n</code></pre>\n\n<p>, e.g.</p>\n\n<p>/\n|_doccontrol\n    |_DC-10 Incoming Correspondence\n    |_DC-11 Outgoing Correspondence</p>\n\n<p>If when I first run nutch the folders DC-10 and DC-11 contain all the files to be indexed then nutch crawls everything without a problem - GOOD :-)</p>\n\n<p>If I add a new folder or documents to the root or doccontrol folders then the next time nutch runs it crawls all the new files and indexes them - GOOD :-)</p>\n\n<p>However any new files that are added to the DC-10 or DC-11 directories are not indexed with nutch's output as follows (summarised):</p>\n\n<pre><code>Injector: starting at 2014-08-29 15:19:59\nInjector: crawlDb: crawl/crawldb\nInjector: urlDir: urls\nInjector: Converting injected urls to crawl db entries.\nInjector: total number of urls rejected by filters: 0\nInjector: total number of urls injected after normalization and filtering: 1\nInjector: Merging injected urls into crawl db.\nInjector: overwrite: true\nInjector: update: false\nInjector: finished at 2014-08-29 15:20:02, elapsed: 00:00:02\nFri Aug 29 15:20:02 EST 2014 : Iteration 1 of 4\nGenerating a new segment\nGenerator: starting at 2014-08-29 15:20:02\nGenerator: Selecting best-scoring urls due for fetch.\nGenerator: filtering: false\nGenerator: normalizing: true\nGenerator: topN: 50000\nGenerator: Partitioning selected urls for politeness.\nGenerator: segment: crawl/segments/20140829152005\nGenerator: finished at 2014-08-29 15:20:06, elapsed: 00:00:03\nOperating on segment : 20140829152005\nFetching : 20140829152005\nFetcher: Your 'http.agent.name' value should be listed first in 'http.robots.agents' property.\nFetcher: starting at 2014-08-29 15:20:06\nFetcher: segment: crawl/segments/20140829152005\nFetcher Timelimit set for : 1409354406733\nUsing queue mode : byHost\nFetcher: threads: 50\nFetcher: time-out divisor: 2\nQueueFeeder finished: total 1 records + hit by time limit :0\nUsing queue mode : byHost\nUsing queue mode : byHost\nUsing queue mode : byHost\n-finishing thread FetcherThread, activeThreads=1\nfetching http://ws0895/doccontrol/ (queue crawl delay=5000ms)\n-finishing thread FetcherThread, activeThreads=1\nUsing queue mode : byHost\nUsing queue mode : byHost\n-finishing thread FetcherThread, activeThreads=1\nUsing queue mode : byHost\nUsing queue mode : byHost\n-finishing thread FetcherThread, activeThreads=1\n.\n.\n.\n-finishing thread FetcherThread, activeThreads=1\nUsing queue mode : byHost\nFetcher: throughput threshold: -1\n-finishing thread FetcherThread, activeThreads=1\nFetcher: throughput threshold retries: 5\n-finishing thread FetcherThread, activeThreads=0\n-activeThreads=0, spinWaiting=0, fetchQueues.totalSize=0\n-activeThreads=0\nFetcher: finished at 2014-08-29 15:20:09, elapsed: 00:00:02\nParsing : 20140829152005\nParseSegment: starting at 2014-08-29 15:20:09\nParseSegment: segment: crawl/segments/20140829152005\nParsed (3ms):http://ws0895/doccontrol/\nParseSegment: finished at 2014-08-29 15:20:10, elapsed: 00:00:01\nCrawlDB update\nCrawlDb update: starting at 2014-08-29 15:20:11\nCrawlDb update: db: crawl/crawldb\nCrawlDb update: segments: [crawl/segments/20140829152005]\nCrawlDb update: additions allowed: true\nCrawlDb update: URL normalizing: false\nCrawlDb update: URL filtering: false\nCrawlDb update: 404 purging: false\nCrawlDb update: Merging segment data into db.\nCrawlDb update: finished at 2014-08-29 15:20:12, elapsed: 00:00:01\nLink inversion\nLinkDb: starting at 2014-08-29 15:20:13\nLinkDb: linkdb: crawl/linkdb\nLinkDb: URL normalize: true\nLinkDb: URL filter: true\nLinkDb: internal links will be ignored.\nLinkDb: adding segment: crawl/segments/20140829152005\nLinkDb: merging with existing linkdb: crawl/linkdb\nLinkDb: finished at 2014-08-29 15:20:15, elapsed: 00:00:02\nDedup on crawldb\nIndexing 20140829152005 on SOLR index -&gt; http://localhost:8983/solr/collection1\nIndexer: starting at 2014-08-29 15:20:19\nIndexer: deleting gone documents: false\nIndexer: URL filtering: false\nIndexer: URL normalizing: false\nActive IndexWriters :\nSOLRIndexWriter\n        solr.server.url : URL of the SOLR instance (mandatory)\n        solr.commit.size : buffer size when sending to SOLR (default 1000)\n        solr.mapping.file : name of the mapping file for fields (default solrindex-mapping.xml)\n        solr.auth : use authentication (default false)\n        solr.auth.username : use authentication (default false)\n        solr.auth : username for authentication\n        solr.auth.password : password for authentication\n\n\nIndexer: finished at 2014-08-29 15:20:20, elapsed: 00:00:01\nCleanup on SOLR index -&gt; http://localhost:8983/solr/collection1\nFri Aug 29 15:20:22 EST 2014 : Iteration 2 of 4\nGenerating a new segment\nGenerator: starting at 2014-08-29 15:20:23\nGenerator: Selecting best-scoring urls due for fetch.\nGenerator: filtering: false\nGenerator: normalizing: true\nGenerator: topN: 50000\nGenerator: 0 records selected for fetching, exiting ...\n</code></pre>\n\n<p>BAD - :-(</p>\n\n<p>What I'd like nutch to do is to index any newly added docs whatever level they were added at.</p>\n\n<p>My nutch command is as follows:</p>\n\n<pre><code>bin/crawl urls crawl http://localhost:8983/solr/collection1 4\n</code></pre>\n\n<p>My nutch-site.xml contains:</p>\n\n<pre><code>&lt;property&gt;\n  &lt;name&gt;db.update.additions.allowed&lt;/name&gt;\n  &lt;value&gt;true&lt;/value&gt;\n  &lt;description&gt;If true, updatedb will add newly discovered URLs, if false\n  only already existing URLs in the CrawlDb will be updated and no new\n  URLs will be added.\n  &lt;/description&gt;\n &lt;/property&gt;\n &lt;name&gt;db.max.outlinks.per.page&lt;/name&gt;\n  &lt;value&gt;-1&lt;/value&gt;\n  &lt;description&gt;The maximum number of outlinks that we'll process for a page.\n  If this value is nonnegative (&gt;=0), at most db.max.outlinks.per.page outlinks\n  will be processed for a page; otherwise, all outlinks will be processed.\n  &lt;/description&gt;\n &lt;/property&gt;\n &lt;property&gt;\n  &lt;name&gt;db.injector.overwrite&lt;/name&gt;\n  &lt;value&gt;true&lt;/value&gt;\n  &lt;description&gt;Whether existing records in the CrawlDB will be overwritten\n  by injected records.\n  &lt;/description&gt;\n &lt;/property&gt;\n &lt;property&gt;\n  &lt;name&gt;db.fetch.schedule.class&lt;/name&gt;\n  &lt;value&gt;org.apache.nutch.crawl.AdaptiveFetchSchedule&lt;/value&gt;\n  &lt;description&gt;The implementation of fetch schedule. DefaultFetchSchedule simply\n  adds the original fetchInterval to the last fetch time, regardless of\n  page changes.&lt;/description&gt;\n &lt;/property&gt;\n\n &lt;property&gt;\n  &lt;name&gt;db.fetch.schedule.adaptive.min_interval&lt;/name&gt;\n  &lt;value&gt;86400.0&lt;/value&gt;\n  &lt;description&gt;Minimum fetchInterval, in seconds.&lt;/description&gt;\n &lt;/property&gt;\n  &lt;property&gt;\n  &lt;name&gt;db.fetch.interval.default&lt;/name&gt;\n  &lt;value&gt;1209600&lt;/value&gt;\n  &lt;description&gt;The default number of seconds between re-fetches of a page (14 days).\n  &lt;/description&gt;\n &lt;/property&gt;\n</code></pre>\n\n<p>Is what I am trying to do (recrawl any newly added documents at any level) impossible?</p>\n\n<p>Or (more likely) am I missing something in the config?</p>\n\n<p>Can anyone point me in the right direction?</p>\n\n<p>Many thanks</p>\n\n<p>Paul</p>\n", "creation_date": 1410211835, "score": 0},
{"title": "Error when running Apache Nutch application from JAR", "view_count": 127, "is_answered": false, "answers": [{"question_id": 20217269, "owner": {"user_id": 582789, "link": "http://stackoverflow.com/users/582789/alfonso-nishikawa", "user_type": "registered", "reputation": 1263}, "body": "<p>Onejar is quite complicate to use as Hadoop job. Try <a href=\"http://maven.apache.org/plugins/maven-shade-plugin/\" rel=\"nofollow\">shade</a>; it becomes an ugly package but much probably will work.\nPacking Nutch with your applications is quite complicated because of the dependencies. In Hadoop one possibility is put them in <code>/lib</code> inside your <code>.jar</code>, but you will face more problems for sure.</p>\n", "creation_date": 1385470853, "is_accepted": false, "score": 0, "last_activity_date": 1385470853, "answer_id": 20217816}], "question_id": 20217269, "tags": ["java", "maven", "jar", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/20217269/error-when-running-apache-nutch-application-from-jar", "last_activity_date": 1434771169, "owner": {"user_id": 921193, "answer_count": 20, "creation_date": 1314779230, "accept_rate": 62, "view_count": 874, "reputation": 2600}, "body": "<p>I use <code>onejar-maven-plugin</code> to package Apache Nutch application. </p>\n\n<p>Application works fine when I run it from IDE. </p>\n\n<p><code>onejar-maven-plugin</code> packages fine, include all dependent JARs, without any errors, but when I try to launch this JAR from command line I get an exception: </p>\n\n<blockquote>\n  <p>java.lang.RuntimeException: java.io.FileNotFoundException:\n  \\my-jar-0.0.1-S NAPSHOT.one-jar.jar\n          at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:\n  1243)\n          at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java\n  :1107)\n          at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:1053\n  )\n          at org.apache.hadoop.conf.Configuration.set(Configuration.java:420)\n          at org.apache.nutch.util.NutchConfiguration.setUUID(NutchConfiguration.j\n  ava:41)\n          at org.apache.nutch.util.NutchConfiguration.create(NutchConfiguration.ja\n  va:73)</p>\n</blockquote>\n\n<p>On this lines:</p>\n\n<pre><code>Properties p = new Properties();    \nconf = NutchConfiguration.create(false, p); // here exception\n</code></pre>\n\n<p>Where can be problem? </p>\n", "creation_date": 1385469347, "score": 0},
{"title": "Nutch 2.3 + Elasticsearch / results not visualizing in Kibana", "view_count": 310, "is_answered": false, "question_id": 30876772, "tags": ["elasticsearch", "nutch", "kibana-4"], "answer_count": 0, "link": "http://stackoverflow.com/questions/30876772/nutch-2-3-elasticsearch-results-not-visualizing-in-kibana", "last_activity_date": 1434483894, "owner": {"age": 35, "answer_count": 50, "creation_date": 1421703565, "user_id": 4471711, "accept_rate": 91, "view_count": 62, "location": "Virginia", "reputation": 1637}, "body": "<p>FYI, this was cross-posted to the Apache Nutch mailing list.</p>\n\n<p>I'm really not sure where the issue lies with this problem, whether it's a Nutch problem, Kibana or Elasticsearch.  I'm using Nutch 2.3, HBase 0.94.14 and Elasticsearch 1.6 with Kibana 4.1.0 to crawl, archive and index.</p>\n\n<p>I primarily followed the below tutorial, with the only exception being the upgrade to ES 1.6 from the tutorial's version of 1.4 (which I am now wondering if that's a problem).</p>\n\n<p><a href=\"https://gist.github.com/xrstf/b48a970098a8e76943b9\" rel=\"nofollow\">https://gist.github.com/xrstf/b48a970098a8e76943b9</a></p>\n\n<p>Following this tutorial, I am using the /bin/nutch script.</p>\n\n<p>Most everything works; Nutch follows my seed URL's, HBase stores the downloads and Elasticsearch seems to be indexing the content, however I can't get Kibana to visualize the content coming from Nutch.  Kibana recognizes the index and its fields, however shows no content.  I've loaded the index in Kibana with and without time-based events to no avail.</p>\n\n<p>I have other indexes and 'types' in that Elasticsearch instance which Kibana can visualize AND I can query Elasticsearch with cURL and get the nutch results just fine, I just can't get Kibana to visualize the specific content from Nutch.</p>\n\n<p>I've tried two different ES + Kibana setups and just redirected the Nutch indexing output and am having the same problem on both.  I have also tried deleting the index and starting over, creating the index first and then running 'nutch index -all' and trying a clean Elasticsearch / Kibana install.</p>\n\n<p>I even went so far as to deploy Elasticsearch 1.4, however that requires downgrading Kibana to v3 and I am having difficulty getting that to work, but I have confirmed (again) that the content is IN Elasticsearch via cURL.</p>\n\n<p>My guess is there is something about the different in the ES version, though if there were a problem with that, wouldn't the Transport Client simply fail on inserting?</p>\n\n<p>Below are the logs from Kibana, which doesn't appear to show anything interesting.</p>\n\n<pre><code>{\n  \"name\": \"Kibana\",\n  \"hostname\": \"VirtualBeast\",\n  \"pid\": 6695,\n  \"level\": 30,\n  \"req\": {\n    \"method\": \"POST\",\n    \"url\": \"\\/elasticsearch\\/_msearch?timeout=0&amp;ignore_unavailable=true&amp;preference=1434483458287\",\n    \"headers\": {\n      \"host\": \"localhost:5601\",\n      \"connection\": \"keep-alive\",\n      \"content-length\": \"732\",\n      \"accept\": \"application\\/json, text\\/plain, *\\/*\",\n      \"origin\": \"http:\\/\\/localhost:5601\",\n      \"user-agent\": \"Mozilla\\/5.0 (X11; Linux x86_64) AppleWebKit\\/537.36 (KHTML, like Gecko) Chrome\\/43.0.2357.125 Safari\\/537.36\",\n      \"content-type\": \"application\\/json;charset=UTF-8\",\n      \"referer\": \"http:\\/\\/localhost:5601\\/\",\n      \"accept-encoding\": \"gzip, deflate\",\n      \"accept-language\": \"en-US,en;q=0.8\"\n    },\n    \"remoteAddress\": \"127.0.0.1\",\n    \"remotePort\": 51632\n  },\n  \"res\": {\n    \"statusCode\": 200,\n    \"responseTime\": 12,\n    \"contentLength\": 4992\n  },\n  \"msg\": \"POST \\/_msearch?timeout=0&amp;ignore_unavailable=true&amp;preference=1434483458287 200 - 12ms\",\n  \"time\": \"2015-06-16T19:39:57.372Z\",\n  \"v\": 0\n}\n</code></pre>\n\n<p>Any help would be appreciated, do I need to upgrade the Indexer to match the Elasticsearch version?</p>\n\n<p>Thanks! </p>\n", "creation_date": 1434483894, "score": 2},
{"title": "java.lang.Exception: java.lang.IncompatibleClassChangeError: Found interface org.apache.hadoop.mapreduce.TaskAttemptContext, but class was expected", "view_count": 2002, "is_answered": false, "answers": [{"question_id": 23965684, "owner": {"user_id": 1586965, "accept_rate": 76, "link": "http://stackoverflow.com/users/1586965/samthebest", "user_type": "registered", "reputation": 11725}, "body": "<p>This will be some kind of horrible mismatching jars problem.  Make sure the jar you have built is using the same coordinates as the jars on the cluster - particularly the versions and distros of hadoop.  There is no easy way to do this, you'll have to work closely with your DevOps/sysadm.</p>\n", "creation_date": 1401620783, "is_accepted": false, "score": 0, "last_activity_date": 1401620783, "answer_id": 23979191}], "question_id": 23965684, "tags": ["hadoop", "mapreduce", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/23965684/java-lang-exception-java-lang-incompatibleclasschangeerror-found-interface-org", "last_activity_date": 1434408762, "owner": {"user_id": 3693269, "view_count": 0, "answer_count": 0, "creation_date": 1401500884, "reputation": 16}, "body": "<p>I'm getting this exception using Hadoop 2.4.0 and Nutch 2.2.</p>\n\n<p>When I attempt to run this command:</p>\n\n<pre><code>./hadoop jar apache-nutch-2.2.1.job org.apache.nutch.crawl.Crawler urls -solr //:8983 -depth 2\n</code></pre>\n\n<p>I get:</p>\n\n<pre><code>Java.lang.Exception: java.lang.IncompatibleClassChangeError: Found interface org.apache.hadoop.mapreduce.TaskAttemptContext, but class was expected\n    at org.apache.hadoop.mapred.LocalJobRunner$Job.runTasks(LocalJobRunner.java:462)\n    at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:522)\nCaused by: java.lang.IncompatibleClassChangeError: Found interface org.apache.hadoop.mapreduce.TaskAttemptContext, but class was expected\n    at org.apache.gora.mapreduce.GoraOutputFormat.getRecordWriter(GoraOutputFormat.java:83)\n    at org.apache.hadoop.mapred.MapTask$NewDirectOutputCollector.&lt;init&gt;(MapTask.java:624)\n    at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:744)\n    at org.apache.hadoop.mapred.MapTask.run(MapTask.java:340)\n    at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:243)\n    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)\n    at java.util.concurrent.FutureTask.run(FutureTask.java:262)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n    at java.lang.Thread.run(Thread.java:745)\n14/05/30 02:16:20 INFO mapreduce.Job: Job job_local2137479825_0001 failed with state FAILED due to: NA\n14/05/30 02:16:21 INFO mapreduce.Job: Counters: 0\nException in thread \"main\" java.lang.RuntimeException: job failed: name=inject urls, jobid=job_local2137479825_0001\n    at org.apache.nutch.util.NutchJob.waitForCompletion(NutchJob.java:54)\n    at org.apache.nutch.crawl.InjectorJob.run(InjectorJob.java:233)\n    at org.apache.nutch.crawl.Crawler.runTool(Crawler.java:68)\n    at org.apache.nutch.crawl.Crawler.run(Crawler.java:136)\n    at org.apache.nutch.crawl.Crawler.run(Crawler.java:250)\n    at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)\n    at org.apache.nutch.crawl.Crawler.main(Crawler.java:257)\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n    at java.lang.reflect.Method.invoke(Method.java:606)\n    at org.apache.hadoop.util.RunJar.main(RunJar.java:212)\n</code></pre>\n\n<p>Any thoughts on how I could resolve?</p>\n\n<p>Thank you!</p>\n", "creation_date": 1401501338, "score": 3},
{"title": "Nutch 2.3 REST curl syntax", "view_count": 183, "owner": {"user_id": 4584258, "answer_count": 31, "creation_date": 1424355773, "accept_rate": 83, "view_count": 21, "reputation": 469}, "is_answered": true, "answers": [{"last_edit_date": 1434127570, "owner": {"user_id": 4584258, "accept_rate": 83, "link": "http://stackoverflow.com/users/4584258/jgloves", "user_type": "registered", "reputation": 469}, "body": "<p>From the user mailing list, I learned the args to use for generate are:</p>\n\n<p>\"normalize\":boolean</p>\n\n<p>\"filter\":boolean</p>\n\n<p>\"crawlId\":String</p>\n\n<p>\"curTime\":long</p>\n\n<p>\"batch\":String</p>\n", "question_id": 30791399, "creation_date": 1434125718, "is_accepted": true, "score": 0, "last_activity_date": 1434127570, "answer_id": 30807734}], "question_id": 30791399, "tags": ["rest", "curl", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/30791399/nutch-2-3-rest-curl-syntax", "last_activity_date": 1434127570, "accepted_answer_id": 30807734, "body": "<p>I'm trying to use curl to test out the Nutch 2.X REST API. I'm able to start the nutchserver and inject URLS, but I'm having trouble getting the generate step to work.</p>\n\n<p>Here's what I've done:</p>\n\n<pre><code>curl -i -X POST -H \"Content-Type:application/json\" http://localhost:8081/job/create -d '{\"crawlId\":\"crawl-01\",\"type\":\"INJECT\",\"confId\":\"default\",\"args\":{\"seedDir\":\"/Users/username/myNutchFolder/apache-nutch-2.3/runtime/local/urls/\"}}'\n</code></pre>\n\n<p>which when I look at jobs, shows that it finished and injected the appropriate number of urls.</p>\n\n<p>Then I try to generate using</p>\n\n<pre><code>curl -i -X POST -H \"Content-Type:application/json\" http://localhost:8081/job/create -d '{\"crawlId\":\"crawl-01\",\"type\":\"GENERATE\",\"confId\":\"default\",\"args\":{}}'\n</code></pre>\n\n<p>which fails, and has the following job status:</p>\n\n<pre><code>{\n    \"args\": {},\n    \"confId\": \"default\",\n    \"crawlId\": \"crawl-01\",\n    \"id\": \"crawl-01-default-GENERATE-94689123\",\n    \"msg\": \"ERROR: java.lang.RuntimeException: job failed: name=[crawl-01]generate: null, jobid=job_local473690964_0003\",\n    \"result\": null,\n    \"state\": \"FAILED\",\n    \"type\": \"GENERATE\"\n},\n</code></pre>\n\n<p>I can't seem to find any documentation beyond the official API page: <a href=\"https://wiki.apache.org/nutch/NutchRESTAPI#Create_job\" rel=\"nofollow\">https://wiki.apache.org/nutch/NutchRESTAPI#Create_job</a>\nso I was hoping someone here might know how to use the REST API to crawl (inject, generate, fetch, parse, updatedb) Any help in understanding even why my generate job failed would be greatly appreciated. </p>\n", "creation_date": 1434056610, "score": 0},
{"title": "How can I integrate Solr5.1.0 with Nutch1.10", "view_count": 512, "is_answered": true, "answers": [{"last_edit_date": 1433867814, "owner": {"user_id": 4963025, "link": "http://stackoverflow.com/users/4963025/bruno-dos-santos", "user_type": "registered", "reputation": 696}, "body": "<p>The problem is that Nutch <code>schema.xml</code> file doesn't contains the field type <code>int</code> used by <code>cityConfidence</code> field. To solve this problem just include the followed line in your <code>schema.xml</code> file:</p>\n\n<pre><code>&lt;fieldType name=\"int\" class=\"solr.TrieIntField\" precisionStep=\"0\" omitNorms=\"true\" positionIncrementGap=\"0\"/&gt;\n</code></pre>\n\n<p>Make sure all field types used by your fields are declared in your <code>schema.xml</code> file.</p>\n", "question_id": 30722014, "creation_date": 1433822842, "is_accepted": false, "score": 1, "last_activity_date": 1433867814, "answer_id": 30722832}, {"question_id": 30722014, "owner": {"user_id": 1959771, "link": "http://stackoverflow.com/users/1959771/douglee", "user_type": "registered", "reputation": 1}, "body": "<p>try <strong>schema-solr4.xml</strong> instead of schema.xml </p>\n", "creation_date": 1433901024, "is_accepted": false, "score": 0, "last_activity_date": 1433901024, "answer_id": 30745762}], "question_id": 30722014, "tags": ["apache", "solr", "lucene", "nutch", "solr-schema"], "answer_count": 2, "link": "http://stackoverflow.com/questions/30722014/how-can-i-integrate-solr5-1-0-with-nutch1-10", "last_activity_date": 1433901024, "owner": {"user_id": 4213245, "view_count": 0, "answer_count": 0, "creation_date": 1415085663, "reputation": 11}, "body": "<p>I replaced the Solr <code>schema.xml</code> with nutch <code>schema.xml</code>.  But when I run Solr again\uff0cSolr log prints this error:</p>\n\n<blockquote>\n  <p>ERROR - 2015-06-09 09:54:30.279; [   ]\n  org.apache.solr.core.CoreContainer; Error creating core [mycore]:\n  Could not load conf for core mycore: Unknown fieldType 'int' specified\n  on field cityConfidence. Schema file is\n  /opt/solr-5.1.0/server/solr/mycore/conf/schema.xml\n  org.apache.solr.common.SolrException: Could not load conf for core\n  mycore: Unknown fieldType 'int' specified on field cityConfidence.\n  Schema file is /opt/solr-5.1.0/server/solr/mycore/conf/schema.xml</p>\n</blockquote>\n", "creation_date": 1433816546, "score": 2},
{"title": "Optimize nutch performance on hadoop cluster", "view_count": 93, "owner": {"age": 27, "answer_count": 4, "creation_date": 1347015423, "user_id": 1654519, "accept_rate": 50, "view_count": 9, "reputation": 57}, "is_answered": true, "answers": [{"question_id": 29389465, "owner": {"user_id": 1654519, "accept_rate": 50, "link": "http://stackoverflow.com/users/1654519/seldon", "user_type": "registered", "reputation": 57}, "body": "<p>It was problem in Nutch, it takes about 50 000 000 from one site and 500 000 from all other. So when it creating queue by host we see one extremely big queue and other very small.</p>\n", "creation_date": 1433756021, "is_accepted": true, "score": 0, "last_activity_date": 1433756021, "answer_id": 30705632}], "question_id": 29389465, "tags": ["performance", "hadoop", "nutch", "cloudera", "yarn"], "answer_count": 1, "link": "http://stackoverflow.com/questions/29389465/optimize-nutch-performance-on-hadoop-cluster", "last_activity_date": 1433756021, "accepted_answer_id": 30705632, "body": "<p>I'm trying to optimize nutch performance for crawling sites. Now i test performance on small hadoop cluster, only two nodes 32gb RAM, cpu Intel Xeon E3 1245v2 4c/8t.\nMy config for nutch <a href=\"http://pastebin.com/bBRHpFuq\" rel=\"nofollow\">http://pastebin.com/bBRHpFuq</a></p>\n\n<p>So, the problem: fetching jobs works not optimal. Some reduce task has 4k pages for fetching, some 1kk pages. For example see screenshot <a href=\"https://docs.google.com/file/d/0B98dgNxOqKMvT1doOVVPUU1PNXM/edit\" rel=\"nofollow\">https://docs.google.com/file/d/0B98dgNxOqKMvT1doOVVPUU1PNXM/edit</a> Some reduce task finished in 10 minutes, but one task work 11 hours and still continue working, so it's like a bottle neck when i have 24 reduce task, but works only one.</p>\n\n<p>May be someone can give usefull advices or links where i can read about problem.</p>\n", "creation_date": 1427887396, "score": 0},
{"title": "how can I decrease the number of exception fetcher status in apache nutch?", "view_count": 26, "is_answered": false, "answers": [{"question_id": 30680216, "owner": {"user_id": 2493828, "link": "http://stackoverflow.com/users/2493828/zigot", "user_type": "registered", "reputation": 16}, "body": "<p>I think you're looking for the conf/regex-urlfilter.txt file. In this file you can define which you'd like to include and exclude with regular expressions. By default some things are excluded, like URLs which contain a question mark or URLs ending with media formats.</p>\n\n<p>Any URL that does not match the patterns you defined will not be stored in the DB, therefore will not be used in the next cycle.</p>\n", "creation_date": 1433755316, "is_accepted": false, "score": 0, "last_activity_date": 1433755316, "answer_id": 30705391}], "question_id": 30680216, "tags": ["exception", "hadoop", "cluster-computing", "fetch", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/30680216/how-can-i-decrease-the-number-of-exception-fetcher-status-in-apache-nutch", "last_activity_date": 1433755316, "owner": {"age": 29, "answer_count": 1, "creation_date": 1430300722, "user_id": 4845751, "view_count": 2, "location": "Tehran, Iran", "reputation": 1}, "body": "<p>I clustered apache Nutch with hadoop but I am not very familiar with either of them. In fetch part, when I look at FetcherStatus, I found the number of the exception quite alot(one thrid of the urls).My question is how I can decrease the amount of exception and increase the number of success urls...</p>\n\n<p>and one more question I wonder  what happens to  exception urls in apache nutch. Are exception urls ignored in next crawls or they will be still used?</p>\n\n<p>Any links, answers and comments are much appreciated</p>\n", "creation_date": 1433576085, "score": 0},
{"title": "Gora MongoDb Exception, can&#39;t serialize Utf8", "view_count": 359, "owner": {"user_id": 2493828, "view_count": 5, "answer_count": 4, "creation_date": 1371480136, "reputation": 16}, "is_answered": true, "answers": [{"question_id": 30662489, "owner": {"user_id": 2493828, "link": "http://stackoverflow.com/users/2493828/zigot", "user_type": "registered", "reputation": 16}, "body": "<p>The solution is to apply the following patch: <a href=\"https://issues.apache.org/jira/browse/NUTCH-1946\" rel=\"nofollow\">https://issues.apache.org/jira/browse/NUTCH-1946</a> to your project. This patch updates gora to 0.6, which contains the fix for this problem.</p>\n\n<p>If you run into a RuntimeException during the GeneratorJob, please add the following to your nutch-site.xml</p>\n\n<pre><code>&lt;property&gt;\n    &lt;name&gt;io.serializations&lt;/name&gt;\n    &lt;value&gt;org.apache.hadoop.io.serializer.WritableSerialization&lt;/value&gt;\n    &lt;description&gt;A list of serialization classes that can be used for\n        obtaining serializers and deserializers.&lt;/description&gt;\n&lt;/property&gt;\n</code></pre>\n", "creation_date": 1433754393, "is_accepted": true, "score": 0, "last_activity_date": 1433754393, "answer_id": 30705098}], "question_id": 30662489, "tags": ["mongodb", "nutch", "gora"], "answer_count": 1, "link": "http://stackoverflow.com/questions/30662489/gora-mongodb-exception-cant-serialize-utf8", "last_activity_date": 1433754393, "accepted_answer_id": 30705098, "body": "<p>I'm trying to get nutch 2.3 work with mongoDB but I get the following exception:</p>\n\n<pre><code>java.lang.IllegalArgumentException: can't serialize class org.apache.avro.util.Utf8\nat org.bson.BasicBSONEncoder._putObjectField(BasicBSONEncoder.java:284)\nat org.bson.BasicBSONEncoder.putObject(BasicBSONEncoder.java:185)\n</code></pre>\n\n<p>I've found the following ticket related to this problem, which says it should be resolved in nutch 2.3: <a href=\"https://issues.apache.org/jira/browse/NUTCH-1843\" rel=\"nofollow\">https://issues.apache.org/jira/browse/NUTCH-1843</a></p>\n\n<p>There's another ticket for the Gora project which says this issue is actually resolved in Gora 0.6 which can be found in <a href=\"https://issues.apache.org/jira/browse/GORA-388\" rel=\"nofollow\">https://issues.apache.org/jira/browse/GORA-388</a> . However Nutch 2.3 uses gora 0.5. So I don't see how this issue would be resolved in nutch 2.3.</p>\n\n<p>I really would like to use MongoDB, but I can't seem to overcome the issue.\nIs there anyone who has insight into this problem? Is it a configuration issue?</p>\n", "creation_date": 1433495022, "score": 1},
{"title": "Nutch plugin that crawls for specific keywords", "view_count": 137, "is_answered": false, "question_id": 21464397, "tags": ["nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/21464397/nutch-plugin-that-crawls-for-specific-keywords", "last_activity_date": 1433352578, "owner": {"user_id": 3248024, "view_count": 0, "answer_count": 0, "creation_date": 1390986022, "reputation": 6}, "body": "<p>Is there any Nutch plugin that help me crawl only web pages that contains specific keywords that can be set by the Nutch user? </p>\n\n<p>For example I only want to crawl web pages that contain both \"job\" &amp; \"apply\" words or web pages that contain words like: \"education\", \"experience\", \"benefits\"? </p>\n\n<p>If a plugin like this does not exist, can an existing one be adjusted?</p>\n", "creation_date": 1391105748, "score": 1},
{"title": "index crawled data from Apache nutch using elasticsearch?", "view_count": 1122, "owner": {"age": 25, "answer_count": 0, "creation_date": 1419850091, "user_id": 4401880, "view_count": 12, "location": "Pune", "reputation": 12}, "is_answered": true, "answers": [{"question_id": 29363782, "owner": {"user_id": 1699191, "link": "http://stackoverflow.com/users/1699191/ameertawfik", "user_type": "registered", "reputation": 618}, "body": "<p>Enable elasticsearch indexer in the configuration. add the elastic-indexer to the plugin linclude property list. see below:</p>\n\n<pre><code>    &lt;property&gt;\n            &lt;name&gt;plugin.includes&lt;/name&gt;\n            &lt;value&gt;protocol-http|urlfilter-regex|parse-(html|tika)|index-(basic|anchor)|indexer-elastic|scoring-opic|urlnormalizer-(pass|regex|basic)&lt;/value&gt;\n    &lt;/property&gt;\n</code></pre>\n", "creation_date": 1428512475, "is_accepted": false, "score": 1, "last_activity_date": 1428512475, "answer_id": 29520904}, {"question_id": 29363782, "owner": {"user_id": 2702341, "link": "http://stackoverflow.com/users/2702341/sujen-shah", "user_type": "registered", "reputation": 183}, "body": "<p>In your nutch-site.xml add the following properties:</p>\n\n<pre><code>&lt;property&gt;\n        &lt;name&gt;plugin.includes&lt;/name&gt;\n        &lt;value&gt;protocol-http|urlfilter-regex|parse-(html|tika)|index-(basic|anchor)|indexer-elastic|scoring-opic|urlnormalizer-(pass|regex|basic)&lt;/value&gt;\n&lt;/property&gt;\n</code></pre>\n\n<p>The above would would make elasticsearch as the indexer. \nFollowing is specifying the host of elasticsearch</p>\n\n<pre><code>&lt;property&gt;\n        &lt;name&gt;elastic.host&lt;/name&gt;\n        &lt;value&gt;localhost&lt;/value&gt;\n&lt;/property&gt;\n</code></pre>\n\n<p>The other optional properties you can set are elastic.port, elastic.cluster, etc. </p>\n\n<p>Now you specified that you have already crawled the data and now want to index it, so you can use the</p>\n\n<pre><code>./bin/nutch index &lt;crawldb&gt; -dir &lt;segment_dir&gt;\n</code></pre>\n\n<p>This would index all the crawled data residing in the segments. The you can check your elasticsearch index for the documents. </p>\n", "creation_date": 1433282582, "is_accepted": true, "score": 1, "last_activity_date": 1433282582, "answer_id": 30607397}], "question_id": 29363782, "tags": ["elasticsearch", "ubuntu-14.04", "nutch", "aws-ec2"], "answer_count": 2, "link": "http://stackoverflow.com/questions/29363782/index-crawled-data-from-apache-nutch-using-elasticsearch", "last_activity_date": 1433282582, "accepted_answer_id": 30607397, "body": "<p>I have apache nutch 1.7 and Elasticsearch 1.4.4 on aws ec2 ubuntu instance. I crawled data using Nutch but how we can index data using elasticsearch? No official documentation is available related to it.</p>\n", "creation_date": 1427789712, "score": 0},
{"title": "Error : org.apache.hadoop.mapred.InvalidInputException: Input path does not exist", "view_count": 525, "is_answered": true, "answers": [{"question_id": 30527212, "owner": {"user_id": 1699191, "link": "http://stackoverflow.com/users/1699191/ameertawfik", "user_type": "registered", "reputation": 618}, "body": "<p>If you are using the <code>bin/crawl</code> from Mac OS or any Unix-based operating system like FreeBSD, then switch to Ubuntu. I believe this is a bug the crawl script has. I faced this before and used Ubuntu instead.</p>\n", "creation_date": 1433014738, "is_accepted": false, "score": 1, "last_activity_date": 1433014738, "answer_id": 30550575}], "question_id": 30527212, "tags": ["java", "hadoop", "solr", "fetch", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/30527212/error-org-apache-hadoop-mapred-invalidinputexception-input-path-does-not-exis", "last_activity_date": 1433014738, "owner": {"user_id": 4662134, "answer_count": 1, "creation_date": 1426152609, "accept_rate": 0, "view_count": 11, "reputation": 21}, "body": "<p>I am new in nutch and solr integration.</p>\n\n<p>I want to crawl new urls so I installed both solr version 4.6.0 and nutch version 1.6 in ubuntu.First I start with some configuration but i still get this error:</p>\n\n<blockquote>\n  <p>org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: File:/home/cloudera/apache-nutch-1.6/bin/20150529030452/crawl_fetch</p>\n  \n  <p>Input path does not exist: file:/home/cloudera/apache-nutch-1.6/bin   /20150529030452/crawl_parse</p>\n  \n  <p>Input path does not exist: file:/home/cloudera/apache-nutch-1.6/bin/20150529030452/parse_data</p>\n  \n  <p>Input path does not exist: file:/home/cloudera/apache-nutch-1.6/bin/20150529030452/parse_text</p>\n</blockquote>\n\n<p>In the file logs I get this error:</p>\n\n<blockquote>\n  <p>2015-05-29 03:05:41,153 ERROR security.UserGroupInformation -PriviledgedActionException as:cloudera </p>\n  \n  <p>cause:org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/home/cloudera/apache-nutch-1.6/bin/20150529030452/crawl_fetch</p>\n  \n  <p>Input path does not exist: file:/home/cloudera/apache-nutch-1.6/bin/20150529030452/crawl_parse</p>\n  \n  <p>Input path does not exist: file:/home/cloudera/apache-nutch-1.6/bin/20150529030452/parse_data</p>\n  \n  <p>Input path does not exist: file:/home/cloudera/apache-nutch-1.6/bin/20150529030452/parse_text</p>\n  \n  <p>2015-05-29 03:05:41,153 ERROR solr.SolrIndexer - org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/home/cloudera/apache-nutch-1.6/bin/20150529030452/crawl_fetch</p>\n  \n  <p>Input path does not exist: file:/home/cloudera/apache-nutch-1.6/bin/20150529030452/crawl_parse</p>\n  \n  <p>Input path does not exist: file:/home/cloudera/apache-nutch-1.6/bin/20150529030452/parse_data</p>\n  \n  <p>Input path does not exist: file:/home/cloudera/apache-nutch-1.6/bin/20150529030452/parse_text</p>\n</blockquote>\n\n<p>Whats the meaning of this, can you please explain whats the issue and how can I solve it.</p>\n\n<p>I will highly appreciate your help.</p>\n", "creation_date": 1432895144, "score": 0},
{"title": "EOFException when running nutch on the hadoop", "view_count": 283, "is_answered": true, "answers": [{"question_id": 30298706, "owner": {"user_id": 3511339, "link": "http://stackoverflow.com/users/3511339/masoud-sagharichian", "user_type": "registered", "reputation": 198}, "body": "<p>I had exactly the same problem with the same config. My problem is solved by adding </p>\n\n<pre><code>&lt;property&gt;\n  &lt;name&gt;io.serializations&lt;/name&gt;\n  &lt;value&gt;org.apache.hadoop.io.serializer.WritableSerialization&lt;/value&gt;\n  &lt;description&gt;A list of serialization classes that can be used for\n  obtaining serializers and deserializers.&lt;/description&gt;\n&lt;/property&gt;\n</code></pre>\n\n<p>to the nutch-site.xml. Thanks to <a href=\"http://quabr.com/26180364/cant-run-nutch2-on-hadoop2-nutch-2-x-hadoop-2-4-0-hbase-0-94-18-gora-0-5\" rel=\"nofollow\">http://quabr.com/26180364/cant-run-nutch2-on-hadoop2-nutch-2-x-hadoop-2-4-0-hbase-0-94-18-gora-0-5</a></p>\n", "creation_date": 1433000066, "is_accepted": false, "score": 3, "last_activity_date": 1433000066, "answer_id": 30547988}], "question_id": 30298706, "tags": ["hadoop", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/30298706/eofexception-when-running-nutch-on-the-hadoop", "last_activity_date": 1433000066, "owner": {"user_id": 4899063, "view_count": 0, "answer_count": 0, "creation_date": 1431593079, "reputation": 6}, "body": "<p>I am running nutch2.3 on the hadoop2.5.2 and hbase 0.98.12 with gora 0.6, when doing the process of nutch generate, hadoop throw an eofexception. any suggestion is welcome.</p>\n\n<blockquote>\n  <p>2015-05-18 15:22:06,578 INFO  [main] mapreduce.Job\n  (Job.java:monitorAndPrintJob(1362)) -  map 100% reduce 0% 2015-05-18\n  15:22:13,697 INFO  [main] mapreduce.Job\n  (Job.java:monitorAndPrintJob(1362)) -  map 100% reduce 50% 2015-05-18\n  15:22:14,720 INFO  [main] mapreduce.Job\n  (Job.java:printTaskEvents(1441)) - Task Id :\n  attempt_1431932258783_0006_r_000001_0, Status : FAILED Error:\n  java.io.EOFException  at\n  org.apache.avro.io.BinaryDecoder.ensureBounds(BinaryDecoder.java:473)\n    at org.apache.avro.io.BinaryDecoder.readInt(BinaryDecoder.java:128)\n    at org.apache.avro.io.BinaryDecoder.readIndex(BinaryDecoder.java:423)\n    at\n  org.apache.avro.io.ResolvingDecoder.doAction(ResolvingDecoder.java:229)\n    at org.apache.avro.io.parsing.Parser.advance(Parser.java:88)    at\n  org.apache.avro.io.ResolvingDecoder.readIndex(ResolvingDecoder.java:206)\n    at\n  org.apache.avro.generic.GenericDatumReader.read(GenericDatumReader.java:152)\n    at\n  org.apache.avro.generic.GenericDatumReader.readRecord(GenericDatumReader.java:177)\n    at\n  org.apache.avro.generic.GenericDatumReader.read(GenericDatumReader.java:148)\n    at\n  org.apache.avro.generic.GenericDatumReader.read(GenericDatumReader.java:139)\n    at\n  org.apache.hadoop.io.serializer.avro.AvroSerialization$AvroDeserializer.deserialize(AvroSerialization.java:127)\n    at\n  org.apache.hadoop.mapreduce.task.ReduceContextImpl.nextKeyValue(ReduceContextImpl.java:146)\n    at\n  org.apache.hadoop.mapreduce.task.ReduceContextImpl.nextKey(ReduceContextImpl.java:121)\n    at\n  org.apache.hadoop.mapreduce.lib.reduce.WrappedReducer$Context.nextKey(WrappedReducer.java:302)\n    at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:170)    at\n  org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:627)\n    at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:389)     at\n  org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)  at\n  java.security.AccessController.doPrivileged(Native Method)    at\n  javax.security.auth.Subject.doAs(Subject.java:415)    at\n  org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1614)\n    at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:163)</p>\n  \n  <p>2015-05-18 15:22:21,901 INFO  [main] mapreduce.Job\n  (Job.java:printTaskEvents(1441)) - Task Id :\n  attempt_1431932258783_0006_r_000001_1, Status : FAILED Error:\n  java.io.EOFException  at\n  org.apache.avro.io.BinaryDecoder.ensureBounds(BinaryDecoder.java:473)\n    at org.apache.avro.io.BinaryDecoder.readInt(BinaryDecoder.java:128)\n    at org.apache.avro.io.BinaryDecoder.readIndex(BinaryDecoder.java:423)\n    at\n  org.apache.avro.io.ResolvingDecoder.doAction(ResolvingDecoder.java:229)\n    at org.apache.avro.io.parsing.Parser.advance(Parser.java:88)    at\n  org.apache.avro.io.ResolvingDecoder.readIndex(ResolvingDecoder.java:206)\n    at\n  org.apache.avro.generic.GenericDatumReader.read(GenericDatumReader.java:152)\n    at\n  org.apache.avro.generic.GenericDatumReader.readRecord(GenericDatumReader.java:177)\n    at\n  org.apache.avro.generic.GenericDatumReader.read(GenericDatumReader.java:148)\n    at\n  org.apache.avro.generic.GenericDatumReader.read(GenericDatumReader.java:139)\n    at\n  org.apache.hadoop.io.serializer.avro.AvroSerialization$AvroDeserializer.deserialize(AvroSerialization.java:127)\n    at\n  org.apache.hadoop.mapreduce.task.ReduceContextImpl.nextKeyValue(ReduceContextImpl.java:146)\n    at\n  org.apache.hadoop.mapreduce.task.ReduceContextImpl.nextKey(ReduceContextImpl.java:121)\n    at\n  org.apache.hadoop.mapreduce.lib.reduce.WrappedReducer$Context.nextKey(WrappedReducer.java:302)\n    at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:170)    at\n  org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:627)\n    at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:389)     at\n  org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)  at\n  java.security.AccessController.doPrivileged(Native Method)    at\n  javax.security.auth.Subject.doAs(Subject.java:415)    at\n  org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1614)\n    at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:163)</p>\n  \n  <p>2015-05-18 15:22:28,986 INFO  [main] mapreduce.Job\n  (Job.java:printTaskEvents(1441)) - Task Id :\n  attempt_1431932258783_0006_r_000001_2, Status : FAILED Error:\n  java.io.EOFException  at\n  org.apache.avro.io.BinaryDecoder.ensureBounds(BinaryDecoder.java:473)\n    at org.apache.avro.io.BinaryDecoder.readInt(BinaryDecoder.java:128)\n    at org.apache.avro.io.BinaryDecoder.readIndex(BinaryDecoder.java:423)\n    at\n  org.apache.avro.io.ResolvingDecoder.doAction(ResolvingDecoder.java:229)\n    at org.apache.avro.io.parsing.Parser.advance(Parser.java:88)    at\n  org.apache.avro.io.ResolvingDecoder.readIndex(ResolvingDecoder.java:206)\n    at\n  org.apache.avro.generic.GenericDatumReader.read(GenericDatumReader.java:152)\n    at\n  org.apache.avro.generic.GenericDatumReader.readRecord(GenericDatumReader.java:177)\n    at\n  org.apache.avro.generic.GenericDatumReader.read(GenericDatumReader.java:148)\n    at\n  org.apache.avro.generic.GenericDatumReader.read(GenericDatumReader.java:139)\n    at\n  org.apache.hadoop.io.serializer.avro.AvroSerialization$AvroDeserializer.deserialize(AvroSerialization.java:127)\n    at\n  org.apache.hadoop.mapreduce.task.ReduceContextImpl.nextKeyValue(ReduceContextImpl.java:146)\n    at\n  org.apache.hadoop.mapreduce.task.ReduceContextImpl.nextKey(ReduceContextImpl.java:121)\n    at\n  org.apache.hadoop.mapreduce.lib.reduce.WrappedReducer$Context.nextKey(WrappedReducer.java:302)\n    at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:170)    at\n  org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:627)\n    at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:389)     at\n  org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)  at\n  java.security.AccessController.doPrivileged(Native Method)    at\n  javax.security.auth.Subject.doAs(Subject.java:415)    at\n  org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1614)\n    at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:163)</p>\n  \n  <p>2015-05-18 15:22:37,078 INFO  [main] mapreduce.Job\n  (Job.java:monitorAndPrintJob(1362)) -  map 100% reduce 100% 2015-05-18\n  15:22:37,109 INFO  [main] mapreduce.Job\n  (Job.java:monitorAndPrintJob(1375)) - Job job_1431932258783_0006\n  failed with state FAILED due to: Task failed\n  task_1431932258783_0006_r_000001 Job failed as tasks failed.\n  failedMaps:0 failedReduces:1</p>\n  \n  <p>2015-05-18 15:22:37,256 INFO  [main] mapreduce.Job\n  (Job.java:monitorAndPrintJob(1380)) - Counters: 50    File System\n  Counters      FILE: Number of bytes read=22       FILE: Number of bytes\n  written=232081        FILE: Number of read operations=0       FILE: Number of\n  large read operations=0       FILE: Number of write operations=0      HDFS:\n  Number of bytes read=612      HDFS: Number of bytes written=0         HDFS:\n  Number of read operations=1       HDFS: Number of large read operations=0\n        HDFS: Number of write operations=0  Job Counters        Failed reduce\n  tasks=4       Launched map tasks=1        Launched reduce tasks=5         Rack-local\n  map tasks=1       Total time spent by all maps in occupied slots\n  (ms)=10399        Total time spent by all reduces in occupied slots\n  (ms)=23225        Total time spent by all map tasks (ms)=10399        Total time\n  spent by all reduce tasks (ms)=23225      Total vcore-seconds taken by\n  all map tasks=10399       Total vcore-seconds taken by all reduce\n  tasks=23225       Total megabyte-seconds taken by all map tasks=10648576\n        Total megabyte-seconds taken by all reduce tasks=23782400\n    Map-Reduce Framework        Map input records=1         Map output records=1\n        Map output bytes=32         Map output materialized bytes=62        Input split\n  bytes=612         Combine input records=0         Combine output records=0\n        Reduce input groups=0       Reduce shuffle bytes=14         Reduce input\n  records=0         Reduce output records=0         Spilled Records=1       Shuffled\n  Maps =1       Failed Shuffles=0       Merged Map outputs=1        GC time elapsed\n  (ms)=175      CPU time spent (ms)=6860        Physical memory (bytes)\n  snapshot=628305920        Virtual memory (bytes) snapshot=3198902272\n        Total committed heap usage (bytes)=481820672    Shuffle Errors\n        BAD_ID=0        CONNECTION=0        IO_ERROR=0      WRONG_LENGTH=0      WRONG_MAP=0\n        WRONG_REDUCE=0  File Input Format Counters          Bytes Read=0    File\n  Output Format Counters        Bytes Written=0 2015-05-18 15:22:37,266\n  ERROR [main] crawl.GeneratorJob (GeneratorJob.java:run(310)) -\n  GeneratorJob: java.lang.RuntimeException: job failed:\n  name=[t2]generate: 1431933684-12185, jobid=job_1431932258783_0006     at\n  org.apache.nutch.util.NutchJob.waitForCompletion(NutchJob.java:54)    at\n  org.apache.nutch.crawl.GeneratorJob.run(GeneratorJob.java:213)    at\n  org.apache.nutch.crawl.GeneratorJob.generate(GeneratorJob.java:241)\n    at org.apache.nutch.crawl.GeneratorJob.run(GeneratorJob.java:308)   at\n  org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)     at\n  org.apache.nutch.crawl.GeneratorJob.main(GeneratorJob.java:316)   at\n  sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)   at\n  sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n    at\n  sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n    at java.lang.reflect.Method.invoke(Method.java:606)     at\n  org.apache.hadoop.util.RunJar.main(RunJar.java:212)</p>\n  \n  <p>Error running:   /usr/pro/nutch2.3/deploy/bin/nutch generate -D\n  mapred.reduce.tasks=2 -D mapred.child.java.opts=-Xmx1000m -D\n  mapred.reduce.tasks.speculative.execution=false -D\n  mapred.map.tasks.speculative.execution=false -D\n  mapred.compress.map.output=true -topN 50000 -noNorm -noFilter -adddays\n  0 -crawlId t2 -batchId 1431933684-12185</p>\n</blockquote>\n", "creation_date": 1431938888, "score": 1},
{"title": "Understanding the Nutch Regex pattern", "view_count": 265, "is_answered": false, "answers": [{"question_id": 30526856, "owner": {"user_id": 3832970, "link": "http://stackoverflow.com/users/3832970/wiktor-stribi%c5%bcew", "user_type": "registered", "reputation": 137962}, "body": "<p>As per the <a href=\"https://wiki.apache.org/nutch/NutchTutorial\" rel=\"nofollow\">Nutch Tutorial</a>, you need to use a regex to the variable part, and you are doing that right, but your regular expression needs a small correction. </p>\n\n<p>The literal dot <code>.</code> in regex must be escaped. <code>([a-z]*\\.)/([0-20]*\\.).html</code> means <em>lowercase English letters, 0 or more occurrences, then a literal <code>.</code>, then a literal <code>/</code>, then 0 or more <code>0</code>s, <code>1</code>s, <code>2</code>s (<code>[0-20]*</code>), followed by a literal <code>.</code>, and then any character and <code>html</code> literal string</em>.</p>\n\n<p>So, you are trying to allow strings like</p>\n\n<pre><code>http://abc.test.com/profiles/people/./.^html\n</code></pre>\n\n<p>See, you require <code>.</code> to be before the final <code>/</code>. It is not the case, I believe.</p>\n\n<p>So, use </p>\n\n<pre><code>+^http://abc\\.test\\.com/profiles/people/[a-zA-Z]+/([1-9]|1[0-9]|20)\\.html\n</code></pre>\n\n<p>See <a href=\"https://regex101.com/r/vJ2uV1/1\" rel=\"nofollow\">demo</a></p>\n", "creation_date": 1432899279, "is_accepted": false, "score": 0, "last_activity_date": 1432899279, "answer_id": 30528574}], "question_id": 30526856, "tags": ["regex", "web-crawler", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/30526856/understanding-the-nutch-regex-pattern", "last_activity_date": 1432899279, "owner": {"user_id": 4034435, "answer_count": 1, "creation_date": 1410517844, "accept_rate": 50, "view_count": 26, "reputation": 65}, "body": "<p>I am very new to Nutch. I am using Nutch 2.3 to crawl a website, here i want to crawl only some links which matches a specific pattern. As we know for this we have to use <code>regex-urlfilter.txt</code>.. So I have configured something like below in my regex-urlfilter.txt file, but the result is 0 crawled URL's.</p>\n\n<p>URL's I want to crawl should match below pattern:</p>\n\n<pre><code>http://abc.test.com/profiles/people/a/1.html\n</code></pre>\n\n<p>In the above URL, we can see <code>/a/1.html</code> at the end. I want to apply regex only at this part. the letter <code>'a' could be [a-z]</code> and the number <code>'1' could be [1-20]</code>. There is no change in the rest of the URL<code>( http://abc.test.com/profiles/people)</code>.</p>\n\n<p>Hope my requirement is clear now</p>\n\n<p>Below is what I have written in /bin/seed/urls.txt</p>\n\n<pre><code>http://abc.test.com/profiles/people/a/1.html\n</code></pre>\n\n<p>Also I tried with <code>http://abc.test.com/</code></p>\n\n<p>And below is my regex pattern in <strong>regex-urlfilter.txt</strong> file</p>\n\n<pre><code># accept anything else\n+^http://abc.test.com/profiles/people/([a-z]*\\.)/([0-20]*\\.).html\n</code></pre>\n\n<p>I am getting 0 URL's when I am crawling. I am assuming that I am making some mistakes either in <strong>urls.txt</strong> file or in <strong>regex-urlfilter.txt</strong></p>\n\n<p>Will somebody please help me in configuring properly</p>\n\n<p>Thanks in advance..</p>\n", "creation_date": 1432894140, "score": 1},
{"title": "Apache NUTCH, relevant crawling", "view_count": 89, "is_answered": false, "answers": [{"question_id": 27022037, "owner": {"user_id": 3769220, "link": "http://stackoverflow.com/users/3769220/bigdata", "user_type": "registered", "reputation": 184}, "body": "<p>In regex-urlfilter.txt you can specify the list of urls you want to ignore. You can specify the http link for \"contact us\"(typically all header, footer information that you don't want to crawl) etc.. in that regex list. While crawling web, nutch will ignore those urls and will only fetch the requires content. You can find regex-urlfilter.txt under apache-nutch-2.2.1/conf folder</p>\n", "creation_date": 1432846255, "is_accepted": false, "score": 0, "last_activity_date": 1432846255, "answer_id": 30516860}], "question_id": 27022037, "tags": ["apache", "web-crawler", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/27022037/apache-nutch-relevant-crawling", "last_activity_date": 1432846255, "owner": {"user_id": 2116143, "answer_count": 0, "creation_date": 1361980865, "accept_rate": 0, "view_count": 3, "reputation": 16}, "body": "<p>I am crawling websites using Apache NUTCH 2.2.1, which provides me content to index on SOLR. When NUTCH fetches content, there are contextual information such as \"contact us\",\"legal notice\" or some other irrelevant information (generally coming from upper menu, left menu or from footer of the page) that I do not need to index.</p>\n\n<p>One of the solution would be to automatically select the most relevant part of the content to index, which can be done by an automatic summarizer. There is a plugin \"summary-basic\", is it used for this purpose? If so how is it configured? Other solutions are also welcome.</p>\n", "creation_date": 1416414797, "score": 0},
{"title": "How to crawl images in Nutch 2.3 as HBase as backend?", "view_count": 339, "owner": {"user_id": 1495574, "answer_count": 17, "creation_date": 1341219032, "accept_rate": 61, "view_count": 58, "reputation": 183}, "is_answered": true, "answers": [{"question_id": 30488071, "owner": {"user_id": 1727204, "accept_rate": 43, "link": "http://stackoverflow.com/users/1727204/ali-n", "user_type": "registered", "reputation": 524}, "body": "<p>In order to fetch and store images using Nutch you have to follow these steps:</p>\n\n<p>1- Adding regular expression to not filter image formats, such as jpg, jpeg, tif, gif, png and etc... (which you already did)</p>\n\n<p>2- Implementing a parse plugin for parsing images. For more information about Nutch extension points and writing required plugin follow these links:</p>\n\n<p><a href=\"http://wiki.apache.org/nutch/AboutPlugins\" rel=\"nofollow\">http://wiki.apache.org/nutch/AboutPlugins</a></p>\n\n<p><a href=\"http://wiki.apache.org/nutch/WritingPluginExample\" rel=\"nofollow\">http://wiki.apache.org/nutch/WritingPluginExample</a></p>\n\n<p>3- Tell Nutch about the implemented plugin and using that for image file formats:</p>\n\n<p>For this purpose you have to follow two different steps, first, modify conf/parse-plugins.xml and map your implemented plugin to image file formats:</p>\n\n<pre><code>&lt;mimeType name=\"image/jpeg\"&gt;\n        &lt;plugin id=\"parse-image\" /&gt;\n&lt;/mimeType&gt;\n&lt;mimeType name=\"image/gif\"&gt;\n        &lt;plugin id=\"parse-image\" /&gt;\n&lt;/mimeType&gt;\n&lt;mimeType name=\"image/png\"&gt;\n        &lt;plugin id=\"parse-image\" /&gt;\n&lt;/mimeType&gt;\n</code></pre>\n\n<p>second, add the implemented plugin to <code>nutch-site.xml</code> to be run at Nutch runtime. You have to add the implemented plugin to <code>&lt;plugin.includes&gt;</code> property.</p>\n", "creation_date": 1432796669, "is_accepted": true, "score": 1, "last_activity_date": 1432796669, "answer_id": 30499428}], "question_id": 30488071, "tags": ["nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/30488071/how-to-crawl-images-in-nutch-2-3-as-hbase-as-backend", "last_activity_date": 1432796669, "accepted_answer_id": 30499428, "body": "<p>I want to crawl images from certain sites. So far I tried modifiying\nregex-urlfilter.txt.</p>\n\n<p>I changed:</p>\n\n<pre><code> -\\.(gif|GIF|jpg|JPG|png|PNG|ico|ICO|css|CSS|sit|SIT|eps|EPS|wmf|WMF|zip|ZIP|ppt|PP\n T|mpg|MPG|xls|XLS|gz|GZ|rpm|RPM|tgz|TGZ|mov|MOV|exe|EXE|jpeg|JPEG|bmp|BMP|js|JS)$\n</code></pre>\n\n<p>To:</p>\n\n<pre><code>-\\.(css|CSS|sit|SIT|eps|EPS|wmf|WMF|zip|ZIP|ppt|PPT|mpg|MPG|xls|XLS|gz|GZ|rpm|RPM|t\ngz|TGZ|mov|MOV|exe|EXE|js|JS)$\n</code></pre>\n\n<p>But it didn't work. I am surprised that I didn't find any documentation regarding <strong><em>crawling images using Nutch 2.3</em></strong>. Referal to any existing documentation would really be a great help.</p>\n", "creation_date": 1432743775, "score": 1},
{"title": "How to auto-index data using solr and nutch?", "view_count": 163, "is_answered": false, "answers": [{"question_id": 30498465, "owner": {"user_id": 3636071, "accept_rate": 100, "link": "http://stackoverflow.com/users/3636071/abhijit-bashetti", "user_type": "registered", "reputation": 3104}, "body": "<p>You can write a scheduler and call the solrJ code which is doing indexing/reindexing.</p>\n\n<p>For writing the scheduler please refer below links</p>\n\n<p><a href=\"http://www.mkyong.com/java/how-to-run-a-task-periodically-in-java/\" rel=\"nofollow\">http://www.mkyong.com/java/how-to-run-a-task-periodically-in-java/</a></p>\n\n<p><a href=\"http://archive.oreilly.com/pub/a/java/archive/quartz.html\" rel=\"nofollow\">http://archive.oreilly.com/pub/a/java/archive/quartz.html</a></p>\n", "creation_date": 1432793774, "is_accepted": false, "score": 0, "last_activity_date": 1432793774, "answer_id": 30498585}, {"question_id": 30498465, "owner": {"user_id": 1727204, "accept_rate": 43, "link": "http://stackoverflow.com/users/1727204/ali-n", "user_type": "registered", "reputation": 524}, "body": "<p>If you are using Apache Nutch, you have to use Nutch solr-index plugin. With using this plugin you can index web documents as soon as they be crawled by Nutch. But the main question would be how can you schedule Nutch to start periodically. </p>\n\n<p>As far as I know you have to use a scheduler for this purpose. I did know an old Nutch project called Nutch-base which uses <a href=\"http://quartz-scheduler.org/\" rel=\"nofollow\">Apache Quartz</a> for the purpose of scheduling Nutch jobs. You can find the source code of Nutch-base from the following link:</p>\n\n<p><a href=\"https://github.com/mathieuravaux/nutchbase\" rel=\"nofollow\">https://github.com/mathieuravaux/nutchbase</a></p>\n\n<p>If you consider this project there is a plugin called admin-scheduling. Although it is implemented for and old version of Nutch but it could be a nice start point for developing scheduler plugin for Nutch.</p>\n\n<p>It is worth to say that if you are going to crawl website periodically and fetch the new arrival links you can use <a href=\"http://pascaldimassimo.com/2010/06/11/how-to-re-crawl-with-nutch/\" rel=\"nofollow\">this tutorial</a>. </p>\n", "creation_date": 1432795576, "is_accepted": false, "score": 0, "last_activity_date": 1432795576, "answer_id": 30499086}], "question_id": 30498465, "tags": ["apache", "solr", "nutch", "solrj", "moss2007enterprisesearch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/30498465/how-to-auto-index-data-using-solr-and-nutch", "last_activity_date": 1432795576, "owner": {"user_id": 4193280, "answer_count": 114, "creation_date": 1414573314, "accept_rate": 27, "view_count": 114, "location": "India", "reputation": 1229}, "body": "<p>i want to automatically index a document or a website when it is fed to apache solr . How we can achieve this ? I have seen examples of using a CRON job that need to be called via a php script , but they are not quite clear in explaination. Using java api SolrJ , is there any way that we can index data automatically , without having the need to manually do it ??</p>\n", "creation_date": 1432793316, "score": 0},
{"title": "focused crawler by modifying nutch", "view_count": 233, "owner": {"user_id": 2789026, "view_count": 15, "answer_count": 7, "creation_date": 1379446030, "reputation": 60}, "is_answered": true, "answers": [{"question_id": 30404292, "owner": {"user_id": 1727204, "accept_rate": 43, "link": "http://stackoverflow.com/users/1727204/ali-n", "user_type": "registered", "reputation": 524}, "body": "<p>If the extracted urls could be differentiated by Regular expression you can do that with current Nutch by adding the specific regex filter. But if you are going to classify URL according to some metadata features related to page you have to implement a customized HTMLParseFilter to filter Outlink[] during parse step. \nFor more information about How to develop a plugin for Nutch follow these links:</p>\n\n<p><a href=\"http://wiki.apache.org/nutch/AboutPlugins\" rel=\"nofollow\">http://wiki.apache.org/nutch/AboutPlugins</a></p>\n\n<p><a href=\"http://wiki.apache.org/nutch/WritingPluginExample\" rel=\"nofollow\">http://wiki.apache.org/nutch/WritingPluginExample</a></p>\n", "creation_date": 1432550099, "is_accepted": true, "score": 1, "last_activity_date": 1432550099, "answer_id": 30436340}], "question_id": 30404292, "tags": ["web-crawler", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/30404292/focused-crawler-by-modifying-nutch", "last_activity_date": 1432550099, "accepted_answer_id": 30436340, "body": "<p>I want to create a focused crawler using nutch. Is there any way to modify nutch so as to make crawling faster? Can we use the metadata in nutch to train a classifier that would reduce the number of urls nutch has to crawl for a given topic??</p>\n", "creation_date": 1432322071, "score": 0},
{"title": "Language Detection in Solr for Nutch documents", "view_count": 288, "is_answered": true, "answers": [{"question_id": 30321669, "owner": {"user_id": 1699191, "link": "http://stackoverflow.com/users/1699191/ameertawfik", "user_type": "registered", "reputation": 618}, "body": "<p>You need to enable the language detection of Nutch. Copy the xml tag below to <code>Nutch_HOME/conf/nutch-site.xml</code>:</p>\n\n<p><code>&lt;property&gt;\n  &lt;name&gt;plugin.includes&lt;/name&gt;\n  &lt;value&gt;protocol-http|urlfilter-regex|parse-(html|tika)|index-(basic|anchor)|indexer-solr|scoring-opic|urlnormalizer-(pass|regex|basic)|language-identifier&lt;/value&gt;\n&lt;/property&gt;\n</code></p>\n\n<p>The above tag enables the language-detection plugin bundled with Nutch. As described in <a href=\"https://wiki.apache.org/nutch/IndexStructure\" rel=\"nofollow\">Nutch's wiki</a>, the plugin will add a field named \"lang\" which contains the language code of your documents.</p>\n", "creation_date": 1432067711, "is_accepted": false, "score": 2, "last_activity_date": 1432067711, "answer_id": 30335642}], "question_id": 30321669, "tags": ["apache", "solr", "nutch", "language-detection"], "answer_count": 1, "link": "http://stackoverflow.com/questions/30321669/language-detection-in-solr-for-nutch-documents", "last_activity_date": 1432067711, "owner": {"user_id": 4915392, "view_count": 2, "answer_count": 0, "creation_date": 1432026006, "reputation": 1}, "body": "<p>How can I use Solr for language identification of documents obtained by crawling with nutch?</p>\n\n<p>I installed <em>Nutch 1.9</em> and <em>Solr 4.8.1</em>. \nI added a new core, named <code>\"core-test\"</code>\n to solr by means of Core Admin in the Solr Admin page and I followed the steps in <a href=\"https://cwiki.apache.org/confluence/display/solr/Detecting+Languages+During+Indexing\" rel=\"nofollow\">Solr wiki</a> for language detection during documents indexing.</p>\n\n<p>I modified the <em>schema.xml</em> in core-test/conf by adding the field</p>\n\n<pre class=\"lang-xml prettyprint-override\"><code>&lt;field name=\"language_s\" type=\"string\" stored=\"true\" indexed=\"true\"/&gt;\n</code></pre>\n\n<p>Then, I used Nutch for crawling a set of web pages by</p>\n\n<pre><code>crawl seed.txt Test http://localhost:8983/solr/core-test 2\n</code></pre>\n\n<p>Nutch works appropriately but the language of the documents is not identified, i.e. I don't obtain the field <code>language_s</code> when I make a query in <a href=\"http://localhost:8983/solr/#/core-test/query\" rel=\"nofollow\">http://localhost:8983/solr/#/core-test/query</a> with <code>q</code> set to <code>\":\"</code>.</p>\n", "creation_date": 1432027650, "score": 0},
{"title": "How to crawl specific pages with nutch 2.3?", "view_count": 541, "is_answered": false, "question_id": 30296759, "tags": ["apache", "web-crawler", "nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/30296759/how-to-crawl-specific-pages-with-nutch-2-3", "last_activity_date": 1431935521, "owner": {"user_id": 4910797, "view_count": 0, "answer_count": 0, "creation_date": 1431930324, "reputation": 6}, "body": "<p>I am new to nutch. After serveral hours of searching I couldn't figure out how to choose the right settings for my crawler.</p>\n\n<p>First of all I have installed nutch 2.3 on Ubuntu 14.04 using hbase 0.94.14 and elasticsearch 1.4.2. </p>\n\n<p>I started using nutch running the following commands in nutch's runtime/local directory:</p>\n\n<pre><code>bin/nutch inject seedfolder\nbin/nutch generate -topN 20\nbin/nutch fetch -all\nbin/nutch parse -all\nbin/nutch updatedb -all\nbin/nutch index -all\n</code></pre>\n\n<p>I can then access the crawled data via elasticsearch. So everything seems to work nicely.</p>\n\n<p>My problem starts when I want to bring nutch to only crawl the sites I am interested in. I read lots of tutorials including the ones I found on the apache web site. One thing that really confuses me is the big difference between the versions of nutch. But also some of the questions I have never have been asked or answered.</p>\n\n<p>What I want to do:\nI want to tell nutch what page to crawl (of course it will be more that one but let's keep it simple). I do that by adding an url to my seed file and calling nutch inject. Now let's say I want to crawl <a href=\"http://www.pagetocrawl.com/intresting-facts\" rel=\"nofollow\">http://www.pagetocrawl.com/intresting-facts</a> more precisely I am interested in the contents of</p>\n\n<pre><code>http://www.pagetocrawl.com/intresting-facts?interesting-fact-id=1 \nhttp://www.pagetocrawl.com/intresting-facts?interesting-fact-id=2\nhttp://www.pagetocrawl.com/intresting-facts?interesting-fact-id=3 \n...\n</code></pre>\n\n<p>What I thought was the necessary thing to do was editing NUTCH_HOME/runtime/local/conf/regex-urlfilter.xml and adding something like</p>\n\n<pre><code>+http://www.pagetocrawl.com/intresting-facts\n</code></pre>\n\n<p>When I tried to run the generate and fetch command I noticed that in fact nutch only crawled sites starting with pagetocrawl.com and didn't touch the other sites I had injected earlier. But then it crawled all the pages that</p>\n\n<pre><code>http://www.pagetocrawl.com/interesting-facts\n</code></pre>\n\n<p>linked to. That is the imprint, where-to-find-us page etc. In the end it didn't crawl even one sigle interesting-fact site. So my two most imporant questions are: How can I tell nutch to crawl only that subsites of my sites filtered by regex-urlfilter.xml that do also match a specific pattern? And in the next step: How can I be sure to crawl all of the relevant subsites (as long as thy are linked to from <a href=\"http://www.pagetocrawl.com/interesting-facts\" rel=\"nofollow\">http://www.pagetocrawl.com/interesting-facts</a> site)?</p>\n\n<p>I've looked at</p>\n\n<pre><code>http://www.stackoverflow.com/questions/19731904/exclude-urls-without-www-from-nutch-1-7-crawl\n</code></pre>\n\n<p>but here the problem seems to appear one step earlier I added my url to regex-urlfilter.xml and it seems to be working - just not the way I expected it to work.</p>\n\n<p>I've also read that question:</p>\n\n<pre><code>http://www.stackoverflow.com/questions/3253525/how-to-index-only-pages-with-certain-urls-with-nutch\n</code></pre>\n\n<p>This seems to describe the very same problem I have. But since I am working with nutch 2.3 the mergedb command doesn't seem to work any longer.</p>\n\n<p>I do really hope I described my problem propery and that somebody can help me with that.</p>\n", "creation_date": 1431931922, "score": 1},
{"title": "How to run apache nutch different jobs in parallel manner", "view_count": 180, "owner": {"user_id": 3454410, "answer_count": 20, "creation_date": 1395646154, "accept_rate": 48, "view_count": 174, "reputation": 832}, "is_answered": true, "answers": [{"last_edit_date": 1431888109, "owner": {"user_id": 1495574, "accept_rate": 61, "link": "http://stackoverflow.com/users/1495574/computergodzilla", "user_type": "registered", "reputation": 183}, "body": "<p>If you check out the nutch web app server, you will find out that it can execute multiple crawl job in parallel.You should check out the source code of the Nutch 2.3 for webapp[NutchUiServer]. Hope this helps.</p>\n", "question_id": 30045799, "creation_date": 1431887391, "is_accepted": true, "score": 4, "last_activity_date": 1431888109, "answer_id": 30290565}], "question_id": 30045799, "tags": ["java", "apache", "web-crawler", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/30045799/how-to-run-apache-nutch-different-jobs-in-parallel-manner", "last_activity_date": 1431895252, "accepted_answer_id": 30290565, "body": "<p>I am using nutch 2.3. All jobs run one after other i.e. first generator, fetch, parse, index etc. I want to run some jobs simultaneously. I know some jobs cannot run in parallel but other can e.g parse job, dbupdate, indexjob should be run with fetch.</p>\n\n<p>Is it possible ? My basic objective is to run fetcher job all the time. I suppose that we can do it with different timestamp. \nCan anyone guide me the proper way ?</p>\n", "creation_date": 1430807752, "score": 9},
{"title": "run nutch 1.8 or 1.9 as a hadoop job", "view_count": 152, "is_answered": false, "answers": [{"question_id": 29546056, "owner": {"user_id": 1699191, "link": "http://stackoverflow.com/users/1699191/ameertawfik", "user_type": "registered", "reputation": 618}, "body": "<p>Your understanding is wrong. You should use the script bin/crawl. In each step, you should see the corresponding class that you should call (in case you want to use it outside the crawl script). In addition, as far as i know the class you quoted is deprecated.</p>\n", "creation_date": 1428733618, "is_accepted": false, "score": 0, "last_activity_date": 1428733618, "answer_id": 29574862}, {"question_id": 29546056, "owner": {"user_id": 1727204, "accept_rate": 43, "link": "http://stackoverflow.com/users/1727204/ali-n", "user_type": "registered", "reputation": 524}, "body": "<p>There are two different options for your scenario.</p>\n\n<p>1- You can implement your crawl class and make ANT to build hadoop job with considering your crawl class.</p>\n\n<p>2- You can schedule different classes to run on hadoop cluster in the desire order. (Inject-Generate-Fetch-Parse ...)</p>\n\n<p>However I chose the first solution. But the second one works too.</p>\n", "creation_date": 1429275222, "is_accepted": false, "score": 0, "last_activity_date": 1429275222, "answer_id": 29699772}, {"question_id": 29546056, "owner": {"user_id": 994000, "link": "http://stackoverflow.com/users/994000/betolink", "user_type": "registered", "reputation": 83}, "body": "<p>Also, take a look at <a href=\"https://github.com/b-cube/nutch-crawler\" rel=\"nofollow\">https://github.com/b-cube/nutch-crawler</a> it's a fork of Nutch 1.9 with the Crawl class so you can run it as a Hadoop job.</p>\n", "creation_date": 1431894275, "is_accepted": false, "score": 0, "last_activity_date": 1431894275, "answer_id": 30291759}], "question_id": 29546056, "tags": ["hadoop", "nutch"], "answer_count": 3, "link": "http://stackoverflow.com/questions/29546056/run-nutch-1-8-or-1-9-as-a-hadoop-job", "last_activity_date": 1431894275, "owner": {"user_id": 1058511, "answer_count": 22, "creation_date": 1321905325, "accept_rate": 47, "view_count": 109, "reputation": 591}, "body": "<p>If I understand correctly, you can not run nutch 1.8 and 1.9 as a hadoop job, because these versions do not have a Crawl class which serves as a wrapper for all the crawl steps. This means there is no one class you can specify in the hadoop call to run a whole job. In nutch 1.7, this used to be the org.apache.nutch.crawl.Crawl class. </p>\n\n<p>Am I missing something? Any one figure out a way around this?</p>\n", "creation_date": 1428603625, "score": 0},
{"title": "How to view fetch list in apache nutch 2.3", "view_count": 197, "is_answered": false, "answers": [{"question_id": 28847225, "owner": {"user_id": 2176123, "accept_rate": 50, "link": "http://stackoverflow.com/users/2176123/steven", "user_type": "registered", "reputation": 191}, "body": "<p>I think the new Wicket-based UI that was introduced in Nutch 2.3 could help here. There is not much documentation, but you can get it running like this:</p>\n\n<pre><code>bin/nutch nutchserver -port 8000 &amp;\nbin/nutch webapp -port 9000 &amp;\n</code></pre>\n", "creation_date": 1426511864, "is_accepted": false, "score": 0, "last_activity_date": 1426511864, "answer_id": 29077820}, {"question_id": 28847225, "owner": {"user_id": 1495574, "accept_rate": 61, "link": "http://stackoverflow.com/users/1495574/computergodzilla", "user_type": "registered", "reputation": 183}, "body": "<p>I don't think that is possible through web app. But you can manually view them in hbase itself. :)</p>\n", "creation_date": 1431876723, "is_accepted": false, "score": 0, "last_activity_date": 1431876723, "answer_id": 30288614}], "question_id": 28847225, "tags": ["apache", "web-crawler", "generator", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/28847225/how-to-view-fetch-list-in-apache-nutch-2-3", "last_activity_date": 1431876723, "owner": {"user_id": 3454410, "answer_count": 20, "creation_date": 1395646154, "accept_rate": 48, "view_count": 174, "reputation": 832}, "body": "<p>I am using apache nutch 2.3 for crawling along with hbase and hadoop. I know that first injector job starts then generator job and then fetcher and so on. </p>\n\n<p>I want to view fetcher list that is generated by generaorjob so some information and process on then before fetching starts?</p>\n", "creation_date": 1425446750, "score": 1},
{"title": "Nutch Crawler taking very long", "view_count": 164, "is_answered": true, "answers": [{"question_id": 30219415, "owner": {"user_id": 2702341, "link": "http://stackoverflow.com/users/2702341/sujen-shah", "user_type": "registered", "reputation": 183}, "body": "<p>Nutch performs parsing after fetching the URL, to get all the outlinks from the fetched URL. The outlinks from a URL are used as the new fetchlist for the next round. </p>\n\n<p>If parsing is skipped, no new URLs might be generated and hence no more fetching. \nOne way I can think of is to configure the parse plugins to include only the type of content you require to be parsed(in your case its the outlinks).\nOne example here - <a href=\"https://wiki.apache.org/nutch/IndexMetatags\" rel=\"nofollow\">https://wiki.apache.org/nutch/IndexMetatags</a></p>\n\n<p>This links describes the features of the parser <a href=\"https://wiki.apache.org/nutch/Features\" rel=\"nofollow\">https://wiki.apache.org/nutch/Features</a></p>\n\n<p>Now, to get only the list of the URLs fetched with their statuses you can use the</p>\n\n<p><code>$bin/nutch readdb crawldb -stats</code> command. </p>\n\n<p>Regarding the status code of 38, looking at the document you have linked, it seems like the status of the URL is \n<code>public static final byte STATUS_FETCH_NOTMODIFIED = 0x26</code></p>\n\n<p>Since, Hex(26) corresponds to Dec(38). </p>\n\n<p>Hope the answer gives some direction :) </p>\n", "creation_date": 1431693342, "is_accepted": false, "score": 1, "last_activity_date": 1431693342, "answer_id": 30259693}], "question_id": 30219415, "tags": ["apache", "web-crawler", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/30219415/nutch-crawler-taking-very-long", "last_activity_date": 1431693342, "owner": {"user_id": 4555234, "answer_count": 6, "creation_date": 1423665405, "accept_rate": 64, "view_count": 10, "reputation": 69}, "body": "<p>I only want Nutch to give me a list of the urls it crawled and the status of that link. I do not need the entire page content or fluff. Is there a way I can do this? Crawling a seedlist of 991 urls with a depth of 3 takes over 3 hours to crawl and parse. I'm hoping this will speed things up. </p>\n\n<p>In the nutch-default.xml file there is </p>\n\n<pre><code>&lt;property&gt;\n  &lt;name&gt;file.content.limit&lt;/name&gt;\n  &lt;value&gt;65536&lt;/value&gt;\n  &lt;description&gt;The length limit for downloaded content using the file\n   protocol, in bytes. If this value is nonnegative (&gt;=0), content longer\n   than it will be truncated; otherwise, no truncation at all. Do not\n   confuse this setting with the http.content.limit setting.\n  &lt;/description&gt;\n&lt;/property&gt;\n\n&lt;property&gt;\n  &lt;name&gt;file.content.ignored&lt;/name&gt;\n  &lt;value&gt;true&lt;/value&gt;\n  &lt;description&gt;If true, no file content will be saved during fetch.\n  And it is probably what we want to set most of time, since file:// URLs\n  are meant to be local and we can always use them directly at parsing\n  and indexing stages. Otherwise file contents will be saved.\n  !! NO IMPLEMENTED YET !!\n  &lt;/description&gt;\n&lt;/property&gt;\n\n&lt;property&gt;\n  &lt;name&gt;http.content.limit&lt;/name&gt;\n  &lt;value&gt;65536&lt;/value&gt;\n  &lt;description&gt;The length limit for downloaded content using the http\n  protocol, in bytes. If this value is nonnegative (&gt;=0), content longer\n  than it will be truncated; otherwise, no truncation at all. Do not\n  confuse this setting with the file.content.limit setting.\n  &lt;/description&gt;\n&lt;/property&gt;\n\n&lt;property&gt;\n  &lt;name&gt;ftp.content.limit&lt;/name&gt;\n  &lt;value&gt;65536&lt;/value&gt; \n  &lt;description&gt;The length limit for downloaded content, in bytes.\n  If this value is nonnegative (&gt;=0), content longer than it will be truncated;\n  otherwise, no truncation at all.\n  Caution: classical ftp RFCs never defines partial transfer and, in fact,\n  some ftp servers out there do not handle client side forced close-down very\n  well. Our implementation tries its best to handle such situations smoothly.\n  &lt;/description&gt;\n&lt;/property&gt;\n</code></pre>\n\n<p>These properties are ones I think may have something to do with it but i'm not sure. Can someone give me some help and clarification? Also I was getting many urls with the status code of 38. I cannot find what that status code indicates in <a href=\"http://svn.apache.org/viewvc/nutch/trunk/src/java/org/apache/nutch/crawl/CrawlDatum.java?view=markup\" rel=\"nofollow\">this</a> document. Thanks for the help!</p>\n", "creation_date": 1431531341, "score": 0},
{"title": "nutch2.3+mysql+solr ClassNotFoundException", "view_count": 439, "is_answered": false, "question_id": 30205311, "tags": ["mysql", "nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/30205311/nutch2-3mysqlsolr-classnotfoundexception", "last_activity_date": 1431693055, "owner": {"user_id": 4893943, "view_count": 4, "answer_count": 0, "creation_date": 1431487466, "reputation": 6}, "body": "<p>I wrote mysql config to <code>gora.properties</code> and update sql lib in <code>ivy.xml</code>.</p>\n\n<p>but when I run the command: </p>\n\n<pre><code>bin/crawl urls/seed.txt Test http://127.0.0.1:8080/solr 2\n</code></pre>\n\n<p>it throws exception:</p>\n\n<pre><code>[root@li257-46 local]# bin/crawl urls/seed.txt Test http://127.0.0.1:8080/solr 2\nInjecting seed URLs\n/home/work/nutch2/runtime/local/bin/nutch inject urls/seed.txt -crawlId Test\nInjectorJob: starting at 2015-05-13 03:25:35\nInjectorJob: Injecting urlDir: urls/seed.txt\n**\nException in thread \"main\" java.lang.NoClassDefFoundError: org/apache/avro/ipc/ByteBufferOutputStream\n    at java.lang.Class.forName0(Native Method)\n    at java.lang.Class.forName(Class.java:259)\n    at org.apache.nutch.storage.StorageUtils.getDataStoreClass(StorageUtils.java:93)\n    at org.apache.nutch.storage.StorageUtils.createWebStore(StorageUtils.java:77)\n    at org.apache.nutch.crawl.InjectorJob.run(InjectorJob.java:218)\n    at org.apache.nutch.crawl.InjectorJob.inject(InjectorJob.java:252)\n    at org.apache.nutch.crawl.InjectorJob.run(InjectorJob.java:275)\n    at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n    at org.apache.nutch.crawl.InjectorJob.main(InjectorJob.java:284)\n\nCaused by: java.lang.ClassNotFoundException: org.apache.avro.ipc.ByteBufferOutputStream\n    at java.net.URLClassLoader$1.run(URLClassLoader.java:372)\n    at java.net.URLClassLoader$1.run(URLClassLoader.java:361)\n    at java.security.AccessController.doPrivileged(Native Method)\n    at java.net.URLClassLoader.findClass(URLClassLoader.java:360)\n    at java.lang.ClassLoader.loadClass(ClassLoader.java:424)\n    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)\n    at java.lang.ClassLoader.loadClass(ClassLoader.java:357)\n    ... 9 more\nError running:\n  /home/work/nutch2/runtime/local/bin/nutch inject urls/seed.txt -crawlId Test\nFailed with exit value 1.**\n</code></pre>\n\n<p>First, I think the avro jar not add, but I found the jar in lib:</p>\n\n<pre><code>/home/work/nutch2/runtime/local/lib\n\n[root@li257-46 lib]# ls\n\navro-1.7.6.jar\n\navro-compiler-1.7.6.jar\n\navro-ipc-1.7.6.jar\n\navro-mapred-1.7.6.jar \n</code></pre>\n\n<p>Can you help me?</p>\n", "creation_date": 1431488768, "score": 1},
{"title": "org.apache.hadoop.mapred.InvalidInputException: Input path does not exist", "view_count": 239, "is_answered": false, "answers": [{"question_id": 30081974, "owner": {"user_id": 1699191, "link": "http://stackoverflow.com/users/1699191/ameertawfik", "user_type": "registered", "reputation": 618}, "body": "<p>I believe this is a bug in Unix based systems like OSX and FreeBsd . Nutch's crawl will not work in them. Try ubuntu.</p>\n", "creation_date": 1431637739, "is_accepted": false, "score": 0, "last_activity_date": 1431637739, "answer_id": 30247360}], "question_id": 30081974, "tags": ["hadoop", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/30081974/org-apache-hadoop-mapred-invalidinputexception-input-path-does-not-exist", "last_activity_date": 1431637739, "owner": {"age": 34, "answer_count": 17, "creation_date": 1338868561, "user_id": 1436468, "accept_rate": 50, "view_count": 33, "location": "Singapore", "reputation": 168}, "body": "<p>I have set up Apache Nutch with single node of Hadoop. When I execute the crawl command it starts the crawling. However there is an exception throwing after few minutes.</p>\n\n<p>cause:org.apache.hadoop.mapred.InvalidInputException: Input path does\nnot exist: (please refer to the image 1)</p>\n\n<p><strong>This is the invalid path according to the exception</strong>\nhdfs://localhost:54310/user/duleendra/TestCrawl/segments/drwxrwxrwx/crawl_generate</p>\n\n<p>Actually there is no such path in hdfs. </p>\n\n<p>How does this drwxrwxrwx come ?</p>\n\n<p><strong>In hdfs I can see the following path</strong> </p>\n\n<p>hdfs://localhost:54310/user/duleendra/TestCrawl/segments/20150506222506/crawl_generate</p>\n\n<p>(please refer to the image 2 as well).</p>\n\n<p><img src=\"http://i.stack.imgur.com/q1Og5.png\" alt=\"enter image description here\"></p>\n\n<p><img src=\"http://i.stack.imgur.com/0blFh.png\" alt=\"enter image description here\"></p>\n\n<p>Have I missed anything?</p>\n\n<p>Thanks</p>\n\n<p>Duleendra</p>\n", "creation_date": 1430928314, "score": 0},
{"title": "Cannot ant runtime in Apache nutch 2.3", "view_count": 424, "owner": {"age": 21, "answer_count": 6, "creation_date": 1415100811, "user_id": 4214037, "accept_rate": 89, "view_count": 16, "location": "Slovakia", "reputation": 109}, "is_answered": true, "answers": [{"question_id": 30138199, "owner": {"user_id": 4214037, "accept_rate": 89, "link": "http://stackoverflow.com/users/4214037/32cupo", "user_type": "registered", "reputation": 109}, "body": "<p>I finally solved the problem. It only needed Java 7. So I ran </p>\n\n<pre><code>sudo add-apt-repository ppa:webupd8team/java\nsudo apt-get update\nsudo apt-get install oracle-java7-installer\nsudo update-alternatives --config java\n</code></pre>\n\n<p>to get path like this </p>\n\n<blockquote>\n  <p>/usr/lib/jvm/java-7-openjdk-amd64</p>\n</blockquote>\n\n<p>ran </p>\n\n<pre><code>sudo pico /etc/environment\n</code></pre>\n\n<p>set Java home folder path </p>\n\n<blockquote>\n  <p>JAVA_HOME=\"/usr/lib/jvm/java-7-openjdk-amd64\"</p>\n</blockquote>\n\n<p>and finally ran </p>\n\n<pre><code> source /etc/environment\n</code></pre>\n", "creation_date": 1431377550, "is_accepted": true, "score": 0, "last_activity_date": 1431377550, "answer_id": 30177446}], "question_id": 30138199, "tags": ["java", "apache", "ant", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/30138199/cannot-ant-runtime-in-apache-nutch-2-3", "last_activity_date": 1431377550, "accepted_answer_id": 30177446, "body": "<p>I followed this tutorial <a href=\"https://wiki.apache.org/nutch/Nutch2Tutorial\" rel=\"nofollow\">https://wiki.apache.org/nutch/Nutch2Tutorial</a>. When I tried to run </p>\n\n<pre><code>ant runtime\n</code></pre>\n\n<p>I was getting this message </p>\n\n<blockquote>\n  <p>BUILD FAILED</p>\n  \n  <p>/usr/local/nutch/framework/apache-nutch-2.3/build.xml:113: The following error occurred while executing this line:</p>\n  \n  <p>/usr/local/nutch/framework/apache-nutch-2.3/src/plugin/build.xml:35: The following error occurred while executing this line:</p>\n  \n  <p>/usr/local/nutch/framework/apache-nutch-2.3/src/plugin/build-plugin.xml:117: Compile failed; see the compiler error output for details.</p>\n</blockquote>\n\n<p>This is on line 117 in build-plugin.xml</p>\n\n<blockquote>\n  <p>deprecation=\"${javac.deprecation}\"</p>\n</blockquote>\n\n<p>How can I fix it?</p>\n", "creation_date": 1431163269, "score": 2},
{"title": "How to extend Nutch for article crawling", "view_count": 974, "is_answered": true, "answers": [{"question_id": 13893369, "owner": {"user_id": 1274085, "link": "http://stackoverflow.com/users/1274085/hari", "user_type": "registered", "reputation": 166}, "body": "<p>If article extraction from a few websites is all that you are looking for, then check out <a href=\"http://www.crawl-anywhere.com/\" rel=\"nofollow\">http://www.crawl-anywhere.com/</a></p>\n\n<p>It comes with an admin UI where you can specify that you want to use boilerpipe article  extractor (which is great). You can also specify by URL pattern matching which pages you want crawled vs which page you want crawled AND indexed.</p>\n", "creation_date": 1356632845, "is_accepted": false, "score": 1, "last_activity_date": 1356632845, "answer_id": 14059408}], "question_id": 13893369, "tags": ["web-crawler", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/13893369/how-to-extend-nutch-for-article-crawling", "last_activity_date": 1431010216, "owner": {"user_id": 1633272, "answer_count": 3, "creation_date": 1346245673, "accept_rate": 29, "view_count": 13, "reputation": 98}, "body": "<p>I'm look for a framework to grab articles, then I find Nutch 2.1. Here's my plan and questions in each:</p>\n\n<p>1</p>\n\n<p>Add article list pages into url/seed.txt\n    Here's one problem. What I actually want to be indexed is the article pages, not the article list pages. But, if I don't allow the list page to be indexed, Nutch will do nothing because the list page is the entrance. So, how can I index only the article page without list pages?</p>\n\n<p>2</p>\n\n<p>Write a plugin to parse out the 'author', 'date', 'article body', 'headline' and maybe other information from html.\n    The 'Parser' plugin interface in Nutch 2.1 is:\n    Parse getParse(String url, WebPage page)\n    And the 'WebPage' class has some predefined attributs:</p>\n\n<pre><code>public class WebPage extends PersistentBase {\n  // ...\n  private Utf8 baseUrl;\n  // ...\n  private ByteBuffer content; // &lt;== This becomes null in IndexFilter\n  // ...\n  private Utf8 title;\n  private Utf8 text;\n  // ...\n  private Map&lt;Utf8,Utf8&gt; headers;\n  private Map&lt;Utf8,Utf8&gt; outlinks;\n  private Map&lt;Utf8,Utf8&gt; inlinks;\n  private Map&lt;Utf8,Utf8&gt; markers;\n  private Map&lt;Utf8,ByteBuffer&gt; metadata;\n  // ...\n}\n\nSo, as you can see, there are 5 maps I can put my specified attributes in. But, 'headers', 'outlinks', 'inlinks' seem not used for this. Maybe I could put those information into markers or metadata. Are they designed for this purpose?\nBTW, the Parser in trunk looks like: 'public ParseResult getParse(Content content)', and seems more reasonable for me.\n</code></pre>\n\n<p>3</p>\n\n<p>After the articles are indexed into Solr, another application can query it by 'date' then store the article information into Mysql.\n    My question here is: can Nutch store the article directly into Mysql? Or can I write a plugin to specify the index behavior?</p>\n\n<p>Is Nutch a good choice for my purpose? If not, do you guys suggest another good quality framework/library for me?\nThanks for your help.</p>\n", "creation_date": 1355584417, "score": 10},
{"title": "Why apache nutch Generatorjob takes so mutch time", "view_count": 31, "is_answered": false, "question_id": 30022904, "tags": ["apache", "hadoop", "web-crawler", "nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/30022904/why-apache-nutch-generatorjob-takes-so-mutch-time", "last_activity_date": 1430715635, "owner": {"user_id": 3454410, "answer_count": 20, "creation_date": 1395646154, "accept_rate": 48, "view_count": 174, "reputation": 832}, "body": "<p>I am using apache nutch 2.3 to crawl some data from web along with hadoop 1.2.1 and hbase 0.94.14. I have already crawled some data and now when my crawler starts, it takes too much time before it start fetching. It takes about 50 minutes in generatorjob and in fetching only 1.5 hours. What is this issue.</p>\n\n<p>I want to generatorjob should take minimum time and fetcher job should take maximum time.</p>\n", "creation_date": 1430715635, "score": 0},
{"title": "Parsing huge amount of HTML with java", "view_count": 105, "owner": {"user_id": 4841524, "view_count": 4, "answer_count": 0, "creation_date": 1430216619, "reputation": 15}, "is_answered": true, "answers": [{"question_id": 29917013, "owner": {"user_id": 1811348, "accept_rate": 94, "link": "http://stackoverflow.com/users/1811348/swapyonubuntu", "user_type": "registered", "reputation": 638}, "body": "<p>I think you should look for html parsers from this page :</p>\n\n<p><a href=\"http://en.wikipedia.org/wiki/Comparison_of_HTML_parsers\" rel=\"nofollow\">Comparison of HTML parsers</a></p>\n\n<p>Creating a script might not be a good idea. You may have inline css, javascript, escape quotes already. It will be a huge amount of pain to do this correctly.Previously, I had tried writing a script but found it cumbersome.Finally, I tried with html parsers and it worked like a charm!</p>\n", "creation_date": 1430217777, "is_accepted": true, "score": 1, "last_activity_date": 1430217777, "answer_id": 29917188}, {"last_edit_date": 1430218394, "owner": {"user_id": 4428462, "accept_rate": 86, "link": "http://stackoverflow.com/users/4428462/jonascz", "user_type": "registered", "reputation": 5887}, "body": "<p>You should do it with Jsoup.</p>\n\n<p><a href=\"http://jsoup.org/\" rel=\"nofollow\">http://jsoup.org/</a></p>\n\n<p>With it, you can easily extract the data you want, such as URL's or links using a simple API, and you can feed them into your program. It can also be used in a multithreaded environment, and is also quite fast.</p>\n\n<p>Check <a href=\"http://stackoverflow.com/a/2835555/4428462\">this</a> answer also, it will be very helpful.</p>\n\n<p>For a comparison of Java HTML parsers, go <a href=\"http://stackoverflow.com/questions/3152138/what-are-the-pros-and-cons-of-the-leading-java-html-parsers\">here</a>.</p>\n\n<p>For your question:</p>\n\n<blockquote>\n  <p>Do I create a script to escape all special characters in HTML and then pass it on as an argument.</p>\n</blockquote>\n\n<p>Jsoup does this for you. If all you want is the text of the HTML document, you might want to use a regex instead, though.</p>\n\n<blockquote>\n  <p>do I write it to a file and pass the path of the file or is there a better way</p>\n</blockquote>\n\n<p>Yes, you could pass it to your program as a string. Writing 2tb of files would be very ineficient.</p>\n\n<p>Note that whatever you do, processing 2000gb oh HTML is going to take a loooong time!</p>\n\n<p>Hope this helps.</p>\n", "question_id": 29917013, "creation_date": 1430217991, "is_accepted": false, "score": 0, "last_activity_date": 1430218394, "answer_id": 29917272}], "question_id": 29917013, "tags": ["java", "html", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/29917013/parsing-huge-amount-of-html-with-java", "last_activity_date": 1430218394, "accepted_answer_id": 29917188, "body": "<p>What's the best way to pass HTML to Java?<br>\nSpecifically, I need to crawl through 2TB of HTML files (.warc format, using nutchWAX) and feed them to my java program one at a time.  </p>\n\n<p>Workflow:  </p>\n\n<ul>\n<li>crawl a page  </li>\n<li>send page to java program  </li>\n<li>wait for answer and then continue crawling  </li>\n</ul>\n\n<p><strong>Question:</strong> Do I create a script to escape all special characters in HTML and then pass it on as an argument, do I write it to a file and pass the path of the file or is there a better way (bear in mind, 2TB of data)?</p>\n", "creation_date": 1430217271, "score": -1},
{"title": "Nutch 2.3+Gora+Hbase : Strange Values Parsed", "view_count": 32, "is_answered": false, "question_id": 29859111, "tags": ["nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/29859111/nutch-2-3gorahbase-strange-values-parsed", "last_activity_date": 1429919326, "owner": {"user_id": 3604265, "view_count": 0, "answer_count": 0, "creation_date": 1399292603, "reputation": 6}, "body": "<p>I have started playing around with Nutch2.3+Gora+HBase, but the some  results look strange in HBase, is there any configuration or a simple step I might be missing?</p>\n\n<pre><code>status: value=\\x00\\x00\\x00\\x01\nfetchTime: value=\\x00\\x00\\x01L\\x93\\x92\\x0F\\x5C\nprevFetchTime: value=\\x00\\x00\\x01L\\x91]\\xF5\\x1C\n</code></pre>\n\n<p>Expected values should be: </p>\n\n<ul>\n<li>status: 4</li>\n<li>fetchTime:  1426888912463  (i.e. in timestamp format)</li>\n<li>prevFetchTime:  1424296904936  (i.e. in timestamp format)</li>\n</ul>\n\n<p>Why am I getting the actual values instead of the expected ones?</p>\n", "creation_date": 1429917780, "score": 1},
{"title": "Nutch 2.2.1 &amp; HBase - Can I create a new property in nutch-site.xml", "view_count": 46, "owner": {"user_id": 4787211, "view_count": 0, "answer_count": 0, "creation_date": 1429013382, "reputation": 5}, "is_answered": true, "answers": [{"question_id": 29786729, "owner": {"user_id": 1727204, "accept_rate": 43, "link": "http://stackoverflow.com/users/1727204/ali-n", "user_type": "registered", "reputation": 524}, "body": "<p>There are two different solutions available for your problem:</p>\n\n<ol>\n<li><p>Implementing a customized <code>HtmlParseFilter</code> plugin to filter pages\nbased on your desired keywords. For more information about Nutch\nextension points and writing customized plugin for Nutch take a look\nat these manuals:</p>\n\n<p><a href=\"http://wiki.apache.org/nutch/AboutPlugins\" rel=\"nofollow\">http://wiki.apache.org/nutch/AboutPlugins</a></p>\n\n<p><a href=\"http://wiki.apache.org/nutch/WritingPluginExample\" rel=\"nofollow\">http://wiki.apache.org/nutch/WritingPluginExample</a></p></li>\n<li><p>Using an indexer to filter documents based on desired keywords;\nHowever, this solution is available if you have indexer in your\nsystem design architecture. In this case Apache Solr could help you\nfor filtering documents before indexing. Here you have to implement\na customized <code>UpdateRequestProcessor</code>. For more information about\nSolr and its extension points take a look at these pages:</p>\n\n<p><a href=\"https://wiki.apache.org/solr/FrontPage\" rel=\"nofollow\">https://wiki.apache.org/solr/FrontPage</a></p>\n\n<p><a href=\"https://wiki.apache.org/solr/UpdateRequestProcessor\" rel=\"nofollow\">https://wiki.apache.org/solr/UpdateRequestProcessor</a></p></li>\n</ol>\n", "creation_date": 1429863988, "is_accepted": true, "score": 0, "last_activity_date": 1429863988, "answer_id": 29842323}], "question_id": 29786729, "tags": ["web", "web-crawler", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/29786729/nutch-2-2-1-hbase-can-i-create-a-new-property-in-nutch-site-xml", "last_activity_date": 1429863988, "accepted_answer_id": 29842323, "body": "<p>I wanna develop a topical web robot using Nutch 2.2.1. And I wanna create a new property with some topic keywords,like following:</p>\n\n<pre><code>&lt;property&gt;\n    &lt;name&gt;html.metatitle.keys&lt;/name&gt;\n    &lt;value&gt;movie,actor,firm&lt;/value&gt;\n    &lt;description&gt;\n    &lt;/description&gt;\n&lt;/property&gt;\n</code></pre>\n", "creation_date": 1429670150, "score": 0},
{"title": "Nutch 2.2.1 &amp; HBase - Infer whether to save Webpage into HBase or not according to some key words", "view_count": 29, "is_answered": false, "question_id": 29787654, "tags": ["web", "web-crawler", "nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/29787654/nutch-2-2-1-hbase-infer-whether-to-save-webpage-into-hbase-or-not-according", "last_activity_date": 1429676069, "owner": {"user_id": 4787211, "view_count": 0, "answer_count": 0, "creation_date": 1429013382, "reputation": 5}, "body": "<p>Did function <em><code>output</code></em> in <em><code>FetcherReducer.java</code></em> response for storing webpage in HBase in Fetcher phase using code as followed?</p>\n\n<pre><code>context.write(key, fit.page)  \n</code></pre>\n\n<p>If yes, is parameter <code>content</code> the content of fetched webpage? Can I use it to abandon webpage by comparing with key words defined by myself?\nOtherwise, can you propose an effective approach to inferring whether to save Webpage into HBase or not  according to some key words?</p>\n", "creation_date": 1429676069, "score": 0},
{"title": "Apache Nutch 1.9 in local Eclipse to run on Amazon EMR remotely", "view_count": 129, "is_answered": false, "answers": [{"question_id": 29734911, "owner": {"user_id": 1727204, "accept_rate": 43, "link": "http://stackoverflow.com/users/1727204/ali-n", "user_type": "registered", "reputation": 524}, "body": "<p>As far as I know you can not run Nutch in distributed mode from Eclipse. In order to run Nutch on hadoop cluster you have to follow these steps:</p>\n\n<ul>\n<li><p>Apply your required configuration in nutch-site.xml and other config files (according to the selected plugins)</p></li>\n<li><p>Build Nutch using <code>ant runtime</code></p></li>\n<li><p>Follow the runtime/deploy directory to find nutch hadoop job.</p></li>\n<li><p>Run following command:</p>\n\n<p>hadoop jar nutch-${version}.job ${your_main_class} ${class_parameters}</p></li>\n</ul>\n\n<p>For example suppose your main crawler class in org.apache.nutch.crawl.crawler in this case the running command would be:</p>\n\n<pre><code>hadoop jar nutch-${version}.job org.apache.nutch.crawl.crawler urls -dir crawl -depth 2 -topN 1000\n</code></pre>\n", "creation_date": 1429517402, "is_accepted": false, "score": 0, "last_activity_date": 1429517402, "answer_id": 29742419}], "question_id": 29734911, "tags": ["java", "eclipse", "hadoop", "amazon-web-services", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/29734911/apache-nutch-1-9-in-local-eclipse-to-run-on-amazon-emr-remotely", "last_activity_date": 1429517402, "owner": {"user_id": 2110873, "answer_count": 14, "creation_date": 1361875542, "accept_rate": 32, "view_count": 108, "reputation": 459}, "body": "<p>I am on Windows 8 32 bit, running Eclipse Juno.</p>\n\n<p>I have just started working on Amazon EMR. So far, I am being able to connect to EMR remotely from my local using SSH and inside Eclipse. I could run my custom JAR on EMR remotely by creating AWS project in Eclipse and using th Custom JAR execution on EMR commands.</p>\n\n<p>I am now trying to run Apache Nutch 1.9 from inside my Eclipse. I did Ant build to create Nutch Eclipse project and I am being to export inside Eclipse workspace successfully. Now, when I am running the Injector I am getting the following error:</p>\n\n<pre><code>Injector: starting at 2015-04-20 00:56:08\nInjector: crawlDb: crawl/crawldb\nInjector: urlDir: urls\nInjector: Converting injected urls to crawl db entries.\nInjector: java.io.IOException: Failed to set permissions of path: \\tmp\\hadoop-Kajari_G\\mapred\\staging\\Kajari_G881485826\\.staging to 0700\n</code></pre>\n\n<p>I found out this is sue to permission issues of Hadoop. After lots of search online I realized this is a common issue in Windows. I ran it via Cygwin as Admin and still couldn't fix it. </p>\n\n<p>So, now I want to still run the Injector code, but I want to run it on my remote EMR cluster, instead of in my local. </p>\n\n<p>Can you please guide me how to tell my Apache Nutch Eclipse project to run on Amazon EMR and not locally? I don't want to create a JAR and run it. I want to run it as an usual Run As --> in Eclipse.</p>\n\n<p>Is this possible to do at all? I did search this online, but couldn't find any working solution.</p>\n\n<p>Thanks!</p>\n", "creation_date": 1429472406, "score": 0},
{"title": "why does nutch always create the linkdb, even though it&#39;s not need for content fetching?", "view_count": 55, "owner": {"user_id": 1058511, "answer_count": 22, "creation_date": 1321905325, "accept_rate": 47, "view_count": 109, "reputation": 591}, "is_answered": true, "answers": [{"last_edit_date": 1429305905, "owner": {"user_id": 1699191, "link": "http://stackoverflow.com/users/1699191/ameertawfik", "user_type": "registered", "reputation": 618}, "body": "<p>That is because Nutch uses the page rank (which is being calculated using link information) to prioritize crawling. For instance, a link that has a high page rank will be crawled before than the one with low page rank. </p>\n\n<p>Nutch was designed to be used as a large scale web crawler,therefore calculating page rank and scoring web pages with it was and still an important component. If you are crawling a few sites, then you probably should use scrappy (a python library). </p>\n\n<p>I hope  that answers your question.</p>\n", "question_id": 29663323, "creation_date": 1429177020, "is_accepted": true, "score": 1, "last_activity_date": 1429305905, "answer_id": 29670792}], "question_id": 29663323, "tags": ["nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/29663323/why-does-nutch-always-create-the-linkdb-even-though-its-not-need-for-content-f", "last_activity_date": 1429305905, "accepted_answer_id": 29670792, "body": "<p>I am reading thru the chapter on nutch in hadoop, the definitive guide. I understand the concept of ranking a page using inverse link. However, I don't see that playing a role when you just want to crawl a few sites. Since creation of the linkdb is a map reduce job, it's bound to take up a lot of computing resources. I am just wondering why is linkdb always generated when most of nutch use cases is just getting web content for designated urls.</p>\n", "creation_date": 1429144316, "score": 0},
{"title": "How to skip Apache nutch from one step to next", "view_count": 55, "is_answered": true, "answers": [{"question_id": 28272768, "owner": {"user_id": 1727204, "accept_rate": 43, "link": "http://stackoverflow.com/users/1727204/ali-n", "user_type": "registered", "reputation": 524}, "body": "<p>Since Apache Nutch is based on Hadoop, different tasks has been applied step by step. It means Generate comes after finishing Inject. Fetch comes after Generate and etc. However for the Parsing step in order to change the default nutch behavior there is a configuration available.</p>\n\n<pre><code>&lt;property&gt;\n    &lt;name&gt;fetcher.parse&lt;/name&gt;\n    &lt;value&gt;true&lt;/value&gt;\n    &lt;description&gt;For merge parse and fetch set it true&lt;/description&gt;\n&lt;/property&gt;\n</code></pre>\n\n<p>The default value of fetcher.parse is false. You have to change it to true and put that inside <code>nutch-site.xml</code>. This parameter changes the default parsing mechanism for do the parse and fetch simultaneously. For other tasks of nutch I am afraid you have to do the development yourself.</p>\n", "creation_date": 1429258550, "is_accepted": false, "score": 1, "last_activity_date": 1429258550, "answer_id": 29693837}], "question_id": 28272768, "tags": ["apache", "web-crawler", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/28272768/how-to-skip-apache-nutch-from-one-step-to-next", "last_activity_date": 1429258550, "owner": {"user_id": 3454410, "answer_count": 20, "creation_date": 1395646154, "accept_rate": 48, "view_count": 174, "reputation": 832}, "body": "<p>I am using apache Nutch 2.3. I want to skip fetch step of apache nutch to next step that is parsing i.e suppose that my crawling is in fetcing state and I want to go to parse step so that It completes early. </p>\n\n<p>How I skip apache nutch from one step to next?</p>\n", "creation_date": 1422861504, "score": 1},
{"title": "nutch on hadoop keeps quitting before completion", "view_count": 25, "is_answered": false, "question_id": 29684010, "tags": ["hadoop", "nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/29684010/nutch-on-hadoop-keeps-quitting-before-completion", "last_activity_date": 1429211723, "owner": {"user_id": 1058511, "answer_count": 22, "creation_date": 1321905325, "accept_rate": 47, "view_count": 109, "reputation": 591}, "body": "<p>I am running nutch 1.7 using a 3 node hadoop cluster. I entered more than 3000 urls in the seeds.txt file. The job finishes surprisingly fast with success status. When I check the results in solr, many of the urls returns nothing. Most of the ones that do have content only got one page's worth. </p>\n\n<p>I retry just a single url which got no results before. Now it gets 94 pages of content. So the problem is definitely not due to robot rejection. </p>\n\n<p>everytime I reran the entire list, it would run for sometime and get a few more page content, but never completes the list.</p>\n\n<p>What could cause nutch to quit like that?</p>\n", "creation_date": 1429211723, "score": 0},
{"title": "crawling with Nutch 2.3, Cassandra 2.0, and solr 4.10.3 returns 0 results", "view_count": 1349, "is_answered": false, "answers": [{"question_id": 28446576, "owner": {"user_id": 2086509, "accept_rate": 73, "link": "http://stackoverflow.com/users/2086509/jayesh-bhoyar", "user_type": "registered", "reputation": 594}, "body": "<p>You will have to add the regex for your website that you want to crawl in regex-urlfilter.txt to pick the link that you have added in nutch-site.xml.</p>\n\n<p>Right now it will only crawl \"nutch.apache.org\"</p>\n\n<p>Try adding below line:</p>\n\n<pre><code>+^http://([a-z0-9]*\\.)*ideone.com/\n</code></pre>\n\n<p>Try to set nutch logs in debug level and get the logs while executing the crawl command.</p>\n\n<p>It will clearly shows why you are unable to crawl and index the site.</p>\n\n<p>Regards,</p>\n\n<p>Jayesh Bhoyar</p>\n\n<p><a href=\"http://technical-fundas.blogspot.com/p/technical-profile.html\" rel=\"nofollow\">http://technical-fundas.blogspot.com/p/technical-profile.html</a></p>\n", "creation_date": 1423643553, "is_accepted": false, "score": 0, "last_activity_date": 1423643553, "answer_id": 28449747}, {"question_id": 28446576, "owner": {"user_id": 4126377, "link": "http://stackoverflow.com/users/4126377/yi-zhang", "user_type": "registered", "reputation": 1}, "body": "<p>I got a similar problem recently. I think you can try the following steps to find out the problem.</p>\n\n<p>1 Do some tests to make sure the DB works well.</p>\n\n<p>2 Instead of running the crawl in batch, you can call nutch step by step and watch the log change as well as the change of DB content, in particular, the new urls.</p>\n\n<p>3 Turn off solr and focus on nutch and the DB. </p>\n", "creation_date": 1425245881, "is_accepted": false, "score": 0, "last_activity_date": 1425245881, "answer_id": 28799424}], "question_id": 28446576, "tags": ["solr", "cassandra", "web-crawler", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/28446576/crawling-with-nutch-2-3-cassandra-2-0-and-solr-4-10-3-returns-0-results", "last_activity_date": 1429194908, "owner": {"user_id": 4197047, "view_count": 4, "answer_count": 0, "creation_date": 1414649488, "reputation": 1}, "body": "<p>I mainly followed the guide on <a href=\"https://wiki.apache.org/nutch/Nutch2Cassandra\" rel=\"nofollow\">this page</a>. I installed Nutch 2.3, Cassandra 2.0, and solr 4.10.3. Set up went well. But when I executed the following command. No urls were fetched.</p>\n\n<pre><code>./bin/crawl urls/seed.txt TestCrawl http://localhost:8983/solr/ 2\n</code></pre>\n\n<p>Below are my settings.</p>\n\n<p>nutch-site.xml</p>\n\n<pre><code>http://ideone.com/H8MPcl\n</code></pre>\n\n<p>regex-urlfilter.txt</p>\n\n<pre><code>+^http://([a-z0-9]*\\.)*nutch.apache.org/\n</code></pre>\n\n<p>hadoop.log</p>\n\n<pre><code>http://ideone.com/LnpAw4\n</code></pre>\n\n<p>I don't see any errors in the log file. I am really lost. Any help would be appreciated. Thanks!</p>\n", "creation_date": 1423628573, "score": 0},
{"title": "Modifying the Nutch crawler to parse the page and get certain data from the pages crawled", "view_count": 354, "is_answered": false, "answers": [{"question_id": 29070411, "owner": {"user_id": 4793563, "link": "http://stackoverflow.com/users/4793563/jakub-jano%c5%a1t%c3%adk", "user_type": "registered", "reputation": 21}, "body": "<p>Probably late, but for anyone facing same issue. This is solved by providing your own ParseFilter plugin. </p>\n\n<p>You can read about plugins at <a href=\"http://wiki.apache.org/nutch/AboutPlugins\" rel=\"nofollow\">this documentation</a></p>\n\n<p>Basically you implement method parse which has DocumentFragment object as argument. From DocumentFragment you can then parse whatever info you need using xPath. Parsed data can be saved inside WebPage metadata.</p>\n\n<p>After you implement plugin you just have to include it into source, use in nutch-site.xml, build and you are good to go.</p>\n", "creation_date": 1429128497, "is_accepted": false, "score": 0, "last_activity_date": 1429128497, "answer_id": 29659898}], "question_id": 29070411, "tags": ["java", "web-crawler", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/29070411/modifying-the-nutch-crawler-to-parse-the-page-and-get-certain-data-from-the-page", "last_activity_date": 1429128497, "owner": {"age": 31, "answer_count": 60, "creation_date": 1303499210, "user_id": 1486762, "accept_rate": 55, "view_count": 444, "location": "Ahmedabad, India", "reputation": 879}, "body": "<p>I want to crawl several sites and collect data based on the language i.e. \"Java\" etc. I am new to Nutch crawler. I just finished setup of Nutch 2.3 with HBase. How to customize the crawling so that when each page is parsed I can get the links within that page and extract some data from it. Such as date, topic etc.</p>\n\n<p>Thank you.</p>\n", "creation_date": 1426485351, "score": 2},
{"title": "What is the process to compile Nutch into one Jar file (and run it)?", "view_count": 904, "owner": {"age": 32, "answer_count": 38, "creation_date": 1241127194, "user_id": 98975, "accept_rate": 90, "view_count": 278, "location": "San Francisco, CA", "reputation": 2604}, "is_answered": true, "answers": [{"question_id": 5024711, "owner": {"user_id": 98975, "accept_rate": 90, "link": "http://stackoverflow.com/users/98975/viksit", "user_type": "registered", "reputation": 2604}, "body": "<p>I realized after much looking around that to run Nutch off the command line in a simple manner, the nutch.job file can be used instead. The syntax is,</p>\n\n<pre><code>hadoop jar nutch-1.0.job org.apache.nutch.crawl.Crawl urls -dir crawl -depth 1\n</code></pre>\n", "creation_date": 1299103914, "is_accepted": true, "score": 1, "last_activity_date": 1299103914, "answer_id": 5174179}], "question_id": 5024711, "tags": ["java", "jar", "hadoop", "executable-jar", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/5024711/what-is-the-process-to-compile-nutch-into-one-jar-file-and-run-it", "last_activity_date": 1429093057, "accepted_answer_id": 5174179, "body": "<p>I'm trying to run the Nutch crawler in a way that I can access all its functionality through one JAR file that contains all its dependencies.</p>\n\n<p>For instance,</p>\n\n<pre><code>java -jar nutch-all-1.2.jar -crawl &lt;other params&gt;\n</code></pre>\n\n<p>and at a later stage, call it with hadoop.</p>\n\n<p>Currently, doing a </p>\n\n<pre><code>java -jar nutch-1.2.jar \n</code></pre>\n\n<p>on the JAR file that exists in the nutch directory results in the error,</p>\n\n<pre><code>Failed to load Main-Class manifest attribute from\nnutch-1.2.jar\n</code></pre>\n\n<p>I believe this happens because this particular JAR does not contain the manifest XML files, or other dependent JARs. What would you recommend as the best method to build nutch into one JAR for this purpose?</p>\n\n<p>Thanks!</p>\n", "creation_date": 1297912469, "score": 1},
{"title": "Building Nutch Plugin: class dependency", "view_count": 339, "is_answered": false, "answers": [{"question_id": 29631427, "owner": {"user_id": 1699191, "link": "http://stackoverflow.com/users/1699191/ameertawfik", "user_type": "registered", "reputation": 618}, "body": "<p>As far as I know from my experience the implementation id should have the same package structure as your extension point. Can you try that and see if it solves your problem.</p>\n", "creation_date": 1429051433, "is_accepted": false, "score": 0, "last_activity_date": 1429051433, "answer_id": 29638642}], "question_id": 29631427, "tags": ["java", "ant", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/29631427/building-nutch-plugin-class-dependency", "last_activity_date": 1429051433, "owner": {"user_id": 1727204, "answer_count": 15, "creation_date": 1349638210, "accept_rate": 43, "view_count": 102, "reputation": 524}, "body": "<p>I wrote some Nutch plugin with using different extension points such as Protocol, Parser and etc. These plugins work perfectly inside eclipse. But in order to use them on hadoop cluster it should be built by using ANT. My problem is, I wrote some classes in some new packages inside core folder(src). These classes are shared cross different developed plugins. My problem is at build time of developed plugins, ANT can not find the mentioned shared classes, so I am unable to complete the build process successfully. For better understanding of my problem Here is build.xml of one of my plugins:</p>\n\n<pre><code>&lt;project name=\"filter-news\" default=\"jar-core\"&gt;\n\n  &lt;import file=\"../build-plugin.xml\"/&gt;\n\n    &lt;!-- Build compilation dependencies --&gt;\n        &lt;target name=\"deps-jar\"&gt;\n          &lt;ant target=\"jar\" inheritall=\"false\" dir=\"../lib-xml\"/&gt;\n        &lt;/target&gt;\n\n        &lt;!-- Add compilation dependencies to classpath --&gt;\n        &lt;path id=\"plugin.deps\"&gt;\n          &lt;fileset dir=\"${nutch.root}/build\"&gt;\n            &lt;include name=\"**/lib-xml/*.jar\" /&gt;\n          &lt;/fileset&gt;\n        &lt;/path&gt;\n\n        &lt;!-- Deploy Unit test dependencies --&gt;\n  &lt;!-- Deploy Unit test dependencies --&gt;\n\n  &lt;!-- for junit test --&gt;\n\n&lt;/project&gt;\n</code></pre>\n\n<p>ivy.xml:</p>\n\n<pre><code>&lt;ivy-module version=\"1.0\"&gt;\n  &lt;info organisation=\"org.apache.nutch\" module=\"${ant.project.name}\"&gt;\n    &lt;license name=\"Apache 2.0\"/&gt;\n    &lt;ivyauthor name=\"Apache Nutch Team\" url=\"http://nutch.apache.org\"/&gt;\n    &lt;description&gt;\n        Apache Nutch\n    &lt;/description&gt;\n  &lt;/info&gt;\n\n  &lt;configurations&gt;\n    &lt;include file=\"../../..//ivy/ivy-configurations.xml\"/&gt;\n  &lt;/configurations&gt;\n\n  &lt;publications&gt;\n    &lt;!--get the artifact from our module name--&gt;\n    &lt;artifact conf=\"master\"/&gt;\n  &lt;/publications&gt;\n\n  &lt;dependencies&gt;\n&lt;dependency org=\"mysql\" name=\"mysql-connector-java\" rev=\"5.1.31\"/&gt;\n&lt;dependency org=\"net.sourceforge.htmlcleaner\" name=\"htmlcleaner\" rev=\"2.2\"/&gt;\n&lt;dependency org=\"commons-jxpath\" name=\"commons-jxpath\" rev=\"1.3\"/&gt;\n\n  &lt;/dependencies&gt;\n\n&lt;/ivy-module&gt;\n</code></pre>\n\n<p>plugin.xml:</p>\n\n<pre><code>&lt;plugin id=\"filter-news\" name=\"Apache Nutch XML/HTML Parser/Indexing Filter\" version=\"1.4\" provider-name=\"nutch.org\"&gt;\n\n    &lt;runtime&gt;\n        &lt;library name=\"filter-news.jar\"&gt;\n            &lt;export name=\"*\"/&gt;\n        &lt;/library&gt;\n        &lt;library name=\"ant-1.7.0.jar\"/&gt;\n        &lt;library name=\"ant-launcher-1.7.0.jar\"/&gt;\n        &lt;library name=\"jdom-1.1.jar\"/&gt;\n        &lt;library name=\"commons-jxpath-1.3.jar\"/&gt;\n        &lt;library name=\"htmlcleaner-2.2.jar\"/&gt;\n        &lt;library name=\"mysql-connector-java-5.1.31.jar\"/&gt;\n    &lt;/runtime&gt;\n\n    &lt;requires&gt;\n        &lt;import plugin=\"nutch-extensionpoints\"/&gt;\n    &lt;/requires&gt;\n\n    &lt;extension id=\"org.apache.nutch.parse\" name=\"Nutch XML/HTML Html parser filter\" point=\"org.apache.nutch.parse.HtmlParseFilter\"&gt;\n        &lt;implementation id=\"com.ictcert.nutch.filter.news.NewsHtmlFilter\"                       class=\"com.ictcert.nutch.filter.news.NewsHtmlFilter\" /&gt;\n    &lt;/extension&gt;\n    &lt;extension id=\"org.apache.nutch.indexer\" name=\"Nutch XML/HTML Indexing Filter\" point=\"org.apache.nutch.indexer.IndexingFilter\"&gt;\n        &lt;implementation id=\"com.ictcert.nutch.filter.news.NewsIndexingFilter\" class=\"com.ictcert.nutch.filter.news.NewsIndexingFilter\"/&gt;\n    &lt;/extension&gt;\n\n&lt;/plugin&gt;\n</code></pre>\n\n<p>When I try to build this plugin ant can not find all of the class dependencies related to <code>com.ictcert.nutch</code> package which is located in core part of nutch (src). While for other classes located in <code>org.apache.nutch</code> I have not such problem. Would you please tell me what is wrong with my configuration that the default packages could be found by ANT but the new ones could not.</p>\n", "creation_date": 1429025366, "score": 0},
{"title": "Nutch 1.9 not crawling url with querystring params", "view_count": 447, "owner": {"user_id": 2470400, "view_count": 5, "answer_count": 1, "creation_date": 1370857447, "reputation": 8}, "is_answered": true, "answers": [{"question_id": 29514441, "owner": {"user_id": 1699191, "link": "http://stackoverflow.com/users/1699191/ameertawfik", "user_type": "registered", "reputation": 618}, "body": "<p>By default, links with query parameters are ignored or filtered out. To enable crawling urls with parameters go to <strong>conf/regex-urlfilter.txt</strong> and comment the following line by adding # to the beginning of the line.</p>\n\n<pre><code># skip URLs containing certain characters as probable queries, etc.\n#-[?*!@=]\n</code></pre>\n", "creation_date": 1428512918, "is_accepted": true, "score": 4, "last_activity_date": 1428512918, "answer_id": 29521051}, {"question_id": 29514441, "owner": {"user_id": 2470400, "link": "http://stackoverflow.com/users/2470400/rohan", "user_type": "registered", "reputation": 8}, "body": "<p>Alright i found the solution. It was a mistake at my ends. ifttt domain had filters that particular region that i wanted too crawl through robots.txt</p>\n\n<p>before crawling just check whether or not that site is allowing it self to crawl </p>\n\n<p>by checking robots.txt</p>\n\n<p>like this : <a href=\"https://ifttt.com/robots.txt\" rel=\"nofollow\">https://ifttt.com/robots.txt</a></p>\n", "creation_date": 1429021767, "is_accepted": false, "score": 0, "last_activity_date": 1429021767, "answer_id": 29630042}], "question_id": 29514441, "tags": ["java", "web-crawler", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/29514441/nutch-1-9-not-crawling-url-with-querystring-params", "last_activity_date": 1429021767, "accepted_answer_id": 29521051, "body": "<p>I am using Nutch1.9 and trying to crawl using individual commands. as can be seen in the output when going in to the 2nd level generater returned with 0 records.\nany one has faced this issue ? i am stuck in here from past 2 days. have searched all possible options.  any leads/helps would be much appreciated.</p>\n\n<pre><code>&lt;br&gt;#######  INJECT   ######&lt;br&gt;\nInjector: starting at 2015-04-08 17:36:20 &lt;br&gt;\nInjector: crawlDb: crawl/crawldb&lt;br&gt;\nInjector: urlDir: urls&lt;br&gt;\nInjector: Converting injected urls to crawl db entries.&lt;br&gt;\nInjector: overwrite: false&lt;br&gt;\nInjector: update: false&lt;br&gt;\nInjector: Total number of urls rejected by filters: 0&lt;br&gt;\nInjector: Total number of urls after normalization: 1&lt;br&gt;\nInjector: Total new urls injected: 1&lt;br&gt;\nInjector: finished at 2015-04-08 17:36:21, elapsed: 00:00:01&lt;br&gt;\n####  GENERATE  ###&lt;br&gt;\nGenerator: starting at 2015-04-08 17:36:22&lt;br&gt;\nGenerator: Selecting best-scoring urls due for fetch.&lt;br&gt;\nGenerator: filtering: true&lt;br&gt;\nGenerator: normalizing: true&lt;br&gt;\nGenerator: topN: 100000&lt;br&gt;\nGenerator: jobtracker is 'local', generating exactly one partition.&lt;br&gt;\nGenerator: Partitioning selected urls for politeness.&lt;br&gt;\nGenerator: segment: crawl/segments/20150408173625&lt;br&gt;\nGenerator: finished at 2015-04-08 17:36:26, elapsed: 00:00:03&lt;br&gt;\ncrawl/segments/20150408173625&lt;br&gt;\n#### FETCH  ####&lt;br&gt;\nFetcher: starting at 2015-04-08 17:36:26&lt;br&gt;\nFetcher: segment: crawl/segments/20150408173625&lt;br&gt;\nUsing queue mode : byHost&lt;br&gt;\nFetcher: threads: 10&lt;br&gt;\nFetcher: time-out divisor: 2&lt;br&gt;\nQueueFeeder finished: total 1 records + hit by time limit :0&lt;br&gt;\nUsing queue mode : byHost&lt;br&gt;\nfetching https://ifttt.com/recipes/search?q=SmartThings (queue crawl delay=5000ms)&lt;br&gt;\nUsing queue mode : byHost&lt;br&gt;\nUsing queue mode : byHost&lt;br&gt;\nThread FetcherThread has no more work available&lt;br&gt;\n-finishing thread FetcherThread, activeThreads=1&lt;br&gt;\nUsing queue mode : byHost&lt;br&gt;\nThread FetcherThread has no more work available&lt;br&gt;\n-finishing thread FetcherThread, activeThreads=1&lt;br&gt;\nUsing queue mode : byHost&lt;br&gt;\nThread FetcherThread has no more work available&lt;br&gt;\n-finishing thread FetcherThread, activeThreads=1&lt;br&gt;\nUsing queue mode : byHost&lt;br&gt;\nThread FetcherThread has no more work available&lt;br&gt;\n-finishing thread FetcherThread, activeThreads=1&lt;br&gt;\nUsing queue mode : byHost&lt;br&gt;\nThread FetcherThread has no more work available&lt;br&gt;\n-finishing thread FetcherThread, activeThreads=1&lt;br&gt;\nThread FetcherThread has no more work available&lt;br&gt;\n-finishing thread FetcherThread, activeThreads=1&lt;br&gt;\nUsing queue mode : byHost&lt;br&gt;\nThread FetcherThread has no more work available&lt;br&gt;\n-finishing thread FetcherThread, activeThreads=1&lt;br&gt;\nUsing queue mode : byHost&lt;br&gt;\nThread FetcherThread has no more work available&lt;br&gt;\n-finishing thread FetcherThread, activeThreads=1&lt;br&gt;\nUsing queue mode : byHost&lt;br&gt;\nFetcher: throughput threshold: -1&lt;br&gt;\nThread FetcherThread has no more work available&lt;br&gt;\nFetcher: throughput threshold retries: 5&lt;br&gt;\n-finishing thread FetcherThread, activeThreads=1&lt;br&gt;\n-activeThreads=1, spinWaiting=0, fetchQueues.totalSize=0, fetchQueues.getQueueCount=1&lt;br&gt;\n-activeThreads=1, spinWaiting=0, fetchQueues.totalSize=0, fetchQueues.getQueueCount=1&lt;br&gt;\n-activeThreads=1, spinWaiting=0, fetchQueues.totalSize=0, fetchQueues.getQueueCount=1&lt;br&gt;\n-activeThreads=1, spinWaiting=0, fetchQueues.totalSize=0, fetchQueues.getQueueCount=1&lt;br&gt;\nThread FetcherThread has no more work available&lt;br&gt;\n-finishing thread FetcherThread, activeThreads=0&lt;br&gt;\n-activeThreads=0, spinWaiting=0, fetchQueues.totalSize=0, fetchQueues.getQueueCount=0&lt;br&gt;\n-activeThreads=0&lt;br&gt;\nFetcher: finished at 2015-04-08 17:36:33, elapsed: 00:00:06&lt;br&gt;\n#### PARSE ####&lt;br&gt;\nParseSegment: starting at 2015-04-08 17:36:33&lt;br&gt;\nParseSegment: segment: crawl/segments/20150408173625&lt;br&gt;\nParseSegment: finished at 2015-04-08 17:36:35, elapsed: 00:00:01&lt;br&gt;\n########   UPDATEDB   ##########&lt;br&gt;\nCrawlDb update: starting at 2015-04-08 17:36:36&lt;br&gt;\nCrawlDb update: db: crawl/crawldb&lt;br&gt;\nCrawlDb update: segments: [crawl/segments/20150408173625]&lt;br&gt;\nCrawlDb update: additions allowed: true&lt;br&gt;\nCrawlDb update: URL normalizing: false&lt;br&gt;\nCrawlDb update: URL filtering: false&lt;br&gt;\nCrawlDb update: 404 purging: false&lt;br&gt;\nCrawlDb update: Merging segment data into db.&lt;br&gt;\nCrawlDb update: finished at 2015-04-08 17:36:37, elapsed: 00:00:01&lt;br&gt;\n#####  GENERATE  ######&lt;br&gt;\nGenerator: starting at 2015-04-08 17:36:38&lt;br&gt;\nGenerator: Selecting best-scoring urls due for fetch.&lt;br&gt;\nGenerator: filtering: true&lt;br&gt;\nGenerator: normalizing: true&lt;br&gt;\nGenerator: topN: 100000&lt;br&gt;\nGenerator: jobtracker is 'local', generating exactly one partition.&lt;br&gt;\nGenerator: 0 records selected for fetching, exiting ...&lt;br&gt;\n#######   EXTRACT  #########&lt;br&gt;\ncrawl/segments/20150408173625&lt;br&gt;\n#### Segments #####&lt;br&gt;\n20150408173625&lt;br&gt;\n</code></pre>\n\n<p>EDIT : \nSo i checked with another URL with query params (         <a href=\"http://queue.acm.org/detail.cfm?id=988409\" rel=\"nofollow\">http://queue.acm.org/detail.cfm?id=988409</a>    ) and it crawled it fine...</p>\n\n<p>so this means that it is specifically not crawling my original url :     <a href=\"https://ifttt.com/recipes/search?q=SmartThings&amp;ac=true\" rel=\"nofollow\">https://ifttt.com/recipes/search?q=SmartThings&amp;ac=true</a>    </p>\n\n<p>i have tried crawling urls without querystring for this ifttt domain and nutch crawls it successfully... </p>\n\n<p>i think the issue is with crawling https website with query strings. \nany help regarding this issue ?</p>\n", "creation_date": 1428495587, "score": 1},
{"title": "Apache Nutch steps explaination", "view_count": 430, "owner": {"user_id": 3089214, "answer_count": 4, "creation_date": 1386725546, "accept_rate": 38, "view_count": 15, "reputation": 75}, "is_answered": true, "answers": [{"question_id": 29589400, "owner": {"user_id": 1699191, "link": "http://stackoverflow.com/users/1699191/ameertawfik", "user_type": "registered", "reputation": 618}, "body": "<p>Your  assumption for the first and second steps are correct. However, you need to understand how the whole workflow takes place. When Nutch fetches urls, it fetches data like web page data or images as binary and stored them into segements as crawl data using a class named Content. </p>\n\n<p>Later, in the parsing step, the stored Content objects are parsed into another data format called <strong>ParsedData</strong> that includes text of the data plus its outlinks if avaiable. The ParsedData are put back to segements to be processed in the next job batch. After this step comes the crawldb update job, here the links from the previous step are put back into the crawldb to update the page rank and web links details. </p>\n\n<p>At the indexing step, the information from parsed data at segments are structured into fields. Nutch uses a classed named \"NutchDocument\" to store the structured data, The nutch documents are put back into segments to be processed in the next step. Lastly, Nutch sends Nutch documents to indexing storage like Solr or Elasticsearch. This is the last step, at this stage you can remove the segments if you do not want to send them again to indexing storage. In another words, this is the follow of data</p>\n\n<p>seed list -> inject urls -> crawl item (simply the urls) -> Contents-> parsed data -> nutch documents.</p>\n\n<p>I hope that answers some of your questions.</p>\n", "creation_date": 1428850681, "is_accepted": true, "score": 1, "last_activity_date": 1428850681, "answer_id": 29590836}], "question_id": 29589400, "tags": ["apache", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/29589400/apache-nutch-steps-explaination", "last_activity_date": 1428851483, "accepted_answer_id": 29590836, "body": "<p>I have followed article: <a href=\"https://wiki.apache.org/nutch/NutchTutorial\" rel=\"nofollow\">https://wiki.apache.org/nutch/NutchTutorial</a> and set up apache nutch +solr. But i want to clarify if i understood correct about working of nutch steps.</p>\n\n<p>1). Inject: In this part, apache reads url list from given seed.txt, compare urls with regex-urlfiler regex and update crawldb with supported urls.</p>\n\n<p>2). Generate: bin/nutch generate crawl/crawldb crawl/segments\n     Nutch takes URLs from crawldb and create fetch list of URLs which are ready to be fetched. it takes input like -topN and timegap etc then create directory with current time under segments. </p>\n\n<p>I believe, In first two steps there was no interaction with internet. Everything was happening locally.</p>\n\n<p>Q: Where is fetch list kept ?</p>\n\n<p>3). Fetch: bin/nutch fetch crawl/segments/</p>\n\n<p>Fetch run fetchList and fetch contents (and URLs) from given URLs and keep it somewhere.</p>\n\n<p>Q: Does fetch read the whole given page of URL (Text + another URLs)?\nQ: Where Nutch keeps fetched data ?</p>\n\n<p>4). Parse: bin/nutch parse crawl/segments/</p>\n\n<p>It parses the entries.</p>\n\n<p>Q: What is meant by parse here ? \nQ: Where i can find result of this step ?</p>\n\n<p>5). bin/nutch updatedb crawl/crawldb crawl/segments/</p>\n\n<p>When this is complete, Nutch update the database with the results of the fetch.</p>\n\n<p>Q: Does it update crawldb with parsed data only or something else also?</p>\n\n<p>Please clear my doubts.</p>\n", "creation_date": 1428841284, "score": 0},
{"title": "Where is the crawled data stored when running nutch crawler?", "view_count": 2016, "owner": {"user_id": 4728385, "answer_count": 27, "creation_date": 1427695136, "accept_rate": 67, "view_count": 52, "reputation": 682}, "is_answered": true, "answers": [{"question_id": 29342945, "owner": {"user_id": 2702341, "link": "http://stackoverflow.com/users/2702341/sujen-shah", "user_type": "registered", "reputation": 183}, "body": "<p>After your crawl is over, you could use the bin/nutch dump command to dump all the urls fetched in plain html format. </p>\n\n<p>The usage is as follows : </p>\n\n<pre><code>$ bin/nutch dump [-h] [-mimetype &lt;mimetype&gt;] [-outputDir &lt;outputDir&gt;]\n   [-segment &lt;segment&gt;]\n -h,--help                show this help message\n -mimetype &lt;mimetype&gt;     an optional list of mimetypes to dump, excluding\n                      all others. Defaults to all.\n -outputDir &lt;outputDir&gt;   output directory (which will be created) to host\n                      the raw data\n -segment &lt;segment&gt;       the segment(s) to use\n</code></pre>\n\n<p>So for example you could do something like </p>\n\n<pre><code>$ bin/nutch dump -segment crawl/segments -outputDir crawl/dump/\n</code></pre>\n\n<p>This would create a new dir at the -outputDir location and dump all the pages crawled in html format. </p>\n\n<p>There are many more ways of dumping out specific data from Nutch, have a look at <a href=\"https://wiki.apache.org/nutch/CommandLineOptions\" rel=\"nofollow\">https://wiki.apache.org/nutch/CommandLineOptions</a></p>\n", "creation_date": 1428038070, "is_accepted": true, "score": 3, "last_activity_date": 1428038070, "answer_id": 29426580}], "question_id": 29342945, "tags": ["web-crawler", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/29342945/where-is-the-crawled-data-stored-when-running-nutch-crawler", "last_activity_date": 1428038070, "accepted_answer_id": 29426580, "body": "<p>I am new to Nutch. I need to crawl the web (say, a few hundred web pages), read the crawled data and do some analysis.\nI followed <a href=\"https://wiki.apache.org/nutch/NutchTutorial\" rel=\"nofollow\">https://wiki.apache.org/nutch/NutchTutorial</a> link (and integrated Solr since I may require to search text in future) and ran the crawl using a few URLs as the seed.  Now, I don't find the text/html data in my local machine.  Where can i find the data and what is the best way to read the data in text format?  Versions: apache-nutch-1.9 &amp; solr-4.10.4</p>\n", "creation_date": 1427708598, "score": 2},
{"title": "Apache Nutch: crawl only new pages for sematics analysis", "view_count": 205, "is_answered": false, "answers": [{"question_id": 24034599, "owner": {"user_id": 2641268, "accept_rate": 60, "link": "http://stackoverflow.com/users/2641268/nh-narumi", "user_type": "registered", "reputation": 13}, "body": "<p>I'm not using solr too. I just checked this documentation: <a href=\"https://today.java.net/pub/a/today/2006/01/10/introduction-to-nutch-1.html\" rel=\"nofollow\">https://today.java.net/pub/a/today/2006/01/10/introduction-to-nutch-1.html</a></p>\n\n<p>It seems like there are command prompts that can show the data fetched using WebDB. I'm new to Nutch but I just follow this documentation. Check it out.</p>\n", "creation_date": 1427959006, "is_accepted": false, "score": 0, "last_activity_date": 1427959006, "answer_id": 29407159}], "question_id": 24034599, "tags": ["nutch", "web-crawler"], "answer_count": 1, "link": "http://stackoverflow.com/questions/24034599/apache-nutch-crawl-only-new-pages-for-sematics-analysis", "last_activity_date": 1427959006, "owner": {"user_id": 3706373, "answer_count": 3, "creation_date": 1401874190, "accept_rate": 67, "view_count": 5, "reputation": 56}, "body": "<p>I plan to tune up Nutch 2.2.X such way, that after initial crawling of the list of sites I launch the crawler daily and get HTML or plain text of new pages appeared on those sites this day only. Number of sites: hundreds.</p>\n\n<p>Please be noted, that I'm not interested on updated, only new pages. Also I need new pages only starting from a date. Let's suppose it is the date of \"Initial crawling\".</p>\n\n<p>Reading documentation and searching the Web iI got following questions can't find anywhere else:</p>\n\n<ol>\n<li><p><strong>What backend I should better use for Nutch for my task?</strong> I need page's text only once, then I never return to it. MySQL seems isn't an option as it is not supported by gora anymore. I tried use HBase, but seems I have to rollback to Nutch 2.1.x to get it working correctly. What are your ideas? How I may minimize disk space and other resources utilization?</p></li>\n<li><p><strong>May I perform my task not using indexing engine, like Solr?</strong> Not sure I need store large fulltext indexes. May Nutch >2.2 be launched without Solr and does it needs specific options for launching such way? Tutorials aren't clearly explain this question: everybody needs Solr, except me. </p></li>\n<li><p><strong>If I'd like to add a site to the crawling list, how I better perform it?</strong> Let's suppose I already crawling a list of sites and want to add a site to the list to monitor it from now. So I need to crawl the new site skipping pages content to add it to WebDB, and then run daily crawl as usual. For Nutch 1.x it may be possible to perform separate crawls and then merge them. How it may looks like for Nutch 2.x?</p></li>\n<li><p><strong>May this task be performed without custom plugins, and may it be performed with Nutch at all?</strong> Probably, I may write a custom plugin which detects somehow is the page already indexed, or it is new, and we need put the content to XML, or a database, etc. Should I write the plugin at all, or there is a way to solve the task with lesser blood? And how the plugin's algorithm may look like, if there is no way to live without it?</p></li>\n</ol>\n\n<p>Thanks to all.</p>\n\n<p>P.S. There is a lot of Nutch questions/answers/tutorials, and I honestly searched in the Web for 2 weeks, but haven't found answers to questions above.</p>\n", "creation_date": 1401876479, "score": 0},
{"title": "Can&#39;t get apache nutch to crawl - permissions and JAVA_HOME suspected", "view_count": 2410, "owner": {"user_id": 340648, "answer_count": 11, "creation_date": 1273781720, "accept_rate": 94, "view_count": 119, "location": "New York, NY, United States", "reputation": 695}, "is_answered": true, "answers": [{"last_edit_date": 1427850268, "owner": {"user_id": 2483976, "link": "http://stackoverflow.com/users/2483976/mehul-rathod", "user_type": "registered", "reputation": 1021}, "body": "<p>It seems that, as a normal user, you don't have permission to write to <code>/usr/share/nutch/logs/hadoop.log</code>, which makes sense as security feature.</p>\n\n<p>To get around this, create a simple bash script:</p>\n\n<pre><code>#!/bin/sh\nexport JAVA_HOME=/usr/lib/jvm/java-1.6.0-openjdk-amd64\nbin/nutch crawl urls -dir crawl -depth 3 -topN 5\n</code></pre>\n\n<p>Save it as <code>nutch.sh</code>, then run it with <code>sudo</code>:</p>\n\n<pre><code>sudo sh nutch.sh\n</code></pre>\n", "question_id": 17374028, "creation_date": 1372456441, "is_accepted": true, "score": 1, "last_activity_date": 1427850268, "answer_id": 17374251}, {"last_edit_date": 1427837062, "owner": {"user_id": 3272907, "link": "http://stackoverflow.com/users/3272907/lotusjeff", "user_type": "registered", "reputation": 6}, "body": "<p>The key to solving this issue is to add the <code>JAVA_HOME</code> variable to your <code>sudo</code> environment.  For example, type <code>env</code> and <code>sudo env</code> and you will see that <code>JAVA_HOME</code> is not set for <code>sudo</code>.  To remedy this, you will need to add the path.</p>\n\n<ol>\n<li>Run <code>sudo visudo</code> to edit your <code>/etc/sudoers</code> file. (Do not use a standard text editor. This special vi text editor that will validate syntax before allowing you to save it.)  </li>\n<li><p>Add this line:</p>\n\n<pre><code>Defaults env_keep+=\"JAVA_HOME\"\n</code></pre>\n\n<p>at the end of the <code>Defaults env_keep</code> section.</p></li>\n<li>Reboot</li>\n</ol>\n", "question_id": 17374028, "creation_date": 1427835746, "is_accepted": false, "score": 0, "last_activity_date": 1427837062, "answer_id": 29378513}], "question_id": 17374028, "tags": ["java", "solr", "ubuntu-12.04", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/17374028/cant-get-apache-nutch-to-crawl-permissions-and-java-home-suspected", "last_activity_date": 1427850268, "accepted_answer_id": 17374251, "body": "<p>I am trying to run a basic crawl as per the <a href=\"http://wiki.apache.org/nutch/NutchTutorial\" rel=\"nofollow\">NutchTutorial</a>:</p>\n\n<pre><code>bin/nutch crawl urls -dir crawl -depth 3 -topN 5\n</code></pre>\n\n<p>So I have Nutch all installed and set up with Solr. I set my $JAVA_HOME in my <code>.bashrc</code> to <code>/usr/lib/jvm/java-1.6.0-openjdk-amd64</code>.</p>\n\n<p>I don't see any problems when I run <code>bin/nutch</code> from the nutch home directory, but when I try to run the crawl as above I get the following error:</p>\n\n<pre><code>log4j:ERROR setFile(null,true) call failed.\njava.io.FileNotFoundException: /usr/share/nutch/logs/hadoop.log (Permission denied)\n        at java.io.FileOutputStream.openAppend(Native Method)\n        at java.io.FileOutputStream.&lt;init&gt;(FileOutputStream.java:207)\n        at java.io.FileOutputStream.&lt;init&gt;(FileOutputStream.java:131)\n        at org.apache.log4j.FileAppender.setFile(FileAppender.java:290)\n        at org.apache.log4j.FileAppender.activateOptions(FileAppender.java:164)\n        at org.apache.log4j.DailyRollingFileAppender.activateOptions(DailyRollingFileAppender.java:216)\n        at org.apache.log4j.config.PropertySetter.activate(PropertySetter.java:257)\n        at org.apache.log4j.config.PropertySetter.setProperties(PropertySetter.java:133)\n        at org.apache.log4j.config.PropertySetter.setProperties(PropertySetter.java:97)\n        at org.apache.log4j.PropertyConfigurator.parseAppender(PropertyConfigurator.java:689)\n        at org.apache.log4j.PropertyConfigurator.parseCategory(PropertyConfigurator.java:647)\n        at org.apache.log4j.PropertyConfigurator.configureRootCategory(PropertyConfigurator.java:544)\n        at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:440)\n        at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:476)\n        at org.apache.log4j.helpers.OptionConverter.selectAndConfigure(OptionConverter.java:471)\n        at org.apache.log4j.LogManager.&lt;clinit&gt;(LogManager.java:125)\n        at org.slf4j.impl.Log4jLoggerFactory.getLogger(Log4jLoggerFactory.java:73)\n        at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:270)\n        at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:281)\n        at org.apache.nutch.crawl.Crawl.&lt;clinit&gt;(Crawl.java:43)\nlog4j:ERROR Either File or DatePattern options are not set for appender [DRFA].\nsolrUrl is not set, indexing will be skipped...\ncrawl started in: crawl\nrootUrlDir = urls\nthreads = 10\ndepth = 3\nsolrUrl=null\ntopN = 5\nInjector: starting at 2013-06-28 16:24:53\nInjector: crawlDb: crawl/crawldb\nInjector: urlDir: urls\nInjector: Converting injected urls to crawl db entries.\nInjector: total number of urls rejected by filters: 0\nInjector: total number of urls injected after normalization and filtering: 1\nInjector: Merging injected urls into crawl db.\nException in thread \"main\" java.io.IOException: Job failed!\n        at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:1357)\n        at org.apache.nutch.crawl.Injector.inject(Injector.java:296)\n        at org.apache.nutch.crawl.Crawl.run(Crawl.java:132)\n        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n        at org.apache.nutch.crawl.Crawl.main(Crawl.java:55)\n</code></pre>\n\n<p>I suspect it might have something to do with file permissions as I have to run <code>sudo</code> on almost everything on this server, but if I run the same crawl command with <code>sudo</code> I get: </p>\n\n<pre><code>Error: JAVA_HOME is not set.\n</code></pre>\n\n<p>So I feel like I've got a catch-22 situation going on here. Should I be able to run this command with <code>sudo</code>, or is there something else I need to do such that I don't have to run it with <code>sudo</code> and it will work, or is there something else entirely going on here?</p>\n", "creation_date": 1372455230, "score": 0},
{"title": "Nutch job failed to post to Solr", "view_count": 133, "is_answered": false, "question_id": 29307871, "tags": ["hadoop", "solr", "nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/29307871/nutch-job-failed-to-post-to-solr", "last_activity_date": 1427481362, "owner": {"user_id": 1454081, "answer_count": 1, "creation_date": 1339601241, "accept_rate": 50, "view_count": 8, "reputation": 32}, "body": "<p>Environment: <strong>NUTCH 1.6 &amp; SOLR 4.2.1</strong>.</p>\n\n<p>Nutch &amp; SOLR are running successfully. Posting to SOLR works well. But when trying to post from NUTCH, I am getting \"Job failed\".</p>\n\n<p>Following command is used to run Nutch crawling and posting.</p>\n\n<p><strong>bin/nutch crawl urls -solr <a href=\"http://10.10.10.10:8983/solr/tsdcr\" rel=\"nofollow\">http://10.10.10.10:8983/solr/tsdcr</a> -dir crawl -depth 5 -topN 5 -threads 50</strong></p>\n\n<p><strong>Output:</strong></p>\n\n<pre><code>SolrIndexer: finished at 2015-03-27 03:04:29, elapsed: 00:01:46\nSolrDeleteDuplicates: starting at 2015-03-27 03:04:29\nSolrDeleteDuplicates: Solr url: http://10.10.10.10:8983/solr/tsdcr\nException in thread \"main\" java.io.IOException: Job failed!\n  at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:1265)\n  at org.apache.nutch.indexer.solr.SolrDeleteDuplicates.dedup(SolrDeleteDuplicates.java:373)\n  at org.apache.nutch.indexer.solr.SolrDeleteDuplicates.dedup(SolrDeleteDuplicates.java:353)\n  at org.apache.nutch.crawl.Crawl.run(Crawl.java:153)\n  at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n  at org.apache.nutch.crawl.Crawl.main(Crawl.java:55)\n</code></pre>\n\n<p><strong>Hadoop log:</strong></p>\n\n<pre><code>2015-03-26 03:04:00,832 DEBUG httpclient.HttpMethodBase - Resorting to protocol version default close connection policy\n2015-03-26 03:04:00,832 DEBUG httpclient.HttpMethodBase - Should NOT close connection, using HTTP/1.1\n2015-03-26 03:04:00,832 TRACE httpclient.HttpConnection - enter HttpConnection.isResponseAvailable()\n2015-03-26 03:04:00,832 TRACE httpclient.HttpConnection - enter HttpConnection.releaseConnection()\n2015-03-26 03:04:00,832 DEBUG httpclient.HttpConnection - Releasing connection back to connection manager.\n2015-03-26 03:04:00,854 WARN  mapred.FileOutputCommitter - Output path is null in cleanup\n2015-03-26 03:04:00,854 WARN  mapred.LocalJobRunner - job_local_0017\njava.lang.NullPointerException\n  at org.apache.hadoop.io.Text.encode(Text.java:388)\n  at org.apache.hadoop.io.Text.set(Text.java:178)\n  at org.apache.nutch.indexer.solr.SolrDeleteDuplicates$SolrInputFormat$1.next(SolrDeleteDuplicates.java:270)\n  at org.apache.nutch.indexer.solr.SolrDeleteDuplicates$SolrInputFormat$1.next(SolrDeleteDuplicates.java:241)\n  at org.apache.hadoop.mapred.MapTask$TrackedRecordReader.moveToNext(MapTask.java:236)\n  at org.apache.hadoop.mapred.MapTask$TrackedRecordReader.next(MapTask.java:216)\n  at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:48)\n  at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:436)\n  at org.apache.hadoop.mapred.MapTask.run(MapTask.java:372)\n  at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:212)\n</code></pre>\n\n<p>Any help is much appreciated. </p>\n\n<p>Thank you.</p>\n", "creation_date": 1427481362, "score": 2},
{"title": "Confusion in Apache Nutch, HBase, Hadoop, Solr, Gora", "view_count": 394, "is_answered": false, "answers": [{"question_id": 29287204, "owner": {"user_id": 1867412, "link": "http://stackoverflow.com/users/1867412/mike-delong", "user_type": "registered", "reputation": 13}, "body": "<ol>\n<li>Here is a good short discussion of HBase vs. Hadoop: <a href=\"http://stackoverflow.com/questions/16929832/difference-between-hbase-and-hadoop\">Difference between HBase and Hadoop</a> </li>\n</ol>\n\n<p>Because HBase is built on top of Hadoop you can't really have HBase without Hadoop.</p>\n\n<ol start=\"2\">\n<li><p>Yes you can run Nutch without Solr; there do not seem to be lots of use cases, however, much less living examples in the wild.</p></li>\n<li><p>Yes, you can run Nutch without Hadoop, but again there don't seem to be a lot of real-world examples of people doing this.</p></li>\n<li><p>Yes Hadoop is part of HBase, in that there is no HBase without Hadoop, but of course Hadoop is used for other things as well.</p></li>\n</ol>\n\n<p>Zookeeper is used for configuration, naming, synchronization, etc. in Hadoop stack workflows. Gora is a memory management/persistence framework and is built on top of Hadoop.</p>\n", "creation_date": 1427401469, "is_accepted": false, "score": 0, "last_activity_date": 1427401469, "answer_id": 29288133}], "question_id": 29287204, "tags": ["hadoop", "solr", "hbase", "nutch", "zookeeper"], "answer_count": 1, "link": "http://stackoverflow.com/questions/29287204/confusion-in-apache-nutch-hbase-hadoop-solr-gora", "last_activity_date": 1427401469, "owner": {"user_id": 3089214, "answer_count": 4, "creation_date": 1386725546, "accept_rate": 38, "view_count": 15, "reputation": 75}, "body": "<p>I am new to all these terms and given some time to understand it. But i have some confusions in it. Please correct me if i am wrong.</p>\n\n<p>Nutch: It's for web crawling, using it we can crawl web pages. We can store these web pages somewhere in db.</p>\n\n<p>Solr: Solr can be used for indexing web pages crawled by Apache Nutch. It helps in searching the indexes web pages.</p>\n\n<p>HBase: It's used as an interface to interact with Hadoop. It helps in getting data at real time from HDFS. It provides simple SQL type interface for interacting.</p>\n\n<p>Hadoop: It provides two functionalities: One is HDFS (Hadoop data file system) and other is Map-Reduce functionality taken from Google algorithms. Its basically used for offline data backup etc.</p>\n\n<p>Gora and ZooKeeper: I am not sure of.</p>\n\n<p>Confusions:\n1). Is HBase a key-value pair DB or just an interface to Hadoop ? or i should ask, can HBase exist without Hadoop ?\nIf yes, can you explain a bit more about its usage.</p>\n\n<p>2). Is there any use of crawling data using Apache Nutch without indexing into Solr ?</p>\n\n<p>3). For running apache nutch, do we need HBase and Hadoop ? If no, how we can make it work without it?</p>\n\n<p>4). Is Hadoop part of HBase ?</p>\n", "creation_date": 1427398003, "score": 0},
{"title": "Using hbase to save data while parsing pages via nutch1.8", "view_count": 36, "is_answered": false, "question_id": 29274145, "tags": ["hbase", "nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/29274145/using-hbase-to-save-data-while-parsing-pages-via-nutch1-8", "last_activity_date": 1427362803, "owner": {"age": 27, "answer_count": 0, "creation_date": 1400665200, "user_id": 3660164, "view_count": 2, "location": "\u4e2d\u56fd\u6d59\u6c5f\u7701Hangzhou Shi", "reputation": 13}, "body": "<p>I'm using nutch 1.8 to crawl pages,and want to save <code>parseText</code> into hbase while parsing. So I wrote a filter ,but when saving <code>parseText</code>, I got this error:</p>\n\n<blockquote>\n  <p>Thread was interrupted while trying to connect to master.</p>\n</blockquote>\n\n<pre><code>static {\n        conf = HBaseConfiguration.create();\n    }\n\npublic static void creatTable(String tableName, String[] familys) throws Exception {\n\n        HBaseAdmin admin = new HBaseAdmin(conf);\n        if (admin.tableExists(tableName)) {\n            System.out.println(\"table already exists!\");\n        } else {\n            HTableDescriptor tableDesc = new HTableDescriptor(tableName);\n            for (int i = 0; i &lt; familys.length; i++) {\n                tableDesc.addFamily(new HColumnDescriptor(familys[i]));\n            }\n            admin.createTable(tableDesc);\n            System.out.println(\"create table \" + tableName + \" ok.\");\n        }\n    }\n</code></pre>\n\n<p>This code runs well on unitTest, but when integrated with nutch the error occurs. Can anyone help?</p>\n", "creation_date": 1427359429, "score": 1},
{"title": "Nutch not crawling url specified in seed.txt", "view_count": 204, "is_answered": false, "answers": [{"last_edit_date": 1427237733, "owner": {"user_id": 1162491, "link": "http://stackoverflow.com/users/1162491/momer", "user_type": "registered", "reputation": 1850}, "body": "<p>As mentioned in <a href=\"http://stackoverflow.com/a/16065297/1162491\">an answer</a> to a similar question, the old URLs are still in Nutch's <code>crawldb</code>.</p>\n\n<p>You can nuke your previous runs completely like <a href=\"http://mail-archives.apache.org/mod_mbox/nutch-user/201408.mbox/%3COF5F439F05.12C3136D-ONC1257D32.0045E0A0-C1257D32.00481094@adv-boeblingen.de%3E\" rel=\"nofollow\">this user did</a> and start fresh, or you can remove the unwanted URLs a few different ways via <a href=\"https://nutch.apache.org/apidocs/apidocs-1.9/org/apache/nutch/crawl/CrawlDbMerger.Merger.html\" rel=\"nofollow\">CrawlDbMerger</a>:</p>\n\n<ul>\n<li><p>CLI via <a href=\"https://wiki.apache.org/nutch/bin/nutch%20mergedb\" rel=\"nofollow\">bin/nutch mergedb</a></p></li>\n<li><p>CLI via <a href=\"https://wiki.apache.org/nutch/bin/nutch%20updatedb\" rel=\"nofollow\">bin/nutch updatedb</a></p></li>\n</ul>\n", "question_id": 29238444, "creation_date": 1427237372, "is_accepted": false, "score": 0, "last_activity_date": 1427237733, "answer_id": 29244356}], "question_id": 29238444, "tags": ["solr", "lucene", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/29238444/nutch-not-crawling-url-specified-in-seed-txt", "last_activity_date": 1427237733, "owner": {"user_id": 2160015, "answer_count": 0, "creation_date": 1363077220, "accept_rate": 38, "view_count": 9, "location": "Kolkata, India", "reputation": 30}, "body": "<p>I have just installed nutch integrated with solr and started crawling. but the urls I am specifying in seed.txt nutch is not crawling those url immediately. It's injecting old urls which I may have given earlier but now they are commented out.It looks like nutch is injecting url's in some strange order. What is the reason.also could anybody guide me any book or detailed tutorial on nutch becuase most of the tutorial available are only installation.</p>\n", "creation_date": 1427215848, "score": 0},
{"title": "How to get the html content from nutch", "view_count": 4740, "is_answered": true, "answers": [{"question_id": 5123757, "owner": {"user_id": 484046, "accept_rate": 81, "link": "http://stackoverflow.com/users/484046/millebii", "user_type": "registered", "reputation": 976}, "body": "<p>Yes there is a way. Have a look at cache.jsp to see how it displays the cached data.</p>\n", "creation_date": 1299604749, "is_accepted": false, "score": -2, "last_activity_date": 1299604749, "answer_id": 5235930}, {"last_edit_date": 1327499596, "owner": {"user_id": 764618, "accept_rate": 42, "link": "http://stackoverflow.com/users/764618/haya-aziz", "user_type": "registered", "reputation": 93}, "body": "<p>Try this:</p>\n\n<pre><code>public ParseResult filter(Content content, ParseResult parseResult, HTMLMetaTags\n metaTags, DocumentFragment doc) \n{\n Parse parse = parseResult.get(content.getUrl());\n LOG.info(\"parse.getText: \" +parse.getText());\n return parseResult;\n}\n</code></pre>\n\n<p>Then check the content in <code>hadoop.log</code>.</p>\n", "question_id": 5123757, "creation_date": 1327488254, "is_accepted": false, "score": 1, "last_activity_date": 1327499596, "answer_id": 9001349}, {"question_id": 5123757, "owner": {"user_id": 1770388, "link": "http://stackoverflow.com/users/1770388/habi", "user_type": "unregistered", "reputation": 81}, "body": "<p>Yes, you can acutally export the content of the crawled segments. It is not straightforward, but it works well for me. First, create a java project with the following code:</p>\n\n<pre><code>import org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.fs.FileSystem;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.SequenceFile;\nimport org.apache.hadoop.io.Text;\nimport org.apache.nutch.protocol.Content;\nimport org.apache.nutch.util.NutchConfiguration;\n\nimport java.io.File;\nimport java.io.FileOutputStream;\n\npublic class NutchSegmentOutputParser {\n\npublic static void main(String[] args) {\n\n    if (args.length != 2) {\n        System.out.println(\"usage: segmentdir (-local | -dfs &lt;namenode:port&gt;) outputdir\");\n        return;\n    }\n\n    try {\n        Configuration conf = NutchConfiguration.create();\n        FileSystem fs = FileSystem.get(conf);\n\n\n        String segment = args[0];\n\n        File outDir = new File(args[1]);\n        if (!outDir.exists()) {\n            if (outDir.mkdir()) {\n                System.out.println(\"Creating output dir \" + outDir.getAbsolutePath());\n            }\n        }\n\n        Path file = new Path(segment, Content.DIR_NAME + \"/part-00000/data\");\n        SequenceFile.Reader reader = new SequenceFile.Reader(fs, file, conf);\n\n\n        Text key = new Text();\n        Content content = new Content();\n\n        while (reader.next(key, content)) {\n            String filename = key.toString().replaceFirst(\"http://\", \"\").replaceAll(\"/\", \"___\").trim();\n\n            File f = new File(outDir.getCanonicalPath() + \"/\" + filename);\n            FileOutputStream fos = new FileOutputStream(f);\n            fos.write(content.getContent());\n            fos.close();\n            System.out.println(f.getAbsolutePath());\n        }\n        reader.close();\n        fs.close();\n    } catch (Exception e) {\n        e.printStackTrace();\n    }\n\n}\n</code></pre>\n\n<p>}</p>\n\n<p>I recommend using Maven; add the following dependencies:</p>\n\n<pre><code>     &lt;dependency&gt;\n      &lt;groupId&gt;org.apache.nutch&lt;/groupId&gt;\n        &lt;artifactId&gt;nutch&lt;/artifactId&gt;\n        &lt;version&gt;1.5.1&lt;/version&gt;\n    &lt;/dependency&gt;\n\n    &lt;dependency&gt;\n        &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;\n        &lt;artifactId&gt;hadoop-common&lt;/artifactId&gt;\n        &lt;version&gt;0.23.1&lt;/version&gt;\n    &lt;/dependency&gt;\n</code></pre>\n\n<p>and create a jar package (i.e. NutchSegmentOutputParser.jar)</p>\n\n<p>You need Hadoop to be installed on your machine. Then run:</p>\n\n<pre><code>$/hadoop-dir/bin/hadoop --config \\\nNutchSegmentOutputParser.jar:~/.m2/repository/org/apache/nutch/nutch/1.5.1/nutch-1.5.1.jar \\\nNutchSegmentOutputParser nutch-crawled-dir/2012xxxxxxxxx/ outdir\n</code></pre>\n\n<p>where nutch-crawled-dir/2012xxxxxxxxx/ is the crawled directory you want to extract content from (it contains 'segment' subdirectory) and outdir is an output dir. The output file names are generated from URI, however, the slashes are replaced by \"<em>_</em>\".</p>\n\n<p>Hope it helps.</p>\n", "creation_date": 1351061796, "is_accepted": false, "score": 8, "last_activity_date": 1351061796, "answer_id": 13044252}, {"question_id": 5123757, "owner": {"user_id": 368581, "accept_rate": 0, "link": "http://stackoverflow.com/users/368581/peter-dietz", "user_type": "registered", "reputation": 849}, "body": "<p>Its super basic.</p>\n\n<pre><code>public ParseResult getParse(Content content) {\n   LOG.info(\"getContent: \" + new String(content.getContent()));\n</code></pre>\n\n<p>The Content object has a method getContent(), which returns a byte array. Just have Java create a new String() with the BA, and you've got the raw html of whatever nutch had fetched.</p>\n\n<p>I'm using Nutch 1.9</p>\n\n<p>Here's the JavaDoc on org.apache.nutch.protocol.Content\n<a href=\"https://nutch.apache.org/apidocs/apidocs-1.2/org/apache/nutch/protocol/Content.html#getContent()\" rel=\"nofollow\">https://nutch.apache.org/apidocs/apidocs-1.2/org/apache/nutch/protocol/Content.html#getContent()</a></p>\n", "creation_date": 1427218576, "is_accepted": false, "score": 0, "last_activity_date": 1427218576, "answer_id": 29239306}], "question_id": 5123757, "tags": ["nutch"], "answer_count": 4, "link": "http://stackoverflow.com/questions/5123757/how-to-get-the-html-content-from-nutch", "last_activity_date": 1427218576, "owner": {"user_id": 593710, "answer_count": 0, "creation_date": 1296213398, "accept_rate": 0, "view_count": 3, "reputation": 31}, "body": "<p>Is there is any way to get the html content of each webpage in nutch while crawling the web page?</p>\n", "creation_date": 1298675762, "score": 6},
{"title": "Nutch message &quot;No IndexWriters activated&quot; while loading to solr", "view_count": 3734, "is_answered": true, "answers": [{"question_id": 17649567, "owner": {"user_id": 236650, "accept_rate": 38, "link": "http://stackoverflow.com/users/236650/mgfernan", "user_type": "registered", "reputation": 386}, "body": "<p>Make sure that the plugin <code>indexer-solr</code> is included. Go to the file: <code>conf/nutch-site.xml</code> and in the property <code>plugin.includes</code> add the plugin, for instance:</p>\n\n<blockquote>\n  <p>protocol-http|urlfilter-regex|parse-(html|tika)|index-(basic|anchor)|indexer-solr|scoring-opic|urlnormalizer-(pass|regex|basic)</p>\n</blockquote>\n\n<p>After adding the plugin the <code>No IndexWriters activated - check your configuration</code> warning disappeared in my case.</p>\n\n<p>Check this thread: <a href=\"http://lucene.472066.n3.nabble.com/a-plugin-extending-IndexWriter-td4074353.html\">http://lucene.472066.n3.nabble.com/a-plugin-extending-IndexWriter-td4074353.html</a></p>\n", "creation_date": 1375306958, "is_accepted": false, "score": 5, "last_activity_date": 1375306958, "answer_id": 17981512}, {"question_id": 17649567, "owner": {"user_id": 3453082, "link": "http://stackoverflow.com/users/3453082/xthule", "user_type": "registered", "reputation": 39}, "body": "<p>Don't know if this is still an issue, but I was having this problem and then realized that my <code>src/plugin/build.xml</code> was missing the <code>indexer-solr</code> plugin. Adding the following and then recompiling nutch fixed it for me:</p>\n\n<p><code>&lt;ant dir=\"indexer-solr\" target=\"deploy\"/&gt;</code></p>\n", "creation_date": 1408716452, "is_accepted": false, "score": 0, "last_activity_date": 1408716452, "answer_id": 25448951}, {"question_id": 17649567, "owner": {"user_id": 3496666, "accept_rate": 72, "link": "http://stackoverflow.com/users/3496666/kumar", "user_type": "registered", "reputation": 1801}, "body": "<p>Add the below property in conf/nutch-site.xml for plugin</p>\n\n<pre><code>&lt;property&gt;\n&lt;name&gt;plugin.includes&lt;/name&gt;\n&lt;value&gt;protocol-httpclient|urlfilter-regex|index-(basic|more)|query-(basic|site|url|lang)|indexer-solr|nutch-extensionpoints|protocol-httpclient|urlfilter-regex|parse-(text|html|msexcel|msword|mspowerpoint|pdf)|summary-basic|scoring-opic|urlnormalizer-(pass|regex|basic)protocol-http|urlfilter-regex|parse-(html|tika|metatags)|index-(basic|anchor|more|metadata)&lt;/value&gt;\n&lt;/property&gt;\n</code></pre>\n\n<p>Let me know if it solves your problem.</p>\n", "creation_date": 1411193588, "is_accepted": false, "score": 0, "last_activity_date": 1411193588, "answer_id": 25945844}, {"question_id": 17649567, "owner": {"user_id": 2367011, "link": "http://stackoverflow.com/users/2367011/ggmoriyon", "user_type": "registered", "reputation": 21}, "body": "<p>@Tryskele + @Scott101 worked for me:</p>\n\n<p>add plugin.includes property to both /conf/nutch-site.xml and runtime/local/conf/nutch-site.xml files:</p>\n\n<pre><code>&lt;property&gt;\n  &lt;name&gt;plugin.includes&lt;/name&gt;\n  &lt;value&gt;protocol-httpclient|urlfilter-regex|index-(basic|more)|query-(basic|site|url|lang)|indexer-solr|nutch-extensionpoints|protocol-httpclient|urlfilter-regex|parse-(text|html|msexcel|msword|mspowerpoint|pdf)|summary-basic|scoring-opic|urlnormalizer-(pass|regex|basic)protocol-http|urlfilter-regex|parse-(html|tika|metatags)|index-(basic|anchor|more|metadata)&lt;/value&gt;\n&lt;/property&gt;\n</code></pre>\n", "creation_date": 1427214541, "is_accepted": false, "score": 2, "last_activity_date": 1427214541, "answer_id": 29237966}], "question_id": 17649567, "tags": ["solr", "nutch"], "answer_count": 4, "link": "http://stackoverflow.com/questions/17649567/nutch-message-no-indexwriters-activated-while-loading-to-solr", "last_activity_date": 1427214541, "owner": {"user_id": 703665, "answer_count": 2, "creation_date": 1302597494, "accept_rate": 17, "view_count": 26, "reputation": 83}, "body": "<p>I have run nutch crawler as per nutch tutorial <a href=\"http://wiki.apache.org/nutch/NutchTutorial\" rel=\"nofollow\">http://wiki.apache.org/nutch/NutchTutorial</a> but when i started loading it to solr i am getting this message i.e. \"<strong>No IndexWriters activated - check your configuration</strong>\" </p>\n\n<pre><code>bin/nutch solrindex http://localhost:8983/solr crawl/crawldb/ -dir crawl/segments/\nIndexer: starting at 2013-07-15 08:09:13\nIndexer: deleting gone documents: false\nIndexer: URL filtering: false\nIndexer: URL normalizing: false\n**No IndexWriters activated - check your configuration**\n\nIndexer: finished at 2013-07-15 08:09:21, elapsed: 00:00:07\n</code></pre>\n", "creation_date": 1373875984, "score": 2},
{"title": "crawl data from website into hdfs", "view_count": 586, "owner": {"user_id": 4679935, "answer_count": 0, "creation_date": 1426584290, "accept_rate": 100, "view_count": 46, "reputation": 9}, "is_answered": true, "answers": [{"question_id": 29204437, "owner": {"user_id": 3568072, "accept_rate": 75, "link": "http://stackoverflow.com/users/3568072/frb", "user_type": "registered", "reputation": 2285}, "body": "<p>the <code>bind</code> parameter of <a href=\"http://flume.apache.org/FlumeUserGuide.html#http-source\" rel=\"nofollow\"><code>HTTPSource</code></a> specifies the IP address or hostname your agent is going to be listening for data. It is not the crawling endpoint, but the endpoint (together with the port) where the crawler must send the data.</p>\n\n<p>Being said that, I would suggest using the <a href=\"http://flume.apache.org/FlumeUserGuide.html#exec-source\" rel=\"nofollow\"><code>Exec</code></a> source in order to execute a script that crawls openweather.org and produce data at the output; that output is then used as input data for the agent.</p>\n", "creation_date": 1427126214, "is_accepted": true, "score": 0, "last_activity_date": 1427126214, "answer_id": 29214724}], "question_id": 29204437, "tags": ["web-crawler", "hdfs", "nutch", "apache-storm", "flume"], "answer_count": 1, "link": "http://stackoverflow.com/questions/29204437/crawl-data-from-website-into-hdfs", "last_activity_date": 1427126214, "accepted_answer_id": 29214724, "body": "<p>I want to crawl data from website so i am using API from openweather.org. \nThe agent that i have configured to stream in data is as follows</p>\n\n<pre><code>weather.channels= memory-channel\nweather.channels.memory-channel.capacity=10000\nweather.channels.memory-channel.type = memory\nweather.sinks = hdfs-write\nweather.sinks.hdfs-write.channel=memory-channel\nweather.sinks.hdfs-write.type = logger\nweather.sinks.hdfs-write.hdfs.path = hdfs://localhost:8020/user/hadoop/flume/\nweather.sinks.hdfs-write.rollInterval = 1200\nweather.sinks.hdfs-write.hdfs.writeFormat=Text\nweather.sinks.hdfs-write.hdfs.fileType=DataStream\nweather.sources= Weather\nweather.sources.Weather.bind =     api.openweathermap.org/data/2.5/forecast/city?id=285787&amp;APPID=8ce9bbbe446da25b19242763bdddb90a\nweather.sources.Weather.username= abc\nweather.sources.Weather.password= ********\nweather.sources.Weather.channels=memory-channel\nweather.sources.Weather.type = http\nweather.sources.Weather.port = 11111\n</code></pre>\n\n<p>While i am running the flume agent with following command\n   flume-ng agent -f weather.conf -n weather</p>\n\n<p>I am getting following error</p>\n\n<pre><code>15/03/23 05:17:34 INFO node.PollingPropertiesFileConfigurationProvider: Reloading configuration file:weather.conf\n15/03/23 05:17:34 INFO conf.FlumeConfiguration: Processing:hdfs-write\n15/03/23 05:17:34 INFO conf.FlumeConfiguration: Processing:hdfs-write\n15/03/23 05:17:34 INFO conf.FlumeConfiguration: Processing:hdfs-write\n15/03/23 05:17:34 INFO conf.FlumeConfiguration: Processing:hdfs-write\n15/03/23 05:17:34 INFO conf.FlumeConfiguration: Added sinks: hdfs-write Agent: weather\n15/03/23 05:17:34 INFO conf.FlumeConfiguration: Processing:hdfs-write\n15/03/23 05:17:34 INFO conf.FlumeConfiguration: Processing:hdfs-write\n15/03/23 05:17:34 INFO conf.FlumeConfiguration: Post-validation flume configuration contains configuration for agents: [weather]\n15/03/23 05:17:34 INFO node.AbstractConfigurationProvider: Creating channels\n15/03/23 05:17:34 INFO channel.DefaultChannelFactory: Creating instance of channel memory-channel type memory\n15/03/23 05:17:34 INFO node.AbstractConfigurationProvider: Created channel memory-channel\n15/03/23 05:17:34 INFO source.DefaultSourceFactory: Creating instance of sourceWeather, type http\n15/03/23 05:17:35 INFO sink.DefaultSinkFactory: Creating instance of sink: hdfs-write, type: logger\n15/03/23 05:17:35 INFO node.AbstractConfigurationProvider: Channel memory-channel connected to [Weather, hdfs-write]\n15/03/23 05:17:35 INFO node.Application: Starting new configuration:{     \nsourceRunners:{Weather=EventDrivenSourceRunner: {    \nsource:org.apache.flume.source.http.HTTP\nSource{name:Weather,state:IDLE} }} sinkRunners:{hdfs-write=SinkRunner: {   \npolicy:org.apache.flume.sink.DefaultSinkProcessor@529d1dd7 counterGroup:{    \nname:null counters:{} } }} channels:{memory-   \nchannel=org.apache.flume.channel.MemoryChannel{name: memory-channel}} }\n15/03/23 05:17:35 INFO node.Application: Starting Channel memory-channel\n15/03/23 05:17:35 INFO instrumentation.MonitoredCounterGroup: Monitored  \ncountergroup for type: CHANNEL, name: memory-channel: Successfully  \nregistered new MBean.\n15/03/23 05:17:35 INFO instrumentation.MonitoredCounterGroup: Component   \ntype: CHANNEL, name: memory-channel started\n15/03/23 05:17:35 INFO node.Application: Starting Sink hdfs-write\n15/03/23 05:17:35 INFO node.Application: Starting Source Weather\n15/03/23 05:17:35 INFO mortbay.log: Logging to \norg.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via   \norg.mortbay.log.Slf4jLog\n15/3/23 05:17:35 INFO mortbay.log: jetty-6.1.26\n15/03/23 05:17:36 WARN mortbay.log: failed \nSelectChannelConnector@api.openweathermap.org/data/2.5/forecast/city?\nid=285787&amp;APPID=8ce9bbbe446da25b19242763bdddb90a:11111:   \njava.net.SocketException: Unresolved address\n15/03/23 05:17:36 WARN mortbay.log: failed Server@642c189d: \njava.net.SocketException: Unresolved address\n15/03/23 05:17:36 ERROR http.HTTPSource: Error while starting HTTPSource.    \n  Exception follows.java.net.SocketException: Unresolved address\n    at sun.nio.ch.Net.translateToSocketException(Net.java:157)\n    at sun.nio.ch.Net.translateException(Net.java:183)\n    at sun.nio.ch.Net.translateException(Net.java:189)\n    at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:76)\n    at org.mortbay.jetty.nio.SelectChannelConnector.open\n    (SelectChannelConnector.java:216)\n    at org.mortbay.jetty.nio.SelectChannelConnector.doStart(SelectChannelCon\n    nector.java:315)\n    at org.mortbay.component.AbstractLifeCycle.start(AbstractLifeCycle.java_\n    at org.mortbay.jetty.Server.doStart(Server.java:235)\n    at org.mortbay.component.AbstractLifeCycle.start(AbstractLifeCycle.java)\n    at org.apache.flume.source.http.HTTPSource.start(HTTPSource.java:220)\n    at org.apache.flume.source.EventDrivenSourceRunner.start(EventDrivenSour\n    ceRunner.java:44)\n    at org.apache.flume.lifecycle.LifecycleSupervisor$MonitorRunnable.run\n    (LifecycleSupervisor.java:251)\n    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java)\n    at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:304)\n    at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.\n    access$301(ScheduledThreadPoolExecutor.java:178)\n    at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.\n    run(ScheduledThreadPoolExecutor.java:293)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.\n    java:1145)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor\n    .java:615)\n    at java.lang.Thread.run(Thread.java:745)\n    Caused by: java.nio.channels.UnresolvedAddressException\n    at sun.nio.ch.Net.checkAddress(Net.java:127)\n    at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java)\n    at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)\n    ... 15 more\n   15/03/23 05:17:36 ERROR lifecycle.LifecycleSupervisor: Unable to start \n   EventDrivenSourceRunner: {   \n   source:org.apache.flume.source.http.HTTPSource{name:Weather,state:IDLE} } \n   - Exception follows.\n   java.lang.RuntimeException: java.net.SocketException: Unresolved address\n    at com.google.common.base.Throwables.propagate(Throwables.java:156)\n    at org.apache.flume.source.http.HTTPSource.start(HTTPSource.java:224)\n    at org.apache.flume.source.EventDrivenSourceRunner.start\n    (EventDrivenSourceRunner.java:44)\n    at org.apache.flume.lifecycle.LifecycleSupervisor$MonitorRunnable.run(Li\n    fecycleSupervisor.java:251)\n    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java)\n    at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:304)\n    at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.\n    access$301(ScheduledThreadPoolExecutor.java:178)\n    at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.\n    run(ScheduledThreadPoolExecutor.java:293)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.\n    java:1145)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor\n    .java:615)\n    at java.lang.Thread.run(Thread.java:745)\n    Caused by: java.net.SocketException: Unresolved address\n    at sun.nio.ch.Net.translateToSocketException(Net.java:157)\n    at sun.nio.ch.Net.translateException(Net.java:183)\n    at sun.nio.ch.Net.translateException(Net.java:189)\n    at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:76)\n    at org.mortbay.jetty.nio.SelectChannelConnector.open(SelectChannelConnec\n    tor.java:216)\n    at org.mortbay.jetty.nio.SelectChannelConnector.doStart(SelectChannelCon\n    nector.java:315)\n    at org.mortbay.component.AbstractLifeCycle.start(AbstractLifeCycle.java:\n    at org.mortbay.jetty.Server.doStart(Server.java:235)\n    at org.mortbay.component.AbstractLifeCycle.start(AbstractLifeCycle.java:\n    at org.apache.flume.source.http.HTTPSource.start(HTTPSource.java:220)\n    ... 9 more\n    Caused by: java.nio.channels.UnresolvedAddressException\n    at sun.nio.ch.Net.checkAddress(Net.java:127)\n    at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java\n    at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)\n    ... 15 more\n    15/03/23 05:17:39 ERROR lifecycle.LifecycleSupervisor: Unable to start \n    EventDrivenSourceRunner: {   \n    source:org.apache.flume.source.http.HTTPSource{name:Weather,state:IDLE} \n    } - Exception follows.\n    java.lang.IllegalStateException: Running HTTP Server found in source:  \n    Weather before I started one.Will not attempt to start.\n    at com.google.common.base.Preconditions.checkState(Preconditions.java:14\n    at org.apache.flume.source.http.HTTPSource.start(HTTPSource.java:189)\n    at org.apache.flume.source.EventDrivenSourceRunner.start(EventDrivenSour\n    ceRunner.java:44)\n    at org.apache.flume.lifecycle.LifecycleSupervisor$MonitorRunnable.run(Li\n    fecycleSupervisor.java:251)\n    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java)\n    at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:304)\n    at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.\n    access$301(ScheduledThreadPoolExecutor.java:178)\n    at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.\n    run(ScheduledThreadPoolExecutor.java:293)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.\n    java:1145)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor\n    .java:615)\n    at java.lang.Thread.run(Thread.java:745)\n    ^C15/03/23 05:17:41 INFO lifecycle.LifecycleSupervisor: Stopping  \n    lifecycle supervisor 10\n    15/03/23 05:17:41 INFO node.PollingPropertiesFileConfigurationProvider:  \n    Configuration provider stopping\n</code></pre>\n\n<p>Please help me on this issue?</p>\n\n<p>Or do i have to do something else before configuring flume agent.</p>\n\n<p>or should i use nutch to crawl the data in, or should i use storm.</p>\n\n<p>Please help me what is the best alternative to do this</p>\n\n<p>Thank you in advance</p>\n", "creation_date": 1427092528, "score": 0},
{"title": "How to configure seed and urlfilter confg files in Apache Nutch", "view_count": 54, "is_answered": true, "answers": [{"question_id": 29195102, "owner": {"user_id": 1699191, "link": "http://stackoverflow.com/users/1699191/ameertawfik", "user_type": "registered", "reputation": 618}, "body": "<p>Enable Nutch's urlnormalizer-regx plugin. Then, add the following to conf/regex-normalize.xml</p>\n\n<pre><code>&lt;regex&gt;\n&lt;pattern&gt;(.*?)(pg=)\\d{1,2})&lt;/pattern&gt;\n&lt;substitution&gt;$1/view/$3&lt;/substitution&gt;\n&lt;/regex&gt;\n</code></pre>\n\n<p>This plugin allows you to alter urls based on regular expression. In the above example, I am matching your url into three parts. I then replace matched urls with the substitution patterns. </p>\n\n<p>For the second problem you should enable indexer-dummy. This plugin will print data into text files.</p>\n\n<p>If you are crawling a small site, then i will recommend using Scrapyy, it suits better your requirements</p>\n\n<p>I hope that helps.</p>\n", "creation_date": 1427112956, "is_accepted": false, "score": 1, "last_activity_date": 1427112956, "answer_id": 29210000}], "question_id": 29195102, "tags": ["nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/29195102/how-to-configure-seed-and-urlfilter-confg-files-in-apache-nutch", "last_activity_date": 1427112956, "owner": {"user_id": 706838, "answer_count": 1, "creation_date": 1302728558, "accept_rate": 45, "view_count": 213, "reputation": 973}, "body": "<p>I would like to setup Nutch so that it goes through all <code>http://www.domain.com/classifieds/something/?pg=&lt;page&gt;</code> pages, for  goes from <code>1</code> to <code>200</code> and store the urls of the form <code>http://www.domain.com/classifieds/something/view/&lt;number&gt;/</code> where  is a ling number? Then, I would like print out all these urls in my terminal. I am using <code>Apache Nutch 1.9</code> and <code>Apache Solr 4.10.4</code>.</p>\n", "creation_date": 1427031839, "score": 0},
{"title": "Nutch 1.2 Solr 3.6 integration issue", "view_count": 555, "owner": {"user_id": 2526784, "answer_count": 0, "creation_date": 1372313344, "accept_rate": 64, "view_count": 53, "reputation": 77}, "is_answered": true, "answers": [{"question_id": 17523424, "owner": {"user_id": 950178, "link": "http://stackoverflow.com/users/950178/jayendra", "user_type": "registered", "reputation": 34896}, "body": "<p>This is mainly the javabin incompatiblity between the Solrj version jars used by Nutch and the Solr 3.6 which you are trying to integrate.  </p>\n\n<p>You would need to update the Solrj jars and regenerate the jobs.   </p>\n\n<p>Follow the steps as mentioned in the <a href=\"http://lucene.472066.n3.nabble.com/Nutch-1-2-Solr-3-1-0-solrindex-Job-failed-td2838580.html\" rel=\"nofollow\">forum</a>. </p>\n", "creation_date": 1373278472, "is_accepted": true, "score": 0, "last_activity_date": 1373278472, "answer_id": 17524115}], "question_id": 17523424, "tags": ["solr", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/17523424/nutch-1-2-solr-3-6-integration-issue", "last_activity_date": 1427110535, "accepted_answer_id": 17524115, "body": "<p>I have crawled a site successfully using NUTCH 1.2 .Now I want to integrate this with solr 3.6 . Problem is when I am issuing command<br>\n<code>$ bin/nutch solrindex //localhost:8080/solr/ crawl/crawldb crawl/linkdb crawl/segments/*</code> an error occurs</p>\n\n<pre><code> SolrIndexer: starting at 2013-07-08 14:52:27\njava.io.IOException: Job failed!\n</code></pre>\n\n<p>Please help me to solve this issue</p>\n\n<p>Here is my nutch log</p>\n\n<pre><code>java.lang.RuntimeException: Invalid version (expected 2, but 60) or the data in not in 'javabin' format\n    at org.apache.solr.common.util.JavaBinCodec.unmarshal(JavaBinCodec.java:99)\n    at org.apache.solr.client.solrj.impl.BinaryResponseParser.processResponse(BinaryResponseParser.java:41)\n    at org.apache.solr.client.solrj.impl.CommonsHttpSolrServer.request(CommonsHttpSolrServer.java:469)\n    at org.apache.solr.client.solrj.impl.CommonsHttpSolrServer.request(CommonsHttpSolrServer.java:249)\n    at org.apache.solr.client.solrj.request.AbstractUpdateRequest.process(AbstractUpdateRequest.java:105)\n    at org.apache.solr.client.solrj.SolrServer.add(SolrServer.java:69)\n    at org.apache.solr.client.solrj.SolrServer.add(SolrServer.java:54)\n    at org.apache.nutch.indexer.solr.SolrWriter.close(SolrWriter.java:75)\n    at org.apache.nutch.indexer.IndexerOutputFormat$1.close(IndexerOutputFormat.java:48)\n    at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:474)\n    at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:411)\n    at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:216)\n2013-07-08 15:17:39,539 ERROR solr.SolrIndexer - java.io.IOException: Job f\n</code></pre>\n", "creation_date": 1373276367, "score": 1},
{"title": "ERROR* Adding 2 documents java.io.IOException: Job failed! ( solr 3.4, nutch 1.4 bin on windows using Cygwin)", "view_count": 473, "is_answered": false, "answers": [{"question_id": 21143773, "owner": {"user_id": 2623052, "accept_rate": 45, "link": "http://stackoverflow.com/users/2623052/allan-macmillan", "user_type": "registered", "reputation": 866}, "body": "<p>It sounds like the schema files for Solr and Nutch dont match up.  Check out this post, I use Solr 4.3 but I dont feel it shouldnt be too different</p>\n\n<p><a href=\"http://amac4.blogspot.com/2013/07/configuring-nutch-to-crawl-urls.html\" rel=\"nofollow\">http://amac4.blogspot.com/2013/07/configuring-nutch-to-crawl-urls.html</a></p>\n\n<p>The log files have more detailed information about the problem, so you could post them here too.</p>\n", "creation_date": 1389905000, "is_accepted": false, "score": 0, "last_activity_date": 1389905000, "answer_id": 21172459}, {"question_id": 21143773, "owner": {"user_id": 2641268, "accept_rate": 60, "link": "http://stackoverflow.com/users/2641268/nh-narumi", "user_type": "registered", "reputation": 13}, "body": "<p>Your command seems to be wrong. It should be: \n$ ./nutch crawl urls -dir newCrawl -solr <a href=\"http://localhost:8080/solr/\" rel=\"nofollow\">http://localhost:8080/solr/</a> -depth 3 -topN 5                    </p>\n\n<p>Your mistake: Didn't put \"-dir\"                                                                                                                                                                           </p>\n", "creation_date": 1426834504, "is_accepted": false, "score": 0, "last_activity_date": 1426834504, "answer_id": 29161039}], "question_id": 21143773, "tags": ["java", "solr", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/21143773/error-adding-2-documents-java-io-ioexception-job-failed-solr-3-4-nutch-1-4", "last_activity_date": 1426834504, "owner": {"user_id": 2682833, "view_count": 5, "answer_count": 1, "creation_date": 1376490086, "reputation": 3}, "body": "<pre><code>$ ./nutch crawl urls -solr `http://localhost:8080/solr/` -depth 2 -topN 3\ncygpath: can't convert empty path\ncrawl started in: crawl-20140115213017\nrootUrlDir = urls\nthreads = 10\ndepth = 2\nsolrUrl=`http://localhost:8080/solr/`\ntopN = 3\nInjector: starting at 2014-01-15 21:30:17\nInjector: crawlDb: crawl-20140115213017/crawldb\nInjector: urlDir: urls\nInjector: Converting injected urls to crawl db entries.\nInjector: Merging injected urls into crawl db.\nInjector: finished at 2014-01-15 21:30:21, elapsed: 00:00:03\nGenerator: starting at 2014-01-15 21:30:21\nGenerator: Selecting best-scoring urls due for fetch.\nGenerator: filtering: true\nGenerator: normalizing: true\nGenerator: topN: 3\nGenerator: jobtracker is 'local', generating exactly one partition.\nGenerator: Partitioning selected urls for politeness.\nGenerator: segment: crawl-20140115213017/segments/20140115213024\nGenerator: finished at 2014-01-15 21:30:26, elapsed: 00:00:04\nFetcher: Your 'http.agent.name' value should be listed first in 'http.robots.agents' property.\nFetcher: starting at 2014-01-15 21:30:26\nFetcher: segment: crawl-20140115213017/segments/20140115213024\nUsing queue mode : byHost\nFetcher: threads: 10\nFetcher: time-out divisor: 2\nQueueFeeder finished: total 1 records + hit by time limit :0\nUsing queue mode : byHost\nUsing queue mode : byHost\nUsing queue mode : byHost\nUsing queue mode : byHost\nUsing queue mode : byHost\nUsing queue mode : byHost\nUsing queue mode : byHost\nUsing queue mode : byHost\nUsing queue mode : byHost\nUsing queue mode : byHost\nFetcher: throughput threshold: -1\nFetcher: throughput threshold retries: 5\nfetching `http://www.parkinson.org/`\n-finishing thread FetcherThread, activeThreads=3\n-finishing thread FetcherThread, activeThreads=2\n-finishing thread FetcherThread, activeThreads=7\n-finishing thread FetcherThread, activeThreads=6\n-finishing thread FetcherThread, activeThreads=5\n-finishing thread FetcherThread, activeThreads=4\n-finishing thread FetcherThread, activeThreads=3\n-finishing thread FetcherThread, activeThreads=2\n-finishing thread FetcherThread, activeThreads=1\n-activeThreads=1, spinWaiting=0, fetchQueues.totalSize=0\n-activeThreads=1, spinWaiting=0, fetchQueues.totalSize=0\n-activeThreads=1, spinWaiting=0, fetchQueues.totalSize=0\n-finishing thread FetcherThread, activeThreads=0\n-activeThreads=0, spinWaiting=0, fetchQueues.totalSize=0\n-activeThreads=0\nFetcher: finished at 2014-01-15 21:30:32, elapsed: 00:00:06\nParseSegment: starting at 2014-01-15 21:30:32\nParseSegment: segment: crawl-20140115213017/segments/20140115213024\nParsing: `http://www.parkinson.org/`\nParseSegment: finished at 2014-01-15 21:30:34, elapsed: 00:00:01\nCrawlDb update: starting at 2014-01-15 21:30:34\nCrawlDb update: db: crawl-20140115213017/crawldb\nCrawlDb update: segments: [crawl-20140115213017/segments/20140115213024]\nCrawlDb update: additions allowed: true\nCrawlDb update: URL normalizing: true\nCrawlDb update: URL filtering: true\nCrawlDb update: 404 purging: false\nCrawlDb update: Merging segment data into db.\nCrawlDb update: finished at 2014-01-15 21:30:36, elapsed: 00:00:01\nGenerator: starting at 2014-01-15 21:30:36\nGenerator: Selecting best-scoring urls due for fetch.\nGenerator: filtering: true\nGenerator: normalizing: true\nGenerator: topN: 3\nGenerator: jobtracker is 'local', generating exactly one partition.\nGenerator: Partitioning selected urls for politeness.\nGenerator: segment: crawl-20140115213017/segments/20140115213038\nGenerator: finished at 2014-01-15 21:30:39, elapsed: 00:00:03\nFetcher: Your 'http.agent.name' value should be listed first in 'http.robots.agents' property.\nFetcher: starting at 2014-01-15 21:30:39\nFetcher: segment: crawl-20140115213017/segments/20140115213038\nUsing queue mode : byHost\nFetcher: threads: 10\nFetcher: time-out divisor: 2\nQueueFeeder finished: total 3 records + hit by time limit :0\nUsing queue mode : byHost\nUsing queue mode : byHost\nfetching `http://forum.parkinson.org/`\nUsing queue mode : byHost\nUsing queue mode : byHost\nUsing queue mode : byHost\nUsing queue mode : byHost\nUsing queue mode : byHost\nUsing queue mode : byHost\nUsing queue mode : byHost\nUsing queue mode : byHost\nFetcher: throughput threshold: -1\nFetcher: throughput threshold retries: 5\nfetching `http://twitter.com/ParkinsonDotOrg`\nfetching `http://www.youtube.com/user/NPFGuru`\n-finishing thread FetcherThread, activeThreads=9\n-finishing thread FetcherThread, activeThreads=8\n-finishing thread FetcherThread, activeThreads=7\n-finishing thread FetcherThread, activeThreads=6\n-finishing thread FetcherThread, activeThreads=5\n-finishing thread FetcherThread, activeThreads=4\n-finishing thread FetcherThread, activeThreads=3\n-finishing thread FetcherThread, activeThreads=2\n-activeThreads=2, spinWaiting=0, fetchQueues.totalSize=0\n-activeThreads=2, spinWaiting=0, fetchQueues.totalSize=0\n-finishing thread FetcherThread, activeThreads=1\n-finishing thread FetcherThread, activeThreads=0\n-activeThreads=0, spinWaiting=0, fetchQueues.totalSize=0\n-activeThreads=0\nFetcher: finished at 2014-01-15 21:30:44, elapsed: 00:00:04\nParseSegment: starting at 2014-01-15 21:30:44\nParseSegment: segment: crawl-20140115213017/segments/20140115213038\nParsing: `http://forum.parkinson.org/`\nParseSegment: finished at 2014-01-15 21:30:45, elapsed: 00:00:01\nCrawlDb update: starting at 2014-01-15 21:30:45\nCrawlDb update: db: crawl-20140115213017/crawldb\nCrawlDb update: segments: [crawl-20140115213017/segments/20140115213038]\nCrawlDb update: additions allowed: true\nCrawlDb update: URL normalizing: true\nCrawlDb update: URL filtering: true\nCrawlDb update: 404 purging: false\nCrawlDb update: Merging segment data into db.\nCrawlDb update: finished at 2014-01-15 21:30:46, elapsed: 00:00:01\nLinkDb: starting at 2014-01-15 21:30:46\nLinkDb: linkdb: crawl-20140115213017/linkdb\nLinkDb: URL normalize: true\nLinkDb: URL filter: tr`enter code here`ue\nLinkDb: adding segment: file:/C:/cygwin/home/nutch/runtime/local/bin/crawl-20140115213017/segments/20140115213024\nLinkDb: adding segment: file:/C:/cygwin/home/nutch/runtime/local/bin/crawl-20140115213017/segments/20140115213038\nLinkDb: finished at 2014-01-15 21:30:47, elapsed: 00:00:01\nSolrIndexer: starting at 2014-01-15 21:30:47\nAdding 2 documents\njava.io.IOException: Job failed!\nSolrDeleteDuplicates: starting at 2014-01-15 21:30:52\nSolrDeleteDuplicates: Solr url: `http://localhost:8080/solr/`\nSolrDeleteDuplicates: finished at 2014-01-15 21:30:53, elapsed: 00:00:01\ncrawl finished: crawl-20140115213017\n</code></pre>\n\n<p>ERROR* Adding 2 documents java.io.IOException: Job failed! \n( solr 3.4, nutch 1.4 bin on windows using Cygwin)\nI'm new to Apache...Need some help\ntry to send crawled data to solr for searching but getting error \"java.io.IOException: Job failed!\"</p>\n", "creation_date": 1389805617, "score": 0},
{"title": "JetBrains IDEA download sources for ivy project", "view_count": 66, "is_answered": false, "question_id": 29148928, "tags": ["intellij-idea", "ivy", "nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/29148928/jetbrains-idea-download-sources-for-ivy-project", "last_activity_date": 1426779583, "owner": {"age": 27, "answer_count": 4, "creation_date": 1347015423, "user_id": 1654519, "accept_rate": 50, "view_count": 9, "reputation": 57}, "body": "<p>i have project with ivy (Apache Nutch), can i automatically download sources code for it's dependencies with Jetbrains IDEA?</p>\n", "creation_date": 1426779583, "score": 0},
{"title": "Nutch Installation", "view_count": 282, "is_answered": false, "answers": [{"question_id": 8954314, "owner": {"user_id": 1150329, "accept_rate": 89, "link": "http://stackoverflow.com/users/1150329/tejas-patil", "user_type": "registered", "reputation": 4793}, "body": "<p>You need to ensure that the nutch web app can find the index and segments that were generated. Nutch looks for these in the index and segments subdirectories of the directory defined in the <code>searcher.dir</code> property. </p>\n\n<p>Set <code>searcher.dir</code> to be the crawl directory containing the index and segments.</p>\n", "creation_date": 1333465403, "is_accepted": false, "score": 0, "last_activity_date": 1333465403, "answer_id": 9996061}], "question_id": 8954314, "tags": ["nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/8954314/nutch-installation", "last_activity_date": 1426694474, "owner": {"user_id": 1161268, "view_count": 7, "answer_count": 0, "creation_date": 1327085338, "reputation": 1}, "body": "<pre><code>&lt;configuration&gt;\n  &lt;property&gt;\n    &lt;name&gt;searcher.dir&lt;/name&gt;\n    &lt;value&gt;your_crawl_folder_here&lt;/value&gt;\n  &lt;/property&gt;\n&lt;/configuration&gt;\nFor example, if your nutch directory resides at C:\\nutch-0.9.0 and you specified crawl as the directory after the -dir command, then enter C:\\nutch-0.9.0\\crawl\\ instead of your_crawl_folder_here.\n</code></pre>\n\n<p>I don't understand this part of the nutch install guide. Anyone working with nutch can explain for me?</p>\n", "creation_date": 1327160877, "score": 0},
{"title": "Apache nutch and solr : queries", "view_count": 1285, "owner": {"age": 25, "answer_count": 15, "creation_date": 1422981470, "user_id": 4525120, "accept_rate": 38, "view_count": 61, "location": "Earth", "reputation": 92}, "is_answered": true, "answers": [{"question_id": 28535600, "owner": {"user_id": 1699191, "link": "http://stackoverflow.com/users/1699191/ameertawfik", "user_type": "registered", "reputation": 618}, "body": "<p>Although Nutch was built to be a web scale search engine, this is not the case any more. Currently, the main purpose of Nutch is to do a large scale crawling. What you do with that crawled data is then up to your requirements. By default, Nutch allows to send data into Solr. That is why you can run</p>\n\n<pre><code>crawl url crawl solraddress depth level\n</code></pre>\n\n<p>You can emit the solr url parameter also. In that case, nutch will not send the crawled data into Solr. Without sending the crawled data to solr, you will not be able to search data. Crawling data and searching data are two different things but very related.</p>\n\n<p>Generally, you will find the crawled data in the crawl/segments not crawl/crawdb. The crawl db folder stores information about the crawled urls, their fetching status and next time for fetching plus some other useful information for crawling. Nutch stores actual crawled data in crawl/segments.</p>\n\n<p>If you want to have an easy way to view crawled data, you might try nutch 2.x as it can store its crawled data into several back ends like MySQL, Hbase, Cassandra and etc through the Gora component.</p>\n\n<p>To view data at solr, you simplly issue a query to Solr like this:</p>\n\n<pre><code>curl http://127.0.0.1:8983/solr/collection1/select/?q=*:*\n</code></pre>\n\n<p>Otherwise, you can always push your data into different stores via adding indexer plugins. Currently, Nutch supports sending data to Solr and Elasticsearch. These indexer plugins send structured data like title, text, metadata, author and other metadata.</p>\n\n<p>The following summarizes what happens in Nutch:</p>\n\n<pre><code>seed list -&gt; crawldb -&gt; fetching raw data (download site contents) \n-&gt; parsing the raw data -&gt; structuring the parse data into fields (title, text, anchor text, metadata and so on)-&gt; \nsending the structured data to storage for usage (like ElasticSearch and Solr).\n</code></pre>\n\n<p>Each of these stages is extendable and allows you to add your logic to suit your requirements.</p>\n\n<p>I hope that clears your confusion.</p>\n", "creation_date": 1424124556, "is_accepted": true, "score": 1, "last_activity_date": 1424124556, "answer_id": 28551234}, {"question_id": 28535600, "owner": {"user_id": 4682205, "link": "http://stackoverflow.com/users/4682205/veke", "user_type": "unregistered", "reputation": 1}, "body": "<p>u can run nutch on windows-I am also a beginner-yes it is a bit tough to install in windows but it do works!-this input path doesn't exist problem can be solved by:-\nreplacing Hadoop-core-1.2.0.jar file in apache-nutch-1.9/lib with hadoop-core-0.20.2.jar(from maven)\nthen rename this new file as hadoop-core-1.2.0</p>\n", "creation_date": 1426616915, "is_accepted": false, "score": 0, "last_activity_date": 1426616915, "answer_id": 29106982}], "question_id": 28535600, "tags": ["apache", "solr", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/28535600/apache-nutch-and-solr-queries", "last_activity_date": 1426616915, "accepted_answer_id": 28551234, "body": "<p>I have just started using <strong>Nutch</strong> 1.9 and <strong>Solr</strong> 4.10</p>\n\n<p>After browsing through certain pages I see that syntax for running this version has been changed, and I have to update certain xml's for configuring <strong>Nutch</strong> and <strong>Solr</strong></p>\n\n<p>This version of package doesnt require Tomcat for running. I started <strong>Solr</strong>: </p>\n\n<p><em>java -jar start.jar</em></p>\n\n<p>and checked <em>localhost:8983/solr/admin</em>, its working. </p>\n\n<p>I planted a seed in <em>bin/url/seed.txt</em> and seed is \"simpleweb.org\"</p>\n\n<p>Ran <b>Command</b> in Nutch: <code>./crawl urls -dir crawl -depth 3 -topN 5</code></p>\n\n<p>I got few IO exceptions in the middle and so to avoid the IO exception I downloaded\n<em>patch-hadoop_7682-1.0.x-win.jar</em> and made an entry in <em>nutch-site.xml</em> and placed the jar file in lib of <strong>Nutch</strong>.</p>\n\n<p>After running <strong>Nutch</strong>,\nFollowing folders were created:</p>\n\n<pre><code>apache-nutch-1.9\\bin\\-dir\\crawldb\\current\\part-00000\n</code></pre>\n\n<p>I can see following files in that path:</p>\n\n<pre><code>data&lt;br&gt;\nindex&lt;br&gt;\n.data.crc&lt;br&gt;\n.index.crc&lt;br&gt;\n</code></pre>\n\n<p>I want to know what to do with these files, what are the next steps? Can we view these files? If yes, how?</p>\n\n<p>I indexed the crawled data from <strong>Nutch</strong> into <strong>Solr</strong>:</p>\n\n<p>for linking solr with nutch (command completed successfully)\n<b>Command</b> <em>./crawl urls solr <a href=\"http://localhost:8983/solr/\" rel=\"nofollow\">http://localhost:8983/solr/</a> -depth 3 -topN 5</em></p>\n\n<p>Why do we need to index the data crawled by <strong>Nutch</strong> into <strong>Solr</strong>?</p>\n\n<p>After crawling using <strong>Nutch</strong></p>\n\n<p>command used for this: <code>./crawl urls -dir crawl -depth 3 -topN 5;</code> can we view the crawled data, if yes, where?</p>\n\n<p><b>OR</b> only after indexing the data crawled by <strong>Nutch</strong> into <strong>Solr</strong>, can we view the crawled data entires?</p>\n\n<p>How to view the crawled data in Solr web?</p>\n\n<p>command used for this: <code>./crawl urls solr localhost:8983/solr/ -depth 3 -topN 5</code></p>\n", "creation_date": 1424067672, "score": 2},
{"title": "Nutch without Solr &amp; with MySQL:", "view_count": 426, "is_answered": false, "question_id": 29060290, "tags": ["java", "mysql", "ivy", "nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/29060290/nutch-without-solr-with-mysql", "last_activity_date": 1426420408, "owner": {"age": 31, "answer_count": 60, "creation_date": 1303499210, "user_id": 1486762, "accept_rate": 55, "view_count": 444, "location": "Ahmedabad, India", "reputation": 879}, "body": "<p>I am new to Nutch, just setup using following url : <a href=\"https://dcvan24.wordpress.com/2013/05/22/fetch-and-index-data-with-nutch-mysql-and-solr/\" rel=\"nofollow\">Nutch+MySQL</a></p>\n\n<p>I ran following command because the command mentioned in url is deprecated and not available with Nutch 2.3.</p>\n\n<p>I ran the following command.\n    bin/crawl urls 1 null 10</p>\n\n<pre><code>Injecting seed URLs\n/home/Aayush/NUTCH_HOME/runtime/local/bin/nutch inject urls -crawlId 1\nInjectorJob: starting at 2015-03-15 17:14:50\nInjectorJob: Injecting urlDir: urls\nInjectorJob: java.lang.ClassNotFoundException: org.apache.gora.sql.store.SqlStore\n    at java.net.URLClassLoader$1.run(URLClassLoader.java:372)\n    at java.net.URLClassLoader$1.run(URLClassLoader.java:361)\n    at java.security.AccessController.doPrivileged(Native Method)\n    at java.net.URLClassLoader.findClass(URLClassLoader.java:360)\n    at java.lang.ClassLoader.loadClass(ClassLoader.java:424)\n    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)\n    at java.lang.ClassLoader.loadClass(ClassLoader.java:357)\n    at java.lang.Class.forName0(Native Method)\n    at java.lang.Class.forName(Class.java:259)\n    at org.apache.nutch.storage.StorageUtils.getDataStoreClass(StorageUtils.java:93)\n    at org.apache.nutch.storage.StorageUtils.createWebStore(StorageUtils.java:77)\n    at org.apache.nutch.crawl.InjectorJob.run(InjectorJob.java:218)\n    at org.apache.nutch.crawl.InjectorJob.inject(InjectorJob.java:252)\n    at org.apache.nutch.crawl.InjectorJob.run(InjectorJob.java:275)\n    at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n    at org.apache.nutch.crawl.InjectorJob.main(InjectorJob.java:284)\n</code></pre>\n\n<p>And I get the following error.</p>\n\n<pre><code>Error running:\n  /home/Aayush/NUTCH_HOME/runtime/local/bin/nutch inject urls -crawlId 1\nFailed with exit value 255.\n</code></pre>\n\n<p>I found somewhere that Nutch + MySQL is not supported by Gora version 0.5 you have to downgrade the version to 0.2.1. I downgraded the version in ivy.xml and then removed the directory build and runtime. And again rebuild the project using ant. </p>\n\n<p>After rebuilding and rerunning I still get the above mentioned error.</p>\n\n<p>Please provide any information/help.</p>\n\n<p>Thank you for reading.</p>\n", "creation_date": 1426420408, "score": 2},
{"title": "Scheduling multiple possibly parallel nutch crawls based on different configurations", "view_count": 74, "is_answered": false, "question_id": 29053576, "tags": ["hadoop", "web-crawler", "nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/29053576/scheduling-multiple-possibly-parallel-nutch-crawls-based-on-different-configurat", "last_activity_date": 1426363355, "owner": {"user_id": 1858513, "view_count": 2, "answer_count": 0, "creation_date": 1354073064, "reputation": 23}, "body": "<p>I have a use case where I need to define schedules for crawling of certain domains with nutch. I'm having a hard time wrapping my head around how this would be setup. It looks to me that the way nutch is designed it runs with a single instance that can in itself handle a huge number of hosts.</p>\n\n<p>So let's say I have three organizations who I will be crawling their sites. Each organization will have their own set of seeds, configurations, and start and stop times of active crawling. Conceivably each of these three organizations would have their own crawl jobs that get fired up based on the organizations defined schedules. Therefore, it is possible that two or more jobs will be running at the same time. Is this something that can be setup?</p>\n", "creation_date": 1426363355, "score": 1},
{"title": "org.apache.solr.common.SolrException: Not Found", "view_count": 581, "is_answered": false, "question_id": 29047614, "tags": ["hadoop", "solr", "web-crawler", "nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/29047614/org-apache-solr-common-solrexception-not-found", "last_activity_date": 1426326984, "owner": {"user_id": 4583109, "view_count": 12, "answer_count": 0, "creation_date": 1424338554, "reputation": 11}, "body": "<p>I want to do a web crawler using Nutch 1.9 and Solr 4.10.2 \nThe crawling is working but when it comes to indexing there is a problem. I looked for the problem and I tried so many methods but nothing seem to work. This is what I get: </p>\n\n<pre><code>Indexer: starting at 2015-03-13 20:51:08\nIndexer: deleting gone documents: false\nIndexer: URL filtering: false\nIndexer: URL normalizing: false\nActive IndexWriters :\n  SOLRIndexWriter\nsolr.server.url : URL of the SOLR instance (mandatory)\nsolr.commit.size : buffer size when sending to SOLR (default 1000)\nsolr.mapping.file : name of the mapping file for fields (default solrindex-mapping.xml)\nsolr.auth : use authentication (default false)\nsolr.auth.username : use authentication (default false)\nsolr.auth : username for authentication\nsolr.auth.password : password for authentication\n\n\nIndexer: java.io.IOException: Job failed!\nat org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:1357)\nat org.apache.nutch.indexer.IndexingJob.index(IndexingJob.java:114)\nat org.apache.nutch.indexer.IndexingJob.run(IndexingJob.java:176)\nat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\nat org.apache.nutch.indexer.IndexingJob.main(IndexingJob.java:186)\n</code></pre>\n\n<p>And when I see the log file this is what I get: </p>\n\n<pre><code>2015-03-13 20:51:08,768 INFO  indexer.IndexingJob - Indexer: starting at 2015-03-13 20:51:08\n2015-03-13 20:51:08,846 INFO  indexer.IndexingJob - Indexer: deleting gone documents: false\n2015-03-13 20:51:08,846 INFO  indexer.IndexingJob - Indexer: URL filtering: false\n2015-03-13 20:51:08,846 INFO  indexer.IndexingJob - Indexer: URL normalizing: false\n2015-03-13 20:51:09,117 INFO  indexer.IndexWriters - Adding org.apache.nutch.indexwriter.solr.SolrIndexWriter\n2015-03-13 20:51:09,117 INFO  indexer.IndexingJob - Active IndexWriters :\nSOLRIndexWriter\n    solr.server.url : URL of the SOLR instance (mandatory)\n    solr.commit.size : buffer size when sending to SOLR (default 1000)\n    solr.mapping.file : name of the mapping file for fields (default solrindex-mapping.xml)\n    solr.auth : use authentication (default false)\nsolr.auth.username : use authentication (default false)\nsolr.auth : username for authentication\nsolr.auth.password : password for authentication\n\n\n2015-03-13 20:51:09,121 INFO  indexer.IndexerMapReduce - IndexerMapReduce: crawldb: testCrawl/crawldb\n2015-03-13 20:51:09,122 INFO  indexer.IndexerMapReduce - IndexerMapReduce: linkdb: testCrawl/linkdb\n2015-03-13 20:51:09,122 INFO  indexer.IndexerMapReduce - IndexerMapReduces: adding segment: testCrawl/segments/20150311221258\n2015-03-13 20:51:09,234 INFO  indexer.IndexerMapReduce - IndexerMapReduces: adding segment: testCrawl/segments/20150311222328\n2015-03-13 20:51:09,235 INFO  indexer.IndexerMapReduce - IndexerMapReduces: adding segment: testCrawl/segments/20150311222727\n2015-03-13 20:51:09,236 INFO  indexer.IndexerMapReduce - IndexerMapReduces: adding segment: testCrawl/segments/20150312085908\n2015-03-13 20:51:09,282 WARN  util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n2015-03-13 20:51:09,747 INFO  anchor.AnchorIndexingFilter - Anchor deduplication is: off\n2015-03-13 20:51:20,904 INFO  indexer.IndexWriters - Adding org.apache.nutch.indexwriter.solr.SolrIndexWriter\n2015-03-13 20:51:20,929 INFO  solr.SolrMappingReader - source: content dest: content\n2015-03-13 20:51:20,929 INFO  solr.SolrMappingReader - source: title dest: title\n2015-03-13 20:51:20,929 INFO  solr.SolrMappingReader - source: host dest: host\n2015-03-13 20:51:20,929 INFO  solr.SolrMappingReader - source: segment dest: segment\n2015-03-13 20:51:20,929 INFO  solr.SolrMappingReader - source: boost dest: boost\n2015-03-13 20:51:20,929 INFO  solr.SolrMappingReader - source: digest dest: digest\n2015-03-13 20:51:20,929 INFO  solr.SolrMappingReader - source: tstamp dest: tstamp\n2015-03-13 20:51:21,192 INFO  solr.SolrIndexWriter - Indexing 250 documents\n2015-03-13 20:51:21,192 INFO  solr.SolrIndexWriter - Deleting 0 documents\n2015-03-13 20:51:21,342 INFO  solr.SolrIndexWriter - Indexing 250 documents\n2015-03-13 20:51:21,437 WARN  mapred.LocalJobRunner - job_local1194740690_0001\norg.apache.solr.common.SolrException: Not Found\n\nNot Found\n\nrequest: http://127.0.0.1:8983/solr/update?wt=javabin&amp;version=2\nat org.apache.solr.client.solrj.impl.CommonsHttpSolrServer.request(CommonsHttpSolrServer.java:430)\nat org.apache.solr.client.solrj.impl.CommonsHttpSolrServer.request(CommonsHttpSolrServer.java:244)\nat org.apache.solr.client.solrj.request.AbstractUpdateRequest.process(AbstractUpdateRequest.java:105)\nat org.apache.nutch.indexwriter.solr.SolrIndexWriter.write(SolrIndexWriter.java:135)\nat org.apache.nutch.indexer.IndexWriters.write(IndexWriters.java:88)\nat org.apache.nutch.indexer.IndexerOutputFormat$1.write(IndexerOutputFormat.java:50)\nat org.apache.nutch.indexer.IndexerOutputFormat$1.write(IndexerOutputFormat.java:41)\nat org.apache.hadoop.mapred.ReduceTask$OldTrackingRecordWriter.write(ReduceTask.java:458)\nat org.apache.hadoop.mapred.ReduceTask$3.collect(ReduceTask.java:500)\nat org.apache.nutch.indexer.IndexerMapReduce.reduce(IndexerMapReduce.java:323)\nat org.apache.nutch.indexer.IndexerMapReduce.reduce(IndexerMapReduce.java:53)\nat org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:522)\nat org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:421)\nat org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:398)\n\n2015-03-13 20:51:21,607 ERROR indexer.IndexingJob - Indexer: java.io.IOException: Job failed!\n\nat org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:1357)\nat org.apache.nutch.indexer.IndexingJob.index(IndexingJob.java:114)\nat org.apache.nutch.indexer.IndexingJob.run(IndexingJob.java:176)\nat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\nat org.apache.nutch.indexer.IndexingJob.main(IndexingJob.java:186)\n</code></pre>\n\n<p>So please any help?</p>\n", "creation_date": 1426326984, "score": 1},
{"title": "Nutch : Indexer: java.io.IOException: Job failed", "view_count": 448, "is_answered": false, "question_id": 29035512, "tags": ["java", "apache", "hadoop", "solr", "nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/29035512/nutch-indexer-java-io-ioexception-job-failed", "last_activity_date": 1426260121, "owner": {"user_id": 4662134, "answer_count": 1, "creation_date": 1426152609, "accept_rate": 0, "view_count": 11, "reputation": 21}, "body": "<p>when I execute this commands line :</p>\n\n<pre><code>ahmed@ubuntu:~/apache-nutch-1.9/bin$ ./crawl seeds fir localhost:8983/solr 1\n</code></pre>\n\n<p>I got this errors Exception in the end of Execution: </p>\n\n<pre><code>Indexer: java.io.IOException: Job failed!\n\nat org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:1357)\nat org.apache.nutch.indexer.IndexingJob.index(IndexingJob.java:114)\nat org.apache.nutch.indexer.IndexingJob.run(IndexingJob.java:176)\nat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\nat org.apache.nutch.indexer.IndexingJob.main(IndexingJob.java:186)   \n</code></pre>\n\n<p>Can I have please a Hand Help. </p>\n", "creation_date": 1426258810, "score": 2},
{"title": "Exception in thread &quot;main&quot; java.lang.NoClassDefFoundError In apach nutch", "view_count": 328, "is_answered": false, "answers": [{"question_id": 29007497, "owner": {"user_id": 582789, "link": "http://stackoverflow.com/users/582789/alfonso-nishikawa", "user_type": "registered", "reputation": 1263}, "body": "<p>It does not exist a command called 'bin/Crawl'.\nIf you execute <code>./bin/nutch</code> you get the list of commands:</p>\n\n<pre><code>Usage: nutch COMMAND\n where COMMAND is one of:\n inject         inject new urls into the database\n hostinject     creates or updates an existing host table from a text file\n generate       generate new batches to fetch from crawl db\n fetch          fetch URLs marked during generate\n parse          parse URLs marked during fetch\n updatedb       update web table after parsing\n updatehostdb   update host table after parsing\n readdb         read/dump records from page database\n readhostdb     display entries from the hostDB\n index          run the plugin-based indexer on parsed batches\n elasticindex   run the elasticsearch indexer - DEPRECATED use the index command instead\n solrindex      run the solr indexer on parsed batches - DEPRECATED use the index command instead\n solrdedup      remove duplicates from solr\n solrclean      remove HTTP 301 and 404 documents from solr - DEPRECATED use the clean command instead\n clean          remove HTTP 301 and 404 documents and duplicates from     indexing backends configured via plugins\n parsechecker   check the parser for a given url\n indexchecker   check the indexing filters for a given url\n plugin         load a plugin and run one of its classes main()\n nutchserver    run a (local) Nutch server on a user defined port\n webapp         run a local Nutch web application\n junit          runs the given JUnit test\n or\n CLASSNAME  run the class named CLASSNAME\nMost commands print help when invoked w/o parameters.\n</code></pre>\n\n<p>Since 'bin/Crawl' command does not exist, it assumes it is a CLASSNAME, therefore the error.</p>\n\n<p>In the past existed a <code>./bin/nutch crawl</code> (deprecated), but now there is a specific script for crawling. Use this:</p>\n\n<pre><code>./bin/crawl\n</code></pre>\n", "creation_date": 1426160598, "is_accepted": false, "score": 0, "last_activity_date": 1426160598, "answer_id": 29008856}], "question_id": 29007497, "tags": ["java", "ubuntu-14.04", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/29007497/exception-in-thread-main-java-lang-noclassdeffounderror-in-apach-nutch", "last_activity_date": 1426160598, "owner": {"user_id": 4662134, "answer_count": 1, "creation_date": 1426152609, "accept_rate": 0, "view_count": 11, "reputation": 21}, "body": "<p>I have set the JAVA_HOME in unix environment.The Problem it looks like a path of Class.I don't know...</p>\n\n<p>when I execute This commad line:</p>\n\n<blockquote>\n  <p>ahmed@ubuntu:~/apache-nutch-1.9/bin$ ./nutch bin/Crawl</p>\n</blockquote>\n\n<p>I got this Exception:</p>\n\n<blockquote>\n  <p>Exception in thread \"main\" java.lang.NoClassDefFoundError: bin/Crawl\n  Caused by: java.lang.ClassNotFoundException: bin.Crawl\n  at java.net.URLClassLoader$1.run(URLClassLoader.java:217)\n  at java.security.AccessController.doPrivileged(Native Method)\n  at java.net.URLClassLoader.findClass(URLClassLoader.java:205)\n  at java.lang.ClassLoader.loadClass(ClassLoader.java:323)\n  at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:294)\n  at java.lang.ClassLoader.loadClass(ClassLoader.java:268)\n  Could not find the main class: bin/Crawl. Program will exit.</p>\n</blockquote>\n\n<p>Can I have A answer. </p>\n", "creation_date": 1426156737, "score": 2},
{"title": "How to count documents using stored fields only in Apache solr", "view_count": 143, "is_answered": false, "answers": [{"question_id": 28984128, "owner": {"user_id": 994000, "link": "http://stackoverflow.com/users/994000/betolink", "user_type": "registered", "reputation": 83}, "body": "<p>The default <strong>nutch-default.xml</strong> config file does not have the index-more plugin activated. You can enable it by adding it to the plugin chain.</p>\n\n<p>Look for the plugin.includes property and change it from</p>\n\n<p><code>&lt;value&gt;protocol-http|urlfilter-regex|parse-(html|tika)|index-(basic|anchor)|indexer-solr|scoring-opic|urlnormalizer-(pass|regex|basic)&lt;/value&gt;\n</code></p>\n\n<p>to</p>\n\n<p><code>&lt;value&gt;protocol-http|urlfilter-regex|parse-(html|tika)|index-(basic|anchor|more)|indexer-solr|scoring-opic|urlnormalizer-(pass|regex|basic)&lt;/value&gt;\n</code></p>\n\n<p><strong>index-more</strong> will index the fetch date.\nNow to know the total number of documents being indexed you need to do a Solr query. </p>\n\n<p>All documents: <code>*:*</code></p>\n\n<p>Documents indexed in the last 24 hours: <code>date:[NOW-1DAY TO NOW]</code></p>\n", "creation_date": 1426101108, "is_accepted": false, "score": 0, "last_activity_date": 1426101108, "answer_id": 28995341}], "question_id": 28984128, "tags": ["apache", "indexing", "solr", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/28984128/how-to-count-documents-using-stored-fields-only-in-apache-solr", "last_activity_date": 1426101108, "owner": {"user_id": 3454410, "answer_count": 20, "creation_date": 1395646154, "accept_rate": 48, "view_count": 174, "reputation": 832}, "body": "<p>I am using solr 4.10.3. Documents are indexed using apache nutch 2.3. There is a field in schema.xml that is tstamp that contains informas when documents was indexed. This field  is not indexed and stored only in solr. I want to count no of documents indexed by nutch in solr. It is clear that I have to use tstamp field. Now how I can do it?</p>\n\n<p>Please explain in details.</p>\n", "creation_date": 1426069743, "score": 0},
{"title": "InjectorJob: java.lang.UnsupportedOperationException: Not implemented by the DistributedFileSystem FileSystem implementation (gora solr, nutch 2)", "view_count": 602, "is_answered": false, "question_id": 28989398, "tags": ["hadoop", "solr", "nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/28989398/injectorjob-java-lang-unsupportedoperationexception-not-implemented-by-the-dis", "last_activity_date": 1426087404, "owner": {"user_id": 4658794, "view_count": 0, "answer_count": 0, "creation_date": 1426082853, "reputation": 6}, "body": "<p>I need some help with apache nutch 2.3.1. \nWith hbase 0.94 everything works ok but when i setup for solrStore I get this erors.</p>\n\n<pre><code>InjectorJob: java.lang.UnsupportedOperationException: Not implemented by theDistributedFileSystem FileSystem implementation \norg.apache.hadoop.fs.FileSystem.getScheme(FileSystem.java:214) at \norg.apache.hadoop.fs.FileSystem.loadFileSystems(FileSystem.java:2365) at \norg.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:2375) at \norg.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2392) at \norg.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:89) at \norg.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2431) at \norg.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2413) at \norg.apache.hadoop.fs.FileSystem.get(FileSystem.java:368) at \norg.apache.hadoop.fs.FileSystem.get(FileSystem.java:167) at \norg.apache.hadoop.fs.FileSystem.get(FileSystem.java:352) at \norg.apache.hadoop.fs.Path.getFileSystem(Path.java:296) at \norg.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath(FileInputFormat.java:372) at \norg.apache.nutch.crawl.InjectorJob.run(InjectorJob.java:212) at \norg.apache.nutch.crawl.InjectorJob.inject(InjectorJob.java:252) at \norg.apache.nutch.crawl.InjectorJob.run(InjectorJob.java:275) at \norg.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70) at \norg.apache.nutch.crawl.InjectorJob.main(InjectorJob.java:284) \n</code></pre>\n\n<p>Here are some of the steps I did</p>\n\n<ul>\n<li><strong>solr</strong>: have solr 4.8.1 with webpage schema (no errors)</li>\n<li><strong>ivy.xml</strong>: uncommented dependency name=\"gora-solr\" rev=\"0.5\"</li>\n<li><p><strong>nutch-site.xml</strong>: org.apache.gora.solr.store.SolrStore</p></li>\n<li><p><strong>gora.properties</strong> </p>\n\n<pre><code>gora.datastore.default=org.apache.gora.solr.store.SolrStore\ngora.solrstore.solr.url=http://localhost:8983/solr\ngora.solrstore.solr.config=solrconfig.xml\ngora.solrstore.solr.schema=gora-solr-webpage-schema.xml\ngora.solrstore.solr.batchSize=100\ngora.solrstore.solr.solrjserver=http\ngora.solrstore.solr.commitWithin=1000\ngora.solrstore.solr.resultsSize=100\n</code></pre></li>\n<li><p>ant runtime</p></li>\n</ul>\n\n<p>Did someone managed to use gora solr 0.5 with apache nutch 2.3?</p>\n", "creation_date": 1426084356, "score": 1},
{"title": "How to index the plugin field in nutch with solr?", "view_count": 910, "owner": {"user_id": 3812643, "answer_count": 5, "creation_date": 1404740615, "accept_rate": 67, "view_count": 17, "location": "Guangzhou, China", "reputation": 83}, "is_answered": true, "answers": [{"question_id": 25259330, "owner": {"user_id": 994000, "link": "http://stackoverflow.com/users/994000/betolink", "user_type": "registered", "reputation": 83}, "body": "<p>Perhaps you forgot to activate those plugins in your nutch-default or nutch-site file.</p>\n\n<pre><code>&lt;property&gt;\n &lt;name&gt;plugin.includes&lt;/name&gt;\n &lt;value&gt;protocol-http|urlfilter-regex|parse-(html|tika)|index-(basic|anchor|more)|scoring- opic|urlnormalizer-(pass|regex|basic)&lt;/value&gt;\n&lt;/property&gt;\n</code></pre>\n\n<p>Then you might want to add them in the solrindex-mapping.xml file as well...</p>\n\n<pre><code>&lt;fields&gt;\n &lt;field dest=\"content\" source=\"content\"/&gt;\n &lt;field dest=\"title\" source=\"title\"/&gt;\n &lt;field dest=\"host\" source=\"host\"/&gt;\n &lt;field dest=\"segment\" source=\"segment\"/&gt;\n &lt;field dest=\"boost\" source=\"boost\"/&gt;\n &lt;field dest=\"digest\" source=\"digest\"/&gt;\n &lt;field dest=\"tstamp\" source=\"tstamp\"/&gt;\n &lt;field dest=\"anchor\" source=\"anchor\"/&gt;\n &lt;field dest=\"type\" source=\"type\"/&gt;\n &lt;field dest=\"id\" source=\"url\"/&gt;\n &lt;copyField source=\"url\" dest=\"url\"/&gt;\n&lt;/fields&gt;\n&lt;uniqueKey&gt;id&lt;/uniqueKey&gt;\n</code></pre>\n\n<p>Compile Nutch and do a new crawl, you should be able to see the index-more and index-anchor fields in solr.</p>\n", "creation_date": 1407868000, "is_accepted": true, "score": 2, "last_activity_date": 1407868000, "answer_id": 25271492}, {"question_id": 25259330, "owner": {"user_id": 156522, "accept_rate": 86, "link": "http://stackoverflow.com/users/156522/vsingh", "user_type": "registered", "reputation": 2017}, "body": "<p>This configuration in nutch-site.xml or maybe nutch-default.xml in your case    </p>\n\n<pre><code>&lt;property&gt;\n&lt;name&gt;plugin.includes&lt;/name&gt;\n&lt;value&gt;protocol-http|urlfilter-regex|parse-(html|tika)|index-(basic|anchor)|indexer-solr|scoring-opic|urlnormalizer-(pass|regex|basic)&lt;/value&gt;\n&lt;/property&gt;\n</code></pre>\n", "creation_date": 1426007048, "is_accepted": false, "score": 0, "last_activity_date": 1426007048, "answer_id": 28969813}], "question_id": 25259330, "tags": ["solr", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/25259330/how-to-index-the-plugin-field-in-nutch-with-solr", "last_activity_date": 1426007048, "accepted_answer_id": 25271492, "body": "<p>I integrate nutch/solr/hbase to construct a search engine, it work well, except that some fileds in the schma.xml are not indexed to solr.\nThe schema.xml likes this:</p>\n\n<pre><code>&lt;schema name=\"nutch\" version=\"1.5\"&gt;\n    &lt;types&gt;\n    &lt;fieldType name=\"string\" class=\"solr.StrField\" sortMissingLast=\"true\"\n        omitNorms=\"true\"/&gt;\n    &lt;fieldType name=\"long\" class=\"solr.TrieLongField\" precisionStep=\"0\"\n        omitNorms=\"true\" positionIncrementGap=\"0\"/&gt;\n    &lt;fieldType name=\"float\" class=\"solr.TrieFloatField\" precisionStep=\"0\"\n        omitNorms=\"true\" positionIncrementGap=\"0\"/&gt;\n    &lt;fieldType name=\"date\" class=\"solr.TrieDateField\" precisionStep=\"0\"\n        omitNorms=\"true\" positionIncrementGap=\"0\"/&gt;\n    &lt;fieldType name=\"text\" class=\"solr.TextField\"\n        positionIncrementGap=\"100\"&gt;\n        &lt;analyzer&gt;\n            &lt;tokenizer class=\"solr.WhitespaceTokenizerFactory\"/&gt;\n            &lt;filter class=\"solr.StopFilterFactory\"\n                ignoreCase=\"true\" words=\"stopwords.txt\"/&gt;\n            &lt;filter class=\"solr.WordDelimiterFilterFactory\"\n                generateWordParts=\"1\" generateNumberParts=\"1\"\n                catenateWords=\"1\" catenateNumbers=\"1\" catenateAll=\"0\"\n                splitOnCaseChange=\"1\"/&gt;\n            &lt;filter class=\"solr.LowerCaseFilterFactory\"/&gt;\n            &lt;filter class=\"solr.RemoveDuplicatesTokenFilterFactory\"/&gt;\n        &lt;/analyzer&gt;\n      &lt;/fieldType&gt;\n      &lt;fieldType name=\"url\" class=\"solr.TextField\"\n        positionIncrementGap=\"100\"&gt;\n        &lt;analyzer&gt;\n            &lt;tokenizer class=\"solr.StandardTokenizerFactory\"/&gt;\n            &lt;filter class=\"solr.LowerCaseFilterFactory\"/&gt;\n            &lt;filter class=\"solr.WordDelimiterFilterFactory\"/&gt;\n        &lt;analyzer&gt;\n            &lt;tokenizer class=\"solr.StandardTokenizerFactory\"/&gt;\n            &lt;filter class=\"solr.LowerCaseFilterFactory\"/&gt;\n            &lt;filter class=\"solr.WordDelimiterFilterFactory\"\n                generateWordParts=\"1\" generateNumberParts=\"1\"/&gt;\n        &lt;/analyzer&gt;\n    &lt;/fieldType&gt;\n&lt;/types&gt;\n&lt;fields&gt;\n    &lt;field name=\"id\" type=\"string\" stored=\"true\" indexed=\"true\"/&gt;\n\n    &lt;!-- core fields --&gt;\n    &lt;field name=\"batchId\" type=\"string\" stored=\"true\" indexed=\"false\"/&gt;\n    &lt;field name=\"digest\" type=\"string\" stored=\"true\" indexed=\"false\"/&gt;\n    &lt;field name=\"boost\" type=\"float\" stored=\"true\" indexed=\"false\"/&gt;\n\n    &lt;!-- fields for index-basic plugin --&gt;\n    &lt;field name=\"host\" type=\"url\" stored=\"false\" indexed=\"true\"/&gt;\n    &lt;field name=\"url\" type=\"url\" stored=\"true\" indexed=\"true\"\n        required=\"true\"/&gt;\n    &lt;field name=\"content\" type=\"text\" stored=\"true\" indexed=\"true\"/&gt;\n    &lt;field name=\"title\" type=\"text\" stored=\"true\" indexed=\"true\"/&gt;\n    &lt;field name=\"cache\" type=\"string\" stored=\"true\" indexed=\"false\"/&gt;\n    &lt;field name=\"tstamp\" type=\"date\" stored=\"true\" indexed=\"true\"/&gt;\n\n    &lt;field name=\"_version_\" type=\"long\" indexed=\"true\" stored=\"true\"/&gt;\n    &lt;!-- fields for index-anchor plugin --&gt;\n    &lt;field name=\"anchor\" type=\"string\" stored=\"true\" indexed=\"true\"\n        multiValued=\"true\"/&gt;\n\n    &lt;!-- fields for index-more plugin --&gt;\n    &lt;field name=\"type\" type=\"string\" stored=\"true\" indexed=\"true\"\n        multiValued=\"true\"/&gt;\n    &lt;field name=\"contentLength\" type=\"long\" stored=\"true\"\n        indexed=\"true\"/&gt;\n    &lt;field name=\"lastModified\" type=\"date\" stored=\"true\"\n        indexed=\"true\"/&gt;\n    &lt;field name=\"date\" type=\"date\" stored=\"true\" indexed=\"true\"/&gt;\n\n    &lt;!-- fields for languageidentifier plugin --&gt;\n    &lt;field name=\"lang\" type=\"string\" stored=\"true\" indexed=\"true\"/&gt;\n\n    &lt;!-- fields for subcollection plugin --&gt;\n    &lt;field name=\"subcollection\" type=\"string\" stored=\"true\"\n        indexed=\"true\" multiValued=\"true\"/&gt;\n\n    &lt;!-- fields for feed plugin (tag is also used by microformats-reltag)--&gt;\n    &lt;field name=\"author\" type=\"string\" stored=\"true\" indexed=\"true\"/&gt;\n    &lt;field name=\"tag\" type=\"string\" stored=\"true\" indexed=\"true\" multiValued=\"true\"/&gt;\n    &lt;field name=\"feed\" type=\"string\" stored=\"true\" indexed=\"true\"/&gt;\n    &lt;field name=\"publishedDate\" type=\"date\" stored=\"true\"\n        indexed=\"true\"/&gt;\n    &lt;field name=\"updatedDate\" type=\"date\" stored=\"true\"\n        indexed=\"true\"/&gt;\n\n    &lt;!-- fields for creativecommons plugin --&gt;\n    &lt;field name=\"cc\" type=\"string\" stored=\"true\" indexed=\"true\"\n        multiValued=\"true\"/&gt;\n\n    &lt;!-- fields for tld plugin --&gt;\n    &lt;field name=\"tld\" type=\"string\" stored=\"false\" indexed=\"false\"/&gt;\n&lt;/fields&gt;\n&lt;uniqueKey&gt;id&lt;/uniqueKey&gt;\n&lt;defaultSearchField&gt;content&lt;/defaultSearchField&gt;\n&lt;solrQueryParser defaultOperator=\"OR\"/&gt;\n&lt;/schema&gt;\n</code></pre>\n\n<p>The fields in \" -- core fields --\" and \"-- fields for index-basic plugin --\" are indexed to solr, but other fields, such as the fields in \"-- fields for index-anchor plugin --\"  -- fields for index-more plugin -- , are not.</p>\n\n<p>what is the problem with that? </p>\n", "creation_date": 1407831427, "score": 0},
{"title": "How to resume a previous incomplete job in apache nutch crawler", "view_count": 130, "is_answered": false, "answers": [{"question_id": 28499432, "owner": {"user_id": 181206, "link": "http://stackoverflow.com/users/181206/capsule", "user_type": "registered", "reputation": 4362}, "body": "<p>There a <code>-resume</code> parameter for these commands, it doesn't work?\n<a href=\"http://wiki.apache.org/nutch/bin/nutch%20fetch\" rel=\"nofollow\">http://wiki.apache.org/nutch/bin/nutch%20fetch</a></p>\n", "creation_date": 1425951558, "is_accepted": false, "score": 0, "last_activity_date": 1425951558, "answer_id": 28954797}], "question_id": 28499432, "tags": ["apache", "web-crawler", "nutch", "resume"], "answer_count": 1, "link": "http://stackoverflow.com/questions/28499432/how-to-resume-a-previous-incomplete-job-in-apache-nutch-crawler", "last_activity_date": 1425968909, "owner": {"user_id": 3454410, "answer_count": 20, "creation_date": 1395646154, "accept_rate": 48, "view_count": 174, "reputation": 832}, "body": "<p>I am using nutch 2.3. There is a possible chance that during any stage of nutch (fetch parse index etc.), network probelm occur or power shutdown happen. How I can resume previous incomplete job. </p>\n\n<p>Please give some example for explaination?</p>\n", "creation_date": 1423829737, "score": 0},
{"title": "could to find or load main class org.apache.nutch.crawl.InjectorJob", "view_count": 328, "is_answered": false, "question_id": 28938612, "tags": ["hadoop", "solr", "nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/28938612/could-to-find-or-load-main-class-org-apache-nutch-crawl-injectorjob", "last_activity_date": 1425894214, "owner": {"age": 29, "answer_count": 0, "creation_date": 1425890473, "user_id": 4648993, "view_count": 0, "location": "France", "reputation": 11}, "body": "<p>I'm using Linux with hadoop cloudera and hbase. </p>\n\n<p>Could you tell me how to correct this error?</p>\n\n<p>The next command give me this error:</p>\n\n<pre><code>src/bin/nutch inject crawl/crawldb dmoz/\n</code></pre>\n\n<p>if you need any other information ask for me.</p>\n", "creation_date": 1425893228, "score": 2},
{"title": "zookeeper unable to open socket to localhost/0:0:0:0:0:0:0:1:2181", "view_count": 2424, "owner": {"user_id": 3454410, "answer_count": 20, "creation_date": 1395646154, "accept_rate": 48, "view_count": 174, "reputation": 832}, "is_answered": true, "answers": [{"question_id": 28109669, "owner": {"user_id": 13447, "accept_rate": 100, "link": "http://stackoverflow.com/users/13447/olaf-kock", "user_type": "registered", "reputation": 30204}, "body": "<p>I don't know zookeeper, but the two IP addresses that you're posting don't have anything to do with each other:</p>\n\n<p><code>1.1.1.1</code> is an IPV4 address. <code>0:0:0:0:0:0:0:1</code> is an IPV6 address (and it's the one for localhost). The shorthand for the IPV6 localhost address is <code>::1</code></p>\n\n<p>Thus, if you're expecting a server on <code>localhost</code>, make sure that it binds to IPV6 as well and not only to <code>127.0.0.1</code> (which is the IPV4 address of localhost). I've seen problems like this with servers that only listened to 127.0.0.1, or with firewalls that only allowed access to localhost on IPV4, but not on IPV6. </p>\n\n<p>I'm not sure what you expect to hit when you're going to 1.1.1.1. Are you using this address locally? AFAIK it's a publically routed address, but I guess it gets a lot of this kind of unsolicited traffic that it's more or less unusable.</p>\n", "creation_date": 1422272860, "is_accepted": false, "score": 1, "last_activity_date": 1422272860, "answer_id": 28149516}, {"last_edit_date": 1422355748, "owner": {"user_id": 2265190, "accept_rate": 50, "link": "http://stackoverflow.com/users/2265190/hms", "user_type": "registered", "reputation": 293}, "body": "<p>( CentOS ) Check your /etc/hosts file and if it conatins like</p>\n\n<pre><code>127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4\n::1         localhost localhost.localdomain localhost6 localhost6.localdomain6\n</code></pre>\n\n<p>change it to</p>\n\n<pre><code>127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4\n::1         ip6-localhost ip6-localhost.localdomain localhost6 localhost6.localdomain6\n</code></pre>\n", "question_id": 28109669, "creation_date": 1422275609, "is_accepted": true, "score": 3, "last_activity_date": 1422355748, "answer_id": 28150212}], "question_id": 28109669, "tags": ["apache", "hbase", "nutch", "zookeeper"], "answer_count": 2, "link": "http://stackoverflow.com/questions/28109669/zookeeper-unable-to-open-socket-to-localhost-000000012181", "last_activity_date": 1425884093, "accepted_answer_id": 28150212, "body": "<p>I am using zookeeper ensemble for hbase. Zookeeper is running on 3 machines. While HBase is also in fully distributed mode. I have Nutch 2.x version. When I start nutch to crawl some data, it gives following buggs in nutch log file.</p>\n\n<pre><code>ERROR zookeeper.ClientCnxnSocketNIO - Unable to open socket to localhost/0:0:0:0:0:0:0:1:2181\n2015-01-23 16:34:21,956 WARN  zookeeper.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect\njava.net.SocketException: Network is unreachable\n        at sun.nio.ch.Net.connect0(Native Method)\n        at sun.nio.ch.Net.connect(Net.java:457)\n        at sun.nio.ch.Net.connect(Net.java:449)\n        at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:647)\n        at org.apache.zookeeper.ClientCnxnSocketNIO.registerAndConnect(ClientCnxnSocketNIO.java:266)\n        at org.apache.zookeeper.ClientCnxnSocketNIO.connect(ClientCnxnSocketNIO.java:276)\n        at org.apache.zookeeper.ClientCnxn$SendThread.startConnect(ClientCnxn.java:958)\n        at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:993)\n2015-01-23 16:34:22,063 WARN  zookeeper.RecoverableZooKeeper - Possibly transient ZooKeeper exception: org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss for /hbase/hbaseid\n</code></pre>\n\n<p>When I run a command on three zookeepers instances </p>\n\n<pre><code>echo ruok | nc 1.1.1.1 2181 it says imok\n</code></pre>\n\n<p>What is wrong with it? My hbase version is 0.94.14 and zookeeper version is 3.4.5 , solr version 4.10.3 ( for indexing )and Nutch version is 2.2.3</p>\n", "creation_date": 1422015214, "score": 3},
{"title": "Nutch 1.7 with Hadoop 2.6.0 &quot;Wrong FS&quot; Error", "view_count": 142, "is_answered": false, "question_id": 28881562, "tags": ["hadoop", "hdfs", "nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/28881562/nutch-1-7-with-hadoop-2-6-0-wrong-fs-error", "last_activity_date": 1425569346, "owner": {"user_id": 4637061, "view_count": 1, "answer_count": 0, "creation_date": 1425564479, "reputation": 11}, "body": "<p>We have been trying to use Nutch 1.7 with Hadoop 2.6.0.</p>\n\n<p>After installation, we we try to submit a job to Nutch, we receive the following error:</p>\n\n<pre><code>INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized Exception in thread \"main\" java.lang.IllegalArgumentException: Wrong FS: hdfs://master:9000/user/ubuntu/crawl/crawldb/436075385, expected: file:///\n</code></pre>\n\n<p>Job is submitted using the following command:</p>\n\n<pre><code>./crawl urls crawl_results 1\n</code></pre>\n\n<p>Also, we have checked fs.default.name setting in core-site.xml is having hdfs protocol:</p>\n\n<pre><code>&lt;property&gt; \n&lt;name&gt;fs.default.name&lt;/name&gt;\n&lt;value&gt;hdfs://master:9000&lt;/value&gt; \n&lt;/property&gt;\n</code></pre>\n\n<p>It is happening when crawl command is sent to Nutch, after it reads the input URLs from file and attempts to insert the data into crawl db.</p>\n\n<p>Any insights would be appreciated.</p>\n\n<p>Thanks in advance.</p>\n", "creation_date": 1425569346, "score": 2},
{"title": "Nutch 2.3 not storing crawl data correctly in Cassandra", "view_count": 693, "owner": {"age": 33, "answer_count": 8, "creation_date": 1292435781, "user_id": 543720, "accept_rate": 80, "view_count": 71, "location": "Groningen, The Netherlands", "reputation": 113}, "is_answered": true, "answers": [{"question_id": 28813709, "owner": {"user_id": 582789, "link": "http://stackoverflow.com/users/582789/alfonso-nishikawa", "user_type": "registered", "reputation": 1263}, "body": "<p>What version of Gora are you using?</p>\n\n<p>Can you, please, delete de database and execute:</p>\n\n<pre><code>nutch inject ~/dev/urls/\nnutch generate -batchId 1\nnutch fetch 1\n</code></pre>\n\n<p>and then</p>\n\n<pre><code>nutch readdb -url &lt;some known url&gt; -content\n</code></pre>\n\n<p>Does it shows the correct information? If the answer is yes, then do:</p>\n\n<pre><code>nutch parse 1\nnutch updatedb\nnutch readdb -url &lt;some known url&gt; -content\n</code></pre>\n", "creation_date": 1425314754, "is_accepted": false, "score": 1, "last_activity_date": 1425314754, "answer_id": 28815075}, {"question_id": 28813709, "owner": {"user_id": 543720, "accept_rate": 80, "link": "http://stackoverflow.com/users/543720/jeroen-vlek", "user_type": "registered", "reputation": 113}, "body": "<p>It's a bug in Gora. A blocker ticket has been opened:</p>\n\n<p><a href=\"http://mail-archives.apache.org/mod_mbox/gora-user/201503.mbox/%3CCAGaRif3NfKmvRE%3DBhLuFw8fmxUOLW1wJhNefp_%2Bk901kjJs2ig%40mail.gmail.com%3E\" rel=\"nofollow\">http://mail-archives.apache.org/mod_mbox/gora-user/201503.mbox/%3CCAGaRif3NfKmvRE%3DBhLuFw8fmxUOLW1wJhNefp_%2Bk901kjJs2ig%40mail.gmail.com%3E</a></p>\n\n<p><a href=\"https://issues.apache.org/jira/browse/GORA-416\" rel=\"nofollow\">https://issues.apache.org/jira/browse/GORA-416</a></p>\n", "creation_date": 1425460333, "is_accepted": true, "score": 0, "last_activity_date": 1425460333, "answer_id": 28850486}], "question_id": 28813709, "tags": ["web-crawler", "nutch", "gora"], "answer_count": 2, "link": "http://stackoverflow.com/questions/28813709/nutch-2-3-not-storing-crawl-data-correctly-in-cassandra", "last_activity_date": 1425460333, "accepted_answer_id": 28850486, "body": "<p>I'm running a crawl with mostly default options with Nutch 2.3 with a Cassandra backend. As a seed list a file with 71 urls is used and I'm crawling with the following command:</p>\n\n<pre><code>bin/crawl ~/dev/urls/ crawlid1 5\n</code></pre>\n\n<p>The keys are stored in Cassandra and the f, p and sc column families are created, however, if I try to read the WebPage objects, the content and text fields are empty, despite the output stating that the fetch and parser jobs supposedly ran.</p>\n\n<p>Furthermore, no new links are added to the link db, despite <em>db.update.additions.allowed</em> having its default value of <em>true</em>.</p>\n\n<p>After completion, I try to read out the crawl data with the code below. This only shows some fields being populated. Looking at the code in FetcherJob and ParserJob, I don't see any reason why the <em>content</em> or <em>text</em> fields should be empty. I'm probably missing some basic setting, but googling for my problem didn't yield anything. I also set breakpoints in the ParserMapper and FetcherMapper and they seem to be executed.</p>\n\n<p>Does anyone know how to store fetched/parsed content in Cassandra with Nutch 2?</p>\n\n<pre><code>import static java.nio.charset.StandardCharsets.UTF_8;\n\nimport java.io.Closeable;\nimport java.io.IOException;\nimport java.util.HashMap;\nimport java.util.Map;\nimport java.util.Map.Entry;\n\nimport org.apache.gora.query.Query;\nimport org.apache.gora.query.Result;\nimport org.apache.gora.store.DataStore;\nimport org.apache.gora.store.DataStoreFactory;\nimport org.apache.gora.util.GoraException;\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.nutch.storage.WebPage;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\n/**\n * Reads the rows from a {@link DataStore} as a {@link WebPage}.\n * \n * @author Jeroen Vlek, jv@datamantics.com Created: Feb 25, 2015\n *\n */\npublic class NutchWebPageReader implements Closeable {\n    private static final Logger LOGGER = LoggerFactory.getLogger(NutchWebPageReader.class);\n\n    DataStore&lt;String, WebPage&gt; dataStore;\n\n    /**\n     * Initializes the datastore field with the {@link Configuration} as defined\n     * in gora.properties in the classpath.\n     */\n    public NutchWebPageReader() {\n        try {\n            dataStore = DataStoreFactory.getDataStore(String.class, WebPage.class, new Configuration());\n        } catch (GoraException e) {\n            throw new RuntimeException(e);\n        }\n    }\n\n    /**\n     * @param args\n     */\n    public static void main(String[] args) {\n        Map&lt;String, WebPage&gt; pages = null;\n        try (NutchWebPageReader pageReader = new NutchWebPageReader()) {\n            pages = pageReader.getAllPages();\n        } catch (IOException e) {\n            LOGGER.error(\"Could not close page reader.\", e);\n        }\n        LOGGER.info(\"Found {} results.\", pages.size());\n\n        for (Entry&lt;String, WebPage&gt; entry : pages.entrySet()) {\n            String key = entry.getKey();\n            WebPage page = entry.getValue();\n            String content = \"null\";\n            if (page.getContent() != null) {\n                new String(page.getContent().array(), UTF_8);\n            }\n            LOGGER.info(\"{} with content {}\", key, content);\n        }\n    }\n\n    /**\n     * @return\n     * \n     */\n    public Map&lt;String, WebPage&gt; getAllPages() {\n        Query&lt;String, WebPage&gt; query = dataStore.newQuery();\n        Result&lt;String, WebPage&gt; result = query.execute();\n        Map&lt;String, WebPage&gt; resultMap = new HashMap&lt;&gt;();\n        try {\n            while (result.next()) {\n                resultMap.put(result.getKey(), dataStore.get(result.getKey()));\n            }\n        } catch (Exception e) {\n            LOGGER.error(\"Something went wrong while processing the query result.\", e);\n        }\n\n        return resultMap;\n    }\n\n    /*\n     * (non-Javadoc)\n     * \n     * @see java.io.Closeable#close()\n     */\n    @Override\n    public void close() throws IOException {\n        dataStore.close();\n    }\n\n}\n</code></pre>\n\n<p>And here is my nutch-site.xml:</p>\n\n<pre><code>&lt;property&gt;\n    &lt;name&gt;storage.data.store.class&lt;/name&gt;\n    &lt;value&gt;org.apache.gora.cassandra.store.CassandraStore&lt;/value&gt;\n    &lt;description&gt;Default class for storing data&lt;/description&gt;\n&lt;/property&gt;\n&lt;property&gt;\n    &lt;name&gt;http.agent.name&lt;/name&gt;\n    &lt;value&gt;Nibbler&lt;/value&gt;\n&lt;/property&gt;\n&lt;property&gt;\n    &lt;name&gt;fetcher.verbose&lt;/name&gt;\n    &lt;value&gt;true&lt;/value&gt;\n    &lt;description&gt;If true, fetcher will log more verbosely.&lt;/description&gt;\n&lt;/property&gt;\n&lt;property&gt;\n    &lt;name&gt;fetcher.parse&lt;/name&gt;\n    &lt;value&gt;true&lt;/value&gt;\n    &lt;description&gt;If true, fetcher will parse content. NOTE: previous\n        releases would\n        default to true. Since 2.0 this is set to false as a safer default.&lt;/description&gt;\n&lt;/property&gt;\n&lt;property&gt;\n    &lt;name&gt;http.content.limit&lt;/name&gt;\n    &lt;value&gt;999999999&lt;/value&gt;\n&lt;/property&gt;\n</code></pre>\n\n<p><strong>EDIT</strong></p>\n\n<p>I was using Cassandra 2.0.12, but I just tried it with 2.0.2 and that didn't resolve the issue. So the versions I'm using:</p>\n\n<ul>\n<li>Nutch: 2.3 (git clone checked out at tag \"release-2.3\") </li>\n<li>Gora: 0.5 in\nNutch </li>\n<li>Cassandra: 2.0.2</li>\n</ul>\n\n<p>Changing <em>result.get()</em> into <em>dataStore.get(result.getKey())</em> resulted in some fields actually being populated, but content and text are still empty.</p>\n\n<p>Some output:</p>\n\n<pre><code>[jvlek@orochimaru nutch]$ runtime/local/bin/nutch inject ~/dev/urls/\nInjectorJob: starting at 2015-03-02 18:34:29\nInjectorJob: Injecting urlDir: /home/jvlek/dev/urls\nInjectorJob: Using class org.apache.gora.cassandra.store.CassandraStore as the Gora storage class.\nInjectorJob: total number of urls rejected by filters: 0\nInjectorJob: total number of urls injected after normalization and filtering: 69\nInjector: finished at 2015-03-02 18:34:32, elapsed: 00:00:02\n[jvlek@orochimaru nutch]$ runtime/local/bin/nutch readdb -url http://www.wired.com/\nkey:    http://www.wired.com/\nbaseUrl:        null\nstatus: 0 (null)\nfetchTime:      1425317669727\nprevFetchTime:  0\nfetchInterval:  2592000\nretriesSinceFetch:      0\nmodifiedTime:   0\nprevModifiedTime:       0\nprotocolStatus: (null)\nparseStatus:    (null)\ntitle:  null\nscore:  1.0\nmarker _injmrk_ :       y\nmarker dist :   0\nreprUrl:        null\nmetadata _csh_ :        ??\n\n[jvlek@orochimaru nutch]$ runtime/local/bin/nutch generate -batchId 1\nGeneratorJob: starting at 2015-03-02 18:34:50\nGeneratorJob: Selecting best-scoring urls due for fetch.\nGeneratorJob: starting\nGeneratorJob: filtering: true\nGeneratorJob: normalizing: true\nGeneratorJob: finished at 2015-03-02 18:34:54, time elapsed: 00:00:03\nGeneratorJob: generated batch id: 1 containing 66 URLs\n[jvlek@orochimaru nutch]$ runtime/local/bin/nutch readdb -url http://www.wired.com/\nkey:    http://www.wired.com/\nbaseUrl:        null\nstatus: 0 (null)\nfetchTime:      1425317669727\nprevFetchTime:  0\nfetchInterval:  2592000\nretriesSinceFetch:      0\nmodifiedTime:   0\nprevModifiedTime:       0\nprotocolStatus: (null)\nparseStatus:    (null)\ntitle:  null\nscore:  1.0\nmarker _injmrk_ :       y\nmarker _gnmrk_ :        1\nmarker dist :   0\nreprUrl:        null\nbatchId:        1\nmetadata _csh_ :        ??\n</code></pre>\n", "creation_date": 1425310869, "score": 0},
{"title": "Nutch does not parse RSS feed file", "view_count": 426, "is_answered": false, "question_id": 28813268, "tags": ["java", "solr", "rss", "web-crawler", "nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/28813268/nutch-does-not-parse-rss-feed-file", "last_activity_date": 1425309971, "owner": {"user_id": 557105, "answer_count": 0, "creation_date": 1293624690, "accept_rate": 25, "view_count": 17, "reputation": 23}, "body": "<p>I am building a news aggregator application. I plan to put in a list of RSS feeds and I am trying to get nutch to parse rss feed and save news articles in Solr.\nBut Nutch does not get links of articles from the RSS feed. It only indexes RSS file as a single document in Solr index. It does not crawl any links inside rss feed file. Here is how nutch-site.xml is configured to use plugins:</p>\n\n<pre><code>  &lt;property&gt;\n    &lt;name&gt;plugin.includes&lt;/name&gt;\n    &lt;value&gt;protocol-http|feed|urlfilter-regex|suffix-urlfilter|parse-html|index-(basic|anchor)|indexer-solr|query-(basic|site|url)|response-(json|xml)|summary-basic|scoring-opic|urlnormalizer-(pass|regex|basic)&lt;/value&gt;\n    &lt;/property&gt;\n</code></pre>\n\n<p>Also parse-plugins.xml has follwing configuration:</p>\n\n<pre><code>&lt;mimeType name=\"application/rss+xml\"&gt;\n        &lt;plugin id=\"feed\" /&gt;\n\n\n         &lt;plugin id=\"parse-tika\" /&gt;\n\n    &lt;/mimeType&gt;\n&lt;mimeType name=\"text/xml\"&gt;\n        &lt;plugin id=\"feed\" /&gt;\n        &lt;plugin id=\"parse-tika\" /&gt;\n\n    &lt;/mimeType&gt;\n</code></pre>\n\n<p>I am using following command to run nutch:\nbin/crawl urls crawldir <a href=\"http://localhost:8082/solr\" rel=\"nofollow\">http://localhost:8082/solr</a> 5</p>\n", "creation_date": 1425309630, "score": 1},
{"title": "Nutch inconsistently ignores redirects", "view_count": 132, "owner": {"user_id": 3868574, "answer_count": 2, "creation_date": 1406115901, "accept_rate": 71, "view_count": 5, "reputation": 49}, "is_answered": true, "answers": [{"question_id": 28760661, "owner": {"user_id": 3868574, "accept_rate": 71, "link": "http://stackoverflow.com/users/3868574/alreadyexists", "user_type": "registered", "reputation": 49}, "body": "<p>This was a bug, 1.10 must to be shipped with the fix:\n<a href=\"https://github.com/apache/nutch/commit/ed052df8822380ccfa89a9ffa1df324933669a59\" rel=\"nofollow\">https://github.com/apache/nutch/commit/ed052df8822380ccfa89a9ffa1df324933669a59</a></p>\n", "creation_date": 1425284291, "is_accepted": true, "score": 1, "last_activity_date": 1425284291, "answer_id": 28805238}], "question_id": 28760661, "tags": ["redirect", "web-crawler", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/28760661/nutch-inconsistently-ignores-redirects", "last_activity_date": 1425284291, "accepted_answer_id": 28805238, "body": "<p>I ran into trouble with crawling (nutch 1.9/openjdk7) pretty simple redirect cases.\nHere is a packet capture for the process.</p>\n\n<pre><code>Time        Source          Destination Protocol Info\n12.988003   99.99.99.99     8.8.4.4     DNS     Standard query 0xc165  A bloomberg.com\n13.032343   8.8.4.4         99.99.99.99 DNS     Standard query response 0xc165  A 69.191.212.191 A 69.191.251.238\n13.124471   99.99.99.99 69.191.212.191  HTTP    GET /robots.txt HTTP/1.0 \n13.228846   69.191.212.191  99.99.99.99 HTTP    HTTP/1.1 301 Moved Permanently  (text/html)\n13.264230   99.99.99.99     8.8.4.4     DNS     Standard query 0x7089  A www.bloomberg.com\n13.344767   8.8.4.4         99.99.99.99 DNS     Standard query response 0x7089  CNAME www.bloomberg.com.edgekey.net CNAME e4569.x.akamaiedge.net A 23.214.189.136\n13.351030   99.99.99.99 23.214.189.136  HTTP    GET /robots.txt HTTP/1.0 \n13.359121   23.214.189.136  99.99.99.99 HTTP    HTTP/1.0 200 OK  (text/plain)\n13.448604   99.99.99.99 69.191.212.191  HTTP    GET / HTTP/1.0 \n13.537211   69.191.212.191  99.99.99.99 HTTP    HTTP/1.1 301 Moved Permanently  (text/html)\n13.640146   99.99.99.99 69.191.212.191  HTTP    GET / HTTP/1.0 \n13.738564   69.191.212.191  99.99.99.99 HTTP    HTTP/1.1 301 Moved Permanently  (text/html)\n</code></pre>\n\n<p>Nutch tries to fetch <a href=\"http://bloomberg.com\" rel=\"nofollow\">http://bloomberg.com</a> which replies with a 301 redirect to <a href=\"http://www.bloomberg.com\" rel=\"nofollow\">http://www.bloomberg.com</a>. The redirect is handled correctly for robots.txt. However, for 'get /', fetcher keeps trying the original hostname, which keeps replying 301. No matter how big http.redirect.max, fetching fails (I've checked 10).</p>\n\n<p>Nutch 1.9 running on\nOpenJDK Runtime Environment (IcedTea 2.5.3) (7u71-2.5.3-0ubuntu0.12.04.1)\nOpenJDK Client VM (build 24.65-b04, mixed mode, sharing)</p>\n\n<p>Is this a bug (could you confirm it then) or just a misconfiguration?</p>\n\n<p>Thanks.</p>\n", "creation_date": 1425026309, "score": 0},
{"title": "./bin/hbase shell command not working", "view_count": 171, "owner": {"user_id": 4615904, "view_count": 2, "answer_count": 0, "creation_date": 1425065901, "reputation": 3}, "is_answered": true, "answers": [{"last_edit_date": 1425067555, "owner": {"user_id": 4615800, "link": "http://stackoverflow.com/users/4615800/gourav-shah-devops-trainer", "user_type": "registered", "reputation": 101}, "body": "<p>Its not able to find java at <code>/etc/java-7-openjdk//bin/java</code>.  That's where you see <code>No such file or directory: error</code>.  Make sure your <code>JAVA_HOME</code> is set correctly, jdk is installed and your <code>hbase</code> is using <code>JAVA_HOME</code> or pointing to the java path which exists. </p>\n", "question_id": 28773066, "creation_date": 1425066453, "is_accepted": true, "score": 0, "last_activity_date": 1425067555, "answer_id": 28773098}], "question_id": 28773066, "tags": ["hbase", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/28773066/bin-hbase-shell-command-not-working", "last_activity_date": 1425067555, "accepted_answer_id": 28773098, "body": "<p>i am integrating nutch with hbase . While dummy testing hbase . by typing ./bin/hbase shell.... i am getting the following error</p>\n\n<p>./bin/hbase: line 392: /etc/java-7-openjdk//bin/java: No such file or directory</p>\n\n<p>thank you</p>\n", "creation_date": 1425066294, "score": 0},
{"title": "nutch gora class not while running nutch in hadoop mode", "view_count": 552, "is_answered": true, "answers": [{"last_edit_date": 1424966593, "owner": {"user_id": 582789, "link": "http://stackoverflow.com/users/582789/alfonso-nishikawa", "user_type": "registered", "reputation": 1263}, "body": "<p>You have problems with gora dependence.\nSurely <code>apache-nutch-2.2.jar</code> does not have <code>org/apache/gora/persistency/impl/PersistentBase.class</code>. You can check with:</p>\n\n<blockquote>\n  <p>jar tf apache-nutch-2.2.jar | grep PersistentBase</p>\n</blockquote>\n\n<p>Check that you compile Nutch with Gora 0.3 version.</p>\n\n<p>I guess you don't have gora-* dependences installed in your hadoop nodes, so a solution is to send them using the .job (instead .jar) witch has all dependences bundled for Hadoop.</p>\n\n<p>If you have this installation:</p>\n\n<blockquote>\n  <p>~<br/>\n  |- nutch/<br/>\n  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|- apache-nutch-2.2.job<br/>\n  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|- bin/<br/>\n  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|- nutch</p>\n</blockquote>\n\n<p>and <code>PATH=~/nutch/bin:.....</code></p>\n\n<p>You can execute Nutch just with:</p>\n\n<p><code>$ nutch inject ...</code></p>\n\n<p><code>$ nutch crawl</code></p>\n\n<p>and <code>nutch</code> command calls Hadoop when needed.</p>\n\n<p>============ updated ==============</p>\n\n<p>Offending line: <a href=\"http://grepcode.com/file/repo1.maven.org/maven2/org.apache.nutch/nutch/2.2.1/org/apache/nutch/crawl/InjectorJob.java/#218\" rel=\"nofollow\">http://grepcode.com/file/repo1.maven.org/maven2/org.apache.nutch/nutch/2.2.1/org/apache/nutch/crawl/InjectorJob.java/#218</a></p>\n\n<p>============ update 2 =============</p>\n\n<p>You are invoking Nutch with the command line:</p>\n\n<pre><code>hadoop jar nutch...jar\n</code></pre>\n\n<p>If you do that, you must assure that <code>gora-core-0.x.jar</code> is in the classpath.</p>\n\n<p>If you invoke a .job, it must have <code>lib/gora-core-0.x.jar</code> inside the zip. Hadoop unpacks that .job and adds <code>lib/*</code> to the classpath, so should not be necessary to do anything.</p>\n", "question_id": 17542126, "creation_date": 1373390059, "is_accepted": false, "score": 1, "last_activity_date": 1424966593, "answer_id": 17554293}], "question_id": 17542126, "tags": ["hadoop", "nutch", "gora"], "answer_count": 1, "link": "http://stackoverflow.com/questions/17542126/nutch-gora-class-not-while-running-nutch-in-hadoop-mode", "last_activity_date": 1424966593, "owner": {"user_id": 725896, "answer_count": 2, "creation_date": 1303841629, "accept_rate": 0, "view_count": 7, "reputation": 44}, "body": "<p>When I try</p>\n\n<pre><code>hadoop jar apache-nutch-2.2.jar org.apache.nutch.crawl.Crawler crawl -dir crawl -depth 3 -topN 5\n</code></pre>\n\n<p>I am getting the following error...  </p>\n\n<pre><code>13/07/09 09:02:46 WARN conf.Configuration: nutch-default.xml:a attempt to override final parameter: hadoop.job.history.user.location;  Ignoring.\n13/07/09 09:02:46 WARN conf.Configuration: nutch-default.xml:a attempt to override final parameter: hadoop.job.history.user.location;  Ignoring.\nException in thread \"main\" java.lang.NoClassDefFoundError: org/apache/gora/persistency/impl/PersistentBase\n    at java.lang.ClassLoader.defineClass1(Native Method)\n    at java.lang.ClassLoader.defineClassCond(ClassLoader.java:631)\n    at java.lang.ClassLoader.defineClass(ClassLoader.java:615)\n    at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:141)\n    at java.net.URLClassLoader.defineClass(URLClassLoader.java:283)\n    at java.net.URLClassLoader.access$000(URLClassLoader.java:58)\n    at java.net.URLClassLoader$1.run(URLClassLoader.java:197)\n    at java.security.AccessController.doPrivileged(Native Method)\n    at java.net.URLClassLoader.findClass(URLClassLoader.java:190)\n    at java.lang.ClassLoader.loadClass(ClassLoader.java:306)\n    at java.lang.ClassLoader.loadClass(ClassLoader.java:247)\n    at org.apache.nutch.crawl.InjectorJob.run(InjectorJob.java:218)\n    at org.apache.nutch.crawl.Crawler.runTool(Crawler.java:68)\n    at org.apache.nutch.crawl.Crawler.run(Crawler.java:136)\n    at org.apache.nutch.crawl.Crawler.run(Crawler.java:250)\n    at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n    at org.apache.nutch.crawl.Crawler.main(Crawler.java:257)\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n    at java.lang.reflect.Method.invoke(Method.java:597)\n    at org.apache.hadoop.util.RunJar.main(RunJar.java:160)\nCaused by: java.lang.ClassNotFoundException: org.apache.gora.persistency.impl.PersistentBase\n    at java.net.URLClassLoader$1.run(URLClassLoader.java:202)\n    at java.security.AccessController.doPrivileged(Native Method)\n    at java.net.URLClassLoader.findClass(URLClassLoader.java:190)\n    at java.lang.ClassLoader.loadClass(ClassLoader.java:306)\n    at java.lang.ClassLoader.loadClass(ClassLoader.java:247)\n    ... 22 more\n</code></pre>\n\n<p>Can somebody help me to get the error fixed?</p>\n", "creation_date": 1373353765, "score": 0},
{"title": "Viewing data in nutch crawl/segment folder", "view_count": 234, "is_answered": true, "answers": [{"question_id": 28686092, "owner": {"user_id": 1699191, "link": "http://stackoverflow.com/users/1699191/ameertawfik", "user_type": "registered", "reputation": 618}, "body": "<p>Run the following command from nutch home:</p>\n\n<pre><code>bin/nutch readseg -dump crawl/segments/your_segment output -nofetch -noparse -noparsetext\n</code></pre>\n\n<p>To know what commands you can use with Nutch, try to run </p>\n\n<pre><code>bin/nutch\n</code></pre>\n\n<p>I hope that helps.</p>\n", "creation_date": 1424897292, "is_accepted": false, "score": 2, "last_activity_date": 1424897292, "answer_id": 28729265}], "question_id": 28686092, "tags": ["nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/28686092/viewing-data-in-nutch-crawl-segment-folder", "last_activity_date": 1424897292, "owner": {"user_id": 2880734, "view_count": 4, "answer_count": 0, "creation_date": 1381794624, "reputation": 6}, "body": "<p>I have a question regarding viewing data in crawldb/segments folder. I see there is a content/part-00000 folder in segment folder. How do I dump the data(or view the data). This is what I am seeing when is type esc :%!xxd in the binary file:(I removed the hex codes) SEQ..org.apache. hadoop.io.Text org.apache.nutch.parse.ParseText. .org.apache.hadoop.io.compress. DefaultCodec <a href=\"http://localhost:8001/a.html\" rel=\"nofollow\">http://localhost:8001/a.html</a> more characters like this.</p>\n\n<p>It does not make much sense. This does not look like the data I have on the local page. Is there another way of looking at this or should I be looking at a different place? Please help.</p>\n", "creation_date": 1424737001, "score": 0},
{"title": "Nutch 2 with Cassandra as a storage is not crawling data properly", "view_count": 643, "is_answered": false, "answers": [{"question_id": 28615433, "owner": {"user_id": 1324769, "link": "http://stackoverflow.com/users/1324769/chris", "user_type": "registered", "reputation": 1}, "body": "<p>I just started using Nutch and Cassandra today.  I am not receiving the same errors in my log file during a crawl.</p>\n\n<p>Did you double check your nutch-site.xml and gora.properties settings? This is how I currently have my files configured.</p>\n\n<p>nutch-site.xml</p>\n\n<pre><code>    &lt;configuration&gt;\n    &lt;property&gt;\n    &lt;name&gt;http.agent.name&lt;/name&gt;\n    &lt;value&gt;My Spider&lt;/value&gt;\n    &lt;/property&gt;\n    &lt;property&gt; \n       &lt;name&gt;storage.data.store.class&lt;/name&gt; \n       &lt;value&gt;org.apache.gora.cassandra.store.CassandraStore&lt;/value&gt;\n       &lt;description&gt;Default class for storing data&lt;/description&gt;\n    &lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre>\n\n<p>gora.properties</p>\n\n<pre><code>#############################\n# CassandraStore properties #\n#############################\ngora.datastore.default=org.apache.gora.cassandra.store.CassandraStore\ngora.cassandrastore.servers=localhost:9160\n</code></pre>\n", "creation_date": 1424384735, "is_accepted": false, "score": 0, "last_activity_date": 1424384735, "answer_id": 28618235}], "question_id": 28615433, "tags": ["cassandra", "web-crawler", "nutch", "gora"], "answer_count": 1, "link": "http://stackoverflow.com/questions/28615433/nutch-2-with-cassandra-as-a-storage-is-not-crawling-data-properly", "last_activity_date": 1424737557, "owner": {"user_id": 3513415, "view_count": 10, "answer_count": 2, "creation_date": 1397011920, "reputation": 32}, "body": "<p>I am using Nutch 2.x using Cassandra as storage. Currently I am just crawling only one website, and data is getting loaded to Cassandra in byte code format.\nWhen I use readdb command in Nutch, I did get any useful crawling data. </p>\n\n<p>Below are the details of different files and output I am getting:</p>\n\n<p><strong>========== command to run crawler =====================</strong></p>\n\n<pre><code>bin/crawl urls/ crawlDir/ http://localhost:8983/solr/ 3\n</code></pre>\n\n<p><strong>======================== seed.txt data ==========================</strong></p>\n\n<pre><code>http://www.ft.com\n</code></pre>\n\n<p><strong>=== Output of readdb command to read data from cassandra webpage.f table======</strong> </p>\n\n<pre><code>~/Documents/Softwares/apache-nutch-2.3/runtime/local$ bin/nutch readdb -dump data -content\n~/Documents/Softwares/apache-nutch-2.3/runtime/local/data$ cat part-r-00000 \nhttp://www.ft.com/  key:    com.ft.www:http/\nbaseUrl:    null    \nstatus: 4 (status_redir_temp)    \nfetchTime:  1426888912463\nprevFetchTime:  1424296904936\nfetchInterval:  2592000\nretriesSinceFetch:  0    \nmodifiedTime:   0    \nprevModifiedTime:   0\nprotocolStatus: (null)    \nparseStatus:    (null)\ntitle:  null\nscore:  1.0\nmarker _injmrk_ :   y\nmarker dist :   0    \nreprUrl:    null    \nbatchId:    1424296906-20007    \nmetadata _csh_ : \n</code></pre>\n\n<p><strong>===============content of regex-urlfilter.txt ======================</strong></p>\n\n<pre><code># skip file: ftp: and mailto: urls\n-^(file|ftp|mailto):\n\n# skip image and other suffixes we can't yet parse\n# for a more extensive coverage use the urlfilter-suffix plugin\n-\\.(gif|GIF|jpg|JPG|png|PNG|ico|ICO|css|CSS|sit|SIT|eps|EPS|wmf|WMF|zip|ZIP|ppt|PPT|mpg|MPG|xls|XLS|gz|GZ|rpm|RPM|tgz|TGZ|mov|MOV|exe|EXE|jpeg|JPEG|bmp|BMP|js|JS)$\n\n# skip URLs containing certain characters as probable queries, etc.    \n-[?*!@=]\n\n# skip URLs with slash-delimited segment that repeats 3+ times, to break loops\n-.*(/[^/]+)/[^/]+\\1/[^/]+\\1/\n\n# accept anything else    \n+.\n</code></pre>\n\n<p><strong>===========content of log file which is bothering me ======================</strong></p>\n\n<pre><code>2015-02-18 13:57:51,253 ERROR store.CassandraStore - \n2015-02-18 13:57:51,253 ERROR store.CassandraStore - [Ljava.lang.StackTraceElement;@653e3e90\n2015-02-18 14:01:45,537 INFO  connection.CassandraHostRetryService - Downed Host Retry service started with queue size -1 and retry delay 10s\n</code></pre>\n\n<p>Please let me know if you need more information.\nCan someone please help me ?</p>\n\n<p>Thanks in advance.\n-Sumant</p>\n", "creation_date": 1424374830, "score": 0},
{"title": "Apache Nutch 2.1 - How get complete source code", "view_count": 712, "owner": {"age": 27, "answer_count": 17, "creation_date": 1332257106, "user_id": 1281305, "accept_rate": 88, "view_count": 129, "location": "Prague, Czech Republic", "reputation": 494}, "is_answered": true, "answers": [{"last_edit_date": 1364820330, "owner": {"user_id": 2225199, "link": "http://stackoverflow.com/users/2225199/nimeshjm", "user_type": "registered", "reputation": 1137}, "body": "<p>If you want to extract content based on an HTML tag, you could look at the xpath-filter plugin: <a href=\"http://www.atlantbh.com/precise-data-extraction-with-apache-nutch/\" rel=\"nofollow\">http://www.atlantbh.com/precise-data-extraction-with-apache-nutch/</a>\nYou can write an xpath query and configure it in the plugin to extract the information you need.</p>\n\n<p>Another option is to write a plugin (as you are doing at the moment) and use an HTML/XML parser to get the information out.\nHere's what I have done when I needed to extract some content out of a specific div:</p>\n\n<pre><code>  @Override\n  public NutchDocument filter(NutchDocument doc, Parse parse, Text url, CrawlDatum datum, Inlinks inlinks) throws IndexingException {\n\n        //LOG.info(\"filter init: \");\n        Metadata metadata = parse.getData().getParseMeta();\n        String fullContent = metadata.get(\"fullcontent\");\n\n        Document document = Jsoup.parse(fullContent); \n        Element contentwrapper = document.select(\"div#content\").first();\n\n        //LOG.info(\"fullcontent\");\n        //LOG.info(contentwrapper);\n\n\n        // Add field\n        doc.add(\"contentwrapper\", contentwrapper.text());\n\n        return doc;\n  }\n</code></pre>\n", "question_id": 15717239, "creation_date": 1364819249, "is_accepted": true, "score": 3, "last_activity_date": 1364820330, "answer_id": 15743466}], "question_id": 15717239, "tags": ["apache", "tags", "nutch", "web-crawler"], "answer_count": 1, "link": "http://stackoverflow.com/questions/15717239/apache-nutch-2-1-how-get-complete-source-code", "last_activity_date": 1424433858, "accepted_answer_id": 15743466, "body": "<p>I am trying to write my own Nutch plugin for crawling webpages. The problem is that I need to identify if there is some special tag, e.g.  on the webpage. There is some note in official documentation that this is possible using Document.getElementsByTagName(\"foo\") but this is not working for me. Do you have any idea?</p>\n\n<p>My second question is that if I identified tag above, I would like to get some other tags from this webpage where tag was identified... is there any way to store complete source code of the webpage which is crawled at some moment?</p>\n\n<p>Thanks, Jan.</p>\n", "creation_date": 1364637744, "score": 1},
{"title": "Apache nutch fetching but not saving file content", "view_count": 60, "is_answered": false, "question_id": 28574566, "tags": ["cassandra", "nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/28574566/apache-nutch-fetching-but-not-saving-file-content", "last_activity_date": 1424223845, "owner": {"user_id": 2880734, "view_count": 4, "answer_count": 0, "creation_date": 1381794624, "reputation": 6}, "body": "<p>I asked nutch to crawl a local file: <a href=\"http://localhost:8080/a.txt\" rel=\"nofollow\">http://localhost:8080/a.txt</a>. I am running the HTTP server and I can see nutch trying to access the file (and before it, /robots.txt). I am using cassandra as backend.</p>\n\n<p>However, I cannot see any data from the crawl. When I do\n./bin/nutch readdb -dump data ..., I get the following output.</p>\n\n<p>Can someone help me with a sane answer to this question? Where is the webpage data?</p>\n\n<p>$ cat data/part-r-00000\n<a href=\"http://localhost:8000/a.html\" rel=\"nofollow\">http://localhost:8000/a.html</a>    key:    localhost:http:8000/a.html\nbaseUrl:    null\nstatus: 2 (status_fetched)\nfetchTime:  1426811920382\nprevFetchTime:  1424219908314\nfetchInterval:  2592000\nretriesSinceFetch:  0\nmodifiedTime:   0\nprevModifiedTime:   0\nprotocolStatus: (null)\nparseStatus:    (null)</p>\n", "creation_date": 1424223845, "score": 1},
{"title": "how to parse html with nutch and index specific tag to solr?", "view_count": 6441, "is_answered": true, "answers": [{"last_edit_date": 1347261105, "owner": {"user_id": 950178, "link": "http://stackoverflow.com/users/950178/jayendra", "user_type": "registered", "reputation": 34896}, "body": "<p>You may want to check <a href=\"https://issues.apache.org/jira/browse/NUTCH-978\" rel=\"nofollow\">Nutch Plugin</a> which should allow you to extract an element from a web page.</p>\n", "question_id": 12338967, "creation_date": 1347200809, "is_accepted": false, "score": 0, "last_activity_date": 1347261105, "answer_id": 12339925}, {"question_id": 12338967, "owner": {"user_id": 1337352, "accept_rate": 32, "link": "http://stackoverflow.com/users/1337352/babu", "user_type": "registered", "reputation": 623}, "body": "<p>I made my own plugin for something similar you want to.\nThe config file for mapping NutchDocument to SolrDocument is in $NUTCH_HOME/conf/<strong>solrindex-mapping.xml</strong>. Here you can add your own tags. But still you have to fill your own tags somewhere.</p>\n\n<p>Here are some tips to plugin:</p>\n\n<ul>\n<li>read <a href=\"http://wiki.apache.org/nutch/WritingPluginExample\" rel=\"nofollow\">http://wiki.apache.org/nutch/WritingPluginExample</a>, here you can find how to make your plugin very simply</li>\n<li>in your plugin extend the <strong>ParseFilter</strong> and <strong>IndexingFilter.</strong></li>\n<li>in <em>YourParseFilter</em> you can use <strong>NodeWalker</strong> to find your specific div</li>\n<li><p>your parsed informations put into page metadata like this</p>\n\n<p><code>page.putToMetadata(new Utf8(\"yourKEY\"), ByteBuffer.wrap(YourByteArrayParsedFromMetaData));</code></p></li>\n<li><p>in <em>YourIndexingFilter</em> add the metadata from page (page.getMetadata) to NutchDocument</p>\n\n<p><code> doc.add(\"your_specific_tag\", value);</code></p></li>\n<li><p><strong>most important!!!!!</strong></p></li>\n<li><p>put <strong>your_specific_tag</strong> to fileds of:</p>\n\n<ul>\n<li><strong>Solr</strong> config file <strong>schema.xml</strong> (and restart Solr)</li>\n</ul>\n\n<p>field name=\"your_specific_tag\" type=\"string\" stored=\"true\" indexed=\"true\"</p>\n\n<ul>\n<li><strong>Nutch</strong> config file <strong>schema.xml</strong> (don't know if it is realy neccessary)</li>\n<li><strong>Nutch</strong> config file <strong>solrindex-mapping.xml</strong></li>\n</ul>\n\n<p>field dest=\"your_specific_tag\" source=\"your_specific_tag\"</p></li>\n</ul>\n", "creation_date": 1365931802, "is_accepted": false, "score": 3, "last_activity_date": 1365931802, "answer_id": 15997598}, {"question_id": 12338967, "owner": {"user_id": 1688068, "link": "http://stackoverflow.com/users/1688068/arul", "user_type": "registered", "reputation": 721}, "body": "<p>u have to just try <a href=\"http://lifelongprogrammer.blogspot.in/2013/08/nutch2-crawl-and-index-extra-tag.html\" rel=\"nofollow\">http://lifelongprogrammer.blogspot.in/2013/08/nutch2-crawl-and-index-extra-tag.html</a> \nthe tutorial said img tag how to get and what all are steps are there mention...</p>\n", "creation_date": 1381820509, "is_accepted": false, "score": 2, "last_activity_date": 1381820509, "answer_id": 19375090}, {"last_edit_date": 1424155580, "owner": {"user_id": 1881318, "link": "http://stackoverflow.com/users/1881318/tahagh", "user_type": "registered", "reputation": 619}, "body": "<p>You can use one of these custom plugins to parse xml files based on xpath (or css selectors):</p>\n\n<ul>\n<li><a href=\"https://github.com/BayanGroup/nutch-custom-search\" rel=\"nofollow\">https://github.com/BayanGroup/nutch-custom-search</a></li>\n<li><a href=\"http://www.atlantbh.com/precise-data-extraction-with-apache-nutch/\" rel=\"nofollow\">http://www.atlantbh.com/precise-data-extraction-with-apache-nutch/</a></li>\n</ul>\n", "question_id": 12338967, "creation_date": 1391075347, "is_accepted": false, "score": 1, "last_activity_date": 1424155580, "answer_id": 21452905}], "question_id": 12338967, "tags": ["solr", "nutch", "apache-tika"], "answer_count": 4, "link": "http://stackoverflow.com/questions/12338967/how-to-parse-html-with-nutch-and-index-specific-tag-to-solr", "last_activity_date": 1424155580, "owner": {"age": 25, "answer_count": 2, "creation_date": 1342958925, "user_id": 1543910, "accept_rate": 62, "view_count": 22, "location": "Iran", "reputation": 72}, "body": "<p>i have installed nutch and solr for crawling a website and search in it; as you know we can index meta tags of webpages into solr with parse meta tags plugin of nutch.(http://wiki.apache.org/nutch/IndexMetatags) now i want to know is there any way to crawl another html tag to solr that isn't meta?(plugin or anyway) like this:</p>\n\n<pre><code>&lt;div id=something&gt;\n      me specific tag\n&lt;/div&gt;\n</code></pre>\n\n<p>indeed i want to add a field to solr (something) that have value of \"me specific tag\" in this page.</p>\n\n<p>any idea?</p>\n", "creation_date": 1347192932, "score": 4},
{"title": "Nutch 1.9 and Javascript generated content", "view_count": 536, "is_answered": false, "question_id": 28167475, "tags": ["selenium", "ant", "nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/28167475/nutch-1-9-and-javascript-generated-content", "last_activity_date": 1424081286, "owner": {"user_id": 2219435, "answer_count": 0, "creation_date": 1364469090, "accept_rate": 75, "view_count": 13, "reputation": 87}, "body": "<p>I am using Nutch 1.9.</p>\n\n<p>When most of the web page is generated with javascript, Nutch ignores Javascripte generated content. Is it possible to fetch it too?</p>\n\n<p>I've found Selenium could be a way to do it, but it seems it is supported only by Nutch 2.x. Is it possible to integrated (and how) with Nutch 1.9?</p>\n\n<p>I've followed the installation instructions on <a href=\"https://github.com/momer/nutch-selenium\" rel=\"nofollow\">nutch-selenium</a>, but a lot's of errors occurred (obviously) when I ran ant. </p>\n\n<pre><code>compile:\n     [echo] Compiling plugin: protocol-selenium\n    [javac] Compiling 2 source files to $NUTCH_HOME/build/protocol-selenium/classes\n    [javac] warning: [options] bootstrap class path not set in conjunction with -source 1.6\n    [javac] $NUTCH_HOME/src/plugin/protocol-selenium/src/java/org/apache/nutch/protocol/selenium/Http.java:14: error: package org.apache.nutch.storage does not exist\n    [javac] import org.apache.nutch.storage.WebPage;\n    [javac]                                ^\n    [javac] $NUTCH_HOME/src/plugin/protocol-selenium/src/java/org/apache/nutch/protocol/selenium/Http.java:15: error: package org.apache.nutch.storage.WebPage does not exist\n    [javac] import org.apache.nutch.storage.WebPage.Field;\n    [javac]                                        ^\n    [javac] $NUTCH_HOME/src/plugin/protocol-selenium/src/java/org/apache/nutch/protocol/selenium/Http.java:26: error: package WebPage does not exist\n    [javac]   private static final Collection&lt;WebPage.Field&gt; FIELDS = new HashSet&lt;WebPage.Field&gt;();\n    [javac]                                          ^\n    [javac] $NUTCH_HOME/src/plugin/protocol-selenium/src/java/org/apache/nutch/protocol/selenium/Http.java:49: error: cannot find symbol\n    [javac]     protected Response getResponse(URL url, WebPage page, boolean redirect)\n    [javac]                                             ^\n    [javac]   symbol:   class WebPage\n    [javac]   location: class Http\n    [javac] $NUTCH_HOME/src/plugin/protocol-selenium/src/java/org/apache/nutch/protocol/selenium/Http.java:55: error: package WebPage does not exist\n    [javac]   public Collection&lt;WebPage.Field&gt; getFields() {\n    [javac]                            ^\n    [javac] $NUTCH_HOME/src/plugin/protocol-selenium/src/java/org/apache/nutch/protocol/selenium/HttpResponse.java:16: error: package org.apache.nutch.storage does not exist\n    [javac] import org.apache.nutch.storage.WebPage;\n    [javac]                                ^\n    [javac] $NUTCH_HOME/src/plugin/protocol-selenium/src/java/org/apache/nutch/protocol/selenium/HttpResponse.java:47: error: cannot find symbol\n    [javac]     public HttpResponse(Http http, URL url, WebPage page, Configuration conf) throws ProtocolException, IOException {\n    [javac]                                             ^\n    [javac]   symbol:   class WebPage\n    [javac]   location: class HttpResponse\n    [javac] $NUTCH_HOME/src/plugin/protocol-selenium/src/java/org/apache/nutch/protocol/selenium/Http.java:26: error: package WebPage does not exist\n    [javac]   private static final Collection&lt;WebPage.Field&gt; FIELDS = new HashSet&lt;WebPage.Field&gt;();\n    [javac]                                                                              ^\n    [javac] $NUTCH_HOME/src/plugin/protocol-selenium/src/java/org/apache/nutch/protocol/selenium/Http.java:29: error: package WebPage does not exist\n    [javac]     FIELDS.add(WebPage.Field.MODIFIED_TIME);\n    [javac]                       ^\n    [javac] $NUTCH_HOME/src/plugin/protocol-selenium/src/java/org/apache/nutch/protocol/selenium/Http.java:30: error: package WebPage does not exist\n    [javac]     FIELDS.add(WebPage.Field.HEADERS);\n    [javac]                       ^\n    [javac] $NUTCH_HOME/src/plugin/protocol-selenium/src/java/org/apache/nutch/protocol/selenium/Http.java:54: error: method does not override or implement a method from a supertype\n    [javac]   @Override\n    [javac]   ^\n    [javac] 11 errors\n    [javac] 1 warning\n\nBUILD FAILED\n$NUTCH_HOME/build.xml:112: The following error occurred while executing this line:\n$NUTCH_HOME/src/plugin/build.xml:77: The following error occurred while executing this line:\n$NUTCH_HOME/src/plugin/build-plugin.xml:133: Compile failed; see the compiler error output for details.\n</code></pre>\n\n<p>Or are there any other options?</p>\n", "creation_date": 1422352297, "score": 0},
{"title": "Nutch skip url containing #", "view_count": 149, "owner": {"age": 24, "answer_count": 8, "creation_date": 1370905721, "user_id": 2472685, "accept_rate": 100, "view_count": 57, "location": "Mumbai, India", "reputation": 336}, "is_answered": true, "answers": [{"question_id": 28520479, "owner": {"user_id": 3506000, "accept_rate": 90, "link": "http://stackoverflow.com/users/3506000/robert-bain", "user_type": "registered", "reputation": 1951}, "body": "<p><a href=\"http://en.wikipedia.org/wiki/Escape_character\" rel=\"nofollow\">Escape</a> the <code>#</code> using a backslash.</p>\n", "creation_date": 1423956456, "is_accepted": true, "score": 3, "last_activity_date": 1423956456, "answer_id": 28521426}], "question_id": 28520479, "tags": ["regex", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/28520479/nutch-skip-url-containing", "last_activity_date": 1423956456, "accepted_answer_id": 28521426, "body": "<p>I am learning Nutch. I have set up nutch and started crawling sites. But one thing I am unable to figure out is how to restrict url containing # as several duplication is going on due to this #. \nI have checked the regex-urlfilter.txt</p>\n\n<pre><code># skip URLs containing certain characters as probable queries, etc.\n-[*!@] \n</code></pre>\n\n<p>If I add # to this line conceptually this should work but after adding # It's not working. Is it due to # used to comment lines? If so how to fix it. </p>\n", "creation_date": 1423949027, "score": 2},
{"title": "Nutch + HBase: hbase versions issue and java exception", "view_count": 616, "is_answered": false, "answers": [{"last_edit_date": 1418749261, "owner": {"user_id": 582789, "link": "http://stackoverflow.com/users/582789/alfonso-nishikawa", "user_type": "registered", "reputation": 1263}, "body": "<p>Are you sure you have this line in <code>gora.properties</code>:</p>\n\n<pre><code>gora.datastore.default=org.apache.gora.hbase.store.HBaseStore\n</code></pre>\n\n<p>with special attention to the namespace:</p>\n\n<p>org.apache.gora.<strong>hbase</strong>.store.HBaseStore</p>\n\n<p>and not</p>\n\n<p>org.apache.gora.<strong>memory</strong>.store.HBaseStore</p>\n\n<p>I hope this will fix the issue :)</p>\n\n<hr>\n\n<p>Edit about versions:</p>\n\n<p>About hbase-0.90.4 returning, Gora-0.3 depends on HBase-0.90.4, which is incompatible with HBase-0.94.14.</p>\n\n<p>In order to run with HBase-0.94.14 you have to use Nutch-2.3-SNAPSHOT (called \"2.x\"). You have a link in <a href=\"http://wiki.apache.org/nutch/Nutch2Tutorial\" rel=\"nofollow\">Nutch2Tutorial</a> or you can <a href=\"http://svn.apache.org/repos/asf/nutch/branches/2.x/\" rel=\"nofollow\">svn checkout http://svn.apache.org/repos/asf/nutch/branches/2.x/</a></p>\n\n<p>Nutch 2.3-SNAPSHOT depends on Gora-0.5 which depends on HBase 0.94.14</p>\n\n<hr>\n\n<p>Seems quite solved:</p>\n\n<p><a href=\"http://mail-archives.apache.org/mod_mbox/nutch-dev/201412.mbox/%3C548F4F1B.7020206@googlemail.com%3E\" rel=\"nofollow\">http://mail-archives.apache.org/mod_mbox/nutch-dev/201412.mbox/%3C548F4F1B.7020206@googlemail.com%3E</a></p>\n\n<p><a href=\"https://issues.apache.org/jira/browse/NUTCH-1899\" rel=\"nofollow\">https://issues.apache.org/jira/browse/NUTCH-1899</a></p>\n", "question_id": 27166644, "creation_date": 1417115589, "is_accepted": false, "score": 0, "last_activity_date": 1418749261, "answer_id": 27177490}, {"question_id": 27166644, "owner": {"user_id": 4299211, "link": "http://stackoverflow.com/users/4299211/gwen-wing", "user_type": "registered", "reputation": 28}, "body": "<p>Alfonso,</p>\n\n<p>I checked about gora.properties, it was OK.</p>\n\n<p>Also, I've tried the latest 2.3 Snapshot but unfortunately it ended into some dependency issue at build time:</p>\n\n<pre><code>[ivy:resolve]       ::::::::::::::::::::::::::::::::::::::::::::::\n[ivy:resolve]       ::          UNRESOLVED DEPENDENCIES         ::\n[ivy:resolve]       ::::::::::::::::::::::::::::::::::::::::::::::\n[ivy:resolve]       ::   org.restlet.jse#org.restlet.lib.org.restlet.lib.org.json;2.0:     java.text.ParseException: inconsistent module descriptor file found in 'http://maven.restlet.org/org/restlet/jse/org.restlet.lib.org.restlet.lib.org.json/2.0/org.restlet.lib.org.restlet.lib.org.json-2.0.pom': bad module name: expected='org.restlet.lib.org.restlet.lib.org.json' found='org.restlet.lib.org.json'; \n[ivy:resolve]       ::::::::::::::::::::::::::::::::::::::::::::::\n[ivy:resolve] :::: ERRORS\n[ivy:resolve]       restlet: bad module name found in http://maven.restlet.org/org/restlet/jse/  org.restlet.lib.org.restlet.lib.org.json/2.0/org.restlet.lib.org.restlet.lib.org.json-2.0.pom: expected='org.restlet.lib.org.restlet.lib.org.json found='org.restlet.lib.org.json'\n[ivy:resolve] \n[ivy:resolve] :: USE VERBOSE OR DEBUG MESSAGE LEVEL FOR MORE DETAILS\n\nBUILD FAILED\n/root/nutch/2.3/build.xml:467: impossible to resolve dependencies:\n        resolve failed - see output for details\n</code></pre>\n", "creation_date": 1417189164, "is_accepted": false, "score": 0, "last_activity_date": 1417189164, "answer_id": 27191952}, {"last_edit_date": 1423563950, "owner": {"user_id": 3139759, "accept_rate": 67, "link": "http://stackoverflow.com/users/3139759/user1337", "user_type": "registered", "reputation": 36}, "body": "<p>Try updating version number:</p>\n\n<ul>\n<li>go in the ivy/ivy.xml;</li>\n<li>change the rev=\"2.2.1\" of org=\"org.restlet.jse\" to rev=\"2.2.3\" (occurs 3 times).</li>\n</ul>\n", "question_id": 27166644, "creation_date": 1423561724, "is_accepted": false, "score": 0, "last_activity_date": 1423563950, "answer_id": 28428545}], "question_id": 27166644, "tags": ["java", "hbase", "nutch", "restlet", "gora"], "answer_count": 3, "link": "http://stackoverflow.com/questions/27166644/nutch-hbase-hbase-versions-issue-and-java-exception", "last_activity_date": 1423563950, "owner": {"user_id": 4299211, "view_count": 10, "answer_count": 4, "creation_date": 1417077732, "reputation": 28}, "body": "<p>I'm trying to setup Nutch 2.2.1 using HBase 0.94.14, on Debian Squeeze.\nI've followed Nutch 1 and 2 tutorials carefully and various documentations. \nI could build HBase 0.94.14, and eventually got it to work (I can create tables etc.)\nI could build Nutch without any issue (it's set on Gora 0.3)</p>\n\n<p>Now issues are:\n1- when trying to launch Nutch, I get the following trace:</p>\n\n<pre><code>./nutch inject /root/nutch/apache-nutch-2.2.1/urls/\nInjectorJob: starting at 2014-11-27 09:43:53\nInjectorJob: Injecting urlDir: /root/nutch/apache-nutch-2.2.1/urls\nInjectorJob: java.lang.ClassNotFoundException: org.apache.gora.memory.store.HBaseStore\n    at java.net.URLClassLoader$1.run(URLClassLoader.java:372)\n    at java.net.URLClassLoader$1.run(URLClassLoader.java:361)\n</code></pre>\n\n<p>etc.</p>\n\n<p>Using strace -f, I've figured out that \"HBaseStore.class\" was not found:</p>\n\n<pre><code>stat(\"/root/nutch/apache-nutch-2.2.1/runtime/local/org/apache/gora/memory/store/HBaseStore.class\",\\\n  &lt;unfinished ...&gt;\n[pid  1827] &lt;... futex resumed&gt; )       = -1 EAGAIN (Resource temporarily unavailable)\n</code></pre>\n\n<p>I tried to figure out if there was an issue with classpath, but eventually found out that:\n- HBaseStore.class was present neither in the Nutch directory tree nor in the Hbase 0.94.4 directory tree\n- HBase jar version in the Nutch tree was surprinsingly: hbase-0.90.4.jar</p>\n\n<p>According to some online discussions I found, I replace hbase-0.90.4.jar in the nutch tree with hbase-0.94.4 from the hbase tree...</p>\n\n<p>But:\n- it doesn't fix the java issue\n- each time I'm rebuilding nutch, hbase-0.90.4.jar is back and I can't find any source for it in the nutch tree :-/</p>\n\n<p>Note that /root/nutch/apache-nutch-2.2.1/conf/hbase-site.xml has:</p>\n\n<pre><code>&lt;property&gt;\n&lt;name&gt;hbase.rootdir&lt;/name&gt;\n&lt;value&gt;/root/nutch/hbase-master/conf/&lt;/value&gt;\n&lt;/property&gt;\n</code></pre>\n\n<p>which corresponds to Nutch 0.94.4 ...</p>\n\n<p>Also tried to rebuild and use Gora 0.5 but it makes Nutch build fail.</p>\n\n<p>I'm not an expert in Java at all, and I don't understand why Nutch is not using the correct version of HBase, why it seems there are missing sources and java classes, and at this point I'm totally stuck. What a mess.</p>\n\n<p>Thanks for any tip that could help to save this situation.</p>\n", "creation_date": 1417078888, "score": 0},
{"title": "Nutch fetches already fetched URLs", "view_count": 292, "is_answered": true, "answers": [{"question_id": 27961354, "owner": {"user_id": 1201652, "link": "http://stackoverflow.com/users/1201652/donatas", "user_type": "registered", "reputation": 11}, "body": "<p>This is an open issue for Nutch 2.X. I faced it this weekend too. </p>\n\n<p>The fix is scheduled for release 2.3.1: <a href=\"https://issues.apache.org/jira/browse/NUTCH-1922\" rel=\"nofollow\">https://issues.apache.org/jira/browse/NUTCH-1922</a>. </p>\n", "creation_date": 1423410010, "is_accepted": false, "score": 1, "last_activity_date": 1423410010, "answer_id": 28395853}], "question_id": 27961354, "tags": ["nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/27961354/nutch-fetches-already-fetched-urls", "last_activity_date": 1423410010, "owner": {"user_id": 4450199, "view_count": 2, "answer_count": 1, "creation_date": 1421168121, "reputation": 3}, "body": "<p>I am trying to crawl website using Nutch. I use commands:</p>\n\n<ul>\n<li>inject for URLs injection to DB</li>\n<li>loop of generate/fetch/parse/updatedb</li>\n</ul>\n\n<p>I noticed what Nutch fetches already fetched URLs on each loop iteration.</p>\n\n<p>Config I have made:</p>\n\n<ul>\n<li>added filter to regex-urlfilter.txt</li>\n</ul>\n\n<p>Added config to nutch-site.xml:</p>\n\n<ul>\n<li>http.agent.name set value MyNutchSpider</li>\n<li>http.robots.agents set value to MyNutchSpider</li>\n<li>file.content.limit -1</li>\n<li>http.content.limit -1</li>\n<li>ftp.content.limit -1</li>\n<li>fetcher.server.delay set value to 1.0</li>\n<li>fetcher.threads.fetch set value to 1</li>\n<li>parser.character.encoding.default </li>\n<li>plugin.includes added protocol protocol-httpclient</li>\n<li>set storage.data.store.class to use custom storage</li>\n</ul>\n\n<p>I use commands:</p>\n\n<ul>\n<li>bin/nutch generate -topN 10</li>\n<li>bin/nutch fetch -all </li>\n<li>bin/nutch parse -all</li>\n<li>bin/nutch updatedb -all</li>\n</ul>\n\n<p>I have tried versions of Nutch 2.2.1 with MySQL and 2.3 with MongoDB. Result is same already fetched URLs are re-feched on each crawl loop iteration.</p>\n\n<p>What I should to do to fetch all not crawled URLs?</p>\n", "creation_date": 1421317673, "score": 0},
{"title": "Nutch 2.3 not able to parse a URL which is correctly parsed through parsechecker", "view_count": 142, "is_answered": false, "question_id": 28346900, "tags": ["apache", "nutch", "apache-tika", "html-parser"], "answer_count": 0, "link": "http://stackoverflow.com/questions/28346900/nutch-2-3-not-able-to-parse-a-url-which-is-correctly-parsed-through-parsechecker", "last_activity_date": 1423409009, "owner": {"user_id": 1495574, "answer_count": 17, "creation_date": 1341219032, "accept_rate": 61, "view_count": 58, "reputation": 183}, "body": "<p>I am experimenting with Apache Nutch 2.3. </p>\n\n<p>When I ran the parse command for the URL <a href=\"http://comptuergodzilla.blogspot.com\" rel=\"nofollow\">http://comptuergodzilla.blogspot.com</a> Nutch parse the content correctly. I mean I get all the outlinks and content in <code>ol</code> and <code>p</code> column family respectively.</p>\n\n<p>But when I did the same for URL <a href=\"http://goal.com/en-india\" rel=\"nofollow\">http://goal.com/en-india</a> it was not able to parse the site outlinks and content. </p>\n\n<p>What makes me scratch my head is after running parsechecker command for URL <a href=\"http://www.goal.com/en-india\" rel=\"nofollow\">http://www.goal.com/en-india</a> I get all the parsed contents and outlinks.</p>\n\n<p>Regarding above my questions are:</p>\n\n<pre><code> i. Why parse command is not working? It should work if parsechecker is parsing the URL correctly.\n ii. Do I have to build the separate HTMLParser plugin for achieving above.\n</code></pre>\n", "creation_date": 1423147591, "score": 1},
{"title": "Intelligent crawler that can prioritize sources based on keywords?", "view_count": 802, "is_answered": true, "answers": [{"question_id": 13255211, "owner": {"user_id": 1274085, "link": "http://stackoverflow.com/users/1274085/hari", "user_type": "registered", "reputation": 166}, "body": "<p>Haven't tried this yet, but I assume you can do this in Nutch by controlling how you score the outlinks. If your current page contains your keywords, you could give a higher score to outlinks for that page.</p>\n\n<p>An implementation of a plugin that extends ScoringFilter is given in <a href=\"http://sujitpal.blogspot.in/2012/01/nutchgora-scoring-and-indexing-plugins.html\" rel=\"nofollow\">http://sujitpal.blogspot.in/2012/01/nutchgora-scoring-and-indexing-plugins.html</a></p>\n", "creation_date": 1354086272, "is_accepted": false, "score": 1, "last_activity_date": 1354086272, "answer_id": 13599590}, {"question_id": 13255211, "owner": {"user_id": 1950616, "link": "http://stackoverflow.com/users/1950616/anonymous", "user_type": "unregistered", "reputation": 11}, "body": "<p>If you're willing to work with a Java based solution, you might want to have a look at <a href=\"http://www.infant-ir.info/\" rel=\"nofollow\">Infant</a>. Infant is a micro web crawling library. It's not an open source project, but is completely free, even for commercial use. You should be able to write your own plug-in for Infant, to control the order in which Infant maintains its URL queue.</p>\n", "creation_date": 1357371970, "is_accepted": false, "score": 1, "last_activity_date": 1357371970, "answer_id": 14169795}, {"question_id": 13255211, "owner": {"user_id": 1645046, "accept_rate": 30, "link": "http://stackoverflow.com/users/1645046/saransh-sharma", "user_type": "registered", "reputation": 73}, "body": "<p>if you are still looking for something similar you can use one of our very similar open source software which is on </p>\n\n<p><a href=\"https://github.com/theupscale/context-parser\" rel=\"nofollow\">Github Link</a></p>\n\n<p>It works on the context based rule engine where you can assign any keyword or text a single value from 0 to 9 as weight, and when it will parse it, will let you crawl what you want </p>\n\n<p>It is based on ROR </p>\n\n<p>MYSQL, MONGO, and Ubuntu 14.04 with other regular gems which you can manage yourself, </p>\n", "creation_date": 1423397569, "is_accepted": false, "score": 0, "last_activity_date": 1423397569, "answer_id": 28393784}], "question_id": 13255211, "tags": ["scrapy", "web-crawler", "nutch"], "answer_count": 3, "link": "http://stackoverflow.com/questions/13255211/intelligent-crawler-that-can-prioritize-sources-based-on-keywords", "last_activity_date": 1423397569, "owner": {"user_id": 597143, "answer_count": 3, "creation_date": 1296490962, "accept_rate": 0, "view_count": 13, "reputation": 62}, "body": "<p>I'm trying to create a webcrawler that continuously crawls the web looking for webpages that contain certain keywords. There are a lot of open source solutions for this (Nutch, Scrapy et cetera), but I need an intelligent one that can prioritize 'rich' sources.</p>\n\n<p>I want the bot to start at a certain page, f.e. <a href=\"http://www.dmoz.org\" rel=\"nofollow\">http://www.dmoz.org</a>, extract all links and continue scraping them. Now if the page contains a certain keyword, f.e. 'foo', it should send this url to a database.</p>\n\n<p>Now, here comes the hard part. If I create and run a bot like this it can take ages before the spider finds new pages containing the specified keywords, since it's crawling the whole web. The process would be dramatically faster if the spider could identify on what domain it often finds the keywords so these can be crawled more often.</p>\n\n<p>Is there an open source solution for this?</p>\n\n<p>So far I've looked at Nutch and Scrapy. Nutch looks to be the best option for what I need, but I don't have any experience with Java and I can't find any specific documentation about this problem.</p>\n", "creation_date": 1352219033, "score": 1},
{"title": "Nutch - Crawl a page for links, but don&#39;t index", "view_count": 106, "is_answered": false, "answers": [{"question_id": 27723292, "owner": {"user_id": 658107, "accept_rate": 33, "link": "http://stackoverflow.com/users/658107/bclingan", "user_type": "registered", "reputation": 18}, "body": "<p>Just as an update, the only solution I found to my problem, was to remove those URLs from the collection after is was crawled as a secondary step.</p>\n", "creation_date": 1423324630, "is_accepted": false, "score": 0, "last_activity_date": 1423324630, "answer_id": 28384193}], "question_id": 27723292, "tags": ["solr", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/27723292/nutch-crawl-a-page-for-links-but-dont-index", "last_activity_date": 1423324630, "owner": {"age": 40, "answer_count": 1, "creation_date": 1300066768, "user_id": 658107, "accept_rate": 33, "view_count": 16, "location": "Bel Air, MD", "reputation": 18}, "body": "<p>I would like to point a site to an index.html page to get it started, but don't want to include index.html in my search results- instead just having the child pages appear in the results. Is there a away to remove specific pages?</p>\n", "creation_date": 1420040922, "score": 0},
{"title": "Nutch 2, add new field in plugin", "view_count": 97, "is_answered": false, "question_id": 28265347, "tags": ["nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/28265347/nutch-2-add-new-field-in-plugin", "last_activity_date": 1422810525, "owner": {"age": 27, "answer_count": 4, "creation_date": 1347015423, "user_id": 1654519, "accept_rate": 50, "view_count": 9, "reputation": 57}, "body": "<p>i try to write plugin for nutch 2, but can't find any info in internet how to add new field in plugin. No information about plugins for nutch 2.</p>\n\n<p>In nutch 1.x \"WebPage\" just have method \"add\"</p>\n", "creation_date": 1422810525, "score": 1},
{"title": "Nutch, NoSuchElementException error after removing table from Hbase", "view_count": 507, "owner": {"age": 27, "answer_count": 4, "creation_date": 1347015423, "user_id": 1654519, "accept_rate": 50, "view_count": 9, "reputation": 57}, "is_answered": true, "answers": [{"question_id": 28250436, "owner": {"user_id": 1654519, "accept_rate": 50, "link": "http://stackoverflow.com/users/1654519/seldon", "user_type": "registered", "reputation": 57}, "body": "<p>The problem was in my config files, i just mixed content nutch-site.xml and hbase-\nsite.xml</p>\n\n<p>But it's very strange, so big mistake and nothing in log.</p>\n", "creation_date": 1422810086, "is_accepted": true, "score": 0, "last_activity_date": 1422810086, "answer_id": 28265264}], "question_id": 28250436, "tags": ["hbase", "nutch", "nosuchelementexception"], "answer_count": 1, "link": "http://stackoverflow.com/questions/28250436/nutch-nosuchelementexception-error-after-removing-table-from-hbase", "last_activity_date": 1422810086, "accepted_answer_id": 28265264, "body": "<p>I use nutch for crawling some sites. One time i decide to clear all crawling result and just remove \"webpage\" table from Hbase store, using hbase shell.</p>\n\n<p>After that nutch trow exception </p>\n\n<pre><code>java.util.NoSuchElementException\nat java.util.TreeMap.key(TreeMap.java:1221)\nat java.util.TreeMap.firstKey(TreeMap.java:285)\nat org.apache.gora.memory.store.MemStore.execute(MemStore.java:125)\nat org.apache.gora.query.impl.QueryBase.execute(QueryBase.java:73)\nat org.apache.gora.mapreduce.GoraRecordReader.executeQuery(GoraRecordReader.java:68)\nat org.apache.gora.mapreduce.GoraRecordReader.nextKeyValue(GoraRecordReader.java:110)\nat org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:531)\nat org.apache.hadoop.mapreduce.MapContext.nextKeyValue(MapContext.java:67)\nat org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)\nat org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:764)\nat org.apache.hadoop.mapred.MapTask.run(MapTask.java:364)\nat org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:223)\nat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)\nat java.util.concurrent.FutureTask.run(FutureTask.java:262)\nat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\nat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\nat java.lang.Thread.run(Thread.java:745)\n</code></pre>\n\n<p>\"inject\" task work without error, but everything else just throw the error (generate, fetch, etc.).</p>\n", "creation_date": 1422698938, "score": 0},
{"title": "Error trying to crawl with nutch - java.net.UnknownHostException on own local hostname", "view_count": 409, "is_answered": false, "answers": [{"question_id": 28228946, "owner": {"user_id": 1563432, "accept_rate": 57, "link": "http://stackoverflow.com/users/1563432/remog", "user_type": "registered", "reputation": 119}, "body": "<p>The fix is as simple as adding your machine's host name to your /etc/hosts file pointing back to your loopback address (127.0.0.1)</p>\n\n<p>I ammended my hosts entry as follows:</p>\n\n<pre><code>127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4 Sparky.LITK\n::1         localhost localhost.localdomain localhost6 localhost6.localdomain6 Sparky.LITK\n</code></pre>\n\n<p>And it worked!</p>\n", "creation_date": 1422590238, "is_accepted": false, "score": 0, "last_activity_date": 1422590238, "answer_id": 28229005}], "question_id": 28228946, "tags": ["java", "hadoop", "solr", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/28228946/error-trying-to-crawl-with-nutch-java-net-unknownhostexception-on-own-local-ho", "last_activity_date": 1422590238, "owner": {"age": 35, "answer_count": 2, "creation_date": 1343663275, "user_id": 1563432, "accept_rate": 57, "view_count": 16, "location": "Moncton, Canada", "reputation": 119}, "body": "<p>Trying to crawl with Nutch 1.9 on Centos 6.6.</p>\n\n<p>When trying to initalize my first crawl after following this guide: </p>\n\n<p><a href=\"http://wiki.apache.org/nutch/NutchTutorial\" rel=\"nofollow\">http://wiki.apache.org/nutch/NutchTutorial</a></p>\n\n<p>However, I am getting the following exception when launching: </p>\n\n<blockquote>\n  <p>Injector: Converting injected urls to crawl db entries. Injector:\n  java.net.UnknownHostException: Sparky.LITK: Sparky.LITK: Name or\n  service not known     at\n  java.net.InetAddress.getLocalHost(InetAddress.java:1473)  at\n  org.apache.hadoop.mapred.JobClient$2.run(JobClient.java:960)  at\n  org.apache.hadoop.mapred.JobClient$2.run(JobClient.java:936)  at\n  java.security.AccessController.doPrivileged(Native Method)    at\n  javax.security.auth.Subject.doAs(Subject.java:415)    at\n  org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)\n    at\n  org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:936)\n    at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:910)\n    at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:1353)   at\n  org.apache.nutch.crawl.Injector.inject(Injector.java:324)     at\n  org.apache.nutch.crawl.Injector.run(Injector.java:380)    at\n  org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)     at\n  org.apache.nutch.crawl.Injector.main(Injector.java:370) Caused by:\n  java.net.UnknownHostException: Sparky.LITK: Name or service not known\n    at java.net.Inet6AddressImpl.lookupAllHostAddr(Native Method)   at\n  java.net.InetAddress$1.lookupAllHostAddr(InetAddress.java:901)    at\n  java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1293)\n    at java.net.InetAddress.getLocalHost(InetAddress.java:1469)     ... 12\n  more</p>\n</blockquote>\n\n<p>It seems to be trying to crawl the machine's own hostname (Sparky.LITK) which is not what I want it to do, I set up a seed.txt list as per the tutorial, but it stuck here.</p>\n", "creation_date": 1422589920, "score": 0},
{"title": "How to restrict Apache Nutch to crawl language specific Documents only", "view_count": 181, "is_answered": true, "answers": [{"question_id": 28209584, "owner": {"user_id": 1455971, "accept_rate": 57, "link": "http://stackoverflow.com/users/1455971/yyz", "user_type": "registered", "reputation": 134}, "body": "<p>Yes, you can do this with Nutch. In your nutch xpathfilterconf, you can specify a language field name, and give it the appropriate xpath expression for whatever websites you're crawling. </p>\n\n<p>Then, inside your XPathIndexingFilter, you can filter on the language field. </p>\n\n<p>Alternatively, if the sites you are crawling don't have their language anywhere in the javascript, you can add a field name for the sites bodytext/main text, and then add some checking in the indexing filter to see if it contains Arabic text.  </p>\n", "creation_date": 1422530974, "is_accepted": false, "score": 1, "last_activity_date": 1422530974, "answer_id": 28213621}], "question_id": 28209584, "tags": ["apache", "web-crawler", "filtering", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/28209584/how-to-restrict-apache-nutch-to-crawl-language-specific-documents-only", "last_activity_date": 1422530974, "owner": {"user_id": 3454410, "answer_count": 20, "creation_date": 1395646154, "accept_rate": 48, "view_count": 174, "reputation": 832}, "body": "<p>I am using apache Nutch 2.3 and I want to crawl only those documents from web that are in some specific language suppose Arabic or atleast documents should conatins some string in Arabic. </p>\n\n<pre><code>`So is there any option in crawler to do this job?`\n</code></pre>\n", "creation_date": 1422518253, "score": 1},
{"title": "How to make nutch plugin dependency jars available at runtime", "view_count": 368, "is_answered": false, "question_id": 28191566, "tags": ["java", "plugins", "ant", "ivy", "nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/28191566/how-to-make-nutch-plugin-dependency-jars-available-at-runtime", "last_activity_date": 1422445817, "owner": {"age": 38, "answer_count": 46, "creation_date": 1240697384, "user_id": 96061, "accept_rate": 83, "view_count": 294, "location": "Denmark", "reputation": 3089}, "body": "<p>I am writing a nutch 1.7 plugin called entity-extractor, that depends on the <a href=\"http://nlp.stanford.edu/software/corenlp.shtml#Download\" rel=\"nofollow\">Stanford Corenlp</a> libraries.</p>\n\n<p>Nutch manages dependencies with Ivy, so in my plugin source folder I have the following ivy.xml:</p>\n\n<pre><code>&lt;!-- ./src/plugins/entity-extractor.ivy.xml --&gt;\n&lt;ivy-module version=\"1.0\" xmlns:maven=\"http://ant.apache.org/ivy/maven\"&gt;\n\n  ...\n\n  &lt;dependencies&gt;                                                                                                                                                                                             \n    &lt;dependency org=\"edu.stanford.nlp\" name=\"stanford-corenlp\" rev=\"3.4.1\"&gt;                                                                                                                                  \n      &lt;artifact name=\"stanford-corenlp\" /&gt;                                                                                                                                                                   \n      &lt;artifact name=\"stanford-corenlp\" maven:classifier=\"models\"/&gt;                                                                                                                                          \n    &lt;/dependency&gt;                                                                                                                                                                                            \n  &lt;/dependencies&gt;                            \n&lt;/ivy-module&gt;\n</code></pre>\n\n<p>In my plugin.xml, I specify that the plugin relies on these jars and their transitive dependencies:</p>\n\n<pre><code>&lt;!-- ./src/plugins/entity-extractor/plugin.xml --&gt;\n&lt;plugin&gt;\n\n  ...\n\n  &lt;runtime&gt;\n    &lt;library name=\"entity-extractor.jar\"&gt;\n      &lt;export name=\"*\"/&gt;\n    &lt;/library&gt;                                                                                                                                                                                             \n    &lt;library name=\"stanford-corenlp-3.4.1.jar\"/&gt;\n    &lt;library name=\"stanford-corenlp-3.4.1-models.jar\"/&gt;\n\n    ...\n\n  &lt;/runtime&gt;\n&lt;/plugin&gt;\n</code></pre>\n\n<p>When I build nutch, both of these dependency jars, as well as the other dependencies, get copied to my plugin folder under ./runtime/local/plugins/my-plugin-name. Specifically, the folder contains the jar <code>stanford-corenlp-3.4.1-models.jar</code>, which doesn't contain any classes, but only tgz files containing the nlp models. In my code I can access the paths of the models inside the jar, via the <a href=\"https://github.com/stanfordnlp/CoreNLP/blob/master/src/edu/stanford/nlp/pipeline/DefaultPaths.java\" rel=\"nofollow\">DefaultPaths</a> class:</p>\n\n<pre><code>// yields \"edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz\"\nString modelPath = DefaultPaths.DEFAULT_NER_THREECLASS_MODEL;\n</code></pre>\n\n<p>and when the models jar is in my classpath, I can access the files:</p>\n\n<pre><code>InputStream modelStream = ClassLoader.getSystemClassLoader().getSystemResourceAsStream(modelPath);\n</code></pre>\n\n<p>My problem is that this only works if I put the following into (nutch-source-home-dir)/ivy/ivy.xml</p>\n\n<pre><code>&lt;!-- ./ivy/ivy.xml --&gt;\n&lt;dependency org=\"edu.stanford.nlp\" name=\"stanford-corenlp\" rev=\"3.4.1\"&gt;\n  &lt;artifact name=\"stanford-corenlp\" /&gt;\n  &lt;artifact name=\"stanford-corenlp\" maven:classifier=\"models\"/&gt;\n&lt;/dependency&gt;\n</code></pre>\n\n<p>And this doesn't feel like the correct way, since I already specify the jar dependencies in my plugin.xml and ivy.xml under my plugin source folder. In my code, I can look at the jars which the <code>SystemClassLoader</code> knows about:</p>\n\n<pre><code>ClassLoader cl = ClassLoader.getSystemClassLoader();\n\nURL[] urls = ((URLClassLoader)cl).getURLs();\n\nfor(URL myurl: urls){\n    System.out.println(myurl.getFile());\n}\n</code></pre>\n\n<p>And by inspection, my desired jars are available when I specify the dependencies in <code>./ivy/ivy.xml</code>, and not available when I don't. How can I tell nutch to make my jars available at runtime, without messing with <code>./ivy/ivy.xml</code>?</p>\n", "creation_date": 1422445817, "score": 1},
{"title": "Solr dedup error Failed with exit value 255", "view_count": 815, "is_answered": false, "question_id": 28185576, "tags": ["java", "apache", "solr", "web-crawler", "nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/28185576/solr-dedup-error-failed-with-exit-value-255", "last_activity_date": 1422424421, "owner": {"user_id": 3454410, "answer_count": 20, "creation_date": 1395646154, "accept_rate": 48, "view_count": 174, "reputation": 832}, "body": "<p>I am crawling few data from web using apache nutch 2.3. My solr version is 4.10.3. Data is crawled successfully in hbase and indexed also in solr but at end (dedup stage ) Follwoing error appears in console;</p>\n\n<pre><code>IndexingJob: done.\nSOLR dedup -&gt; http://solr:8983/solr\n/home/crawler/nutch-2.3/bin/nutch solrdedup -D mapred.reduce.tasks=2 -D mapred.child.java.opts=-Xmx1000m -D mapred.reduce.tasks.speculative.execution=false -D mapred.map.tasks.speculative.execution=false -D mapred.compress.map.output=true http://solr:8983/solr\nError running:\n  /home/crawler/nutch-2.3/bin/nutch solrdedup -D mapred.reduce.tasks=2 -D mapred.child.java.opts=-Xmx1000m -D mapred.reduce.tasks.speculative.execution=false -D mapred.map.tasks.speculative.execution=false -D mapred.compress.map.output=true http://solr:8983/solr\nFailed with exit value 255.\n</code></pre>\n\n<p>Where solr is IP of a machine running apache solr. In apache nutch log file corresponding error ( in details is following)</p>\n\n<pre><code>2015-01-28 10:39:47,830 WARN  mapred.FileOutputCommitter - Output path is null in cleanup\n2015-01-28 10:39:47,830 WARN  mapred.LocalJobRunner - job_local345700287_0001\njava.lang.Exception: java.lang.NullPointerException\n        at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:354)\nCaused by: java.lang.NullPointerException\n        at org.apache.hadoop.io.Text.encode(Text.java:388)\n        at org.apache.hadoop.io.Text.set(Text.java:178)\n        at org.apache.nutch.indexer.solr.SolrDeleteDuplicates$SolrRecordReader.nextKeyValue(SolrDeleteDuplicates.java:233)\n        at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:531)\n        at org.apache.hadoop.mapreduce.MapContext.nextKeyValue(MapContext.java:67)\n        at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)\n        at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:764)\n        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:364)\n        at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:223)\n        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n        at java.util.concurrent.FutureTask.run(FutureTask.java:266)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at java.lang.Thread.run(Thread.java:745)\n</code></pre>\n\n<p>What is the problem with nutch or solr? How to SOlve it?</p>\n", "creation_date": 1422424421, "score": 5},
{"title": "Errors when trying to compile Nutch 2.2.1 with MongoDB, could not load definitions from sonar/ant/antlib.xml", "view_count": 404, "owner": {"user_id": 4450199, "view_count": 2, "answer_count": 1, "creation_date": 1421168121, "reputation": 3}, "is_answered": true, "answers": [{"question_id": 27931703, "owner": {"user_id": 256618, "link": "http://stackoverflow.com/users/256618/mark-oconnor", "user_type": "registered", "reputation": 54719}, "body": "<p>The error is being thrown because the sonar jar is not present in one of the classpaths (see <a href=\"http://svn.apache.org/viewvc/nutch/trunk/build.xml?revision=1651193&amp;view=markup\" rel=\"nofollow\">build.xml, line 883</a>) </p>\n\n<pre><code>&lt;taskdef uri=\"antlib:org.sonar.ant\" resource=\"org/sonar/ant/antlib.xml\"&gt;\n    &lt;classpath path=\"${ant.library.dir}\"/&gt;\n    &lt;classpath path=\"${mysql.library.dir}\"/&gt;\n&lt;/taskdef&gt;\n</code></pre>\n\n<p>It looks like the warning can be safely ignored, because it's only required when running the \"sonar\" target.</p>\n\n<p>One simple solution would be to move the taskdef within the \"sonar\" target. That would avoid the error message.</p>\n\n<p>I would suggest raising a JIRA issue to get this fixed.</p>\n", "creation_date": 1421186820, "is_accepted": true, "score": 0, "last_activity_date": 1421186820, "answer_id": 27932388}, {"question_id": 27931703, "owner": {"user_id": 4450199, "link": "http://stackoverflow.com/users/4450199/jevgenij-l", "user_type": "registered", "reputation": 3}, "body": "<p>After correcting this error I got another ;-). After research I found what it is not possible to compile Nutch 2.2.1 with Gora 0.5 to use MongoDB (gora-mongodb rev=0.5).</p>\n\n<p>I have cloned Nutch 2.3 and successfully compiled it. </p>\n", "creation_date": 1421244362, "is_accepted": false, "score": 0, "last_activity_date": 1421244362, "answer_id": 27944842}], "question_id": 27931703, "tags": ["mongodb", "ant", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/27931703/errors-when-trying-to-compile-nutch-2-2-1-with-mongodb-could-not-load-definitio", "last_activity_date": 1421244362, "accepted_answer_id": 27932388, "body": "<p>I want to compile Nutch 2.2.1 to use with MongoDB for data storage.</p>\n\n<p>I changed gora-core to 0.5 in file ivy.xml:</p>\n\n<pre><code>&lt;dependency org=\"org.apache.gora\" name=\"gora-core\" rev=\"0.5\" conf=\"*-&gt;default\"/&gt;\n</code></pre>\n\n<p>Also added dependency for mongodb in ivy/ivy.xml file:</p>\n\n<pre><code>&lt;dependency org=\"org.apache.gora\" name=\"gora-mongodb\" rev=\"0.5\" conf=\"*-&gt;default\" /&gt;\n</code></pre>\n\n<p>Added mongodb config in conf/gora.properties:</p>\n\n<pre><code>############################\n# MongoDBStore properties  #\n############################\ngora.datastore.default=org.apache.gora.mongodb.store.MongoStore\ngora.mongodb.override_hadoop_configuration=false\ngora.mongodb.mapping.file=/gora-mongodb-mapping.xml\ngora.mongodb.servers=localhost:27017\ngora.mongodb.db=nutch\n</code></pre>\n\n<p>Added gora-mongodb-mapping.xml to conf directory from Nutch-2.3-SNAPSHOT.</p>\n\n<p>When I am trying to compile I get error:</p>\n\n<pre><code>Could not load definitions from resource org/sonar/ant/antlib.xml. It could not be found.\n</code></pre>\n\n<p>After which I get many compiler errors. </p>\n\n<p>When I try to configure and compile Nutch with MySQL every compiles and work perfectly.</p>\n\n<p>I am trying to compile on Debian.</p>\n", "creation_date": 1421183979, "score": 0},
{"title": "Bypassing authentication for localhost in order to implement search in Etherpad", "view_count": 315, "is_answered": false, "answers": [{"question_id": 3413661, "owner": {"user_id": 1113772, "accept_rate": 100, "link": "http://stackoverflow.com/users/1113772/paul-sweatte", "user_type": "registered", "reputation": 14979}, "community_owned_date": 1420558520, "creation_date": 1420558520, "is_accepted": false, "body": "<blockquote>\n  <p>Which leaves me with my initial problem : how can I determine if the host is localhost in Javascript ?</p>\n</blockquote>\n\n<p>Use the <code>location</code> API:</p>\n\n<pre><code>if (/localhost/.test(location.hostname) )\n</code></pre>\n\n<p><strong>References</strong></p>\n\n<ul>\n<li><a href=\"http://msdn.microsoft.com/en-us/library/ie/ms535866\" rel=\"nofollow\">MSDN: location Object</a></li>\n</ul>\n", "score": 0, "last_activity_date": 1420558520, "answer_id": 27801939}], "question_id": 3413661, "tags": ["javascript", "authentication", "solr", "nutch", "etherpad"], "answer_count": 1, "link": "http://stackoverflow.com/questions/3413661/bypassing-authentication-for-localhost-in-order-to-implement-search-in-etherpad", "last_activity_date": 1420558520, "owner": {"age": 31, "answer_count": 16, "creation_date": 1264704657, "user_id": 261251, "accept_rate": 68, "view_count": 81, "location": "Paris, France", "reputation": 840}, "body": "<p>I'm trying to implement Nutch + Solr based search engine into my Etherpad installation. The main issue I'm having is that Nutch doesn't support <code>POST</code> authentication. Etherpad and Nutch are installed on the same machine, so an obvious solution would be to find a way to bypass authentication for <code>localhost</code>.</p>\n\n<p>This is where I'm stuck. I don't know the Etherpad codebase very well, I've mostly done cosmetic tweaks until now.</p>\n\n<p>Can someone point me to where I should be looking, or potential solutions ? I've found some interesting bits of code having do to with auth in the codebase, but it's Javascript, so there's no obvious way to check if the request host is <code>localhost</code>.</p>\n\n<p>Edit :</p>\n\n<p>I've found the code that handles the auth policy, and tested it with a simple condition so that it always returns <code>true</code>. Authentication can then be bypassed. Which leaves me with my initial problem : how can I determine if the host is <code>localhost</code> in Javascript ?</p>\n\n<pre><code> if (guestPolicy == \"allow\") {\n    return;\n  }\n</code></pre>\n", "creation_date": 1281002780, "score": 2},
{"title": "Sharing crawled nutch data between multiple solr indexes", "view_count": 195, "is_answered": true, "answers": [{"question_id": 27567444, "owner": {"user_id": 1699191, "link": "http://stackoverflow.com/users/1699191/ameertawfik", "user_type": "registered", "reputation": 618}, "body": "<p>You will need to write a new indexer plugin to do that; look at the SolrIndexer of Nutch to understand how to write a new indexer. In that indexer, you should do the following:</p>\n\n<ol>\n<li>Define three or four Solr server instances, one for each core.</li>\n<li>Inside the write method of the indexer, examine the type of the document and  use the right Solr core to add the document. By right, you should have a field at Nutch that you can use to determine where to send the document.</li>\n</ol>\n", "creation_date": 1419761848, "is_accepted": false, "score": 1, "last_activity_date": 1419761848, "answer_id": 27675475}], "question_id": 27567444, "tags": ["solr", "web-crawler", "search-engine", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/27567444/sharing-crawled-nutch-data-between-multiple-solr-indexes", "last_activity_date": 1420380205, "owner": {"user_id": 1602316, "answer_count": 80, "creation_date": 1345094807, "accept_rate": 50, "view_count": 206, "location": "Italy", "reputation": 2590}, "body": "<p>We have thousands of solr indexes/collections that share pages being crawled by nutch. </p>\n\n<p>Currently these pages are being crawled multiple times, once for each solr index that contains them.</p>\n\n<p>It is possible to crawl these sites once, and share the crawl data between indexes?</p>\n\n<p>Maybe by checking existing crawldbs if a site has been crawled and get the data from there for parsing and indexing.</p>\n\n<p>Or crawl all sites in one go, and then selectively submit crawl data to each index. (eg: one site per segment, but not sure how to identify which segment  belongs to what site due to segment names are numeric)</p>\n\n<p>Any ideas or help appreciated :)</p>\n", "creation_date": 1418996906, "score": 1},
{"title": "put Crawled data and mysql data together in Solr", "view_count": 66, "is_answered": false, "answers": [{"question_id": 27685119, "owner": {"user_id": 137650, "link": "http://stackoverflow.com/users/137650/matslindh", "user_type": "registered", "reputation": 13901}, "body": "<p>You'll need to create a separate collection for the import task from MySQL. These are defined in your Solr configuration directory - see <code>example/solr</code> in the distribution for a minimal setup. You can create as many collections as you need, and each collection will be handled separately from existing collections.</p>\n\n<p>There's also the possibility of having different content in the same index, but that might cause an issue with Nutch - it will require you to change <code>preImportDeleteQuery</code> in the DataImportHandler, and handle deletes from the data set by yourself.</p>\n", "creation_date": 1419854979, "is_accepted": false, "score": 0, "last_activity_date": 1419854979, "answer_id": 27688610}], "question_id": 27685119, "tags": ["php", "mysql", "solr", "nutch", "dataimporthandler"], "answer_count": 1, "link": "http://stackoverflow.com/questions/27685119/put-crawled-data-and-mysql-data-together-in-solr", "last_activity_date": 1419854979, "owner": {"user_id": 4401413, "view_count": 0, "answer_count": 0, "creation_date": 1419838177, "reputation": 1}, "body": "<p>i am using `</p>\n\n<blockquote>\n  <p>Nutch</p>\n</blockquote>\n\n<p>` and '</p>\n\n<blockquote>\n  <p>Solr</p>\n</blockquote>\n\n<p>integration process. i am able to crawl some website data and pass it into solr system,which i am able to access easily.Now i want to import data from mysql also and put both data together but different index. i already tried to import data from mysql and i was also successful in doing that but then it replaced all the indexes created on crawled data , indirectly i lost mine crawled data. so can anyone help me out for doing the above process?</p>\n", "creation_date": 1419838564, "score": 0},
{"title": "is /etc/bin/hadoop a synchronous call?", "view_count": 149, "owner": {"user_id": 1058511, "answer_count": 22, "creation_date": 1321905325, "accept_rate": 47, "view_count": 109, "reputation": 591}, "is_answered": true, "answers": [{"question_id": 27596653, "owner": {"user_id": 4247626, "accept_rate": 40, "link": "http://stackoverflow.com/users/4247626/ravi-h", "user_type": "registered", "reputation": 281}, "body": "<p>No, <code>Map Reduce</code> jobs not done same time.\nFirst Mapper class finishes its work, then output of mapper will submitted to reducer phase(in between <code>shuffling,sorting,combing</code>).\nIn mapper phase nodes share their results to NameNode, if namenode found complete output then it will tell to mapper to stop mapper phase.\nAfter then start reduce phase.</p>\n", "creation_date": 1419226864, "is_accepted": false, "score": 0, "last_activity_date": 1419226864, "answer_id": 27597098}, {"question_id": 27596653, "owner": {"user_id": 2226545, "link": "http://stackoverflow.com/users/2226545/phoenix", "user_type": "registered", "reputation": 71}, "body": "<p>Yes , it is a synchronous call because for the final result you have to wait till the completion of job at all the nodes. In fact I would say it also depends on how you have written the job i.e. you can add some lines of code to print your current status of crawl on terminal , by which you will get the live status update of your crawl.\nBut to see the final output of the crawl you have to wait till the completion of job.</p>\n", "creation_date": 1419490099, "is_accepted": true, "score": 1, "last_activity_date": 1419490099, "answer_id": 27644880}], "question_id": 27596653, "tags": ["hadoop", "asynchronous", "mapreduce", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/27596653/is-etc-bin-hadoop-a-synchronous-call", "last_activity_date": 1419490099, "accepted_answer_id": 27644880, "body": "<p>when you call /etc/bin/hadoop jar myjar.jar myclass args</p>\n\n<p>Does the map reduce job run synchronousely? another words, does the call return only after all the map reduce jobs on all the nodes have completed? </p>\n\n<p>This is especially relevant in case a nutch job is dispatched. I want to know whether site crawling has been completed by the time the command returns. </p>\n", "creation_date": 1419223045, "score": 0},
{"title": "run hadoop nutch on oozie gets old racing condition", "view_count": 204, "is_answered": false, "question_id": 27612719, "tags": ["hadoop", "nutch", "oozie"], "answer_count": 0, "link": "http://stackoverflow.com/questions/27612719/run-hadoop-nutch-on-oozie-gets-old-racing-condition", "last_activity_date": 1419296387, "owner": {"user_id": 1058511, "answer_count": 22, "creation_date": 1321905325, "accept_rate": 47, "view_count": 109, "reputation": 591}, "body": "<p>I can run hadoop jar apache-nutch-1.7.job org.apache.nutch.crawl.Crawl args in command line just fine, but when run in oozie, I get an exception</p>\n\n<p>org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.io.FileNotFoundException: File does not exist: hdfs://server:8020/user/hdfs/.staging/job_1416525929767_0494/job.splitmetainfo\n    at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl$InitTransition.createSplits(JobImpl.java:1566)\n    at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl$InitTransition.transition(JobImpl.java:1430)\n    at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl$InitTransition.transition(JobImpl.java:1388)</p>\n\n<p>an old Jira reported this exception</p>\n\n<p><a href=\"https://issues.apache.org/jira/browse/MAPREDUCE-5471\" rel=\"nofollow\">https://issues.apache.org/jira/browse/MAPREDUCE-5471</a></p>\n\n<p>but it was supposely fixed back in version 2.1.1-beta. I am on yarn 2.5.0.</p>\n\n<p>Any one else see this?</p>\n", "creation_date": 1419296387, "score": 0},
{"title": "nutch 1.7 keeps change filesystem to local when run from oozie", "view_count": 122, "is_answered": false, "answers": [{"question_id": 27595686, "owner": {"user_id": 1058511, "accept_rate": 47, "link": "http://stackoverflow.com/users/1058511/user1058511", "user_type": "registered", "reputation": 591}, "body": "<p>it seems the oozie conf directory is missing the proper *-site.xml files. I added mapred-site.xml to /etc/oozie/conf/hadoop-conf directory, and this problem went away.</p>\n", "creation_date": 1419296179, "is_accepted": false, "score": 0, "last_activity_date": 1419296179, "answer_id": 27612692}], "question_id": 27595686, "tags": ["hadoop", "nutch", "oozie"], "answer_count": 1, "link": "http://stackoverflow.com/questions/27595686/nutch-1-7-keeps-change-filesystem-to-local-when-run-from-oozie", "last_activity_date": 1419296179, "owner": {"user_id": 1058511, "answer_count": 22, "creation_date": 1321905325, "accept_rate": 47, "view_count": 109, "reputation": 591}, "body": "<p>I built and ran nutch 1.7 from command line just fine</p>\n\n<p>hadoop jar apache-ntuch-1.7.job org.apache.nutch.crawl.Crawl hdfs://myserver/nutch/urls -dir hdfs://myserver/nutch/crawl -depth 5 -topN100</p>\n\n<p>but when I ran the same thing from oozie, it keeps getting \nWrong FS: hdfs://myserver/nutch/crawl/crawldb/current, expected: file:///</p>\n\n<p>I checked into the source, every time the code does </p>\n\n<p>FileSystem fs = new JobClient(job).getFs();</p>\n\n<p>the fs gets changed back to local fs.</p>\n\n<p>I override all the instance of these statements, the job then dies in the fetch stage, simply says \njava.io.IOException: Job failed!\n    at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:838)</p>\n\n<p>it really appears that running from oozie causes the wrong version of JobClient class (from hadoop-core.jar) to be loaded.</p>\n\n<p>Anyone saw this before?</p>\n", "creation_date": 1419213382, "score": 0},
{"title": "Filtering a Url via Regex", "view_count": 42, "is_answered": false, "answers": [{"question_id": 27592153, "owner": {"user_id": 1602049, "accept_rate": 40, "link": "http://stackoverflow.com/users/1602049/nash-ag", "user_type": "registered", "reputation": 3623}, "body": "<p>If you want seperate regex. Then the first one can be:</p>\n\n<pre><code>http\\:\\/\\/greenbook\\.americansalon\\.com\\/leaf|cat\\/(?=Haircolor|Cosmetics|Shampoos).*\n</code></pre>\n\n<p>The other two look fine.</p>\n", "creation_date": 1419185198, "is_accepted": false, "score": 0, "last_activity_date": 1419185198, "answer_id": 27592220}, {"question_id": 27592153, "owner": {"user_id": 3297613, "accept_rate": 76, "link": "http://stackoverflow.com/users/3297613/avinash-raj", "user_type": "registered", "reputation": 126216}, "body": "<p>Seems like you want something like this,</p>\n\n<pre><code>http\\:\\/\\/greenbook\\.americansalon\\.com\\/(?:(leaf|cat)\\/(?:Haircolor|Cosmetics|Shampoos)\\b|company\\b).*\n</code></pre>\n\n<p><a href=\"https://www.regex101.com/r/wP7pR2/8\" rel=\"nofollow\">DEMO</a></p>\n", "creation_date": 1419185273, "is_accepted": false, "score": 0, "last_activity_date": 1419185273, "answer_id": 27592225}, {"question_id": 27592153, "owner": {"user_id": 1627235, "link": "http://stackoverflow.com/users/1627235/sven-hohenstein", "user_type": "registered", "reputation": 51036}, "body": "<p>This regex matches valid URLs according to your rules:</p>\n\n<pre><code>http((\\:\\/\\/greenbook\\.americansalon\\.com\\/(((leaf|cat)\\/(Haircolor|Cosmetics|Shampoos))|company\\b).*)|(s?\\:\\/\\/(?!greenbook\\.americansalon\\.com).*))\n</code></pre>\n\n<p><a href=\"https://regex101.com/r/mE0gG2/1\" rel=\"nofollow\"><strong>Online demo</strong></a></p>\n", "creation_date": 1419186731, "is_accepted": false, "score": 0, "last_activity_date": 1419186731, "answer_id": 27592454}], "question_id": 27592153, "tags": ["regex", "nutch"], "answer_count": 3, "link": "http://stackoverflow.com/questions/27592153/filtering-a-url-via-regex", "last_activity_date": 1419204303, "owner": {"user_id": 1780484, "answer_count": 7, "creation_date": 1351412224, "accept_rate": 50, "view_count": 21, "reputation": 124}, "body": "<p>Given these seed urls: </p>\n\n<pre><code>http://greenbook.americansalon.com/cat/Haircolor.htm\nhttp://greenbook.americansalon.com/cat/Cosmetics.htm\nhttp://greenbook.americansalon.com/cat/Shampoos-and-Conditioners.htm\n</code></pre>\n\n<p>I'd like to  crawl (via Nutch 1.4) through urls that match only the following rules:</p>\n\n<ul>\n<li>Only Categories that are \"Haircolor\", \"Cosmetics\" and \"Shampoos-and-Conditioners\" (as seed urls). I.e. nothing else can come in url after <code>http://greenbook.americansalon.com/cat/</code> .</li>\n<li>Any  <code>http://greenbook.americansalon.com/company/...</code> is accptable.</li>\n<li>Any site other than \"greenbook.americansalon\" is acceptable.</li>\n</ul>\n\n<p>Mentioning the seed urls is not a guarantee, of course, since it's possible to get to other categories from them.</p>\n\n<p>I want a regex negating:</p>\n\n<pre><code>http\\:\\/\\/greenbook\\.americansalon\\.com\\/([leaf|cat]+\\/[^Haircolor|Cosmetics|Shampoos].*)\n</code></pre>\n\n<p>and combining </p>\n\n<pre><code>http\\:\\/\\/greenbook.americansalon.com\\/company\\/.*\n</code></pre>\n\n<p>and any other site ( <code>.+</code> ) -  (For example - <code>http://www.spilo.com/index.asp</code> should also be caught).</p>\n\n<p>In other words - give me all the urls (including other domains than \"greenbook.americansalon\") that follow the rules I mentioned.</p>\n", "creation_date": 1419184732, "score": 0},
{"title": "Nutch 2.2.1 Error", "view_count": 54, "is_answered": false, "answers": [{"question_id": 27420091, "owner": {"user_id": 1965370, "link": "http://stackoverflow.com/users/1965370/do-do", "user_type": "registered", "reputation": 389}, "body": "<p>It shows that the CLASSPATH for your executable is incorrect. Please verify your CLASSPATH. Note that Unix/Linux style paths and Windows style paths (cygpath) are different.</p>\n\n<p>Hopes this helps.</p>\n", "creation_date": 1419001781, "is_accepted": false, "score": 0, "last_activity_date": 1419001781, "answer_id": 27568745}], "question_id": 27420091, "tags": ["nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/27420091/nutch-2-2-1-error", "last_activity_date": 1419001781, "owner": {"user_id": 676567, "answer_count": 1, "creation_date": 1301050634, "accept_rate": 45, "view_count": 105, "reputation": 269}, "body": "<p>I'm trying to run Nutch 2.2.1 using Cygwin64/Windows7 and getting the following error: </p>\n\n<pre><code>MY-PC /home/apache-nutch-2.2.1/src/bin\n$ ./nutch crawl urls -dir test -depth 3 -topN 4\ncygpath: can't convert empty path\nError: Could not find or load main class org.apache.nutch.crawl.Crawler\n</code></pre>\n\n<p>Has anyone seen or resolved this error before?</p>\n\n<p>Thanks,</p>\n\n<p>O.</p>\n", "creation_date": 1418292393, "score": 0},
{"title": "HBase Nutch error [Ljava.lang.StackTraceElement", "view_count": 99, "owner": {"user_id": 3454410, "answer_count": 20, "creation_date": 1395646154, "accept_rate": 48, "view_count": 174, "reputation": 832}, "is_answered": true, "answers": [{"question_id": 27056507, "owner": {"user_id": 2265190, "accept_rate": 50, "link": "http://stackoverflow.com/users/2265190/hms", "user_type": "registered", "reputation": 293}, "body": "<p>This problem \"Connection refused\" is simply because your region server is not running properly</p>\n", "creation_date": 1418974495, "is_accepted": true, "score": 0, "last_activity_date": 1418974495, "answer_id": 27561587}], "question_id": 27056507, "tags": ["apache", "hbase", "web-crawler", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/27056507/hbase-nutch-error-ljava-lang-stacktraceelement", "last_activity_date": 1418974495, "accepted_answer_id": 27561587, "body": "<p>My apache nutch is crawling and in log file following error is appeared.</p>\n\n<p>ERROR store.HBaseStore - Connection refused 2014-11-17 00:00:38,255 ERROR store.HBaseStore - [Ljava.lang.StackTraceElement;@6dce5061</p>\n\n<p>How to remove this error. According to my search this error is because of hbase and not in nutch. This question is posted <a href=\"http://stackoverflow.com/questions/24384793/error-store-hbasestore-ljava-lang-stacktraceelement15a3cc7b\">here</a> but it has no answer.I have to bounty this question if do not get an answer that's why I am posting again.</p>\n\n<p>Some informations of my small cluster is following ( 2 machine cluster)</p>\n\n<p>On machine one, hadoop and hbase are running</p>\n\n<p>On machine two, apache nutch crawler(2.2.1) is running.</p>\n\n<p>When I check log files of hbase and hadoop, there isn't any information about bug. Because of this bug, crawled data in not going to be saved in hbase(machine1). That's a real problem for me and my crawler in not crawler properly. There is about 266 GB already crawled data in table.</p>\n", "creation_date": 1416556152, "score": 0},
{"title": "Apache nutch is not crawling any more", "view_count": 278, "owner": {"user_id": 3454410, "answer_count": 20, "creation_date": 1395646154, "accept_rate": 48, "view_count": 174, "reputation": 832}, "is_answered": true, "answers": [{"question_id": 27097905, "owner": {"user_id": 1320510, "accept_rate": 70, "link": "http://stackoverflow.com/users/1320510/charles-merriam", "user_type": "registered", "reputation": 7478}, "body": "<p>Not quite my field, but looks like thread exhaustion on the underlying machines.</p>\n", "creation_date": 1417584570, "is_accepted": false, "score": 0, "last_activity_date": 1417584570, "answer_id": 27264309}, {"question_id": 27097905, "owner": {"user_id": 2265190, "accept_rate": 50, "link": "http://stackoverflow.com/users/2265190/hms", "user_type": "registered", "reputation": 293}, "body": "<p>As I was also facing similiar problem. Actual problem was with regionserver (Hbase deamon ). So try to restart it as it is shutdown when used with default seeting and data is too mutch in hbase. For more information, see log files of regionserver.</p>\n", "creation_date": 1418973621, "is_accepted": true, "score": 0, "last_activity_date": 1418973621, "answer_id": 27561395}], "question_id": 27097905, "tags": ["java", "hadoop", "hbase", "web-crawler", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/27097905/apache-nutch-is-not-crawling-any-more", "last_activity_date": 1418973621, "accepted_answer_id": 27561395, "body": "<p>I have a two machine cluster. On one machine nutch is configured and on second hbase and hadoop are configured. hadoop is in fully distributed mode and hbase in pseudo distributed mode. I have crawled about 280GB data. But now when I start crawling . It gives following message and do not crawl any more in previous table</p>\n\n<p>INFO  mapreduce.GoraRecordReader - gora.buffer.read.limit = 10000\nINFO  crawl.FetchScheduleFactory - Using FetchSchedule impl: org.apache.nutch.crawl.DefaultFetchSchedule</p>\n\n<p>and following bug</p>\n\n<p>ERROR store.HBaseStore\n - [Ljava.lang.StackTraceElement;@7ae0c96b</p>\n\n<p>Documents are fetched but they are not saved in hbase.\nBut if I crawl data in a new table, it works well and crawl properly witout any error. I think this is not a connection problem as for new table it works. I think it is bacause of some property etc. </p>\n\n<p>Can anyone guide me as I am not an expert in apache nutch?</p>\n", "creation_date": 1416803582, "score": 1},
{"title": "Hadoop-2.5.1 + Nutch-2.2.1: Found interface org.apache.hadoop.mapreduce.TaskAttemptContext, but class was expected", "view_count": 344, "is_answered": false, "answers": [{"question_id": 26236112, "owner": {"user_id": 3891482, "link": "http://stackoverflow.com/users/3891482/tmsprgl", "user_type": "registered", "reputation": 6}, "body": "<p>Probably you are using Gora (or smth else) compiled with Hadoop 1 (from maven repo?). You can download Gora (0.5?) and build it with Hadoop 2.</p>\n\n<p>Perhaps it is just the first trouble in the series of problems.\nPlease notify us about your future steps.</p>\n", "creation_date": 1412924957, "is_accepted": false, "score": 0, "last_activity_date": 1412924957, "answer_id": 26293779}, {"question_id": 26236112, "owner": {"user_id": 4368212, "link": "http://stackoverflow.com/users/4368212/rrydziu", "user_type": "registered", "reputation": 56}, "body": "<p>I had similar error on nutch 2.x with hadoop 2.4.0</p>\n\n<p>Recompile nutch with hadoop 2.5.1 dependencies (ivy) and exclude all hadoop 1.x dependencies - you can find them in lib - probably hadoop-core.</p>\n", "creation_date": 1418770003, "is_accepted": false, "score": 0, "last_activity_date": 1418770003, "answer_id": 27515374}], "question_id": 26236112, "tags": ["hadoop", "solr", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/26236112/hadoop-2-5-1-nutch-2-2-1-found-interface-org-apache-hadoop-mapreduce-taskatte", "last_activity_date": 1418770003, "owner": {"user_id": 4116941, "view_count": 0, "answer_count": 0, "creation_date": 1412681589, "reputation": 11}, "body": "<p><strong>Command: ./crawl /urls /mydir XXXXX 2</strong></p>\n\n<p>When I run this command in Hadoop-2.5.1 and Nutch-2.2.1, I get the wrong information as following.</p>\n\n<p>14/10/07 19:58:10 INFO mapreduce.Job: Running job: job_1411692996443_0016<br>\n14/10/07 19:58:17 INFO mapreduce.Job: Job job_1411692996443_0016 running in uber mode : false\n14/10/07 19:58:17 INFO mapreduce.Job:  map 0% reduce 0%<br>\n14/10/07 19:58:21 INFO mapreduce.Job: Task Id : attempt_1411692996443_0016_m_000000_0, Status : FAILED<br>\n<strong>Error: Found interface org.apache.hadoop.mapreduce.TaskAttemptContext, but class was expected</strong><br>\n14/10/07 19:58:26 INFO mapreduce.Job: Task Id : attempt_1411692996443_0016_m_000000_1, Status : FAILED<br>\n<strong>Error: Found interface org.apache.hadoop.mapreduce.TaskAttemptContext, but class was expected</strong>\n14/10/07 19:58:31 INFO mapreduce.Job: Task Id : attempt_1411692996443_0016_m_000000_2, Status : FAILED<br>\n<strong>Error: Found interface org.apache.hadoop.mapreduce.TaskAttemptContext, but class was expected</strong>\n14/10/07 19:58:36 INFO mapreduce.Job:  map 100% reduce 0%\n14/10/07 19:58:36 INFO mapreduce.Job: Job job_1411692996443_0016 failed with state FAILED due to: Task failed task_1411692996443_0016_m_000000<br>\nJob failed as tasks failed. failedMaps:1 failedReduces:0<br>\n14/10/07 19:58:36 INFO mapreduce.Job: Counters: 12 </p>\n\n<pre><code>Job Counters \n    Failed map tasks=4\n    Launched map tasks=4\n    Other local map tasks=3\n    Data-local map tasks=1\n    Total time spent by all maps in occupied slots (ms)=11785\n    Total time spent by all reduces in occupied slots (ms)=0\n    Total time spent by all map tasks (ms)=11785\n    Total vcore-seconds taken by all map tasks=11785\n    Total megabyte-seconds taken by all map tasks=12067840\nMap-Reduce Framework\n    CPU time spent (ms)=0\n    Physical memory (bytes) snapshot=0\n    Virtual memory (bytes) snapshot=0\n</code></pre>\n\n<p>14/10/07 19:58:36 ERROR crawl.InjectorJob: InjectorJob: java.lang.RuntimeException: job failed: name=[/mydir]inject /urls, jobid=job_1411692996443_0016</p>\n\n<pre><code>at org.apache.nutch.util.NutchJob.waitForCompletion(NutchJob.java:55)\nat org.apache.nutch.crawl.InjectorJob.run(InjectorJob.java:233)\nat org.apache.nutch.crawl.InjectorJob.inject(InjectorJob.java:251)\nat org.apache.nutch.crawl.InjectorJob.run(InjectorJob.java:273)\nat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)\nat org.apache.nutch.crawl.InjectorJob.main(InjectorJob.java:282)\nat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\nat java.lang.reflect.Method.invoke(Method.java:483)\nat org.apache.hadoop.util.RunJar.main(RunJar.java:212)\n</code></pre>\n", "creation_date": 1412685188, "score": 1},
{"title": "Nutch fetched pages aren&#39;t being indexed into Solr", "view_count": 88, "owner": {"age": 33, "answer_count": 173, "creation_date": 1316553539, "user_id": 955697, "accept_rate": 77, "view_count": 192, "location": "Montevideo, Uruguay", "reputation": 5385}, "is_answered": true, "answers": [{"last_edit_date": 1418645717, "owner": {"user_id": 955697, "accept_rate": 77, "link": "http://stackoverflow.com/users/955697/deleteman", "user_type": "registered", "reputation": 5385}, "body": "<p>Found my problem, I'll leave it as an answer in case anyone else has the same symptoms:</p>\n\n<p>My problem was the proxy configuration. My linux box has the proxy configured to be applied system-wide, but I also had to configure Nutch to use the same proxy. Once I changed that, it started to work.</p>\n\n<p>The configuration is under config/nutch-default.xml</p>\n\n<p><strong>Edit with more info</strong></p>\n\n<p>To be more specific, here is the Proxy configuration I had to change:</p>\n\n<pre><code>&lt;property&gt;\n  &lt;name&gt;http.proxy.host&lt;/name&gt;\n  &lt;value&gt;xxx.xxx.xxx&lt;/value&gt;\n  &lt;description&gt;The proxy hostname.  If empty, no proxy is used.&lt;/description&gt;\n&lt;/property&gt;\n</code></pre>\n", "question_id": 27255361, "creation_date": 1417545269, "is_accepted": true, "score": 1, "last_activity_date": 1418645717, "answer_id": 27256514}], "question_id": 27255361, "tags": ["solr", "indexing", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/27255361/nutch-fetched-pages-arent-being-indexed-into-solr", "last_activity_date": 1418645717, "accepted_answer_id": 27256514, "body": "<p>Ok, so I'm trying to setup nutch to crawl a site and index the pages into solr. I'm currently using Nutch 1.9 with Solr 4.10.2 \nI've followed these instructions: <a href=\"http://wiki.apache.org/nutch/NutchTutorial#A4._Setup_Solr_for_search\" rel=\"nofollow\">http://wiki.apache.org/nutch/NutchTutorial#A4._Setup_Solr_for_search</a></p>\n\n<p>The crawling appears to go just fine but when I check the collection on Solr (using the web ui) there are no documents indexed...any idea where I could check for problems? </p>\n", "creation_date": 1417541184, "score": 1},
{"title": "How to programmatically create indexing from the list of urls in java", "view_count": 238, "owner": {"user_id": 4356312, "view_count": 2, "answer_count": 0, "creation_date": 1418448157, "reputation": 8}, "is_answered": true, "answers": [{"question_id": 27455886, "owner": {"user_id": 137650, "link": "http://stackoverflow.com/users/137650/matslindh", "user_type": "registered", "reputation": 13901}, "body": "<p>How you programmatically do something really depends on which language you plan on writing your code in - fetching content from a URL and making sense of that content before indexing will be largely dependent on the libraries available for your programming language of choice.</p>\n\n<p>You can still use nutch with the Solr backend - give it the list of urls as input and set <code>--depth</code> to <code>1</code> (so that it doesn't spider anything further).</p>\n\n<p>There are also other \"ready\" options, such as <a href=\"http://www.crawl-anywhere.com/\" rel=\"nofollow\">Crawl Anywhere</a> (which has a Solr backend) and <a href=\"http://scrapy.org/\" rel=\"nofollow\">Scrapy</a>.</p>\n\n<p>\"Not as good as Google\" is not a good description of what you want to accomplish and how to approach that (keep in mind that Search is a core product for Google and they have a very, very large set of custom technologies for handling search). If you have specific issues with your own data and how to display that (usually you can do more useful results as you have domain knowledge of the task you're trying to solve), ask concrete, specific questions.</p>\n", "creation_date": 1418464619, "is_accepted": true, "score": 0, "last_activity_date": 1418464619, "answer_id": 27457599}, {"question_id": 27455886, "owner": {"user_id": 417864, "accept_rate": 75, "link": "http://stackoverflow.com/users/417864/alexandre-rafalovitch", "user_type": "registered", "reputation": 6649}, "body": "<p>You can use <a href=\"https://cwiki.apache.org/confluence/display/solr/Uploading+Structured+Data+Store+Data+with+the+Data+Import+Handler\" rel=\"nofollow\">Data Import Handler</a> to load the list of URLs from file and then read and index them.</p>\n\n<p>You'd need to use nested entity with outside entity having <em>rootEntity</em> flag set to false.</p>\n\n<p>You'd need to practice a little bit with DIH. So, I recommend that you first learn how to import just the URLs into individual Solr documents and then enhance it with actually parsing of URL content.</p>\n", "creation_date": 1418618705, "is_accepted": false, "score": 0, "last_activity_date": 1418618705, "answer_id": 27477517}], "question_id": 27455886, "tags": ["solr", "lucene", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/27455886/how-to-programmatically-create-indexing-from-the-list-of-urls-in-java", "last_activity_date": 1418618705, "accepted_answer_id": 27457599, "body": "<p>I want to create search engine. So I had used nutch and solr for the developing it.\nBut it is not able to crawl each and every url of the website and search results are not as \ngood as Google.So I started using jcrawler to get list of url.\nNow I have list of urls.But I have to index them.\nSo is there any way where I can index list of urls stored line by line in a file.\nand show results vis lucene or solr or any other Java API </p>\n", "creation_date": 1418448660, "score": 1},
{"title": "gora-mongodb.mapping.XML properties File", "view_count": 410, "owner": {"user_id": 676567, "answer_count": 1, "creation_date": 1301050634, "accept_rate": 45, "view_count": 105, "reputation": 269}, "is_answered": true, "answers": [{"question_id": 27460399, "owner": {"user_id": 582789, "link": "http://stackoverflow.com/users/582789/alfonso-nishikawa", "user_type": "registered", "reputation": 1263}, "body": "<p>You need 2 files in <code>nutch/conf</code>:</p>\n\n<ol>\n<li><code>gora.properties</code>: where you declare you are going to use mongodb backend.</li>\n<li><code>gora-mongodb-mapping.xml</code> (notice the dash, not the dot you wrote): where you create a mapping between names in Gora entities and the fields in the datastore.</li>\n</ol>\n\n<p>The version you are using I really think it is not prepared to work with Gora 0.5, but give it a shot. Copy <code>gora-mongodb-mapping.xml</code> from <a href=\"http://svn.apache.org/viewvc/nutch/branches/2.x/conf/\" rel=\"nofollow\">Nutch-2.3-SNAPSHOT</a> to <code>nutch/conf/</code></p>\n\n<p>If it does not work, try using Nutch-2.3-SNAPSHOT instead of 2.2.1.</p>\n", "creation_date": 1418515729, "is_accepted": true, "score": 0, "last_activity_date": 1418515729, "answer_id": 27464993}], "question_id": 27460399, "tags": ["nutch", "gora"], "answer_count": 1, "link": "http://stackoverflow.com/questions/27460399/gora-mongodb-mapping-xml-properties-file", "last_activity_date": 1418515729, "accepted_answer_id": 27464993, "body": "<p>I'm new to Nutch (2.2.1) and trying to run it on Cygwin/Windows 7 with the latest version of Gora (0.5) so I can persist data to a MongoDB (2.6) datastore. I changed the Nutch-Site.XML File to include my Mongo property but I'm a little confused about the gora-mongodb.mapping.XML properties file <a href=\"http://gora.apache.org/current/gora-mongodb.html#gora-mongodb-mappings\" rel=\"nofollow\">here</a> that's needed. Just wondering do I need to:</p>\n\n<p>1) create a Java class within the Nutch/Gora project which I specify in class-name property in the gora-mongodb.mapping File or will Gora create this for me?  The documentation doesn't appear to be very clear.</p>\n\n<p>2) I created a sample File in my apache-nutch-2.2.1\\runtime\\local\\conf folder and added the name of my MongoDB collection.  When I run Nutch I get the following error:</p>\n\n<pre><code>$ ./nutch crawl urls -dir testCrawl -depth 3 -topN 5\ncygpath: can't convert empty path\nException in thread \"main\" org.apache.gora.util.GoraException: java.lang.IllegalStateException: A collection is not specified\n        at org.apache.gora.store.DataStoreFactory.createDataStore(DataStoreFactory.java:167)\n        at org.apache.gora.store.DataStoreFactory.createDataStore(DataStoreFactory.java:135)\n        at org.apache.nutch.storage.StorageUtils.createWebStore(StorageUtils.java:75)\n        at org.apache.nutch.crawl.InjectorJob.run(InjectorJob.java:221)\n        at org.apache.nutch.crawl.Crawler.runTool(Crawler.java:68)\n        at org.apache.nutch.crawl.Crawler.run(Crawler.java:136)\n        at org.apache.nutch.crawl.Crawler.run(Crawler.java:250)\n        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n        at org.apache.nutch.crawl.Crawler.main(Crawler.java:257)\nCaused by: java.lang.IllegalStateException: A collection is not specified\n        at org.apache.gora.mongodb.store.MongoMappingBuilder.build(MongoMappingBuilder.java:77)\n        at org.apache.gora.mongodb.store.MongoStore.initialize(MongoStore.java:168)\n        at org.apache.gora.store.DataStoreFactory.initializeDataStore(DataStoreFactory.java:102)\n        at org.apache.gora.store.DataStoreFactory.createDataStore(DataStoreFactory.java:161)\n        ... 8 more\n</code></pre>\n\n<p>Any help or clarification around this file would be appreciated.</p>\n", "creation_date": 1418484197, "score": 0},
{"title": "Nutch, Gora and MongoDB", "view_count": 948, "owner": {"user_id": 676567, "answer_count": 1, "creation_date": 1301050634, "accept_rate": 45, "view_count": 105, "reputation": 269}, "is_answered": true, "answers": [{"question_id": 27450666, "owner": {"user_id": 582789, "link": "http://stackoverflow.com/users/582789/alfonso-nishikawa", "user_type": "registered", "reputation": 1263}, "body": "<p>Know that in Nutch 2.2.1 you have to change the storage in <code>nutch/conf/gora.properties</code>.</p>\n\n<p>About the testing error, you can do <code>mvn package -DskipTests</code>.</p>\n", "creation_date": 1418415706, "is_accepted": true, "score": 0, "last_activity_date": 1418415706, "answer_id": 27451523}], "question_id": 27450666, "tags": ["nutch", "gora"], "answer_count": 1, "link": "http://stackoverflow.com/questions/27450666/nutch-gora-and-mongodb", "last_activity_date": 1418415706, "accepted_answer_id": 27451523, "body": "<p>I'm trying to run Nutch (2.2.1) on Cygwin/Windows 7 with the latest version of Gora (0.5) so I can persist data to MongoDB datastore. I changed the Nutch-Site.XML File to include my Mongo property:</p>\n\n<pre><code>&lt;property&gt;\n    &lt;name&gt;storage.data.store.class&lt;/name&gt;\n    &lt;value&gt;org.apache.gora.mongodb.store.MongoStore&lt;/value&gt;\n    &lt;description&gt;Default class for storing data&lt;/description&gt;\n&lt;/property&gt;\n</code></pre>\n\n<p>My problem occurs when I try building the \"gora-mongodb\" project from the command line using Maven, 2 of the tests Fail - testCountQuery and testWordCount with the following permissions error: </p>\n\n<pre><code>14/12/12 19:09:28 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n14/12/12 19:09:28 ERROR security.UserGroupInformation: PriviledgedActionException as:MyPC cause:java.io.IOException: Failed to set permissions of path: \\tmp\\hadoop-MyPC\\mapred\\staging\\MyPC555128998\\.staging to 0700\n21738 [main] INFO org.apache.gora.GoraTestDriver - tearing down test  \n</code></pre>\n\n<p>Is it possible to resolve this permissions error without moving over to Ubuntu etc?</p>\n\n<p>Thanks,</p>\n\n<p>O.</p>\n", "creation_date": 1418412076, "score": 1},
{"title": "setting up and running apache nutch 2.2.1", "view_count": 452, "owner": {"user_id": 4340496, "answer_count": 1, "creation_date": 1418112590, "accept_rate": 81, "view_count": 7, "reputation": 96}, "is_answered": true, "answers": [{"question_id": 27374344, "owner": {"user_id": 1965370, "link": "http://stackoverflow.com/users/1965370/do-do", "user_type": "registered", "reputation": 389}, "body": "<p>you should go to <code>$NUTCH_HOME/runtime/deploy</code>  to run the command </p>\n", "creation_date": 1418315758, "is_accepted": true, "score": 1, "last_activity_date": 1418315758, "answer_id": 27427887}], "question_id": 27374344, "tags": ["apache", "hbase", "nutch", "gora"], "answer_count": 1, "link": "http://stackoverflow.com/questions/27374344/setting-up-and-running-apache-nutch-2-2-1", "last_activity_date": 1418315758, "accepted_answer_id": 27427887, "body": "<p>I am trying to set up and run apache nutch 2.2.1 on my ubuntu desktop. As a newbie, I found some parts of the tutorial given by the official website a bit confusing. </p>\n\n<ol>\n<li><p>If I were to run it on my own desktop, is it correct to go to the </p>\n\n<pre><code>$NUTCH_HOME/runtime/local \n</code></pre></li>\n</ol>\n\n<p>to run the bin/nutch command?</p>\n\n<ol start=\"2\">\n<li><p>Where should I put the file named urls? (in which there a seed list seed.txt) Is it under  </p>\n\n<pre><code>$NUTCH_HOME/runtime/local\n</code></pre></li>\n</ol>\n\n<p>If I am in the right directory, I had this problem executing the command </p>\n\n<pre><code>bin/nutch crawl urls -dir crawl -depth 1\n</code></pre>\n\n<p>InjectorJob: Using class org.apache.gora.memory.store.MemStore as the Gora storage class.\nInjectorJob: total number of urls rejected by filters: 0\nInjectorJob: total number of urls injected after normalization and filtering: 0\nException in thread \"main\" java.lang.RuntimeException: job failed: name=generate: null, jobid=job_local1613558008_0002\n    at org.apache.nutch.util.NutchJob.waitForCompletion(NutchJob.java:54)\n    at org.apache.nutch.crawl.GeneratorJob.run(GeneratorJob.java:199)\n    at org.apache.nutch.crawl.Crawler.runTool(Crawler.java:68)\n    at org.apache.nutch.crawl.Crawler.run(Crawler.java:152)\n    at org.apache.nutch.crawl.Crawler.run(Crawler.java:250)\n    at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n    at org.apache.nutch.crawl.Crawler.main(Crawler.java:257)</p>\n\n<p>I am following the tutorial 1 <a href=\"http://wiki.apache.org/nutch/NutchTutorial\" rel=\"nofollow\">http://wiki.apache.org/nutch/NutchTutorial</a> until 3.3\nand have yet to configure GORA Hbase etc. \nIt seems that this problem arises because the injector did not get the urls. \nDoes anyone know how to solve this problem? Thanks a lot! </p>\n", "creation_date": 1418113621, "score": 0},
{"title": "scala nutch gora-cassandra - RuntimeException: job failed", "view_count": 235, "is_answered": false, "question_id": 27380599, "tags": ["cassandra", "runtime-error", "nutch", "gora"], "answer_count": 0, "link": "http://stackoverflow.com/questions/27380599/scala-nutch-gora-cassandra-runtimeexception-job-failed", "last_activity_date": 1418195106, "owner": {"user_id": 1579289, "answer_count": 17, "creation_date": 1344257801, "accept_rate": 91, "view_count": 51, "reputation": 405}, "body": "<p>I'm trying to run nutch and load the crawled data into cassandra.</p>\n\n<p>I've got my sbt file</p>\n\n<pre><code>\"org.apache.gora\" % \"gora-cassandra\" % \"0.3\",\n\"org.apache.nutch\" % \"nutch\" % \"2.2.1\",\n\"com.datastax.cassandra\" % \"cassandra-driver-core\" % \"2.1.2\"\n</code></pre>\n\n<p>and am kicking off the job</p>\n\n<pre><code>ToolRunner.run(NutchConfiguration.create(), new Crawler(), Array(\"urls\"));\n</code></pre>\n\n<p>but am hitting the slightly vague error \nEDIT - updated to be full logs from start of request</p>\n\n<pre><code>[Ljava.lang.String;@526950c7\n****file:/home/abdev/Working/Qordaoba/gl/web-crawling-services/crawling-services/urls\n[error] play - Cannot invoke the action, eventually got an error: java.lang.RuntimeException: job failed: name=generate: null, jobid=job_local_0002\n[error] application - \n\n! @6kemm159h - Internal server error, for (POST) [/nutch/job] -&gt;\n\nplay.api.Application$$anon$1: Execution exception[[RuntimeException: job failed: name=generate: null, jobid=job_local_0002]]\n    at play.api.Application$class.handleError(Application.scala:296) ~[play_2.11-2.3.6.jar:2.3.6]\n    at play.api.DefaultApplication.handleError(Application.scala:402) [play_2.11-2.3.6.jar:2.3.6]\n    at play.core.server.netty.PlayDefaultUpstreamHandler$$anonfun$3$$anonfun$applyOrElse$4.apply(PlayDefaultUpstreamHandler.scala:320) [play_2.11-2.3.6.jar:2.3.6]\n    at play.core.server.netty.PlayDefaultUpstreamHandler$$anonfun$3$$anonfun$applyOrElse$4.apply(PlayDefaultUpstreamHandler.scala:320) [play_2.11-2.3.6.jar:2.3.6]\n    at scala.Option.map(Option.scala:145) [scala-library-2.11.1.jar:na]\nCaused by: java.lang.RuntimeException: job failed: name=generate: null, jobid=job_local_0002\n    at org.apache.nutch.util.NutchJob.waitForCompletion(NutchJob.java:54) ~[nutch-2.2.1.jar:na]\n    at org.apache.nutch.crawl.GeneratorJob.run(GeneratorJob.java:199) ~[nutch-2.2.1.jar:na]\n    at org.apache.nutch.crawl.Crawler.runTool(Crawler.java:68) ~[nutch-2.2.1.jar:na]\n    at org.apache.nutch.crawl.Crawler.run(Crawler.java:152) ~[nutch-2.2.1.jar:na]\n    at org.apache.nutch.crawl.Crawler.run(Crawler.java:250) ~[nutch-2.2.1.jar:na]\n</code></pre>\n\n<p>In cassandra - the <code>keyspace</code> webpage and tables <code>sc  p  f</code> are being created before the error is thrown.</p>\n\n<p>EDIT --- If I put all (sorry its a long list I know) the below jars in my lib folder - then the job runs; and the first few logs are about connecting to cassandra. I don't see those logs when I'm trying to just use the SBT dependencies.</p>\n\n<p>Logs when running with below jar files:</p>\n\n<pre><code>SLF4J: The following set of substitute loggers may have been accessed\nSLF4J: during the initialization phase. Logging calls during this\nSLF4J: phase were not honored. However, subsequent logging calls to these\nSLF4J: loggers will work as normally expected.\nSLF4J: See also http://www.slf4j.org/codes.html#substituteLogger\nSLF4J: org.webjars.WebJarExtractor\n[info] Compiling 5 Scala sources and 1 Java source to /home/abdev/Working/Qordaoba/gl/web-crawling-services/crawling-services/target/scala-2.11/classes...\n14/12/10 07:31:03 INFO play: Application started (Dev)\n14/12/10 07:31:03 INFO slf4j.Slf4jLogger: Slf4jLogger started\n[Ljava.lang.String;@3a6f1296\n14/12/10 07:31:05 INFO connection.CassandraHostRetryService: Downed Host Retry service started with queue size -1 and retry delay 10s\n14/12/10 07:31:05 INFO service.JmxMonitor: Registering JMX me.prettyprint.cassandra.service_Test Cluster:ServiceType=hector,MonitorType=hector\n14/12/10 07:31:06 INFO crawl.InjectorJob: InjectorJob: Using class org.apache.gora.cassandra.store.CassandraStore as the Gora storage class.\n14/12/10 07:31:06 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n14/12/10 07:31:06 INFO input.FileInputFormat: Total input paths to process : 1\n</code></pre>\n\n<p>Full list of Jar files</p>\n\n<pre><code>activation-1.1.jar\nantlr-3.2.jar\naopalliance-1.0.jar\napache-cassandra-1.2.19.jar\napache-cassandra-clientutil-1.2.19.jar\napache-cassandra-thrift-1.2.19.jar\napache-nutch-2.2.1.jar\nasm-3.2.jar\navro-1.3.3.jar\ncommons-beanutils-1.7.0.jar\ncommons-beanutils-core-1.8.0.jar\ncommons-cli-1.1.jar\ncommons-cli-1.2.jar\ncommons-codec-1.2.jar\ncommons-codec-1.4.jar\ncommons-collections-3.2.1.jar\ncommons-configuration-1.6.jar\ncommons-digester-1.8.jar\ncommons-el-1.0.jar\ncommons-httpclient-3.1.jar\ncommons-io-2.4.jar\ncommons-lang-2.6.jar\ncommons-logging-1.1.1.jar\ncommons-math-2.1.jar\ncommons-net-1.4.1.jar\ncompress-lzf-0.8.4.jar\nconcurrentlinkedhashmap-lru-1.3.jar\ncql-internal-only-1.4.1.zip\ncrawler-commons-0.2.jar\ncxf-api-2.5.2.jar\ncxf-common-utilities-2.5.2.jar\ncxf-rt-bindings-xml-2.5.2.jar\ncxf-rt-core-2.5.2.jar\ncxf-rt-frontend-jaxrs-2.5.2.jar\ncxf-rt-transports-common-2.5.2.jar\ncxf-rt-transports-http-2.5.2.jar\nelasticsearch-0.19.4.jar\ngeronimo-javamail_1.4_spec-1.7.1.jar\ngeronimo-stax-api_1.0_spec-1.0.1.jar\ngora-cassandra-0.3.jar\ngora-core-0.3.jar\nguava-11.0.2.jar\nguava-13.0.1.jar\nhadoop-core-1.2.0.jar\nhamcrest-core-1.3.jar\nhector-core-1.1-4.jar\nhigh-scale-lib-1.1.2.jar\nhsqldb-2.2.8.jar\nhttpclient-4.1.1.jar\nhttpcore-4.1.jar\nicu4j-4.0.1.jar\njackson-core-asl-1.8.8.jar\njackson-core-asl-1.9.2.jar\njackson-jaxrs-1.7.1.jar\njackson-mapper-asl-1.8.8.jar\njackson-mapper-asl-1.9.2.jar\njackson-xc-1.7.1.jar\njamm-0.2.5.jar\njaxb-api-2.2.2.jar\njaxb-impl-2.2.3-1.jar\njbcrypt-0.3m.jar\njdom-1.1.jar\njersey-core-1.8.jar\njersey-json-1.8.jar\njersey-server-1.8.jar\njettison-1.3.1.jar\njetty-6.1.26.jar\njetty-client-6.1.26.jar\njetty-sslengine-6.1.26.jar\njetty-util5-6.1.26.jar\njetty-util-6.1.26.jar\njline-0.9.1.jar\njline-1.0.jar\njson-simple-1.1.jar\njsr305-1.3.9.jar\njsr311-api-1.1.1.jar\njunit-4.11.jar\njuniversalchardet-1.0.3.jar\nlibthrift-0.7.0.jar\nlog4j-1.2.16.jar\nlucene-analyzers-3.6.0.jar\nlucene-core-3.6.0.jar\nlucene-highlighter-3.6.0.jar\nlucene-memory-3.6.0.jar\nlucene-queries-3.6.0.jar\nlz4-1.1.0.jar\nmetrics-core-2.2.0.jar\nneethi-3.0.1.jar\norg.osgi.core-4.0.0.jar\norg.restlet.ext.jackson-2.0.5.jar\norg.restlet-2.0.5.jar\noro-2.0.8.jar\nparanamer-2.2.jar\nparanamer-ant-2.2.jar\nparanamer-generator-2.2.jar\nqdox-1.10.1.jar\nserializer-2.7.1.jar\nservlet-api-2.5-6.1.14.jar\nservlet-api-2.5-20081211.jar\nslf4j-api-1.6.6.jar\nslf4j-api-1.7.2.jar\nslf4j-log4j12-1.6.1.jar\nslf4j-log4j12-1.7.2.jar\nsnakeyaml-1.6.jar\nsnappy-java-1.0.5.jar\nsnaptree-0.1.jar\nsolr-solrj-3.4.0.jar\nspring-aop-3.0.6.RELEASE.jar\nspring-asm-3.0.6.RELEASE.jar\nspring-beans-3.0.6.RELEASE.jar\nspring-context-3.0.6.RELEASE.jar\nspring-core-3.0.6.RELEASE.jar\nspring-expression-3.0.6.RELEASE.jar\nspring-web-3.0.6.RELEASE.jar\nstax2-api-3.1.1.jar\nstax-api-1.0.1.jar\nstax-api-1.0-2.jar\nthrift-python-internal-only-0.7.0.zip\ntika-core-1.3.jar\nwoodstox-core-asl-4.1.1.jar\nwsdl4j-1.6.2.jar\nwstx-asl-3.2.7.jar\nxercesImpl-2.9.1.jar\nxml-apis-1.3.04.jar\nxmlenc-0.52.jar\nxmlParserAPIs-2.6.2.jar\nxmlschema-core-2.0.1.jar\nzookeeper-3.3.1.jar\n</code></pre>\n\n<p>Thanks,\nBrent</p>\n", "creation_date": 1418133190, "score": 0},
{"title": "Errors when configuring Apache Nutch crawler", "view_count": 204, "is_answered": false, "answers": [{"question_id": 25448271, "owner": {"user_id": 3333565, "link": "http://stackoverflow.com/users/3333565/user3333565", "user_type": "registered", "reputation": 11}, "body": "<p>I know this is old, but thought it may help someone in the future:\nHave you tried to run:\nant runtime\nrun it from the nutch folder after you changed the config values.</p>\n", "creation_date": 1418161954, "is_accepted": false, "score": 0, "last_activity_date": 1418161954, "answer_id": 27389539}], "question_id": 25448271, "tags": ["java", "nutch", "web-crawler"], "answer_count": 1, "link": "http://stackoverflow.com/questions/25448271/errors-when-configuring-apache-nutch-crawler", "last_activity_date": 1418161954, "owner": {"user_id": 300327, "answer_count": 45, "creation_date": 1269379533, "accept_rate": 40, "view_count": 224, "reputation": 1084}, "body": "<p>Nutch Gurus,</p>\n\n<p>I am having some trouble running Nutch on a linux server and I hope that you someone can help. I am trying to crawl URLs configured in seed.txt, but I am seeing the following errors. The crawler is  triggered as follows</p>\n\n<pre><code>nohup java  -classpath \"./common-conf/*:*:./plugins/*:\" -jar  crawler-jar-2.0-SNAPSHOT.jar &amp;\n</code></pre>\n\n<p>In this configuration, all the configuration properties are present in <strong>common-conf</strong> directory.  We have some custom configuration that we have set up in our Crawler binary. As a result, we have built a custom binary and don't use standard Apache nutch crawler. I see the following issues:</p>\n\n<ol>\n<li>Our custom <strong>nutch-default.xml</strong> and <strong>nutch-site.xml</strong> are not picked from the <strong>common-conf</strong> classpath directory. They are being picked up from nutch jar file. When I print out the URL path for the both the xmls, I see something like this</li>\n</ol>\n\n<blockquote>\n  <p>nutch default =\n  jar:file:/home/nbsxlwa/crawler/lib/nutch-2.2.1.jar!/nutch-default.xml\n  nutch site =\n  jar:file:/home/nbsxlwa/crawler/lib/nutch-2.2.1.jar!/nutch-site.xml</p>\n</blockquote>\n\n<p>I want the files to be picked up from classpath. I can verify that the files exist.</p>\n\n<ol>\n<li>Our custom <strong>gora.properties</strong> is not being picked up. I see the following log trace</li>\n</ol>\n\n<p><strong>14/08/22 07:18:24 WARN store.DataStoreFactory: gora.properties not found, properties will be empty.\n14/08/22 07:18:24 INFO crawl.InjectorJob: InjectorJob: Using class org.apache.gora.memory.store.MemStore as the Gora storage class.</strong></p>\n\n<p>gora.properties exists in the classpath and I am not sure why it is not being picked up.</p>\n\n<pre><code>/home/nbsxlwa/crawler/ find . -name \"gora.properties\"\n./common-conf/gora.properties\n</code></pre>\n\n<ol>\n<li><strong>http.agent.name</strong> configuration property is not being picked up. I can confirm that the configuration exists in nutch-site.xml</li>\n</ol>\n\n<p>The stack trace is given below.</p>\n\n<pre><code>14/08/22 07:18:36 ERROR fetcher.FetcherJob: Fetcher: No agents listed in 'http.agent.name' property.\n14/08/22 07:18:36 WARN crawl.Crawler: Error running crawler job for configuration. Tool run command raises an exception\njava.lang.IllegalArgumentException: Fetcher: No agents listed in 'http.agent.name' property.\n        at org.apache.nutch.fetcher.FetcherJob.checkConfiguration(FetcherJob.java:252)\n        at org.apache.nutch.fetcher.FetcherJob.run(FetcherJob.java:160)\n        at org.apache.nutch.crawl.Crawler.runTool(Crawler.java:78)\n        at org.apache.nutch.crawl.Crawler.run(Crawler.java:176)\n        at org.apache.nutch.crawl.Crawler.run(Crawler.java:266)\n        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n        at org.apache.nutch.crawl.Crawler.main(Crawler.java:356)\n</code></pre>\n\n<p>1.<strong>regex-normalize.xml</strong> and <strong>regex-urlfilter.txt</strong> are not being picked up from the classpath. I can confirm that the files exist in my classpath. The stack trace is given below</p>\n\n<pre><code>/home/nbsxlwa/crawler : find . -name \"regex-normalize.xml\"\n./common-conf/regex-normalize.xml\n\n/home/nbsxlwa/crawler : find . -name \"regex-urlfilter.txt\"\n./common-conf/regex-urlfilter.txt\n\n\n14/08/22 07:18:29 INFO conf.Configuration: regex-normalize.xml not found\n14/08/22 07:18:29 WARN regex.RegexURLNormalizer: Can't load the default rules! \n14/08/22 07:18:29 INFO conf.Configuration: regex-urlfilter.txt not found\n14/08/22 07:18:29 INFO conf.Configuration: regex-normalize.xml not found\n14/08/22 07:18:29 WARN regex.RegexURLNormalizer: Can't load the default rules!\n</code></pre>\n\n<p>I have gone through the following links to see where I am going wrong. How do I set up Nutch configuration settings here? Please advise.</p>\n\n<ol>\n<li><a href=\"http://mail-archives.apache.org/mod_mbox/nutch-user/201202.mbox/%3CCAGaRif3rtJHokgG5FHSbnJLLUAVGiDnfx7JaW-7kiBjx_ivwSg@mail.gmail.com%3E\" rel=\"nofollow\">http://mail-archives.apache.org/mod_mbox/nutch-user/201202.mbox/%3CCAGaRif3rtJHokgG5FHSbnJLLUAVGiDnfx7JaW-7kiBjx_ivwSg@mail.gmail.com%3E</a> and</li>\n<li><a href=\"http://osdir.com/ml/user.nutch.apache/2012-02/msg00127.html\" rel=\"nofollow\">http://osdir.com/ml/user.nutch.apache/2012-02/msg00127.html</a></li>\n</ol>\n\n<p>I would appreciate any assistance in this matter.</p>\n\n<p>Thanks,</p>\n", "creation_date": 1408714310, "score": 0},
{"title": "How to change configuration of apache nutch when it is crawling", "view_count": 61, "is_answered": false, "answers": [{"question_id": 27142429, "owner": {"user_id": 1965370, "link": "http://stackoverflow.com/users/1965370/do-do", "user_type": "registered", "reputation": 389}, "body": "<p>Nutch 2.2.1 crawling is a loop of Hadoop jobs, we can change the configuration of the Nutch crawler during runtime, however the changing only is activated in the next Hadoop job. For example, if you change the configuration during generating job, the changing is activated in fetching job.</p>\n\n<p>Hope this helps,</p>\n\n<p>Le Quoc Do   </p>\n", "creation_date": 1418148669, "is_accepted": false, "score": 0, "last_activity_date": 1418148669, "answer_id": 27385853}], "question_id": 27142429, "tags": ["apache", "configuration", "web-crawler", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/27142429/how-to-change-configuration-of-apache-nutch-when-it-is-crawling", "last_activity_date": 1418148669, "owner": {"user_id": 3454410, "answer_count": 20, "creation_date": 1395646154, "accept_rate": 48, "view_count": 174, "reputation": 832}, "body": "<p>My crawler (apache nutch2.2.1) is in crawling state. I have to change some configurations of crawler in nutch-site.xml. I have come to know that when crawler is in running state, avoid to change configuration.</p>\n\n<p>My question is.</p>\n\n<ol>\n<li>Can we change configurations of crawler in running state?</li>\n<li>If yes then is there any cations when doing some changes in crawler?</li>\n<li>or If we could not change configuration of crawler, then what are its drawbacks if configurations are changed?</li>\n</ol>\n", "creation_date": 1416982158, "score": 0},
{"title": "How to Set topN via nutch crawl SCRIPT", "view_count": 188, "is_answered": false, "answers": [{"question_id": 27362491, "owner": {"user_id": 1965370, "link": "http://stackoverflow.com/users/1965370/do-do", "user_type": "registered", "reputation": 389}, "body": "<p>Please verify the property <code>db.max.outlinks.per.page</code> in the nutch configuration.\nChange this value to a higher number or to <code>-1</code> to have all the urls crawled and indexed.</p>\n\n<p>Hope this helps,</p>\n\n<p>Le Quoc Do</p>\n", "creation_date": 1418147844, "is_accepted": false, "score": 0, "last_activity_date": 1418147844, "answer_id": 27385597}], "question_id": 27362491, "tags": ["solr", "web-crawler", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/27362491/how-to-set-topn-via-nutch-crawl-script", "last_activity_date": 1418147844, "owner": {"age": 23, "answer_count": 0, "creation_date": 1417935302, "user_id": 4333712, "view_count": 0, "reputation": 1}, "body": "<p>I am trying to crawl a webpage whose url is <a href=\"http://def.com/xyz/(say)\" rel=\"nofollow\">http://def.com/xyz/(say)</a> which has more than 2000 outgoing urls, but when I query the solr it is showing less than 50 documents whereas I am expecting around 2000\ndocuments. \nI am using the Following query :    </p>\n\n<pre><code>./crawl urls TestCrawl http://localhost:8983/solr/ -depth 2 -topN 3000          \n</code></pre>\n\n<p>The console output is :</p>\n\n<pre><code>Injector: starting at 2014-12-08 21:36:15\nInjector: crawlDb: TestCrawl/crawldb\nInjector: urlDir: urls\nInjector: Converting injected urls to crawl db entries.\nInjector: Total number of urls rejected by filters: 0\nInjector: Total number of urls after normalization: 1\nInjector: Merging injected urls into crawl db.\nInjector: overwrite: false\nInjector: update: false\nInjector: URLs merged: 1\nInjector: Total new urls injected: 0\nInjector: finished at 2014-12-08 21:36:18, elapsed: 00:00:02\n</code></pre>\n\n<p>I am assuming that somehow nutch is not being able to get topN value from crawl script.</p>\n", "creation_date": 1418056456, "score": 0},
{"title": "Nutch 1.9 command crawl is only fetching one level", "view_count": 1660, "is_answered": true, "answers": [{"question_id": 27350063, "owner": {"user_id": 3496666, "accept_rate": 72, "link": "http://stackoverflow.com/users/3496666/kumar", "user_type": "registered", "reputation": 1801}, "body": "<p>Try to use individual commands for web crawling. Then check how many pages could crawl in second run. If its 0 pages then check your include path in (should be like <strong>+^<a href=\"http://www.google.com/\" rel=\"nofollow\">http://www.google.com/</a></strong>) regex-urlfilter.txt.</p>\n\n<p>Refer how to run <a href=\"http://wiki.apache.org/nutch/NutchTutorial#A3.4_Using_Individual_Commands_for_Whole-Web_Crawling\" rel=\"nofollow\">Individual command</a></p>\n", "creation_date": 1418015852, "is_accepted": false, "score": 1, "last_activity_date": 1418015852, "answer_id": 27351826}, {"last_edit_date": 1418128583, "owner": {"user_id": 4338097, "link": "http://stackoverflow.com/users/4338097/pengcheng-liu", "user_type": "registered", "reputation": 11}, "body": "<p>It is working now, the first round only 1 page is fetched and second round is fetched lots of pages, I guess the number of rounds is same as depth.</p>\n", "question_id": 27350063, "creation_date": 1418074115, "is_accepted": false, "score": 1, "last_activity_date": 1418128583, "answer_id": 27367283}], "question_id": 27350063, "tags": ["web-crawler", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/27350063/nutch-1-9-command-crawl-is-only-fetching-one-level", "last_activity_date": 1418128583, "owner": {"user_id": 4335610, "view_count": 3, "answer_count": 0, "creation_date": 1418000143, "reputation": 1}, "body": "<p>I am a rookie in nutch. After few weeks playing with it. I can finally start crawling.</p>\n\n<p>I installed nutch 1.9 and solr 4.1 and my seed.txt file only containing 1 url, and my regex-urlfiler.txt is set to accept everything. I am running this command:</p>\n\n<pre><code>bin/crawl urls crawl http://104.131.94.**:8983/solr/ 1 -depth 3 -topN 5\n</code></pre>\n\n<p>Here is the output:</p>\n\n<pre><code>Injector: starting at 2014-12-07 18:41:31\nInjector: crawlDb: crawl/crawldb\nInjector: urlDir: urls\nInjector: Converting injected urls to crawl db entries.\nInjector: overwrite: false\nInjector: update: false\nInjector: Total number of urls rejected by filters: 0\nInjector: Total number of urls after normalization: 1\nInjector: Total new urls injected: 1\nInjector: finished at 2014-12-07 18:41:33, elapsed: 00:00:01\nSun Dec 7 18:41:33 EST 2014 : Iteration 1 of 1\nGenerating a new segment\nGenerator: starting at 2014-12-07 18:41:34\nGenerator: Selecting best-scoring urls due for fetch.\nGenerator: filtering: false\nGenerator: normalizing: true\nGenerator: topN: 50000\nGenerator: Partitioning selected urls for politeness.\nGenerator: segment: crawl/segments/20141207184137\nGenerator: finished at 2014-12-07 18:41:38, elapsed: 00:00:03\nOperating on segment : 20141207184137\nFetching : 20141207184137\nFetcher: starting at 2014-12-07 18:41:39\nFetcher: segment: crawl/segments/20141207184137\nFetcher Timelimit set for : 1418006499487\nUsing queue mode : byHost\nFetcher: threads: 50\nFetcher: time-out divisor: 2\nQueueFeeder finished: total 1 records + hit by time limit :0\nUsing queue mode : byHost\nUsing queue mode : byHost\nfetching http://www.wenxuecity.com/ (queue crawl delay=5000ms)\nThread FetcherThread has no more work available\n-finishing thread FetcherThread, activeThreads=1\nUsing queue mode : byHost\nThread FetcherThread has no more work available\n-finishing thread FetcherThread, activeThreads=1\nUsing queue mode : byHost\nThread FetcherThread has no more work available\n-finishing thread FetcherThread, activeThreads=1\nUsing queue mode : byHost\nThread FetcherThread has no more work available\n-finishing thread FetcherThread, activeThreads=1\nUsing queue mode : byHost\nThread FetcherThread has no more work available\n-finishing thread FetcherThread, activeThreads=1\nUsing queue mode : byHost\nThread FetcherThread has no more work available\n-finishing thread FetcherThread, activeThreads=1\nUsing queue mode : byHost\nThread FetcherThread has no more work available\n-finishing thread FetcherThread, activeThreads=1\nUsing queue mode : byHost\nThread FetcherThread has no more work available\n-finishing thread FetcherThread, activeThreads=1\nUsing queue mode : byHost\nThread FetcherThread has no more work available\n-finishing thread FetcherThread, activeThreads=1\nUsing queue mode : byHost\nThread FetcherThread has no more work available\n-finishing thread FetcherThread, activeThreads=1\nUsing queue mode : byHost\nUsing queue mode : byHost\nThread FetcherThread has no more work available\n-finishing thread FetcherThread, activeThreads=1\nUsing queue mode : byHost\nUsing queue mode : byHost\nThread FetcherThread has no more work available\n-finishing thread FetcherThread, activeThreads=1\nThread FetcherThread has no more work available\n-finishing thread FetcherThread, activeThreads=1\nThread FetcherThread has no more work available\n-finishing thread FetcherThread, activeThreads=1\nUsing queue mode : byHost\nUsing queue mode : byHost\nUsing queue mode : byHost\nThread FetcherThread has no more work available\n-finishing thread FetcherThread, activeThreads=1\nUsing queue mode : byHost\nThread FetcherThread has no more work available\n-finishing thread FetcherThread, activeThreads=1\nThread FetcherThread has no more work available\n-finishing thread FetcherThread, activeThreads=1\nUsing queue mode : byHost\nThread FetcherThread has no more work available\n-finishing thread FetcherThread, activeThreads=1\nUsing queue mode : byHost\nUsing queue mode : byHost\nUsing queue mode : byHost\nUsing queue mode : byHost\nUsing queue mode : byHost\nUsing queue mode : byHost\nThread FetcherThread has no more work available\n-finishing thread FetcherThread, activeThreads=6\nUsing queue mode : byHost\nThread FetcherThread has no more work available\nThread FetcherThread has no more work available\n-finishing thread FetcherThread, activeThreads=5\nThread FetcherThread has no more work available\nThread FetcherThread has no more work available\n-finishing thread FetcherThread, activeThreads=3\nThread FetcherThread has no more work available\n-finishing thread FetcherThread, activeThreads=2\nThread FetcherThread has no more work available\nThread FetcherThread has no more work available\n-finishing thread FetcherThread, activeThreads=2\n-finishing thread FetcherThread, activeThreads=5\n-finishing thread FetcherThread, activeThreads=1\n-finishing thread FetcherThread, activeThreads=4\nUsing queue mode : byHost\nThread FetcherThread has no more work available\n-finishing thread FetcherThread, activeThreads=1\nUsing queue mode : byHost\nThread FetcherThread has no more work available\n-finishing thread FetcherThread, activeThreads=1\nUsing queue mode : byHost\nThread FetcherThread has no more work available\n-finishing thread FetcherThread, activeThreads=1\nUsing queue mode : byHost\nThread FetcherThread has no more work available\n-finishing thread FetcherThread, activeThreads=1\nUsing queue mode : byHost\nThread FetcherThread has no more work available\n-finishing thread FetcherThread, activeThreads=1\nUsing queue mode : byHost\nThread FetcherThread has no more work available\n-finishing thread FetcherThread, activeThreads=1\nUsing queue mode : byHost\nThread FetcherThread has no more work available\n-finishing thread FetcherThread, activeThreads=1\nUsing queue mode : byHost\nThread FetcherThread has no more work available\n-finishing thread FetcherThread, activeThreads=1\nUsing queue mode : byHost\nThread FetcherThread has no more work available\n-finishing thread FetcherThread, activeThreads=1\nUsing queue mode : byHost\nThread FetcherThread has no more work available\n-finishing thread FetcherThread, activeThreads=1\nUsing queue mode : byHost\nThread FetcherThread has no more work available\n-finishing thread FetcherThread, activeThreads=1\nUsing queue mode : byHost\nThread FetcherThread has no more work available\n-finishing thread FetcherThread, activeThreads=1\nUsing queue mode : byHost\nThread FetcherThread has no more work available\n-finishing thread FetcherThread, activeThreads=1\nUsing queue mode : byHost\nThread FetcherThread has no more work available\n-finishing thread FetcherThread, activeThreads=1\nUsing queue mode : byHost\nThread FetcherThread has no more work available\n-finishing thread FetcherThread, activeThreads=1\nUsing queue mode : byHost\nThread FetcherThread has no more work available\n-finishing thread FetcherThread, activeThreads=1\nUsing queue mode : byHost\nThread FetcherThread has no more work available\n-finishing thread FetcherThread, activeThreads=1\nUsing queue mode : byHost\nThread FetcherThread has no more work available\n-finishing thread FetcherThread, activeThreads=1\nUsing queue mode : byHost\nThread FetcherThread has no more work available\n-finishing thread FetcherThread, activeThreads=1\nUsing queue mode : byHost\nThread FetcherThread has no more work available\n-finishing thread FetcherThread, activeThreads=1\nUsing queue mode : byHost\nThread FetcherThread has no more work available\n-finishing thread FetcherThread, activeThreads=1\nUsing queue mode : byHost\nThread FetcherThread has no more work available\n-finishing thread FetcherThread, activeThreads=1\nUsing queue mode : byHost\nFetcher: throughput threshold: -1\nThread FetcherThread has no more work available\nFetcher: throughput threshold retries: 5\n-finishing thread FetcherThread, activeThreads=1\nfetcher.maxNum.threads can't be &lt; than 50 : using 50 instead\nThread FetcherThread has no more work available\n-finishing thread FetcherThread, activeThreads=0\n-activeThreads=0, spinWaiting=0, fetchQueues.totalSize=0, fetchQueues.getQueueCount=0\n-activeThreads=0\nFetcher: finished at 2014-12-07 18:41:42, elapsed: 00:00:02\nParsing : 20141207184137\nParseSegment: starting at 2014-12-07 18:41:43\nParseSegment: segment: crawl/segments/20141207184137\nParsed (17ms):http://www.wenxuecity.com/\nParseSegment: finished at 2014-12-07 18:41:46, elapsed: 00:00:02\nCrawlDB update\nCrawlDb update: starting at 2014-12-07 18:41:48\nCrawlDb update: db: crawl/crawldb\nCrawlDb update: segments: [crawl/segments/20141207184137]\nCrawlDb update: additions allowed: true\nCrawlDb update: URL normalizing: false\nCrawlDb update: URL filtering: false\nCrawlDb update: 404 purging: false\nCrawlDb update: Merging segment data into db.\nCrawlDb update: finished at 2014-12-07 18:41:49, elapsed: 00:00:01\nLink inversion\nLinkDb: starting at 2014-12-07 18:41:51\nLinkDb: linkdb: crawl/linkdb\nLinkDb: URL normalize: true\nLinkDb: URL filter: true\nLinkDb: internal links will be ignored.\nLinkDb: adding segment: crawl/segments/20141207184137\nLinkDb: finished at 2014-12-07 18:41:52, elapsed: 00:00:01\nDedup on crawldb\nIndexing 20141207184137 on SOLR index -&gt; http://104.131.94.36:8983/solr/\nIndexer: starting at 2014-12-07 18:41:58\nIndexer: deleting gone documents: false\nIndexer: URL filtering: false\nIndexer: URL normalizing: false\nActive IndexWriters :\nSOLRIndexWriter\n        solr.server.url : URL of the SOLR instance (mandatory)\n        solr.commit.size : buffer size when sending to SOLR (default 1000)\n        solr.mapping.file : name of the mapping file for fields (default solrindex-mapping.xml)\n        solr.auth : use authentication (default false)\n        solr.auth.username : use authentication (default false)\n        solr.auth : username for authentication\n        solr.auth.password : password for authentication\n\n\nIndexer: finished at 2014-12-07 18:42:01, elapsed: 00:00:03\nCleanup on SOLR index -&gt; http://104.131.94.36:8983/solr/\n</code></pre>\n\n<p>There are couple issues here:</p>\n\n<ol>\n<li><p>the crawl didn't take my topN 5 instead it use topN =50000, then I took a look at the crawl script it is hard coded to 50000 doesn't really take -topN parameters. I guess I can just modify the script.</p></li>\n<li><p>the depth 3 is also get ignored, and seems to me there is also no parameter in the script to take care the depth.  </p></li>\n</ol>\n\n<p>I can see a lot of examples are running command nutch crawl, but with 1.9 the command cannot be used anymore. I am really stuck here, any suggestion would be appreciated.</p>\n\n<p>The solr indexing is working fine, I always got 1 documents indexed. And I tried several crawl-able websites, the script always stopped at the first level.</p>\n\n<p>Thanks\nPengcheng   </p>\n", "creation_date": 1418002156, "score": 0},
{"title": "Nutch Crawler error: Premission denied", "view_count": 445, "owner": {"age": 26, "answer_count": 3, "creation_date": 1352245073, "user_id": 1804659, "accept_rate": 75, "view_count": 24, "location": "Israel", "reputation": 61}, "is_answered": true, "answers": [{"question_id": 27200873, "owner": {"user_id": 1965370, "link": "http://stackoverflow.com/users/1965370/do-do", "user_type": "registered", "reputation": 389}, "body": "<p>This is a permission problem. You should set Read,Write and Execute permission to the folder (<code>&lt;name&gt;hadoop.tmp.dir&lt;/name&gt;</code> value in the Hadoop configuration file <em>core-site.xml</em>).</p>\n\n<p>Hope this help,</p>\n\n<p>Le Quoc Do</p>\n", "creation_date": 1418049188, "is_accepted": true, "score": 0, "last_activity_date": 1418049188, "answer_id": 27360277}], "question_id": 27200873, "tags": ["java", "hadoop", "web-crawler", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/27200873/nutch-crawler-error-premission-denied", "last_activity_date": 1418049188, "accepted_answer_id": 27360277, "body": "<p>I am trying to run a basic crawler. Got the command from the NutchTutorial: bin/crawl urls -dir crawl -depth 3 -topN 5</p>\n\n<p>(after doing all the presets)</p>\n\n<p>Im running from windows so I've installed cygwin64 as a running environment</p>\n\n<p>I don't see any problems when I run bin/nutch from the nutch home directory, but when I try to run the crawl as above I get the following error:</p>\n\n<pre><code>Injector: starting at 2014-11-29 11:31:35\nInjector: crawlDb: -dir/crawldb\nInjector: urlDir: urls\nInjector: Converting injected urls to crawl db entries.\nInjector: java.io.IOException: Failed to set permissions of path: \\tmp\\hadoop-Er\nan\\mapred\\staging\\Eran996102549\\.staging to 0700\n        at org.apache.hadoop.fs.FileUtil.checkReturnValue(FileUtil.java:691)\n        at org.apache.hadoop.fs.FileUtil.setPermission(FileUtil.java:664)\n        at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSys\ntem.java:514)\n        at org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.jav\na:349)\n        at org.apache.hadoop.fs.FilterFileSystem.mkdirs(FilterFileSystem.java:19\n3)\n        at org.apache.hadoop.mapreduce.JobSubmissionFiles.getStagingDir(JobSubmi\nssionFiles.java:126)\n        at org.apache.hadoop.mapred.JobClient$2.run(JobClient.java:942)\n        at org.apache.hadoop.mapred.JobClient$2.run(JobClient.java:936)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Unknown Source)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInforma\ntion.java:1190)\n        at org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:9\n36)\n        at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:910)\n        at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:1353)\n        at org.apache.nutch.crawl.Injector.inject(Injector.java:324)\n        at org.apache.nutch.crawl.Injector.run(Injector.java:380)\n        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n        at org.apache.nutch.crawl.Injector.main(Injector.java:370)\n</code></pre>\n\n<p>Theres no reference to that error on thier tutorial.\nWhat do I do ?</p>\n", "creation_date": 1417254054, "score": 0},
{"title": "apache nutch taking too long time in generate phase", "view_count": 188, "is_answered": false, "answers": [{"question_id": 26460815, "owner": {"user_id": 1965370, "link": "http://stackoverflow.com/users/1965370/do-do", "user_type": "registered", "reputation": 389}, "body": "<p>Before fetching job, the generating job is performed in Nutch. In the generating job, Nutch will select topN URLs, which have the highest scores among all URLs in CrawlDB, for fetching. Therefore the reason of your crawler taking too long time before fetching would be you set topN is too high compared to your system capacity, and the number of URLs in crawlDB is large (selecting process will take time).</p>\n\n<p>Hope this helps</p>\n\n<p>Le Quoc Do</p>\n", "creation_date": 1418048421, "is_accepted": false, "score": 0, "last_activity_date": 1418048421, "answer_id": 27360023}], "question_id": 26460815, "tags": ["apache", "web-crawler", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/26460815/apache-nutch-taking-too-long-time-in-generate-phase", "last_activity_date": 1418048421, "owner": {"user_id": 3454410, "answer_count": 20, "creation_date": 1395646154, "accept_rate": 48, "view_count": 174, "reputation": 832}, "body": "<p>I have two urls in my urls/seed file. My crawler taking too ling time before it start fetching. My already crawled data is about 220 GB . Any idea why nutch is behaving like this</p>\n", "creation_date": 1413791616, "score": 0},
{"title": "How to allow apache nutch to crawl forever", "view_count": 93, "owner": {"user_id": 3454410, "answer_count": 20, "creation_date": 1395646154, "accept_rate": 48, "view_count": 174, "reputation": 832}, "is_answered": true, "answers": [{"question_id": 26462156, "owner": {"user_id": 1965370, "link": "http://stackoverflow.com/users/1965370/do-do", "user_type": "registered", "reputation": 389}, "body": "<p>If you want to crawl forever, the following is the script you need:</p>\n\n<pre><code>#!/bin/bash\n\n./bin/nutch inject urls #urls is the seed data\nwhile [ 1 == 1 ]\ndo\n   ./bin/nutch generate -topN 10000 # 10000 is the number of URLs will be fetch in each crawling round, you can modify it\n   ./bin/nutch fetch -all\n   ./bin/nutch parse -all\n   ./bin/nutch updatedb\n\ndone\n</code></pre>\n\n<p>Hope this helps</p>\n\n<p>Le Quoc Do</p>\n", "creation_date": 1417973823, "is_accepted": true, "score": 1, "last_activity_date": 1417973823, "answer_id": 27345618}], "question_id": 26462156, "tags": ["apache", "web-crawler", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/26462156/how-to-allow-apache-nutch-to-crawl-forever", "last_activity_date": 1417973823, "accepted_answer_id": 27345618, "body": "<p>I am using apache nutch (2.2.1) for crawling. What changes are need if I want to crawl forever. Guide me completely as I am not familiar with nutch too mutch.</p>\n", "creation_date": 1413796591, "score": 0},
{"title": "Error Nutch No agents listed in &#39;http.agent.name&#39;", "view_count": 559, "owner": {"user_id": 3454410, "answer_count": 20, "creation_date": 1395646154, "accept_rate": 48, "view_count": 174, "reputation": 832}, "is_answered": true, "answers": [{"question_id": 27104145, "owner": {"user_id": 4331856, "link": "http://stackoverflow.com/users/4331856/hqc", "user_type": "registered", "reputation": 36}, "body": "<p>You shoule add the property of \"http.robots.agents\" and put the value of http.agent.name as the first agent name, and keep the default * at the end of the list.just like:</p>\n\n<pre><code>&lt;property&gt;\n     &lt;name&gt;http.robots.agents&lt;/name&gt;\n     &lt;value&gt;nutch-spider-2.2.1,*&lt;/value&gt;\n&lt;/property&gt;\n</code></pre>\n", "creation_date": 1417959221, "is_accepted": true, "score": 2, "last_activity_date": 1417959221, "answer_id": 27343156}], "question_id": 27104145, "tags": ["apache", "web-crawler", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/27104145/error-nutch-no-agents-listed-in-http-agent-name", "last_activity_date": 1417959221, "accepted_answer_id": 27343156, "body": "<p>I am using nutch2.2.1. Log file is generating following error</p>\n\n<p>ERROR protocol.RobotRulesParser - Agent we advertise (nutch-spider-2.2.1) not listed first in 'http.robots.agents' property!</p>\n\n<p>My nutch-site.xml is (for above property)</p>\n\n<pre><code>&lt;property&gt;\n&lt;name&gt;http.agent.name&lt;/name&gt;\n&lt;value&gt;nutch-spider-2.2.1&lt;/value&gt;\n&lt;/property&gt;\n</code></pre>\n\n<p>my nutch-default.xml is</p>\n\n<pre><code>&lt;property&gt;\n&lt;name&gt;http.agent.name&lt;/name&gt;\n&lt;value&gt;&lt;/value&gt;\n&lt;/property&gt;\n</code></pre>\n\n<p>Where is actual problem? Please guide it clearly(properly explaination).\nThis question is posted <a href=\"http://stackoverflow.com/questions/6582934/nutch-no-agents-listed-in-http-agent-name\">here</a> but I have to bounty this question (if needed) that's why posting it again.</p>\n", "creation_date": 1416829716, "score": 2},
{"title": "HTTPS crawling issue with Nutch", "view_count": 665, "owner": {"age": 28, "answer_count": 16, "creation_date": 1398342846, "user_id": 3568831, "accept_rate": 71, "view_count": 51, "location": "Cameroon", "reputation": 137}, "is_answered": true, "answers": [{"question_id": 27297622, "owner": {"user_id": 3568831, "accept_rate": 71, "link": "http://stackoverflow.com/users/3568831/nwawel-a-iroume", "user_type": "registered", "reputation": 137}, "body": "<p>I got the solution for crawling the website which has default certificate and i hope this may help others people who will encounter this problem.</p>\n\n<p>Some Post in this forum mentionned about adding argument <code>-Djsse.enableSNIExtension=false</code>\nbut where to put that?\nI edited the nucth file using nano and added this argument in <strong>NUTCH_OPTS</strong>\nin nutch 1.9 it is on  line 195 which is now</p>\n\n<pre><code>NUTCH_OPTS=($NUTCH_OPTS -Dhadoop.log.dir=\"$NUTCH_LOG_DIR\" -Djsse.enableSNIExtension=false)\n</code></pre>\n\n<p>after that the crawling got success without breaking</p>\n", "creation_date": 1417723993, "is_accepted": true, "score": 0, "last_activity_date": 1417723993, "answer_id": 27303165}], "question_id": 27297622, "tags": ["ssl", "web-crawler", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/27297622/https-crawling-issue-with-nutch", "last_activity_date": 1417723993, "accepted_answer_id": 27303165, "body": "<p>He guy i want to crawl https sites using nutch 1.9 with java7</p>\n\n<p>in seed.txt</p>\n\n<pre><code>https://site.com\n</code></pre>\n\n<p>in regex-urlfilter.txt</p>\n\n<pre><code>+^https://([a-z0-9]*\\.)*site.com/\n</code></pre>\n\n<p>but when running crawlig process using <code>bin/crawl ...</code> i got a <strong>javax.net.ssl.SSLProtocolException: handshake alert:  unrecognized_name</strong></p>\n", "creation_date": 1417706545, "score": -1},
{"title": "Nutch regex-urlfilter crawl multiple website", "view_count": 470, "is_answered": false, "answers": [{"question_id": 25895105, "owner": {"user_id": 3568831, "accept_rate": 71, "link": "http://stackoverflow.com/users/3568831/nwawel-a-iroume", "user_type": "registered", "reputation": 137}, "body": "<p>Use these regular expressions in regex-urlfilter.txt</p>\n\n<p>solution1</p>\n\n<pre><code>+^http://([a-z0-9]*\\.)*a.b.c/\n+^http://([a-z0-9]*\\.)*d.e.f/\n</code></pre>\n\n<p>solution2</p>\n\n<pre><code>+^http://([a-z0-9]*\\.)*(a.b.c|d.e.f)/\n</code></pre>\n", "creation_date": 1417705629, "is_accepted": false, "score": 0, "last_activity_date": 1417705629, "answer_id": 27297311}], "question_id": 25895105, "tags": ["regex", "url", "web-crawler", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/25895105/nutch-regex-urlfilter-crawl-multiple-website", "last_activity_date": 1417705629, "owner": {"user_id": 2445001, "answer_count": 1, "creation_date": 1370173486, "accept_rate": 29, "view_count": 16, "reputation": 133}, "body": "<p>I've seen this <a href=\"http://stackoverflow.com/questions/10847893/nutch-does-not-crawl-multiple-sites\">link</a>. But my problems is quite different from that.\n<p> My seed.txt looks like:</p>\n\n<pre><code>http://a.b.c/ \nhttp://d.e.f/\n</code></pre>\n\n<p>And my regex-urlfilter.txt looks like this:</p>\n\n<pre><code># skip file: ftp: and mailto: urls\n-^(file|ftp|mailto):\n\n# skip image and other suffixes we can't yet parse\n# for a more extensive coverage use the urlfilter-suffix plugin\n-\\.(gif|GIF|jpg|JPG|png|PNG|ico|ICO|css|CSS|sit|SIT|eps|EPS|wmf|WMF|zip|ZIP|ppt|PPT|mpg|MPG|xls|XLS|gz|GZ|rpm|RPM|tgz|TGZ|mov|MOV|exe|EXE|jpeg|JPEG|bmp|BMP|js|JS)$\n\n# skip URLs containing certain characters as probable queries, etc.\n-[?*!@=]\n\n# skip URLs with slash-delimited segment that repeats 3+ times, to break loops\n#-.*(/[^/]+)/[^/]+\\1/[^/]+\\1/\n\n# accept anything else\n+^http://a.b.c/*\n</code></pre>\n\n<p>I want to crawl some url like this:</p>\n\n<pre><code>http://a.b.c/index.php?id=1\nhttp://a.b.c/about.php\nhttp://a.b.c/help.html\nhttp://a.b.c/test1/test2/\nhttp://a.b.c/index.php?usv=contact\nhttp://a.b.c/index.php?usv=vdetailpro&amp;id=104&amp;sid=74\n</code></pre>\n\n<p>and something like that</p>\n\n<p>I've tested by command: <code>bin/nutch org.apache.nutch.net.URLFilterChecker -allCombined</code>\nand recognized that regex isn't match.</p>\n\n<p><p>Thanks you!</p>\n", "creation_date": 1410969111, "score": 1},
{"title": "Using nutch in Windows 7", "view_count": 839, "is_answered": false, "answers": [{"question_id": 14018187, "owner": {"user_id": 525590, "accept_rate": 75, "link": "http://stackoverflow.com/users/525590/jpee", "user_type": "registered", "reputation": 111}, "body": "<p>We had same issues in till Nutch 1.3, we recommend using a ubuntu vm environment with virtual box for development and a shared directory so you can develop with your ide on windows and deploy to your vm and run Nutch on Linux.</p>\n", "creation_date": 1362774244, "is_accepted": false, "score": 0, "last_activity_date": 1362774244, "answer_id": 15302664}, {"question_id": 14018187, "owner": {"user_id": 2095321, "accept_rate": 100, "link": "http://stackoverflow.com/users/2095321/mateva", "user_type": "registered", "reputation": 336}, "body": "<p>Did you try <a href=\"http://wiki.apache.org/nutch/GettingNutchRunningWithWindows\" rel=\"nofollow\">GettingNutchRunningWithWindows from the Nutch Wiki</a>?</p>\n\n<p>Some of my students experimented a lot and here is the result of their work:</p>\n\n<p>Tested with <strong>nutch 1.7</strong> - <code>http://www.apache.org/dyn/closer.cgi/nutch/1.7/apache-nutch-1.7-bin.zip</code>\nYou'll also need <em>cygwin</em>.</p>\n\n<p>1) Extract <em>nutch</em> to path without spaces. For example:</p>\n\n<pre><code> d:\\dev\\ir\\nutch-1.7\n</code></pre>\n\n<p>2) Copy jdk to some place without spaces. I attempted to make a symlink inside cygwin instead, but it did not go well. For example</p>\n\n<pre><code>xcopy /S \"C:\\Program Files\\Java\\jdk1.7.0_21\" c:\\jdk1.7.0_21\n</code></pre>\n\n<p>3) In cygwin setup the paths to java</p>\n\n<p>3.1) <code>export JAVA_HOME=/cygdrive/c/jdk1.7.0_21</code></p>\n\n<p>3.2) <code>export PATH=$JAVA_HOME/bin:$PATH</code></p>\n\n<p>3,3)  Check that all is correct by calling which java. Should return <code>/cygdrive/c/jdk1.7.0_21/bin/java</code></p>\n\n<p>SO FAR - fixed the first problem - with incorrect java paths. Now to the second problem - <em>hadoop</em> patching.</p>\n\n<p>4) Patch <em>hadoop</em></p>\n\n<pre><code>https://issues.apache.org/jira/browse/HADOOP-7682\nhttps://github.com/congainc/patch-hadoop_7682-1.0.x-win\n</code></pre>\n\n<p>In short:\n - put <code>patch-hadoop_7682-1.0.x-win.jar</code> in <code>d:\\dev\\ir\\nutch-1.7\\lib</code>\n - edit <code>d:\\dev\\ir\\nutch-1.7\\conf\\nutch-site.xml</code> by adding the following:</p>\n\n<pre><code>&lt;property&gt;\n   &lt;name&gt;fs.file.impl&lt;/name&gt;\n   &lt;value&gt;com.conga.services.hadoop.patch.HADOOP_7682.WinLocalFileSystem&lt;/value&gt;\n   &lt;description&gt;Enables patch for issue HADOOP-7682 on Windows&lt;/description&gt;\n&lt;/property&gt;\n</code></pre>\n\n<p>5) Hadoop temp dir -  I am not sure if this is necessary (try before applying it), because I added it before applying the patch, but in my <code>d:\\dev\\ir\\nutch-1.7\\conf\\nutch-site.xml</code> I have</p>\n\n<pre><code>&lt;property&gt;\n   &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;\n   &lt;value&gt;C:\\tmp\\asd&lt;/value&gt;\n&lt;/property&gt;\n</code></pre>\n\n<p>6) <em>Hadoop version</em> -I am not sure if this is necessary (try before applying it), I downgraded hadoop to hadoop-core-0.20.205.0.jarbefore I found the patch and it still stays this on my setup. \nIf you find this necessary it is here: <code>http://mvnrepository.com/artifact/org.apache.hadoop/hadoop-core/0.20.205.0</code></p>\n\n<p>6.1) Move <code>hadoop-core-1.2.1.jar</code> from <code>d:\\dev\\ir\\nutch-1.7\\lib</code> to some location for backup</p>\n\n<p>6.2) Download <code>hadoop-core-0.20.205.0.jar</code> to <code>d:\\dev\\ir\\nutch-1.7\\lib</code></p>\n\n<p>7) Some crawling optimizations. If you need to crawl lots of sites, don't start crawling with a huge list of urls, and big depth and topN.\nIf you do, you'd see that nutch fetches links one at a time from the same site sequentially, waiting a 5 seconds between fetches.\nThe reason is that depth 30 and topN 200 will most possibly fill the first fetch queue only with links from the same site. Nutch won't try to fetch them at once, because by default it is configured not to fetch in several threads from the same site. So you are doomed to wait. A lot.</p>\n\n<p>7.1) To resolve this, first run several crawls with small depth and topN - e.g. </p>\n\n<pre><code>bin/nutch crawl urls -dir crawl -depth 3 -topN 4\n</code></pre>\n\n<p>This will fill the generated fetch queue with urls from more than one site</p>\n\n<p>7.2) Then you can try a big night's crawl with</p>\n\n<pre><code>bin/nutch crawl urls -dir crawl -depth 20  -topN 150\n</code></pre>\n\n<p>7.3.) To allow for some multi-threading add the following to yours <code>nutch-site.xml</code>. It will allow several threads fetch from the same host at once.</p>\n\n<p><strong>NOTE!</strong> Read the meaning of the properties in internet before using them.</p>\n\n<pre><code>&lt;property&gt;\n  &lt;name&gt;fetcher.threads.fetch&lt;/name&gt;\n  &lt;value&gt;16&lt;/value&gt;\n&lt;/property&gt;\n&lt;property&gt;\n  &lt;name&gt;fetcher.threads.per.queue&lt;/name&gt;\n  &lt;value&gt;4&lt;/value&gt;\n&lt;/property&gt;\n&lt;property&gt;\n&lt;property&gt;\n  &lt;name&gt;fetcher.queue.mode&lt;/name&gt;\n  &lt;value&gt;byDomain&lt;/value&gt;\n&lt;/property&gt;\n&lt;name&gt;fetcher.threads.per.host&lt;/name&gt;\n   &lt;value&gt;8&lt;/value&gt;\n   &lt;description&gt;&lt;/description&gt;\n&lt;/property&gt;\n&lt;property&gt;\n  &lt;name&gt;fetcher.verbose&lt;/name&gt;\n  &lt;value&gt;true&lt;/value&gt;\n&lt;/property&gt;\n&lt;property&gt;\n  &lt;name&gt;fetcher.server.min.delay&lt;/name&gt;\n  &lt;value&gt;5.0&lt;/value&gt;\n  &lt;description&gt;applicable ONLY if fetcher.threads.per.host is greater than 1 (i.e. the host blocking is turned off).&lt;/description&gt;\n&lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre>\n\n<p><strong>Note:</strong> When you crawl lots of sites, make sure your <code>D:\\Dev\\id\\apache-nutch-1.7\\conf\\regex-urlfilter.txt</code> includes only the sites in which you are interested. Otherwise you'll end up with \"The Internet\" on your disk. </p>\n", "creation_date": 1417645133, "is_accepted": false, "score": 0, "last_activity_date": 1417645133, "answer_id": 27282772}], "question_id": 14018187, "tags": ["windows", "windows-7", "cygwin", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/14018187/using-nutch-in-windows-7", "last_activity_date": 1417645133, "owner": {"user_id": 1895298, "answer_count": 5, "creation_date": 1355243827, "accept_rate": 62, "view_count": 81, "reputation": 624}, "body": "<p>I am trying to use nutch 1.6 from the windows environment but every time I try to run as per the procedure given in the site <a href=\"http://wiki.apache.org/nutch/NutchTutorial\">Nutch Tuorial Apache</a> I always end up with the following exception:</p>\n\n<pre><code>Exception in thread \"main\" java.io.IOException: Failed to set permissions of path: \\tmp\\hadoop-ajayn\\mapred\\staging\\ajayn-1231695575\\.staging to 0700\n</code></pre>\n\n<p>I have been searching extensively over the net but there is no concrete solution. Please note that I have no hadoop instances installed or running in the system and my sole purpose is the try out nutch as a web crawling agent. </p>\n\n<p>Is it even possible to run nutch 1.6 in windows and if yes any pointers as to how to go about it and avoid the above exception.</p>\n\n<p>PS: if it helps, the /tmp/ folder has a Read Only attribute attached to it and it does not change even if you try to do so. Also from cygwin I have tried to set the file permissions 777, but every time I try to run the nutch instance, a new folder eg: \"ajayn-1231695575\" is created that does not have any execution rights.</p>\n\n<p>Thanks</p>\n\n<p>Ajay</p>\n", "creation_date": 1356332634, "score": 6},
{"title": "Nutch crawler: accept only english pages", "view_count": 210, "owner": {"user_id": 3529121, "answer_count": 1, "creation_date": 1397397337, "accept_rate": 79, "view_count": 42, "reputation": 139}, "is_answered": true, "answers": [{"question_id": 26422064, "owner": {"user_id": 1965370, "link": "http://stackoverflow.com/users/1965370/do-do", "user_type": "registered", "reputation": 389}, "body": "<p>The value you set: <code>&lt;value&gt;en-us,en-gb,en;q=0.7,*;q=0.3&lt;/value&gt;</code> means that it prefers English but other languages (*) still there. For crawling only English pages, you should set value as below:</p>\n\n<pre><code>&lt;value&gt;en-us,en-gb,en&lt;/value&gt;\n</code></pre>\n\n<p>To make sure, change the value in nutch-default.xml as well.</p>\n\n<p>Hope this helps</p>\n\n<p>-Le Quoc Do</p>\n", "creation_date": 1417514605, "is_accepted": true, "score": 0, "last_activity_date": 1417514605, "answer_id": 27246604}], "question_id": 26422064, "tags": ["java", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/26422064/nutch-crawler-accept-only-english-pages", "last_activity_date": 1417514605, "accepted_answer_id": 27246604, "body": "<p>How can I do to configure the crawler nutch so that crawl only pages in English?</p>\n\n<p>I set in nutch-site.xml file this setting, but it does not work:</p>\n\n<pre><code>&lt;property&gt;\n&lt;name&gt;http.accept.language&lt;/name&gt;\n&lt;value&gt;en-us,en-gb,en;q=0.7,*;q=0.3&lt;/value&gt;\n&lt;description&gt;Value of the \"Accept-Language\" request header field.\nThis allows selecting non-English language as default one to retrieve.\nIt is a useful setting for search engines build for certain national group.\n&lt;/description&gt;\n&lt;/property&gt;\n</code></pre>\n", "creation_date": 1413538337, "score": 1},
{"title": "Apache Nutch to index only part of page content", "view_count": 2112, "owner": {"age": 37, "answer_count": 153, "creation_date": 1256735081, "user_id": 198087, "accept_rate": 71, "view_count": 507, "location": "Ternopil&#39;, Ukraine", "reputation": 2778}, "is_answered": true, "answers": [{"question_id": 6630199, "owner": {"user_id": 484046, "accept_rate": 81, "link": "http://stackoverflow.com/users/484046/millebii", "user_type": "registered", "reputation": 976}, "body": "<p>By default the content is flat after parsing.\nSo I don't think you can do what you want, unless you can get extract your content at the indexing step ie once content has been flattened.</p>\n", "creation_date": 1310333249, "is_accepted": false, "score": 1, "last_activity_date": 1310333249, "answer_id": 6643812}, {"question_id": 6630199, "owner": {"user_id": 12016, "accept_rate": 77, "link": "http://stackoverflow.com/users/12016/mana", "user_type": "registered", "reputation": 2720}, "body": "<p>Building your own ParsingFilter and IndexingFilter is easy. Nutch provides you with the DOM document, which you only need to traverse and search for your div. Then you simply add the new fields to your index and schema and your done.</p>\n\n<p>There are some examples on how to do this: </p>\n\n<p><a href=\"http://wiki.apache.org/nutch/HowToMakeCustomSearch\" rel=\"nofollow\">http://wiki.apache.org/nutch/HowToMakeCustomSearch</a></p>\n\n<p><a href=\"http://sujitpal.blogspot.com/2009/07/nutch-custom-plugin-to-parse-and-add.html\" rel=\"nofollow\">http://sujitpal.blogspot.com/2009/07/nutch-custom-plugin-to-parse-and-add.html</a></p>\n\n<p>Good luck</p>\n", "creation_date": 1310463986, "is_accepted": true, "score": 2, "last_activity_date": 1310463986, "answer_id": 6662252}, {"question_id": 6630199, "owner": {"user_id": 1881318, "link": "http://stackoverflow.com/users/1881318/tahagh", "user_type": "registered", "reputation": 619}, "body": "<p>Instead of writing your own plugins, you can also use these custom plugins which can be configured to extract parts of pages:</p>\n\n<ul>\n<li><a href=\"https://github.com/BayanGroup/nutch-custom-search\" rel=\"nofollow\">https://github.com/BayanGroup/nutch-custom-search</a></li>\n<li><a href=\"http://www.atlantbh.com/precise-data-extraction-with-apache-nutch/\" rel=\"nofollow\">http://www.atlantbh.com/precise-data-extraction-with-apache-nutch/</a></li>\n</ul>\n", "creation_date": 1416927754, "is_accepted": false, "score": 0, "last_activity_date": 1416927754, "answer_id": 27130044}], "question_id": 6630199, "tags": ["solr", "nutch"], "answer_count": 3, "link": "http://stackoverflow.com/questions/6630199/apache-nutch-to-index-only-part-of-page-content", "last_activity_date": 1416927754, "accepted_answer_id": 6662252, "body": "<p>Going to use <code>Apache Nutch v1.3</code> to extract only some specific content from the webpages. Checked parse-html plugin. Seems it normalizes each html page using tagsoup or nekohtml. This is good. I need to extract only text inside <code>&lt;span class='xxx'&gt;</code> and <code>&lt;span class='yyy'&gt;</code> elemetns on the web-page. Would be great if extracted texts are saved into different fields (e.g. <code>content_xxx</code>, <code>content_yyy</code>).\nMy question is: should I write my own plugin or this could be done using some standard way? </p>\n\n<p>The best way would be apply XSLT on normalized web-page and get the result. Is that possible?</p>\n", "creation_date": 1310157270, "score": 1},
{"title": "Issue parsing PDF with Apache Nutch - extractor plugin", "view_count": 590, "is_answered": false, "answers": [{"question_id": 27110016, "owner": {"user_id": 1881318, "link": "http://stackoverflow.com/users/1881318/tahagh", "user_type": "registered", "reputation": 619}, "body": "<p>I think the problem relates to omitNonMatching=\"true\" in your extractor.xml file. </p>\n\n<p>omitNonMatching=\"true\" means \"don't index those pages that don't match in any extracto-to rules of extractor.xml\". The default value is false.</p>\n", "creation_date": 1416921454, "is_accepted": false, "score": 0, "last_activity_date": 1416921454, "answer_id": 27127841}], "question_id": 27110016, "tags": ["pdf", "solr", "nutch", "extractor"], "answer_count": 1, "link": "http://stackoverflow.com/questions/27110016/issue-parsing-pdf-with-apache-nutch-extractor-plugin", "last_activity_date": 1416921454, "owner": {"user_id": 1948962, "answer_count": 3, "creation_date": 1357312148, "view_count": 2, "location": "Edinburgh", "reputation": 181}, "body": "<p>I am trying to index web pages AND pdf documents from a website. I am using Nutch 1.9.</p>\n\n<p>I downloade the nutch-custom-search plugin from <a href=\"https://github.com/BayanGroup/nutch-custom-search\" rel=\"nofollow\">https://github.com/BayanGroup/nutch-custom-search</a>. The plugin is awsome and indeed let me match selected divs to solr fieds. </p>\n\n<p>The problem I am having is that, my site also contains numerous pdf files. I can see that they are fetched but never parsed. There is no pdf when I query solr. Just web pages. I am trying to use tika to parse .PDFs (I hope that I have the right idea)</p>\n\n<p>If on cygwin, I run parsechecker see below, it seems to parse OK:   </p>\n\n<pre><code>    $ bin/nutch parsechecker -dumptext -forceAs application/pdf http://www.immunisationscotland.org.uk/uploads/documents/18304-Tuberculosis.pdf \n</code></pre>\n\n<p>I am not too sure what to do next (see below for my config)</p>\n\n<p><strong>extractor.xml</strong></p>\n\n<pre><code>    &lt;config xmlns=\"http://bayan.ir\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://bayan.ir http://raw.github.com/BayanGroup/nutch-custom-search/master/zal.extractor/src/main/resources/extractors.xsd\" omitNonMatching=\"true\"&gt;\n&lt;fields&gt;\n    &lt;field name=\"pageTitleChris\" /&gt;\n    &lt;field name=\"contentChris\" /&gt;     \n&lt;/fields&gt;\n&lt;documents&gt;\n    &lt;document url=\"^.*\\.(?!pdf$)[^.]+$\" engine=\"css\"&gt;\n        &lt;extract-to field=\"pageTitleChris\"&gt;\n            &lt;text&gt;\n                &lt;expr value=\"head &gt; title\" /&gt;\n            &lt;/text&gt; \n\n        &lt;/extract-to&gt;\n        &lt;extract-to field=\"contentChris\"&gt;\n            &lt;text&gt;\n                &lt;expr value=\"#primary-content\" /&gt;\n            &lt;/text&gt; \n\n        &lt;/extract-to&gt;\n\n    &lt;/document&gt;\n&lt;/documents&gt;\n</code></pre>\n\n<p></p>\n\n<p><strong>Inside my parse-plugins.xml i added</strong></p>\n\n<pre><code>    &lt;mimeType name=\"application/pdf\"&gt;\n       &lt;plugin id=\"parse-tika\" /&gt;\n    &lt;/mimeType&gt;\n</code></pre>\n\n<p><strong>nutch-site.xml</strong></p>\n\n<pre><code>    &lt;name&gt;plugin.includes&lt;/name&gt;\n&lt;value&gt;protocol-http|urlfilter-regex|parse-(html|tika|text)|extractor|index-(basic|anchor)|query-(basic|site|url)|indexer-solr|response-(json|xml)|summary-basic|scoring-opic|urlnormalizer-(pass|regex|basic)&lt;/value&gt;\n</code></pre>\n\n<p></p>\n\n<pre><code>&lt;property&gt;\n  &lt;name&gt;http.content.limit&lt;/name&gt;\n  &lt;value&gt;65536666&lt;/value&gt;\n  &lt;description&gt;&lt;/description&gt;\n&lt;/property&gt;\n\n\n&lt;property&gt;\n  &lt;name&gt;extractor.file&lt;/name&gt;\n  &lt;value&gt;extractor.xml&lt;/value&gt;\n&lt;/property&gt;\n</code></pre>\n\n<p>Help would be much appreciated,</p>\n\n<p>Thanks</p>\n\n<p>Chris</p>\n", "creation_date": 1416848275, "score": 0},
{"title": "Get thrown &quot;Generator: 0 records selected for fetching&quot; when trying to crawl a small majority of websites using Nutch", "view_count": 993, "is_answered": false, "answers": [{"question_id": 19427074, "owner": {"user_id": 2225199, "link": "http://stackoverflow.com/users/2225199/nimeshjm", "user_type": "registered", "reputation": 1137}, "body": "<p>I tried that seed url and I got this error:  </p>\n\n<pre><code>Denied by robots.txt: http://scambs.moderngov.co.uk/uuCoverPage.aspx?bcr=1\n</code></pre>\n\n<p>Looking at the robots.txt file of that site:</p>\n\n<pre><code># Disallow all webbot searching \nUser-agent: *\nDisallow: /\n</code></pre>\n\n<p>You have to set a specific user agent in Nutch and modify the website to accept crawling form your user agent.</p>\n\n<p>The property to change in Nutch is in conf/nutch-site.xml:</p>\n\n<pre><code>&lt;property&gt;\n  &lt;name&gt;http.agent.name&lt;/name&gt;\n  &lt;value&gt;nutch&lt;/value&gt;\n&lt;/property&gt;\n</code></pre>\n", "creation_date": 1382304954, "is_accepted": false, "score": 0, "last_activity_date": 1382304954, "answer_id": 19483145}, {"question_id": 19427074, "owner": {"user_id": 3096857, "link": "http://stackoverflow.com/users/3096857/b3nchai", "user_type": "registered", "reputation": 1}, "body": "<p>try this</p>\n\n<pre><code>     &lt;property&gt; \n   &lt;name&gt;db.fetch.schedule.class&lt;/name&gt; \n   &lt;value&gt;org.apache.nutch.crawl.AdaptiveFetchSchedule&lt;/value&gt; \n  &lt;/property&gt;\n\n&lt;property&gt;\n  &lt;name&gt;db.fetch.interval.default&lt;/name&gt;\n  &lt;value&gt;10&lt;/value&gt;\n  &lt;description&gt;The default number of seconds between re-fetches of a page (30 days).\n  &lt;/description&gt;\n&lt;/property&gt;\n  &lt;property&gt;\n  &lt;name&gt;db.fetch.interval.max&lt;/name&gt;\n          &lt;!-- for now always re-fetch everything --&gt;\n  &lt;value&gt;100&lt;/value&gt;\n  &lt;description&gt;The maximum number of seconds between re-fetches of a page\n  (less than one day). After this period every page in the db will be re-tried, no\n   matter what is its status.\n  &lt;/description&gt;\n&lt;/property&gt;\n</code></pre>\n", "creation_date": 1416903469, "is_accepted": false, "score": 0, "last_activity_date": 1416903469, "answer_id": 27121677}], "question_id": 19427074, "tags": ["regex", "solr", "web-crawler", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/19427074/get-thrown-generator-0-records-selected-for-fetching-when-trying-to-crawl-a-s", "last_activity_date": 1416903469, "owner": {"user_id": 2890376, "view_count": 0, "answer_count": 0, "creation_date": 1382012087, "reputation": 6}, "body": "<p>I have a site that runs using moderngov.co.uk (you send them a template, which they then upload). I'm trying to crawl this site so it can indexed by Solr and searched through a drupal site. I can crawl the vast majority of websites out there, but for some reason I am unable to crawl this one: <a href=\"http://scambs.moderngov.co.uk/uuCoverPage.aspx?bcr=1\" rel=\"nofollow\">http://scambs.moderngov.co.uk/uuCoverPage.aspx?bcr=1</a></p>\n\n<p>The specific error I get is this:</p>\n\n<pre><code>Injector: starting at 2013-10-17 13:32:47\nInjector: crawlDb: X-X/crawldb\nInjector: urlDir: urls/seed.txt\nInjector: Converting injected urls to crawl db entries.\nInjector: total number of urls rejected by filters: 1\nInjector: total number of urls injected after normalization and filtering: 0\nInjector: Merging injected urls into crawl db.\nInjector: finished at 2013-10-17 13:32:50, elapsed: 00:00:02\nThu, Oct 17, 2013 1:32:50 PM : Iteration 1 of 2\nGenerating a new segment\nGenerator: starting at 2013-10-17 13:32:51\nGenerator: Selecting best-scoring urls due for fetch.\nGenerator: filtering: false\nGenerator: normalizing: true\nGenerator: topN: 50000\nGenerator: 0 records selected for fetching, exiting ...\n</code></pre>\n\n<p>I'm not sure if it's got something to do with the regex patterns Nutch uses to parse html, or if there's a redirect that's causing issues, or something else entirely. Below are a few of the nutch config files:</p>\n\n<p>Here are the urlfilters: <a href=\"http://pastebin.com/ZqeZUJa1\" rel=\"nofollow\">http://pastebin.com/ZqeZUJa1</a></p>\n\n<p>sysinfo:\nWindows 7 (64-bit)\nSolr 3.6.2\nApache Nutch 1.7</p>\n\n<p>If anyone has come across this problem before, or might know why this is happening, any help would be greatly appreciated.</p>\n\n<p>Thanks</p>\n", "creation_date": 1382013729, "score": 1},
{"title": "Nutch ReCrawl and extract new links", "view_count": 122, "is_answered": false, "answers": [{"question_id": 23915495, "owner": {"user_id": 3096857, "link": "http://stackoverflow.com/users/3096857/b3nchai", "user_type": "registered", "reputation": 1}, "body": "<p>try making some changes to your nutch-site.xml</p>\n\n<pre><code> &lt;property&gt; \n   &lt;name&gt;db.fetch.schedule.class&lt;/name&gt; \n   &lt;value&gt;org.apache.nutch.crawl.AdaptiveFetchSchedule&lt;/value&gt; \n  &lt;/property&gt;\n\n&lt;property&gt;\n  &lt;name&gt;db.fetch.interval.default&lt;/name&gt;\n  &lt;value&gt;10&lt;/value&gt;\n  &lt;description&gt;The default number of seconds between re-fetches of a page (30 days).\n  &lt;/description&gt;\n&lt;/property&gt;\n  &lt;property&gt;\n  &lt;name&gt;db.fetch.interval.max&lt;/name&gt;\n          &lt;!-- for now always re-fetch everything --&gt;\n  &lt;value&gt;10&lt;/value&gt;\n  &lt;description&gt;The maximum number of seconds between re-fetches of a page\n  (less than one day). After this period every page in the db will be re-tried, no\n   matter what is its status.\n  &lt;/description&gt;\n&lt;/property&gt;\n</code></pre>\n", "creation_date": 1416901772, "is_accepted": false, "score": 0, "last_activity_date": 1416901772, "answer_id": 27121250}], "question_id": 23915495, "tags": ["solr", "lucene", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/23915495/nutch-recrawl-and-extract-new-links", "last_activity_date": 1416901772, "owner": {"user_id": 3496812, "view_count": 0, "answer_count": 0, "creation_date": 1396594099, "reputation": 1}, "body": "<p>I am working on a News gathering application. It should daily monitor more than 300 sites and send new URLs(new news links) to Solr(which is our indexer module). I have installed NUTCH and applied all required configuration. everything work fine ,but its re-crawling module dose not work. I red many articles (such as <a href=\"http://pascaldimassimo.com/2010/06/11/how-to-re-crawl-with-nutch/\" rel=\"nofollow\">http://pascaldimassimo.com/2010/06/11/how-to-re-crawl-with-nutch/</a> and <a href=\"http://wiki.apache.org/nutch/IntranetRecrawl\" rel=\"nofollow\">http://wiki.apache.org/nutch/IntranetRecrawl</a>) and applied their configurations but unfortunately non of them works for me. Is there any scrips or configuration for my issue ?</p>\n\n<p>I also use NUTCH 1.8.</p>\n\n<p>Regards,</p>\n", "creation_date": 1401289988, "score": 0},
{"title": "Apache Nutch - indexing only the modified files in Solr", "view_count": 102, "is_answered": false, "answers": [{"question_id": 26006705, "owner": {"user_id": 1015324, "accept_rate": 0, "link": "http://stackoverflow.com/users/1015324/anoop-isaac", "user_type": "registered", "reputation": 171}, "body": "<p>Answering my own question here, Hope it helps someone\nOnce I set the adaptivefetchschedule, could see that Nutch was not pulling the pages that hasnt changed.Its honoring if-modified-since header.</p>\n", "creation_date": 1414777819, "is_accepted": false, "score": 0, "last_activity_date": 1414777819, "answer_id": 26680641}], "question_id": 26006705, "tags": ["solr", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/26006705/apache-nutch-indexing-only-the-modified-files-in-solr", "last_activity_date": 1416679061, "owner": {"user_id": 1015324, "answer_count": 9, "creation_date": 1319658532, "accept_rate": 0, "view_count": 75, "reputation": 171}, "body": "<p>Iam able to set up the Apache Nutch and get the data indexed in Solr. While indexing I am trying to make sure only modified pages gets indexed. Below are the 2 questions we have regarding this.</p>\n\n<ul>\n<li><p>Is it possible to tell Nutch to send \u2018If-modified-since\u2019 header while\ncrawling the site and download the page only if it has changed since\nthe last time it was crawled. </p></li>\n<li><p>I could see that Nutch is forming the    MD5 digest out of the\nretrieved page content, but even though digest    hasn\u2019t changed\n(compared to previous version), it is still the    indexing the page\nin Solr. Is there any setting with in Nutch to make    sure if the\ncontent hasn\u2019t changed have it not index in Solr?</p></li>\n</ul>\n", "creation_date": 1411518660, "score": 0},
{"title": "Solr and nutch. How to save seed?", "view_count": 55, "is_answered": false, "answers": [{"question_id": 26351214, "owner": {"user_id": 3757127, "accept_rate": 83, "link": "http://stackoverflow.com/users/3757127/yann", "user_type": "registered", "reputation": 601}, "body": "<p>I don't know any way in Nutch to achieve that; you could run Nutch multiple times, with exactly one seed at each run, and index the seed as a static field in Solr, via:</p>\n\n<pre><code>&lt;property&gt;\n        &lt;name&gt;index.static&lt;/name&gt;\n        &lt;value&gt;seedUrl:theSeedForTheCurrentNutchRun&lt;/value&gt;\n&lt;/property&gt;\n</code></pre>\n\n<p>This would work, but depending on how your seeds relate to each other, you may spend more time crawling (if a page is reachable via multiple seeds. Also in this case, the last seed would be the one recorded in Solr).</p>\n", "creation_date": 1413361677, "is_accepted": false, "score": 0, "last_activity_date": 1413361677, "answer_id": 26377900}], "question_id": 26351214, "tags": ["solr", "web-crawler", "config", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/26351214/solr-and-nutch-how-to-save-seed", "last_activity_date": 1416678891, "owner": {"user_id": 1759796, "answer_count": 1, "creation_date": 1350660081, "accept_rate": 45, "view_count": 11, "reputation": 134}, "body": "<p>Hi I followed this Tutorial</p>\n\n<p><a href=\"http://wiki.apache.org/nutch/NutchTutorial\" rel=\"nofollow\">http://wiki.apache.org/nutch/NutchTutorial</a></p>\n\n<p>It worked as expected. </p>\n\n<p><strong>How do I save the seed a crawled page came from to solr?</strong></p>\n\n<p>I want to be able to query solr for a word \"foobar\" and get all the seeds that lead to pages containing  this word. I suppose I have to add a field in the schema.xml, but i don't know what the line in the file should be.</p>\n", "creation_date": 1413249145, "score": 1},
{"title": "Nutch - How to split html page into multiple pages with own url?", "view_count": 138, "is_answered": false, "question_id": 27062253, "tags": ["parsing", "solr", "web-crawler", "nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/27062253/nutch-how-to-split-html-page-into-multiple-pages-with-own-url", "last_activity_date": 1416575566, "owner": {"user_id": 965718, "answer_count": 3, "creation_date": 1317064957, "accept_rate": 38, "view_count": 18, "reputation": 55}, "body": "<p>I have a page (<a href=\"http://www.example.com/content\" rel=\"nofollow\">http://www.example.com/content</a>), which contains multiple blocks</p>\n\n<pre><code> &lt;div&gt;\n &lt;h1 id=\"titleOne\"&gt;First title&lt;/h1&gt;\n Here is custom content\n &lt;h1 id=\"titleTwo\"&gt;Second title&lt;/h1&gt;\n Here is custom content for part 2\n &lt;/div&gt;\n</code></pre>\n\n<p>I want to index this page like 2 separate pages which differ in url and contains on text from &lt;h1&gt; to another &lt;h1&gt; element.</p>\n\n<p>Example:</p>\n\n<p>Document 1: <a href=\"http://www.example.com/content#titleOne\" rel=\"nofollow\">http://www.example.com/content#titleOne</a></p>\n\n<p>Document 2: <a href=\"http://www.example.com/content#titleTwo\" rel=\"nofollow\">http://www.example.com/content#titleTwo</a></p>\n", "creation_date": 1416575566, "score": 1},
{"title": "run nutch1.9 in eclipse got error CrawlDb update: java.io.IOException: Job failed", "view_count": 180, "owner": {"user_id": 4148048, "view_count": 5, "answer_count": 0, "creation_date": 1413428777, "reputation": 6}, "is_answered": true, "answers": [{"question_id": 26839442, "owner": {"user_id": 432844, "link": "http://stackoverflow.com/users/432844/julien-nioche", "user_type": "registered", "reputation": 939}, "body": "<p>Have you tried following the steps from the <a href=\"http://wiki.apache.org/nutch/RunNutchInEclipse\" rel=\"nofollow\">Nutch WIKI</a> instead?</p>\n", "creation_date": 1416571376, "is_accepted": true, "score": 1, "last_activity_date": 1416571376, "answer_id": 27060981}], "question_id": 26839442, "tags": ["eclipse", "hadoop", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/26839442/run-nutch1-9-in-eclipse-got-error-crawldb-update-java-io-ioexception-job-faile", "last_activity_date": 1416571376, "accepted_answer_id": 27060981, "body": "<p>I'm trying to run nutch 1.9 in eclipse, all of my config is according to this article(<a href=\"http://yewintko.wordpress.com/2014/02/02/setting-up-nutch-in-eclipse-indigo/\" rel=\"nofollow\">http://yewintko.wordpress.com/2014/02/02/setting-up-nutch-in-eclipse-indigo/</a>). But I got this error :</p>\n\n<pre><code>CrawlDb update: starting at 2014-11-10 15:50:10\nCrawlDb update: db: urls\nCrawlDb update: segments: [3, crawl]\nCrawlDb update: additions allowed: true\nCrawlDb update: URL normalizing: false\nCrawlDb update: URL filtering: false\nCrawlDb update: 404 purging: false\nCrawlDb update: Merging segment data into db.\nCrawlDb update: java.io.IOException: Job failed!\n    at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:1357)\n    at org.apache.nutch.crawl.CrawlDb.update(CrawlDb.java:119)\n    at org.apache.nutch.crawl.CrawlDb.run(CrawlDb.java:219)\n    at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n    at org.apache.nutch.crawl.CrawlDb.main(CrawlDb.java:179)\n</code></pre>\n", "creation_date": 1415607490, "score": 0},
{"title": "Incremental crawling in Nutch", "view_count": 620, "owner": {"user_id": 3839319, "view_count": 1, "answer_count": 0, "creation_date": 1405398774, "reputation": 5}, "is_answered": true, "answers": [{"question_id": 26964716, "owner": {"user_id": 3496666, "accept_rate": 72, "link": "http://stackoverflow.com/users/3496666/kumar", "user_type": "registered", "reputation": 1801}, "body": "<p>Upto my knowledge,</p>\n\n<p>Nutch crawl the known links and getting inlinks and outlinks from the known pages then add those links into db for next crawl. It seems why nutch didn't crawl all pages at single run.</p>\n\n<p>Incremental crawling means to crawl only new or updated pages and leaves the unmodified pages.</p>\n\n<p>Nutch cralws only limited page because of your configuration settings. change it to crawl all pages. See <a href=\"http://wiki.apache.org/nutch/FAQ#Nutch_doesn.27t_crawl_relative_URLs.3F_Some_pages_are_not_indexed_but_my_regex_file_and_everything_else_is_okay_-_what_is_going_on.3F\" rel=\"nofollow\">here</a></p>\n\n<p>If you want to make a search for one website, then take a look at <a href=\"http://aperture.sourceforge.net/\" rel=\"nofollow\">Aperture</a>. It will crawl whole website at single run. It provides incremental support.  </p>\n", "creation_date": 1416201444, "is_accepted": false, "score": 1, "last_activity_date": 1416201444, "answer_id": 26965991}, {"question_id": 26964716, "owner": {"user_id": 432844, "link": "http://stackoverflow.com/users/432844/julien-nioche", "user_type": "registered", "reputation": 939}, "body": "<p>Why don't you use the Nutch mailing list? you'd get a larger audience and quicker answers from fellow Nutch users.</p>\n\n<p>What value are you setting for the number of rounds when using the crawl script? Setting it to 1 means that you won't go further than the URLs in the seed list. Use a large value to crawl the whole site in a single call to the script.</p>\n\n<p>The difference in the total number of URLs could be the max oulinks per page param as Kumar suggested but it could also be due to the URL filtering.</p>\n", "creation_date": 1416571158, "is_accepted": true, "score": 0, "last_activity_date": 1416571158, "answer_id": 27060918}], "question_id": 26964716, "tags": ["nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/26964716/incremental-crawling-in-nutch", "last_activity_date": 1416571158, "accepted_answer_id": 27060918, "body": "<p>I'm new to Nutch and am doing a POC with Nutch 1.9.  I am only trying to crawl my own site to set up a search on it.  I find that the first crawl I do only crawls one page.  The second crawls 40 pages, the third 300.  the increments reduce and it crawls around 400 pages overall.  Does anyone know why it doesn't just do the full crawl of the website on the first run?  I used the nutch tutorial (<a href=\"http://wiki.apache.org/nutch/NutchTutorial\" rel=\"nofollow\">http://wiki.apache.org/nutch/NutchTutorial</a>) and am running using the script as per section 3.5.</p>\n\n<p>I'm also finding with multiple runs it doesn't crawl the whole site anyway - GSA brings back over 900 pages for the same site - nutch brings back 400.</p>\n\n<p>Thanks kindly</p>\n\n<p>Jason</p>\n", "creation_date": 1416192207, "score": 0},
{"title": "Nutch 2.x integration with neo4j", "view_count": 153, "owner": {"user_id": 832569, "answer_count": 35, "creation_date": 1309994888, "accept_rate": 74, "view_count": 120, "reputation": 779}, "is_answered": true, "answers": [{"question_id": 26978672, "owner": {"user_id": 728812, "link": "http://stackoverflow.com/users/728812/michael-hunger", "user_type": "registered", "reputation": 31128}, "body": "<p>There are many ways of integrating with Neo4j, not sure what Nutch is written in, but for most languages there are <a href=\"http://neo4j.com/developer/language-guides\" rel=\"nofollow\">drivers</a></p>\n", "creation_date": 1416270779, "is_accepted": true, "score": 1, "last_activity_date": 1416270779, "answer_id": 26984600}], "question_id": 26978672, "tags": ["neo4j", "web-crawler", "nutch", "graph-databases", "data-analysis"], "answer_count": 1, "link": "http://stackoverflow.com/questions/26978672/nutch-2-x-integration-with-neo4j", "last_activity_date": 1416302364, "accepted_answer_id": 26984600, "body": "<p>I'm going to write a web crawler for a very big site (more than 5 million pages) so I'm considering using <code>Nutch</code> for the crawling phase. To ease my analyses over the results of the crawling I was thinking on using the neo4j graph database but apparently <code>Nutch</code> doesn't have a data accessor for this database or more precisely, <code>Apache Gora</code> the data accessor Nutch uses doesn't.</p>\n\n<p>Is there a solution for this problem or an alternative graph database that integrates with nutch?</p>\n", "creation_date": 1416246438, "score": 0},
{"title": "apache nutch 2.2.1 Error in execution", "view_count": 765, "is_answered": false, "answers": [{"question_id": 25349745, "owner": {"user_id": 368581, "accept_rate": 0, "link": "http://stackoverflow.com/users/368581/peter-dietz", "user_type": "registered", "reputation": 849}, "body": "<p>I'm using OSX, but I had that same error about <code>Could not find or load main class ...InjectorJob</code>, and I believe that it is the result of a dirty source directory, in my case, I had checked it out via Git, and changed branches a few times, as I was trying out various features. So, you've been running <code>ant</code> or <code>ant runtime</code> to rebuild the runtime/deploy directory, but to solve this, I had to run:</p>\n\n<p><code>\nant clean\n</code></p>\n\n<p>Which deletes this compiled output, and recompiles it properly. After this point, the crawl command runs properly.</p>\n", "creation_date": 1416169954, "is_accepted": false, "score": 0, "last_activity_date": 1416169954, "answer_id": 26961742}], "question_id": 25349745, "tags": ["nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/25349745/apache-nutch-2-2-1-error-in-execution", "last_activity_date": 1416169954, "owner": {"user_id": 3949973, "view_count": 1, "answer_count": 0, "creation_date": 1408280312, "reputation": 1}, "body": "<p>I'm trying to execute nutch 2.2.1 from windows 8 on my local computervia cygdrive.\nI followed the configuration instruction and this is the command I execute in folder src:</p>\n\n<p><strong>./bin/crawl urls/ testCrawl/  //127.0.0.1:8983/solr/ 1</strong></p>\n\n<p>the parameters according to:\n crawl    </p>\n\n<p>The solr link is working on my local machine, and the folders urls &amp; testCrawl exists.</p>\n\n<p>I got error message and I have no idea what should I do:</p>\n\n<p>he-nutch-2.2.1/src\n$ ./bin/crawl urls/ testCrawl/ //127.0.0.1:8181/solr/ 1</p>\n\n<p><strong>cygpath: can't convert empty path</strong></p>\n\n<p><strong>Error: Could not find or load main class org.apache.nutch.crawl.InjectorJob</strong></p>\n\n<p>I saw in other forum to run the commands from $NUTCH_HOME/runtime/local/ but I don't have the runtime folder at all.. I saw that in 2.2.1 it was removed and the instruction is to exec it form the nutch home folder</p>\n\n<p>Anyone know what I need to do to make it work?? maybe change something in the configuration?</p>\n\n<p>Thanks</p>\n", "creation_date": 1408283066, "score": 0},
{"title": "Nutch 1.6 find original url of redirected ones", "view_count": 66, "owner": {"age": 29, "answer_count": 13, "creation_date": 1300893182, "user_id": 673308, "accept_rate": 46, "view_count": 199, "reputation": 608}, "is_answered": true, "answers": [{"question_id": 26865985, "owner": {"user_id": 4046180, "accept_rate": 25, "link": "http://stackoverflow.com/users/4046180/katerina-a", "user_type": "registered", "reputation": 793}, "body": "<p>You can dump the outlinks by doing the following</p>\n\n<pre><code>bin/nutch readseg -dump crawl/segments/segmentname/ outputdir -nocontent -nofetch -    nogenerate -noparse -noparsetext\n</code></pre>\n\n<p>Also to properly follow the redirects, you might want to change this property in nutch-default.xml</p>\n\n<pre><code>&lt;property&gt;\n&lt;name&gt;http.redirect.max&lt;/name&gt;\n&lt;value&gt;5&lt;/value&gt;\n&lt;description&gt;The maximum number of redirects the fetcher will follow when\ntrying to fetch a page. If set to negative or 0, fetcher won't immediately\nfollow redirected URLs, instead it will record them for later fetching.\n&lt;/description&gt;\n&lt;/property&gt;\n</code></pre>\n", "creation_date": 1415715256, "is_accepted": true, "score": 1, "last_activity_date": 1415715256, "answer_id": 26866904}], "question_id": 26865985, "tags": ["java", "nutch", "url-redirection"], "answer_count": 1, "link": "http://stackoverflow.com/questions/26865985/nutch-1-6-find-original-url-of-redirected-ones", "last_activity_date": 1415715256, "accepted_answer_id": 26866904, "body": "<p>I wonder how could I find the original url after it hits a redirection. They're actually found on seedlist but I can not guarantee which url is redirected to which url.  In Fetcher phase I expect to read it from Nutch.WRITABLE_REPR_URL_KEY, but it is overriden by redirected url. </p>\n\n<p>Any suggestion how to read them from crawldb, segments or linkdb?</p>\n\n<p>PS: I only crawl first-level pages (depth:1) on seedlist.</p>\n\n<p>Best,\nTugcem.</p>\n", "creation_date": 1415712303, "score": 0},
{"title": "Convert unstructured data(text) into structured format using java", "view_count": 717, "owner": {"user_id": 2196341, "answer_count": 103, "creation_date": 1363888096, "accept_rate": 70, "view_count": 108, "location": "Los Angeles, CA", "reputation": 1585}, "is_answered": true, "answers": [{"question_id": 26824171, "owner": {"user_id": 2915763, "link": "http://stackoverflow.com/users/2915763/ramanan-r", "user_type": "registered", "reputation": 789}, "body": "<p>You can use nlp tools like GATE (<a href=\"https://gate.ac.uk/\" rel=\"nofollow\">https://gate.ac.uk/</a>), Apache OpenNLP (<a href=\"https://opennlp.apache.org/\" rel=\"nofollow\">https://opennlp.apache.org/</a>), Minorthird (<a href=\"http://sourceforge.net/projects/minorthird/\" rel=\"nofollow\">http://sourceforge.net/projects/minorthird/</a>), etc.</p>\n\n<p>You can write a jape grammar in GATE which creates annotations based on the words present in the text. For example you can annotate <code>dimension, measurements, proportions</code> etc as <code>dimension</code> and then look up for numbers in next sentence.</p>\n\n<p>You can look into other nlp tools here: <a href=\"https://www.quora.com/What-are-the-best-Java-open-source-NLP-toolkits\" rel=\"nofollow\">https://www.quora.com/What-are-the-best-Java-open-source-NLP-toolkits</a></p>\n", "creation_date": 1415595803, "is_accepted": true, "score": 2, "last_activity_date": 1415595803, "answer_id": 26837077}], "question_id": 26824171, "tags": ["solr", "web-crawler", "search-engine", "nutch", "information-retrieval"], "answer_count": 1, "link": "http://stackoverflow.com/questions/26824171/convert-unstructured-datatext-into-structured-format-using-java", "last_activity_date": 1415595803, "accepted_answer_id": 26837077, "body": "<p>I've done a lot of research on this and read few papers, checked all SO Q&amp;A's but I'm still not clear.</p>\n\n<pre><code>        I'm trying to setup a small search engine. I am crawling data using nutch and \n        have integrated it with solr for front end and indexing.\n\n        After crawl, I have unstructured data i.e content of entire page I crawled and \n        I need to structure this data into sets.\n</code></pre>\n\n<p>For example : I crawled a page which contains information about any product and I have raw text which has product description, size, dimension etc.</p>\n\n<p>My goal: extract keywords I want say : dimensions --> store in DB column\nExtract information related to dimensions --> store in same row(another column)</p>\n\n<p>So, I have keyword and description in one table and I fetch as per query and output to user.</p>\n\n<p>I'm not sure how to go about stucturing/fetching the information i need and I wish to do this using java, if possible. I read few articles, but I cant follow. Any help/guidance will be appreciated.</p>\n\n<p>Please let me know, if you need more information. Appreciate your time and help</p>\n", "creation_date": 1415498280, "score": 0},
{"title": "How to remove menu&#39;s from html during crawl or indexing with nutch and solr", "view_count": 449, "is_answered": false, "answers": [{"question_id": 26629079, "owner": {"user_id": 2915763, "link": "http://stackoverflow.com/users/2915763/ramanan-r", "user_type": "registered", "reputation": 789}, "body": "<p>Add this in nutch site.xml</p>\n\n<pre><code>&lt;!-- tika properties to use BoilerPipe, according to Marcus Jelsma --&gt; \n&lt;property&gt; \n  &lt;name&gt;tika.use_boilerpipe&lt;/name&gt; \n  &lt;value&gt;true&lt;/value&gt; \n&lt;/property&gt; \n&lt;property&gt; \n  &lt;name&gt;tika.boilerpipe.extractor&lt;/name&gt; \n  &lt;value&gt;ArticleExtractor&lt;/value&gt; \n&lt;/property&gt; \n</code></pre>\n\n<p>This wont exactly remove header and footer, but it will do a decent job in removing things other than main article in the page.</p>\n\n<p>If you are not satisfied you can use other boilerplates like <a href=\"https://code.google.com/p/boilerpipe/\" rel=\"nofollow\">https://code.google.com/p/boilerpipe/</a></p>\n", "creation_date": 1415595131, "is_accepted": false, "score": 0, "last_activity_date": 1415595131, "answer_id": 26836974}], "question_id": 26629079, "tags": ["solr", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/26629079/how-to-remove-menus-from-html-during-crawl-or-indexing-with-nutch-and-solr", "last_activity_date": 1415595131, "owner": {"user_id": 4193066, "view_count": 0, "answer_count": 0, "creation_date": 1414569077, "reputation": 6}, "body": "<p>We are using Nutch 2.1 with solr 4.8.1 and want to remove header and footer from  the parsed result. \nWe have tried element-selector plugin (<a href=\"https://github.com/kaqqao/nutch-element-selector\" rel=\"nofollow\">https://github.com/kaqqao/nutch-element-selector</a>) to remove header and footer but there is no effect in our search result.</p>\n\n<p>can anyone help to get sort this.</p>\n\n<p>Thanks in advance.   </p>\n", "creation_date": 1414581793, "score": 1},
{"title": "How to make apache Nutch to crawl forever", "view_count": 94, "owner": {"user_id": 3454410, "answer_count": 20, "creation_date": 1395646154, "accept_rate": 48, "view_count": 174, "reputation": 832}, "is_answered": true, "answers": [{"last_edit_date": 1415593023, "owner": {"user_id": 2915763, "link": "http://stackoverflow.com/users/2915763/ramanan-r", "user_type": "registered", "reputation": 789}, "body": "<p>What about a simple while loop in a script file?</p>\n\n<pre><code>for (( ; ; ))\ndo\n#set variables\nbin/nutch crawl $URLS -dir $CRAWL_LOC -depth 1 -topN 1000\ndone\n</code></pre>\n\n<p>//Linux script</p>\n", "question_id": 26771942, "creation_date": 1415361202, "is_accepted": true, "score": 1, "last_activity_date": 1415593023, "answer_id": 26800689}], "question_id": 26771942, "tags": ["apache", "web-crawler", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/26771942/how-to-make-apache-nutch-to-crawl-forever", "last_activity_date": 1415593023, "accepted_answer_id": 26800689, "body": "<p>I have some specific urls( about 17000 urls) and I want to run apache nutch to crawl forever i.e when it complete its all steps then it should start again automatically</p>\n", "creation_date": 1415249401, "score": -1},
{"title": "Getting clear content (without markup) with Nutch 1.9", "view_count": 99, "is_answered": false, "answers": [{"question_id": 26598439, "owner": {"user_id": 2915763, "link": "http://stackoverflow.com/users/2915763/ramanan-r", "user_type": "registered", "reputation": 789}, "body": "<p>Add this in nutch site.xml</p>\n\n<pre><code>&lt;!-- tika properties to use BoilerPipe, according to Marcus Jelsma --&gt; \n&lt;property&gt; \n  &lt;name&gt;tika.use_boilerpipe&lt;/name&gt; \n  &lt;value&gt;true&lt;/value&gt; \n&lt;/property&gt; \n&lt;property&gt; \n  &lt;name&gt;tika.boilerpipe.extractor&lt;/name&gt; \n  &lt;value&gt;ArticleExtractor&lt;/value&gt; \n&lt;/property&gt; \n</code></pre>\n\n<p>// This is for nutch 1.7, I'm not sure about 1.9</p>\n\n<p>Use jsoup to get plain text.</p>\n", "creation_date": 1415363378, "is_accepted": false, "score": 0, "last_activity_date": 1415363378, "answer_id": 26801282}], "question_id": 26598439, "tags": ["solr", "web-crawler", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/26598439/getting-clear-content-without-markup-with-nutch-1-9", "last_activity_date": 1415363378, "owner": {"user_id": 2219435, "answer_count": 0, "creation_date": 1364469090, "accept_rate": 75, "view_count": 13, "reputation": 87}, "body": "<p>Using Nutch 1.9, how do I get clear content (without html markup) of crawled pages and save the .content in readable form. Is Solr way to do that or can it be done without it and how?</p>\n\n<p>And a subquestion, how do I control the crawling depth with bin/crawl script? There was an option to that (and topN) in bin/nutch crawl command, but it is deprecated now and won't execute.</p>\n", "creation_date": 1414451650, "score": 0},
{"title": "Any alternative for Apache Nutch?", "view_count": 607, "is_answered": true, "answers": [{"question_id": 26668934, "owner": {"user_id": 2915763, "link": "http://stackoverflow.com/users/2915763/ramanan-r", "user_type": "registered", "reputation": 789}, "body": "<p>For step-by-step to build a search engine, visit <a href=\"http://blog.building-blocks.com/technical-tips/building-a-search-engine-with-nutch-and-solr-in-10-minutes\" rel=\"nofollow\">http://blog.building-blocks.com/technical-tips/building-a-search-engine-with-nutch-and-solr-in-10-minutes</a></p>\n", "creation_date": 1415362439, "is_accepted": false, "score": 1, "last_activity_date": 1415362439, "answer_id": 26801056}], "question_id": 26668934, "tags": ["python", "web-scraping", "scrapy", "screen-scraping", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/26668934/any-alternative-for-apache-nutch", "last_activity_date": 1415362439, "owner": {"user_id": 2997866, "answer_count": 1, "creation_date": 1384549602, "accept_rate": 0, "view_count": 77, "reputation": 310}, "body": "<p>I'm looking for a solution like below:</p>\n\n<p>1- search a given keyword in search engines (Google,Yahoo...)</p>\n\n<p>2- receive results (links)</p>\n\n<p>3- go to the links and extract contents and ...</p>\n\n<p>After a brief research I found \"Apache Nutch\", I just want to know your suggestion. </p>\n\n<p><strong>do you have any recommendation for this solution?</strong> </p>\n\n<p><strong>Is there any better solution for what I explained?</strong></p>\n\n<p>I'm a python developer and I'm familiar with tools like \"Scrapy\".  </p>\n\n<p>Thank You.</p>\n", "creation_date": 1414738072, "score": 1},
{"title": "I want to crawl twitter and facebook", "view_count": 611, "is_answered": false, "answers": [{"question_id": 26699984, "owner": {"user_id": 2915763, "link": "http://stackoverflow.com/users/2915763/ramanan-r", "user_type": "registered", "reputation": 789}, "body": "<p>What exactly do you want to crawl in facebook/ twitter?</p>\n\n<p>Only specific search engine bots are allowed to crawl facebook.</p>\n\n<p>Visit <a href=\"https://facebook.com/robots.txt\" rel=\"nofollow\">https://facebook.com/robots.txt</a></p>\n\n<p>At the bottom they have disallowed all bots except the one listed.</p>\n\n<p>So to fetch data from facebook (if that's what you need), use API.</p>\n\n<p><a href=\"https://developers.facebook.com/\" rel=\"nofollow\">https://developers.facebook.com/</a></p>\n\n<p>In twitter, you can crawl a few url's</p>\n\n<pre><code>Allow: /?lang=\nAllow: /hashtag/*?src=\nAllow: /search?q=%23\n</code></pre>\n\n<p>Again a better approach is to use API, if your aim is to fetch some data.</p>\n\n<p><a href=\"https://dev.twitter.com/\" rel=\"nofollow\">https://dev.twitter.com/</a></p>\n", "creation_date": 1415362205, "is_accepted": false, "score": 0, "last_activity_date": 1415362205, "answer_id": 26800998}], "question_id": 26699984, "tags": ["facebook", "twitter", "solr", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/26699984/i-want-to-crawl-twitter-and-facebook", "last_activity_date": 1415362205, "owner": {"user_id": 3835707, "view_count": 1, "answer_count": 1, "creation_date": 1405317647, "reputation": 11}, "body": "<p>I am making a crawler now.\nIt supports web, Facebook and Twitter.\nMy mentor says it needs to support getting post using Facebook and Twitter API, but I don't know how.\nI am using Solr as a search engine and planning to use Nutch for web crawling.\nI saw that Nutch does not support those APIs.\nCould you recommend other web crawlers or a way to get posts using Nutch or other ways, whatever.\nI would appreciate very much!</p>\n", "creation_date": 1414936088, "score": 0},
{"title": "How to include Nutch in Simple Java Application?", "view_count": 92, "is_answered": false, "answers": [{"question_id": 26786300, "owner": {"user_id": 2915763, "link": "http://stackoverflow.com/users/2915763/ramanan-r", "user_type": "registered", "reputation": 789}, "body": "<p>Write a webservice, which runs in your server. Call the webservice from client side with the url list. Write url's in seed file on your server side.</p>\n\n<p>Use Apache commons-cli in web service to run a script file which starts nutch:</p>\n\n<pre><code>Executor exec = new DefaultExecutor();\nexec.setWorkingDirectory(file);\nCommandLine cl = new CommandLine(\"./runCommand.sh\");\n\nint exitvalue = exec.execute(cl);\nif (exitvalue == 0)\n    System.out.println(\"./runCommand.sh succeeded....\");\n</code></pre>\n", "creation_date": 1415360714, "is_accepted": false, "score": 0, "last_activity_date": 1415360714, "answer_id": 26800516}], "question_id": 26786300, "tags": ["java", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/26786300/how-to-include-nutch-in-simple-java-application", "last_activity_date": 1415360714, "owner": {"age": 31, "answer_count": 99, "creation_date": 1248783286, "user_id": 146366, "accept_rate": 75, "view_count": 1474, "location": "South Africa", "reputation": 3470}, "body": "<p>I have a simple browser \"hello world\" applet being called:</p>\n\n<pre><code>import java.awt.Graphics;\n\npublic class CrawlCrowd extends java.applet.Applet \n{\n    public void init() \n    {\n        resize(150,25);\n    }\n\n    public void paint(Graphics g) {\n        g.drawString(\"Hello world!\", 50, 25);\n    }\n\n}\n</code></pre>\n\n<p>I want to include <a href=\"http://nutch.apache.org/\" rel=\"nofollow\">Nutch</a>, so that I can crawl urls on the fly in the applet. How do I go about including it and executing a simple crawl?</p>\n", "creation_date": 1415297013, "score": -2},
{"title": "Is there any way to get a thumbsuck idea of how much space your database will need for a search engine?", "view_count": 22, "is_answered": true, "answers": [{"question_id": 26772587, "owner": {"user_id": 137650, "link": "http://stackoverflow.com/users/137650/matslindh", "user_type": "registered", "reputation": 13901}, "body": "<p>You've already done an estimate, but your estimate is probably way off. Almost no modern web page is only 1kb in size (MSN.com is 319KB (58.8KB gzipped) - but 1B web pages is, depending on who you're asking, a measurable amount of the relevant pages on the internet today. And keep in mind, you probably don't just want to store the actual page content, but also index it. This will include several indices, depending on what kind of use you're expecting from the index. Much of the content will probably also be parsed and transformed into other content as well, which will be indexed separately for different usage.</p>\n\n<p>So the only answer possible for such as question is \"it depends\", and \"good luck\". Additionally, 95TB is not A LOT of storage today, and could be handled by a single server (storage wise - index usage and query counts will require more servers, but it all depends on what you're going to be using stuff for).</p>\n\n<p>Start somewhere and see where it takes you.</p>\n", "creation_date": 1415259586, "is_accepted": false, "score": 2, "last_activity_date": 1415259586, "answer_id": 26774116}], "question_id": 26772587, "tags": ["solr", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/26772587/is-there-any-way-to-get-a-thumbsuck-idea-of-how-much-space-your-database-will-ne", "last_activity_date": 1415259586, "owner": {"age": 31, "answer_count": 99, "creation_date": 1248783286, "user_id": 146366, "accept_rate": 75, "view_count": 1474, "location": "South Africa", "reputation": 3470}, "body": "<p>I am creating a crawler that, for arguments sake, will crawl 1 billion pages. I know that is the absolute maximum number of pages I will ever crawl and I know I need to store as much information about each page on the internet. The crawler is <a href=\"http://nutch.apache.org/\" rel=\"nofollow\">nutch</a> with soir. </p>\n\n<p>How can I reliably decide on the size hard disk I will need to maintain this amount of  data? I can't find any information on how much space a record will take up in <a href=\"http://nutch.apache.org/\" rel=\"nofollow\">nutch</a>. And I need to know so I can see how realistic is it is to host this on one drive, and if not, what my other options are.</p>\n\n<p>If it takes up 1 kilobyte per page, 1 billion pages will need = 1 000 000 000 / 1024 / 1024 = 95 Terabyte. This is a LOT. But if it is half a byte per page, or pehaps 25% or less off a byte, which would make storing it on only a few servers far more realistic.</p>\n", "creation_date": 1415253089, "score": 0},
{"title": "apache hadoop, hbase and nutch components distribution for 4 servers cluster", "view_count": 373, "owner": {"user_id": 3454410, "answer_count": 20, "creation_date": 1395646154, "accept_rate": 48, "view_count": 174, "reputation": 832}, "is_answered": true, "answers": [{"last_edit_date": 1415255214, "owner": {"user_id": 3860596, "link": "http://stackoverflow.com/users/3860596/mbaxi", "user_type": "registered", "reputation": 717}, "body": "<p>Say you have 4 nodes n1, n2, n3 and n4.\nYou can install hadoop and hbase in distributed mode.\nIf you are using Hadoop 1.x - </p>\n\n<pre><code>n1 - hadoop master[Namenode and Jobtracker]\nn2, n3 and n3 - hadoop slaves [datanodes and tasktrackers]\n</code></pre>\n\n<p>For HBase, you can choose n1 or any other node as Master node, Since Master node are usually not CPU/Memory intensive, all Masters can be deployed on single node on test setup, However in Production its good to have each Master deployment on a separate node.</p>\n\n<pre><code>Lets say n2 - HBase Master, remaining 3 nodes can act as regionservers.\n</code></pre>\n\n<p>Hive and Nutch can reside on any node.\nHope this helps; For a test setup this should be good to go.</p>\n\n<hr>\n\n<p>Update -</p>\n\n<p>For Hadoop 2.x, since your cluster size is small, Namenode HA deployment can be skipped.\nNamenode HA would require two nodes one each for an active and standby node.</p>\n\n<p>A zookeeper quorum which again requires odd number of nodes so a minimum of three nodes would be required.</p>\n\n<p>A journal quorum again require a minimum of 3 nodes.</p>\n\n<p>But for a cluster this small HA might not be a major concern. So you can keep </p>\n\n<blockquote>\n  <p>n1 - namenode</p>\n  \n  <p>n2 - ResouceManager or Yarn</p>\n</blockquote>\n\n<p>and remaining nodes can act as datanodes, try not to deploy anything else on the yarn node.</p>\n\n<p>Rest of the deployment for HBase, Hive and Nutch would remain same.</p>\n", "question_id": 26647204, "creation_date": 1415193503, "is_accepted": true, "score": 2, "last_activity_date": 1415255214, "answer_id": 26758230}, {"question_id": 26647204, "owner": {"user_id": 2245718, "accept_rate": 87, "link": "http://stackoverflow.com/users/2245718/irishdog", "user_type": "registered", "reputation": 308}, "body": "<p>In my opinion, you should install Hadoop in fully distributed mode, so the jobs could run in parallel manner and much faster, as the MapReduce tasks will be distributed in 4 machines. Of course, the Hadoop's master node should run in one single machine.</p>\n\n<p>If you need to process big amount of data, it's a good choice to install HBase in one single machine and the Hadoop in 3.</p>\n\n<p>You could make all the above very easy using tools/platforms with a very friendly GUI like Cloudera Manager and Hortonworks. They will help you to control and maintain your cluster better but they are also provide Health Monitoring, Cluster Analytics as well as E-Mail notifications for every error occurs in your cluster. </p>\n\n<p>Cloudera Manager\n<a href=\"http://www.cloudera.com/content/cloudera/en/products-and-services/cloudera-enterprise/cloudera-manager.html\" rel=\"nofollow\">http://www.cloudera.com/content/cloudera/en/products-and-services/cloudera-enterprise/cloudera-manager.html</a></p>\n\n<p>Hortonworks\n<a href=\"http://hortonworks.com/\" rel=\"nofollow\">http://hortonworks.com/</a></p>\n\n<p>In these two links, you can find more guidance about how you could costruct your cluster</p>\n", "creation_date": 1415218406, "is_accepted": false, "score": 0, "last_activity_date": 1415218406, "answer_id": 26766228}], "question_id": 26647204, "tags": ["apache", "hadoop", "hive", "hbase", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/26647204/apache-hadoop-hbase-and-nutch-components-distribution-for-4-servers-cluster", "last_activity_date": 1415255214, "accepted_answer_id": 26758230, "body": "<p>I have 4 systems. I want to crawl some data. For that first I need to configure cluster. I am confused about placement of components.</p>\n\n<ol>\n<li>should I place all component (hadoop, hive, hbase, nutch) in one machine and add other machines as nodes in hadoop?</li>\n<li>Should I place hbase in one machine, nutch in other and hadoop in third and add forth machine as slave of hadoop?</li>\n<li>Should HBase be in pseudo distributed mode or full distributed. </li>\n<li>How many slaves I sholud add in hbase if I run it as fully distributed mode.</li>\n</ol>\n\n<p>What should be the best way. PLease guide step by step ( For hbase and hadoop)</p>\n", "creation_date": 1414653751, "score": -1},
{"title": "How to use Nutch API into java application?", "view_count": 1686, "is_answered": false, "answers": [{"question_id": 26729016, "owner": {"user_id": 3496666, "accept_rate": 72, "link": "http://stackoverflow.com/users/3496666/kumar", "user_type": "registered", "reputation": 1801}, "body": "<p>You cannot include the jar normally as other java projects. <a href=\"http://wiki.apache.org/nutch/RunNutchInEclipse\" rel=\"nofollow\">see this to use nutch in eclipse </a></p>\n", "creation_date": 1415084138, "is_accepted": false, "score": 0, "last_activity_date": 1415084138, "answer_id": 26729320}], "question_id": 26729016, "tags": ["java", "web-crawler", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/26729016/how-to-use-nutch-api-into-java-application", "last_activity_date": 1415086151, "owner": {"user_id": 3338663, "answer_count": 0, "creation_date": 1393010667, "accept_rate": 0, "view_count": 10, "reputation": 6}, "body": "<p>I want to use Nutch API into my java application to crawl PDF links from a website for analyses, how can I do it using the Nutch jar in my java application,can I have an example ?</p>\n", "creation_date": 1415082667, "score": 0},
{"title": "Parse data with tika for apache solr", "view_count": 358, "is_answered": false, "answers": [{"question_id": 26454978, "owner": {"user_id": 3974380, "link": "http://stackoverflow.com/users/3974380/pascal-essiembre", "user_type": "registered", "reputation": 71}, "body": "<p><a href=\"http://www.norconex.com/product/collector-http/\" rel=\"nofollow\">Norconex HTTP Collector</a> will store with your document all possible metadata it could find, without restriction. That ranges from the HTTP Header values obtained when downloading a page, to all the  tags in that HTML page.</p>\n\n<p>That may likely be too much fields for you.   If so, you can reject those you do not want, or instead, be explicit about the ones you want to keep by adding a \"KeepOnlyTagger\" to your <code>&lt;importer&gt;</code> section in your configuration:</p>\n\n<pre><code>&lt;tagger class=\"com.norconex.importer.tagger.impl.KeepOnlyTagger\"\n    fields=\"title,pubdate,anotherone,etc\"/&gt;\n</code></pre>\n\n<p>You'll find how to get started quickly along with configuration options here: <a href=\"http://www.norconex.com/product/collector-http/configuration.html\" rel=\"nofollow\">http://www.norconex.com/product/collector-http/configuration.html</a></p>\n", "creation_date": 1414727537, "is_accepted": false, "score": 0, "last_activity_date": 1414727537, "answer_id": 26667284}], "question_id": 26454978, "tags": ["solr", "web-crawler", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/26454978/parse-data-with-tika-for-apache-solr", "last_activity_date": 1414727537, "owner": {"user_id": 3279550, "answer_count": 3, "creation_date": 1391689578, "accept_rate": 29, "view_count": 8, "reputation": 53}, "body": "<p>I have managed to get apache nutch to index a news website and pass the results off to Apache solr. </p>\n\n<p>Using this tutorial \n<a href=\"https://github.com/renepickhardt/metalcon/wiki/simpleNutchSolrSetup\" rel=\"nofollow\">https://github.com/renepickhardt/metalcon/wiki/simpleNutchSolrSetup</a> the only difference is I have decided to use Cassandra instead. </p>\n\n<p>As a test I am trying to crawl Cnn, to extract out the title of article's and the date it was published.</p>\n\n<p>Question 1: </p>\n\n<p><strong>How to parse data from the webpage, to extract the date and the title.</strong> </p>\n\n<p>I have found this article for a plugin. It seems a bit out dated and am not sure that it still applies. I have also read that Tika can be used as well but again most tutorials are quite old.</p>\n\n<p><a href=\"http://www.ryanpfister.com/2009/04/how-to-sort-by-date-with-nutch/\" rel=\"nofollow\">http://www.ryanpfister.com/2009/04/how-to-sort-by-date-with-nutch/</a></p>\n\n<p>Another SO article is this </p>\n\n<p><a href=\"http://stackoverflow.com/q/13893369/3279550\">How to extend Nutch for article crawling</a>. I would prefer to use Nutch, only because that is what I have started with. I have do not really have a preference.</p>\n\n<p>Anything would be a great help. </p>\n", "creation_date": 1413750004, "score": 0},
{"title": "how to create a nutch job in hadoop", "view_count": 502, "is_answered": true, "answers": [{"question_id": 23103403, "owner": {"user_id": 1708119, "accept_rate": 67, "link": "http://stackoverflow.com/users/1708119/h4ck3r", "user_type": "registered", "reputation": 4487}, "body": "<p><code>hadoop jar</code> command should be followed by a jar name. Replace apache-nutch-1.6.job with apache-nutch jar name and try again</p>\n\n<p>hadoop jar <strong>apache-nutch-XXX.jar</strong> org.apache.nutch.crawl.Crawl firstSite/urls -dir urls -depth 1 -topN 5</p>\n", "creation_date": 1397657611, "is_accepted": false, "score": 0, "last_activity_date": 1397657611, "answer_id": 23112009}, {"question_id": 23103403, "owner": {"user_id": 2340715, "link": "http://stackoverflow.com/users/2340715/michael-mcdonald", "user_type": "registered", "reputation": 11}, "body": "<p>The immediate answer is that your apache-nutch-1.6.job is missing or not in the current directory.</p>\n\n<p>The Apache Nutch .job file (e.g. apache-nutch-1.9.job) is created in runtime/deploy/ when you compile nutch (via 'ant')  Note that you need the source distribution of apache-nutch, not the binary version, to create this file.</p>\n\n<p>The .job file is actually a JAR file. It is significantly larger than apache-nutch-1.9.jar because it contains all dependencies (as well as files from conf/) within it, so that it contains everything needed to run a Hadoop job.</p>\n", "creation_date": 1414723263, "is_accepted": false, "score": 1, "last_activity_date": 1414723263, "answer_id": 26666752}], "question_id": 23103403, "tags": ["hadoop", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/23103403/how-to-create-a-nutch-job-in-hadoop", "last_activity_date": 1414723263, "owner": {"age": 27, "answer_count": 0, "creation_date": 1395293421, "user_id": 3440602, "view_count": 4, "location": "India", "reputation": 6}, "body": "<p>hadoop jar apache-nutch-1.6.job org.apache.nutch.crawl.Crawl firstSite/urls -dir urls -depth 1 -topN 5</p>\n\n<pre><code>Exception in thread \"main\" java.io.IOException: Error opening job jar: apache-nutch-1.6.job\n        at org.apache.hadoop.util.RunJar.main(RunJar.java:90)\nCaused by: java.io.FileNotFoundException: apache-nutch-1.6.job (No such file or directory)\n        at java.util.zip.ZipFile.open(Native Method)\n        at java.util.zip.ZipFile.&lt;init&gt;(ZipFile.java:215)\n        at java.util.zip.ZipFile.&lt;init&gt;(ZipFile.java:145)\n        at java.util.jar.JarFile.&lt;init&gt;(JarFile.java:153)\n        at java.util.jar.JarFile.&lt;init&gt;(JarFile.java:90)\n        at org.apache.hadoop.util.RunJar.main(RunJar.java:88)\n</code></pre>\n", "creation_date": 1397635242, "score": 1},
{"title": "Can&#39;t run Nutch2 on Hadoop2 (Nutch 2.x + Hadoop 2.4.0 + HBase 0.94.18 + Gora 0.5 + Avro 1.7.6)", "view_count": 800, "is_answered": true, "answers": [{"question_id": 26180364, "owner": {"user_id": 1466535, "accept_rate": 91, "link": "http://stackoverflow.com/users/1466535/sergey-weiss", "user_type": "registered", "reputation": 2451}, "body": "<p>We solved the problem with <code>EOFException</code>s and instability by setting old (i.e., hadoop-1.2.0) value for <code>io.serializations</code> property in conf/nutch-site.xml:</p>\n\n<pre><code>&lt;property&gt;\n  &lt;name&gt;io.serializations&lt;/name&gt;\n  &lt;value&gt;org.apache.hadoop.io.serializer.WritableSerialization&lt;/value&gt;\n  &lt;description&gt;A list of serialization classes that can be used for\n  obtaining serializers and deserializers.&lt;/description&gt;\n&lt;/property&gt;\n</code></pre>\n\n<p>And it turned out that patching Avro is not needed.</p>\n", "creation_date": 1414585760, "is_accepted": false, "score": 1, "last_activity_date": 1414585760, "answer_id": 26630400}], "question_id": 26180364, "tags": ["hbase", "nutch", "avro", "hadoop2", "gora"], "answer_count": 1, "link": "http://stackoverflow.com/questions/26180364/cant-run-nutch2-on-hadoop2-nutch-2-x-hadoop-2-4-0-hbase-0-94-18-gora-0-5", "last_activity_date": 1414585760, "owner": {"user_id": 3891482, "view_count": 5, "answer_count": 1, "creation_date": 1406723565, "reputation": 6}, "body": "<p>I need to install Nutch 2.3 for EMR in above configuration (subj).</p>\n\n<p>Done on local computer:</p>\n\n<ol>\n<li>Nutch 2.x</li>\n</ol>\n\n<p>1.1 svn current 2.x version</p>\n\n<p>1.2. prepared scripts:</p>\n\n<p>1.2.1 ivy:</p>\n\n<pre>\n    dependency org=\"org.apache.hadoop\" name=\"hadoop-common\" rev=\"2.4.0\"\n    dependency org=\"org.apache.hadoop\" name=\"hadoop-mapreduce-client-core\" rev=\"2.4.0\"\n    dependency org=\"org.apache.gora\" name=\"gora\" rev=\"0.5\"  \n    dependency org=\"org.apache.gora\" name=\"gora-hbase\" rev=\"0.5\"\n</pre>\n\n<p>1.2.2 default.properties:</p>\n\n<pre><code>hadoop.version=2.4.0\nversion=2.3-SNAPSHOT\n</code></pre>\n\n<p>1.3. added</p>\n\n<pre><code>public int getFieldsCount() { return Field.values().length; }\n</code></pre>\n\n<p>to ProtocolStatus.java, ParseStatus.java, Host.java, WebPage.java.</p>\n\n<ol start=\"2\">\n<li>HBase</li>\n</ol>\n\n<p>2.1 svn HBase 0.94.18</p>\n\n<p>2.2 prepared for Protobuf 2.5.0, also thanks to Dobromyslov [ <a href=\"https://github.com/dobromyslov\" rel=\"nofollow\">https://github.com/dobromyslov</a> ]</p>\n\n<p>2.3 also generated hbase-0.94.18-hadoop-2.4.0.jar</p>\n\n<ol start=\"3\">\n<li><p>Gora 0.5 (also was tested for versions 0.4, 0.6-SNAPSHOT, and 0.5.3 from com.argonio.gora)</p></li>\n<li><p>Avro 1.7.6 (also played with versions 1.7.4, 1.7.7)</p></li>\n</ol>\n\n<p>4.1 svn</p>\n\n<p>4.2 patched for AVRO-813</p>\n\n<p>4.3 patched for AVRO-882 and rollbacked</p>\n\n<p>4.4 patched as in [1] - commented throwing EOFException against</p>\n\n<pre><code>org.apache.avro.io.BinaryDecoder.ensureBounds(BinaryDecoder.java:473),\n</code></pre>\n\n<p>etc.</p>\n\n<p>After numerous exceptions, some changes have been made in Nutch 2.x and Avro 1.7.6.</p>\n\n<p>Nutch looks like a bit of running, but is unstable and incorrect.</p>\n\n<p>Cycle (inject, generate, fetch, parse, updatedb) passed but some functionalities are broken and ignored.</p>\n\n<p>It seems that i broke the normal data exchange between Nutch and HBase (also with gora and avro). Some fields (and/or some of the data formats) read and write incorrectly. F.e. many markers are lost (temporary emulated in code); data in batchId field are lost; scoring is broken also.</p>\n\n<p>Please help! I'm ready to publish all my diffs and exception traces.</p>\n\n<p>[1] <a href=\"http://mail-archives.apache.org/mod_mbox/nutch-user/201409.mbox/%3cCAEmTxX9HrRM00SxerFAdRdZy=wVAd9xCchDTuLaxPQ=wi0QEsw@mail.gmail.com%3e\" rel=\"nofollow\">http://mail-archives.apache.org/mod_mbox/nutch-user/201409.mbox/%3cCAEmTxX9HrRM00SxerFAdRdZy=wVAd9xCchDTuLaxPQ=wi0QEsw@mail.gmail.com%3e</a></p>\n", "creation_date": 1412344680, "score": 1},
{"title": "block some part of web page to be indexed", "view_count": 189, "is_answered": true, "answers": [{"question_id": 9913863, "owner": {"user_id": 1150329, "accept_rate": 89, "link": "http://stackoverflow.com/users/1150329/tejas-patil", "user_type": "registered", "reputation": 4793}, "body": "<p>I am working with nutch codebase since past 2 years and as far i have seen, this aint possible. Once the content enters the nutch segments, you cant strip off parts like drop-down menu, navigation etc from it and keep only the required stuff. </p>\n\n<p>If you or anyone else knows how to do it (off course..without modifying the code), please share the same.</p>\n", "creation_date": 1333996240, "is_accepted": false, "score": 0, "last_activity_date": 1333996240, "answer_id": 10077902}, {"question_id": 9913863, "owner": {"user_id": 3745204, "link": "http://stackoverflow.com/users/3745204/user3745204", "user_type": "registered", "reputation": 11}, "body": "<p>Not sure, if you still need to do this, but just in case you do, you could try blacklist_whitelist plug-in which can be found at <a href=\"https://issues.apache.org/jira/browse/NUTCH-585\" rel=\"nofollow\">https://issues.apache.org/jira/browse/NUTCH-585</a>.</p>\n\n<p>The plug-in allows you to have a list of the elements you want to either block or allow but not both.\nfor example:</p>\n\n<pre><code>&lt;property&gt;\n  &lt;name&gt;parser.html.blacklist&lt;/name&gt;\n  &lt;value&gt;noscript,div,#footer&lt;/value&gt;\n  &lt;description&gt;\n   A comma-delimited list of css like tags to identify the elements which should\n   NOT be parsed. Use this to tell the HTML parser to ignore the given elements, e.g. site navigation.\n   It is allowed to only specify the element type (required), and optional its class name ('.')\n   or ID ('#'). More complex expressions will not be parsed.\n   Valid examples: div.header,span,p#test,div#main,ul,div.footercol\n   Invalid expressions: div#head#part1,#footer,.inner#post\n   Note that the elements and their children will be silently ignored by the parser,\n   so verify the indexed content with Luke to confirm results.\n   Use either 'parser.html.blacklist' or 'parser.html.whitelist', but not both of them at once. If so,\n   only the whitelist is used.\n  &lt;/description&gt;\n&lt;/property&gt;\n</code></pre>\n", "creation_date": 1414420630, "is_accepted": false, "score": 1, "last_activity_date": 1414420630, "answer_id": 26589979}], "question_id": 9913863, "tags": ["nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/9913863/block-some-part-of-web-page-to-be-indexed", "last_activity_date": 1414420630, "owner": {"user_id": 1292056, "view_count": 2, "answer_count": 0, "creation_date": 1332729410, "reputation": 6}, "body": "<p>I crawled a web site. there are a lot of common contents on the pages, like drop-down menu, navigation. How to prevent these contents from being indexed?</p>\n", "creation_date": 1332960673, "score": 1},
{"title": "How can i crawl data from hbase using nutch", "view_count": 739, "is_answered": true, "answers": [{"question_id": 8485608, "owner": {"user_id": 608975, "link": "http://stackoverflow.com/users/608975/codingfoo", "user_type": "registered", "reputation": 668}, "body": "<p>Nutch is designed to crawl websites and so does not know how to crawl hbase.</p>\n\n<p>Since you are trying to index content from hbase into solr, you have at least two options:</p>\n\n<p>If you have an application that saves data into hbase, modify it to index the data into solr, after the data is saved in hbase.</p>\n\n<p>You can also write a Map/Reduce job to send data from hbase into solr.</p>\n\n<p>You should also look into the <a href=\"http://www.lilyproject.org/lily/index.html\" rel=\"nofollow\">lily project</a>. It integrates hbase and solr.</p>\n\n<p>Also if your search queries are simple, you could design your hbase schema, so that you dont need solr.</p>\n", "creation_date": 1323788250, "is_accepted": false, "score": 1, "last_activity_date": 1323788250, "answer_id": 8491264}, {"question_id": 8485608, "owner": {"user_id": 1102209, "link": "http://stackoverflow.com/users/1102209/preethi-vinayak-ponangi", "user_type": "registered", "reputation": 381}, "body": "<p>I don't think \"crawl\" is the right terminology to use when it comes to databases. Crawling is a specific use case, when you want to \"crawl\" web pages, you don't know what you are starting with. You don't know what the end point is. Hence you start at some point and try to discover what lies out there. </p>\n\n<p>When it comes to a database, a NOSQL database like HBase. You already \"know\" what is in that database. All you have to do is retrieve the information completely, or retrieve partially what's in it based on specific queries. </p>\n\n<p>I see that probably in your case, you could tweak Lucene to build your indexes from the data queried from HBase and then feed these indexes to Solr to create a full pledged search application.</p>\n\n<p>Hope this might be the direction you are looking for.</p>\n", "creation_date": 1324048617, "is_accepted": false, "score": 1, "last_activity_date": 1324048617, "answer_id": 8536180}], "question_id": 8485608, "tags": ["hbase", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/8485608/how-can-i-crawl-data-from-hbase-using-nutch", "last_activity_date": 1414415961, "owner": {"user_id": 674793, "answer_count": 53, "creation_date": 1300966135, "accept_rate": 33, "view_count": 156, "location": "India", "reputation": 605}, "body": "<p>My requirement is to crawl data from HBASE using Nutch, and then index it into Solr. how can i proceed on this? </p>\n", "creation_date": 1323760586, "score": 2},
{"title": "nutch on hadoop 2 (yarn) log files", "view_count": 166, "is_answered": false, "answers": [{"question_id": 26519734, "owner": {"user_id": 3812643, "accept_rate": 67, "link": "http://stackoverflow.com/users/3812643/jinhong-lu", "user_type": "registered", "reputation": 83}, "body": "<p>Do you mean the logs in $NUTCH_HOME/runtime/local/logs?\nOr the logs on console?  you can run the task by shell, and redirect the long to a file </p>\n", "creation_date": 1414029838, "is_accepted": false, "score": 0, "last_activity_date": 1414029838, "answer_id": 26520168}], "question_id": 26519734, "tags": ["nutch", "yarn"], "answer_count": 1, "link": "http://stackoverflow.com/questions/26519734/nutch-on-hadoop-2-yarn-log-files", "last_activity_date": 1414029838, "owner": {"user_id": 1058511, "answer_count": 22, "creation_date": 1321905325, "accept_rate": 47, "view_count": 109, "reputation": 591}, "body": "<p>I am running nutch on a yarn cluster. I suspect that some of the crawl jobs timed out. I would like to see nutch log to varify this. On cloudera console, I only see log output about the yarn tasks, but nothing that is directly output by nutch. </p>\n\n<p>Does anyone know where the nutch log would go?</p>\n", "creation_date": 1414026705, "score": 0},
{"title": "What are the benefits of applying Apache Tika to Solr instead of Nutch", "view_count": 66, "is_answered": true, "answers": [{"question_id": 26498977, "owner": {"user_id": 417864, "accept_rate": 75, "link": "http://stackoverflow.com/users/417864/alexandre-rafalovitch", "user_type": "registered", "reputation": 6649}, "body": "<p>Apply it as early as you can but make sure to keep the original, full-fidelity, document somewhere as well. </p>\n\n<p>There is no point passing a binary file around if you know that in the end you are going to reduce it to a set of metadata fields and get rid of the rest.</p>\n", "creation_date": 1414009562, "is_accepted": false, "score": 2, "last_activity_date": 1414009562, "answer_id": 26516490}], "question_id": 26498977, "tags": ["solr", "nutch", "apache-tika"], "answer_count": 1, "link": "http://stackoverflow.com/questions/26498977/what-are-the-benefits-of-applying-apache-tika-to-solr-instead-of-nutch", "last_activity_date": 1414009562, "owner": {"user_id": 3279550, "answer_count": 3, "creation_date": 1391689578, "accept_rate": 29, "view_count": 8, "reputation": 53}, "body": "<p>I am trying to crawl data with Apache Nutch and index it with Apache Solr.</p>\n\n<p>As part of this I want to parse the content as well. I am trying to figure out is it better to apply Tika to Nutch  , to Solr or both.</p>\n", "creation_date": 1413940902, "score": 0},
{"title": "Solr+Nutch+AjaxSolr query", "view_count": 526, "owner": {"user_id": 1162512, "answer_count": 27, "creation_date": 1327163131, "accept_rate": 58, "view_count": 772, "reputation": 2192}, "is_answered": true, "answers": [{"last_edit_date": 1406730296, "owner": {"user_id": 1526859, "accept_rate": 82, "link": "http://stackoverflow.com/users/1526859/alexander-chepurnoy", "user_type": "registered", "reputation": 474}, "body": "<ol>\n<li>Look at Solr with multiple cores, it's better not to try mix documents with different nature in one collection</li>\n<li>There are many APIs for SOLR, such as SOLRJ for Java (<a href=\"http://wiki.apache.org/solr/Solrj\" rel=\"nofollow\">http://wiki.apache.org/solr/Solrj</a>), SolPHP for PHP (<a href=\"http://wiki.apache.org/solr/SolPHP\" rel=\"nofollow\">http://wiki.apache.org/solr/SolPHP</a>) and so on.</li>\n</ol>\n", "question_id": 11487394, "creation_date": 1342355303, "is_accepted": true, "score": 1, "last_activity_date": 1406730296, "answer_id": 11491894}], "question_id": 11487394, "tags": ["solr", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/11487394/solrnutchajaxsolr-query", "last_activity_date": 1413970546, "accepted_answer_id": 11491894, "body": "<p>1) I referred <a href=\"https://github.com/evolvingweb/ajax-solr/wiki/reuters-tutorial\" rel=\"nofollow\">https://github.com/evolvingweb/ajax-solr/wiki/reuters-tutorial</a> for Ajax-Solr setup. </p>\n\n<p>I want to know that although ajax-solr is running but it's searching under only reuters data. If I want to crawl the web using nutch and integrate it with solr,then i have to replace solr's schema.xml file with nutch's schema.xml file which will not be according to ajax-solr configuration. By replacing the schema.xml files, ajax-solr wont work(correct me if I am wrong)!!! </p>\n\n<p>How would I now integrate Solr with Nutch along with Ajax-Solr so ajax-Solr can search other data on the web as well?? </p>\n\n<p>2) I would like to ask whether there are any front end API for Solr searching,except Ajax-Solr, which would help in efficient searching of the crawled web?</p>\n", "creation_date": 1342300001, "score": 0},
{"title": "Nutch and Elasticsearch 1.1.1", "view_count": 804, "owner": {"user_id": 3739277, "answer_count": 0, "creation_date": 1402697843, "view_count": 0, "location": "San Francisco, CA", "reputation": 3}, "is_answered": true, "answers": [{"question_id": 24214624, "owner": {"user_id": 432844, "link": "http://stackoverflow.com/users/432844/julien-nioche", "user_type": "registered", "reputation": 939}, "body": "<p>The trunk contains a patch <a href=\"https://issues.apache.org/jira/browse/NUTCH-1745\" rel=\"nofollow\">https://issues.apache.org/jira/browse/NUTCH-1745</a> which has been committed and will be in Nutch-1.9. It should fix your problem - although the message you posted doesn't help determining what the issue actually is.</p>\n\n<p>BTW you'd get a more relevant audience by posting on the <a href=\"http://nutch.apache.org/mailing_lists.html\" rel=\"nofollow\">Nutch user list</a></p>\n", "creation_date": 1403107143, "is_accepted": true, "score": 1, "last_activity_date": 1403107143, "answer_id": 24290140}, {"question_id": 24214624, "owner": {"user_id": 126217, "link": "http://stackoverflow.com/users/126217/digital-illusion", "user_type": "registered", "reputation": 375}, "body": "<p>It happened to me as well, and turned out that the indexer plugin <code>indexer-elastic</code> was still configured to use an old version of elasticsearch as a dependency in <code>src\\plugin\\indexer-elastic\\ivy.xml</code></p>\n", "creation_date": 1413917684, "is_accepted": false, "score": 0, "last_activity_date": 1413917684, "answer_id": 26494173}], "question_id": 24214624, "tags": ["java", "elasticsearch", "search-engine", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/24214624/nutch-and-elasticsearch-1-1-1", "last_activity_date": 1413917684, "accepted_answer_id": 24290140, "body": "<p>I am using Elasticsearch 1.1.1 and trying to get Nutch working with it. I have tried both version 1.8 and 2.2.1. When I finish running the crawl, I get this for final results:</p>\n\n<pre><code>...\nIndexer: starting at 2014-06-13 00:14:33\nIndexer: deleting gone documents: false\nIndexer: URL filtering: false\nIndexer: URL normalizing: false\nActive IndexWriters :\nElasticIndexWriter\nelastic.cluster : elastic prefix cluster\nelastic.host : hostname\nelastic.port : port\nelastic.index : elastic index command \nelastic.max.bulk.docs : elastic bulk index doc counts. (default 250) \nelastic.max.bulk.size : elastic bulk index length. (default 2500500 ~2.5MB)\n\n\nIndexer: java.io.IOException: Job failed!\nat org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:1357)\nat org.apache.nutch.indexer.IndexingJob.index(IndexingJob.java:114)\nat org.apache.nutch.indexer.IndexingJob.run(IndexingJob.java:176)\nat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\nat org.apache.nutch.indexer.IndexingJob.main(IndexingJob.java:186)\n</code></pre>\n\n<p>I have updated the ivy.xml file for the new version, and tried every variation of things I can figure out from various tutorials and posts on here.</p>\n", "creation_date": 1402698706, "score": 0},
{"title": "Web crawler with incremental crawling support for windows", "view_count": 597, "owner": {"age": 25, "answer_count": 126, "creation_date": 1396591478, "user_id": 3496666, "accept_rate": 72, "view_count": 618, "location": "Chennai, India", "reputation": 1801}, "is_answered": true, "answers": [{"question_id": 25973854, "owner": {"user_id": 3974380, "link": "http://stackoverflow.com/users/3974380/pascal-essiembre", "user_type": "registered", "reputation": 71}, "body": "<p>Looks like a perfect match for <a href=\"http://www.norconex.com/product/collector-http/\" rel=\"nofollow\">Norconex HTTP Collector</a>:</p>\n\n<ul>\n<li>It is written 100% in Java.</li>\n<li>It runs fully on Windows (without the need for Cygwin or a Linux/Unix VM).</li>\n<li>It is well documented with examples and a forum to ask questions/raise issues (github).</li>\n<li>It supports incremental crawlings, detecting modified documents as well as deleted ones. </li>\n<li>It supports both Solr and Elasticsearch, and more (via the use of its \"Committers\").</li>\n<li>It is extensively configurable/flexible.  It is easy to integrate with it and provide custom features to it without having to learn complex plugin mechanism (implement an interface, put it in classpath, and voil\u00e0). </li>\n<li>Its development is very active.</li>\n</ul>\n\n<p>It is maintained by Norconex, a company of enterprise search professionals.  Issues addressed quickly.  Version 2.0.0 is heavily being worked on, soon bringing many new features (language detection, document splitting, etc).</p>\n\n<p>It is GPL but Norconex offers a commercial license if GPL is a problem for you.</p>\n\n<p>It also has many other features you did not list, like the ability to manipulate the document content before sending it to your search engine.  It also supports sitemaps, robots rules, etc.  I invite you to give it a try: <a href=\"http://www.norconex.com/product/collector-http/\" rel=\"nofollow\">http://www.norconex.com/product/collector-http/</a></p>\n", "creation_date": 1413012077, "is_accepted": true, "score": 0, "last_activity_date": 1413012077, "answer_id": 26312159}], "question_id": 25973854, "tags": ["java", "solr", "web-crawler", "nutch", "crawler4j"], "answer_count": 1, "link": "http://stackoverflow.com/questions/25973854/web-crawler-with-incremental-crawling-support-for-windows", "last_activity_date": 1413012077, "accepted_answer_id": 26312159, "body": "<ul>\n<li><p>I need a open source web crawler developed in java with incremental crawling support.</p></li>\n<li><p>Web crawler should be easily customized and integrated with solr or elasticsearch.</p></li>\n<li><p>It should be an active one which is developing further with more features.</p></li>\n<li><p>Aperture is one of a good crawler, it has all features i mentioned but its not an active crawler and due to license (if i use it for commercial purpose) of their dependency i ignored.</p></li>\n<li><p>Nutch - a web crawler which has more features with hadoop support. But i go through many websites and tutorials, there is no proper documents, api found for customizing it programmatically in windows. I could edit the code in eclipse but it cause many errors while running map reduce jobs. There is no java api for nutch to implement like aperture.</p></li>\n<li><p>Crawl4j is a good web crawler but it has no incremental crawling features and i haven't checked license problems.</p></li>\n</ul>\n\n<p>Is there any other crawler which have all features that i mentioned or is there any way to use any one of above mentioned crawler for my requirements?</p>\n\n<p>Helpful answers will be greatly appreciated.</p>\n", "creation_date": 1411387993, "score": 1},
{"title": "Nutch error &quot;QueueFeeder finished: total 0 records. Hit by time limit :36&quot;", "view_count": 185, "is_answered": false, "question_id": 26276690, "tags": ["apache", "web-crawler", "nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/26276690/nutch-error-queuefeeder-finished-total-0-records-hit-by-time-limit-36", "last_activity_date": 1412852295, "owner": {"user_id": 3454410, "answer_count": 20, "creation_date": 1395646154, "accept_rate": 48, "view_count": 174, "reputation": 832}, "body": "<p>I have about 17000 urls in my urls/seed.txt file. But When I start my crawler(apache Nutch), after job generation, it gives following message</p>\n\n<p>QueueFeeder finished: total 0 records. Hit by time limit :36</p>\n\n<p>and then display -finishing thread FetcherThread0, activeThreads=19 etc. And crawl 0 page.\nWhat is this problem , can anyone guide me?</p>\n", "creation_date": 1412852295, "score": 1},
{"title": "What Nutch command do I need to invoke from the command line if I update the url filter text", "view_count": 154, "is_answered": false, "answers": [{"question_id": 25538609, "owner": {"user_id": 1825741, "link": "http://stackoverflow.com/users/1825741/bharath", "user_type": "registered", "reputation": 1}, "body": "<p>If you change the regex-urlfilter.txt file, you need to update the nutch job file. This can be done as:</p>\n\n<p><code>jar -uvf /usr/local/nutch-1.2/nutch-1.2.job &lt;path to regex-urlfilter.txt&gt;</code></p>\n", "creation_date": 1412838581, "is_accepted": false, "score": 0, "last_activity_date": 1412838581, "answer_id": 26272343}], "question_id": 25538609, "tags": ["java", "mapreduce", "nutch", "web-crawler"], "answer_count": 1, "link": "http://stackoverflow.com/questions/25538609/what-nutch-command-do-i-need-to-invoke-from-the-command-line-if-i-update-the-url", "last_activity_date": 1412838581, "owner": {"user_id": 300327, "answer_count": 45, "creation_date": 1269379533, "accept_rate": 40, "view_count": 224, "reputation": 1084}, "body": "<p>Nutch Gurus,</p>\n\n<p>If I change files such as <strong>robots.txt</strong>, or <strong>regex-urlfilter.txt</strong> and any such resources, which command do I need to invoke?</p>\n\n<p>I was not sure from the nutch instructions. I am guessing it is parser job, but I am not sure.</p>\n\n<p>Kartik</p>\n\n<p>From the instructions</p>\n\n<pre><code># echo \" crawl one-step crawler for intranets\"\n  echo \" inject     inject new urls into the database\"\n  echo \" hostinject     creates or updates an existing host table from a text file\"\n  echo \" generate   generate new batches to fetch from crawl db\"\n  echo \" fetch      fetch URLs marked during generate\"\n  echo \" parse      parse URLs marked during fetch\"\n  echo \" updatedb   update web table after parsing\"\n  echo \" updatehostdb   update host table after parsing\"\n  echo \" readdb     read/dump records from page database\"\n  echo \" readhostdb     display entries from the hostDB\"\n  echo \" elasticindex   run the elasticsearch indexer\"\n  echo \" solrindex  run the solr indexer on parsed batches\"\n  echo \" solrdedup  remove duplicates from solr\"\n  echo \" parsechecker   check the parser for a given url\"\n  echo \" indexchecker   check the indexing filters for a given url\"\n  echo \" plugin     load a plugin and run one of its classes main()\"\n  echo \" nutchserver    run a (local) Nutch server on a user defined port\"\n  echo \" junit          runs the given JUnit test\"\n  echo \" or\"\n  echo \" CLASSNAME  run the class named CLASSNAME\"\n  echo \"Most commands print help when invoked w/o parameters.\"\n</code></pre>\n", "creation_date": 1409182907, "score": 0},
{"title": "nutch crawling with solr, job failed for depth &gt;= 2", "view_count": 348, "is_answered": false, "question_id": 26266513, "tags": ["nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/26266513/nutch-crawling-with-solr-job-failed-for-depth-2", "last_activity_date": 1412802612, "owner": {"user_id": 2327028, "answer_count": 1, "creation_date": 1367072106, "view_count": 10, "location": "Jersey City, NJ, United States", "reputation": 42}, "body": "<p>I am trying to run Nutch crawler on my local machine and want to index the retrieved data using solr.\nUsing apache-nutch-1.9 and solr-4.10.1</p>\n\n<p>As of now they are installed and seem to run for depth = 1.</p>\n\n<p>I get the following error when depth = 2</p>\n\n<pre><code>bin/crawl urls/ crawl http://localhost:8983/solr 2\n</code></pre>\n\n<p>.....</p>\n\n<pre><code>Indexer: java.io.IOException: Job failed!\n    at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:1357)\n    at org.apache.nutch.indexer.IndexingJob.index(IndexingJob.java:114)\n    at org.apache.nutch.indexer.IndexingJob.run(IndexingJob.java:176)\n    at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n    at org.apache.nutch.indexer.IndexingJob.main(IndexingJob.java:186)\n</code></pre>\n", "creation_date": 1412802612, "score": 0},
{"title": "How to crawl latest articles in a specific domain using a specific set of websites?", "view_count": 159, "is_answered": false, "question_id": 26232688, "tags": ["web-crawler", "nutch", "crawler4j"], "answer_count": 0, "link": "http://stackoverflow.com/questions/26232688/how-to-crawl-latest-articles-in-a-specific-domain-using-a-specific-set-of-websit", "last_activity_date": 1412673679, "owner": {"user_id": 3052875, "answer_count": 3, "creation_date": 1385838274, "accept_rate": 50, "view_count": 7, "reputation": 34}, "body": "<p>I'm interested to build a program to get <strong>all latest articles in a specific domain (\"computer science\") from a specific set of websites (\"ScienceDirect\" for example)</strong>.\nAs you know, some websites publish a page for each research article, such as: <a href=\"http://www.sciencedirect.com/science/article/pii/S108480451400085X\" rel=\"nofollow\">http://www.sciencedirect.com/science/article/pii/S108480451400085X</a>\nEach page contains the information of a specific article.</p>\n\n<p>I'm interested to know what is a best <strong>tool (open source)</strong> for this purpose?\nGeneral web crawlers (such as Apache Nutch) provide a general framework to crawl the whole web but in my case I need a <strong>website(s)-specific crawler</strong>.</p>\n", "creation_date": 1412673679, "score": 0},
{"title": "Why can it run in main method but not in servlet doGet method", "view_count": 218, "is_answered": false, "question_id": 26142606, "tags": ["java", "eclipse", "servlets", "hadoop", "nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/26142606/why-can-it-run-in-main-method-but-not-in-servlet-doget-method", "last_activity_date": 1412302579, "owner": {"user_id": 2357926, "answer_count": 0, "creation_date": 1367922783, "accept_rate": 40, "view_count": 15, "reputation": 131}, "body": "<p>I indeed run Nutch 1.2 in eclipse EE. If I use Nutch API in main method, then everything is fine, it can run and the result is OK, but I just want to make it as a web project, so I use Servlet doGet method (didn't modify the programm, keep the same) , but after I click \"run on server\", it shows:</p>\n\n<blockquote>\n  <p>**INFO ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s).\n  INFO ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s).\n  INFO ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s).\n      Servlet.service() for servlet return_searchresult_webpage threw exception\n      java.net.ConnectException: Call to localhost/127.0.0.1:9000 failed on connection      exception: java.net.ConnectException: Connection refused: no further information\n      at org.apache.hadoop.ipc.Client.wrapException(Client.java:767)\n      at org.apache.hadoop.ipc.Client.call(Client.java:743)\n      at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:220)\n      at com.sun.proxy.$Proxy2.getProtocolVersion(Unknown Source)\n      at org.apache.hadoop.ipc.RPC.getProxy(RPC.java:359)\n      at org.apache.hadoop.hdfs.DFSClient.createRPCNamenode(DFSClient.java:106)\n      at org.apache.hadoop.hdfs.DFSClient.(DFSClient.java:207)\n      at org.apache.hadoop.hdfs.DFSClient.(DFSClient.java:170)\n      at     org.apache.hadoop.hdfs.DistributedFileSystem.initialize(DistributedFileSystem.java:82)\n      at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:1378)\n      at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:66)\n      at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:1390)\n      at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:196)\n      at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:95)\n      at org.apache.nutch.searcher.NutchBean.(NutchBean.java:90)\n      at yuhao.return_searchresult_webpage.doGet(return_searchresult_webpage.java:63)\n      at javax.servlet.http.HttpServlet.service(HttpServlet.java:707)\n      at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)\n      at    org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:290)\n      at   org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206)\n      at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:233)\n      at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:191)\n      at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:127)\n      at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:103)\n      at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:109)\n      at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:293)\n      at org.apache.coyote.http11.Http11Processor.process(Http11Processor.java:861)\n      at   org.apache.coyote.http11.Http11Protocol$Http11ConnectionHandler.process(Http11Protocol.java:606)\n      at org.apache.tomcat.util.net.JIoEndpoint$Worker.run(JIoEndpoint.java:489)\n      at java.lang.Thread.run(Unknown Source) **&lt;</p>\n</blockquote>\n\n<p>Server is tomcat 6. It seems that it is a connection problem for Hadoop, Nutch uses hadoop, and I have \"hadoop-0.20.2-core.jar, hadoop-core-0.20.2-cdh3u0.jar\" . But I didn't find that the 9000 port is occupied by any programm, and the tomcat occupies 8080, 8005, 8009 ports. I am really frustrated. Can anyone give me a guidance on what the problem is? Thanks so much for your help!</p>\n", "creation_date": 1412170210, "score": 0},
{"title": "Using Nutch to Retrive Page Contents", "view_count": 103, "is_answered": true, "answers": [{"question_id": 26121329, "owner": {"user_id": 1699191, "link": "http://stackoverflow.com/users/1699191/ameertawfik", "user_type": "registered", "reputation": 618}, "body": "<p>Well, there are many issues you want to address. Below are the issues with their solutions:</p>\n\n<ol>\n<li><strong>Limiting crawling to seed list</strong>: enable the <strong>scoring-depth</strong> plugin and configure it to allow only 1 level of crawling.</li>\n<li><strong>Getting textual content</strong>: Nutch does that by default.</li>\n<li><strong>Getting HTML raw data</strong>: it is not possible by Nutch 1.9. You need to download Nutch from its trunk repository and build it because the HTML content is scheduled for Nutch's next release (1.10). </li>\n<li><strong>Extracting outlinks</strong>: you can do that, but you have to write a new indexingFilter to index the outlinks.</li>\n<li><strong>Doing all of the above without Solr</strong>: you can do that. However, you have to write a new indexer that stores the extract data in whatever format you want.</li>\n</ol>\n", "creation_date": 1412100780, "is_accepted": false, "score": 1, "last_activity_date": 1412100780, "answer_id": 26127357}], "question_id": 26121329, "tags": ["java", "web-crawler", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/26121329/using-nutch-to-retrive-page-contents", "last_activity_date": 1412100780, "owner": {"age": 28, "answer_count": 8, "creation_date": 1260730497, "user_id": 230780, "accept_rate": 79, "view_count": 136, "location": "Tehran, Iran", "reputation": 1021}, "body": "<p>I have a very large list of seeds to be crawled (only those seeds are needed without any deepening). How can I use Nutch to retrieve:</p>\n\n<ol>\n<li>the HTML of</li>\n<li>the text content of</li>\n<li>(Preferably) the out-links of </li>\n</ol>\n\n<p>the seed pages? (without any indexing and integration into any other platform like Solr).</p>\n\n<p>Thanks</p>\n", "creation_date": 1412081414, "score": 0},
{"title": "Cygwin command through java application on windows", "view_count": 152, "is_answered": false, "question_id": 26031402, "tags": ["java", "windows", "hadoop", "cygwin", "nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/26031402/cygwin-command-through-java-application-on-windows", "last_activity_date": 1412081368, "owner": {"age": 25, "answer_count": 126, "creation_date": 1396591478, "user_id": 3496666, "accept_rate": 72, "view_count": 618, "location": "Chennai, India", "reputation": 1801}, "body": "<ol>\n<li>I am working with nutch 1.9 on windows using cygwin and it works well.</li>\n<li>Already asked <a href=\"http://stackoverflow.com/questions/25933941/how-to-run-nutch-1-9-in-eclipse-on-windows\">How to run nutch 1.9 in eclipse on windows?</a> but not getting any answer yet.</li>\n<li>Is there any way to run cygwin command through java program?</li>\n<li>I created a script file and tried to execute from batch file using cygwin bash, but it cause \"file does not exists\" for storing crawled data.</li>\n</ol>\n\n<p><strong>print.sh</strong><br/></p>\n\n<blockquote>\n  <p><em>#!/bin/bash <br/>\n  cd /cygdrive/c/cygwin/home/apache-nutch-1.9/\n  <br/> bin/crawl urls/ crawlresult/ http://localhost:8983/solr/test 1</em></p>\n</blockquote>\n\n<p><strong>sample.bat</strong></p>\n\n<blockquote>\n  <p><em>C:\\cygwin\\bin\\bash C:\\Users\\user\\Desktop\\print.sh</em></p>\n</blockquote>\n\n<p><strong>Error</strong></p>\n\n<blockquote>\n  <p><em>Fetcher: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/C:/cygwin/home/apache-nutch-1.9/crawlresult/segments/crawl_generate\n          at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.j\n  ava:190)</em></p>\n</blockquote>\n\n<p>I can run nutch command from cygwin terminal, but fails when running script file as above.</p>\n", "creation_date": 1411623765, "score": 1},
{"title": "Need an open source crawler like Apache Nutch without Hadoop", "view_count": 789, "is_answered": true, "answers": [{"question_id": 22842728, "owner": {"user_id": 1038826, "link": "http://stackoverflow.com/users/1038826/zsxwing", "user_type": "registered", "reputation": 9192}, "body": "<p>I think you only need the linux command <code>wget</code>.\nFor example, assume that the hosts are put in the file <code>hosts.txt</code>. You can use the following command to download them:</p>\n\n<pre><code>for host in `cat hosts.txt` ; do wget -r -d 1 -H \"$host\"; done\n</code></pre>\n\n<p>-r means recursive, -d 1 means only download the level 1, -H means do not limit the domain name.</p>\n\n<p>You can search <code>wget recursive download</code> in google, or run <code>man wget</code> to get more information.</p>\n", "creation_date": 1396576248, "is_accepted": false, "score": 1, "last_activity_date": 1396576248, "answer_id": 22852100}, {"question_id": 22842728, "owner": {"user_id": 230780, "accept_rate": 79, "link": "http://stackoverflow.com/users/230780/shayan", "user_type": "registered", "reputation": 1021}, "body": "<p>Nutch is no longer bound to Hadoop:</p>\n\n<blockquote>\n  <p>By default, Nutch no longer comes with a Hadoop distribution, however\n  when run in local mode e.g. running Nutch in a single process on one\n  machine, then we use Hadoop as a dependency. This may suit you fine if\n  you have a small site to crawl and index, but most people choose Nutch\n  because of its capability to run on in deploy mode, within a Hadoop\n  cluster.</p>\n</blockquote>\n\n<p>from: <a href=\"http://wiki.apache.org/nutch/NutchHadoopTutorial\" rel=\"nofollow\">http://wiki.apache.org/nutch/NutchHadoopTutorial</a></p>\n", "creation_date": 1412080503, "is_accepted": false, "score": 0, "last_activity_date": 1412080503, "answer_id": 26121027}], "question_id": 22842728, "tags": ["hadoop", "web-crawler", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/22842728/need-an-open-source-crawler-like-apache-nutch-without-hadoop", "last_activity_date": 1412080503, "owner": {"user_id": 956730, "answer_count": 52, "creation_date": 1316601948, "accept_rate": 86, "view_count": 99, "reputation": 731}, "body": "<p>I need an open source crawler with features like url normalizer, url filter, parser, politeness, excluding some urls but what i'm doing is not big in anyway. It is just about 500 hosts with their 1 level outlinks that i need to keep up to date. I don't like to reinvent the wheel by implementing all these good features that Nutch has meanwhile i don't like the overhead of Hadoop for this small task.</p>\n\n<p>Is there any fork of Nutch without Hadoop? or any other simple crawler with these features?\nI don't need any adaptive fetch scheduling, ranking, etc. I just have a list of hosts that i should fetch their outlinks using one single machine.</p>\n\n<p>My preference is some fork of Nutch because i have experience using it.</p>\n", "creation_date": 1396540130, "score": 2},
{"title": "Nutch in Windows: Failed to set permissions of path", "view_count": 5388, "owner": {"age": 36, "answer_count": 7, "creation_date": 1355334507, "user_id": 1898687, "accept_rate": 77, "view_count": 19, "location": "Hamburg", "reputation": 77}, "is_answered": true, "answers": [{"question_id": 15188050, "owner": {"user_id": 486130, "accept_rate": 75, "link": "http://stackoverflow.com/users/486130/mille-bii", "user_type": "registered", "reputation": 63}, "body": "<p>I have Nutch running on windows, no custom build. It's a long time since I haven't used it though. But one thing that took me a while to catch, is that you need to run cygwin as a windows admin to get the necessary rights.</p>\n", "creation_date": 1362387578, "is_accepted": false, "score": 1, "last_activity_date": 1362387578, "answer_id": 15197795}, {"question_id": 15188050, "owner": {"user_id": 525590, "accept_rate": 75, "link": "http://stackoverflow.com/users/525590/jpee", "user_type": "registered", "reputation": 111}, "body": "<p>We are using Nutch too, but it is not supported for running on Windows, on Cygwin our 1.4 version had similar problems as you had, something like mapreduce too. </p>\n\n<p>We solved it by using a vm (Virtual box) with Ubuntu and a shared directory between Windows and Linux, so we can develop and built on Windows and run Nutch (crawling) on Linux.</p>\n", "creation_date": 1362516647, "is_accepted": false, "score": 2, "last_activity_date": 1362516647, "answer_id": 15233861}, {"question_id": 15188050, "owner": {"user_id": 973840, "accept_rate": 85, "link": "http://stackoverflow.com/users/973840/diaa", "user_type": "registered", "reputation": 385}, "body": "<p>It took me a while to get this working but here's the solution which works on nutch 1.7.</p>\n\n<ol>\n<li>Download <a href=\"http://mvnrepository.com/artifact/org.apache.hadoop/hadoop-core/0.20.2\" rel=\"nofollow\">Hadoop Core 0.20.2</a> from the MVN repository </li>\n<li>Replace (nutch-directory)/lib/hadoop-core-1.2.0.jar with the downloaded file renaming it with the same name.</li>\n</ol>\n\n<p>That should be it.</p>\n\n<p><strong>Explanation</strong></p>\n\n<p>This issue is caused by hadoop since it assumes you're running on unix and abides by the file permission rules. The issue was resolved in 2011 actually but nutch didn't update the hadoop version they use. The relevant fixes are <a href=\"https://issues.apache.org/jira/browse/HADOOP-7126\" rel=\"nofollow\">here</a> and <a href=\"https://issues.apache.org/jira/browse/HDFS-1084\" rel=\"nofollow\">here</a></p>\n", "creation_date": 1391806181, "is_accepted": true, "score": 4, "last_activity_date": 1391806181, "answer_id": 21637481}, {"last_edit_date": 1392738736, "owner": {"user_id": 294657, "accept_rate": 83, "link": "http://stackoverflow.com/users/294657/kaqqao", "user_type": "registered", "reputation": 2275}, "body": "<p>I suggest a different approach. Check <a href=\"https://github.com/congainc/patch-hadoop_7682-1.0.x-win\" rel=\"nofollow\">this link</a> out. It explains how to swallow the error on Windows, and does not require you to downgrade Hadoop or rebuild Nutch. I tested on Nutch 2.1, but it applies to other versions as well.\nI also made <a href=\"https://github.com/veggen/nutch-windows-script\" rel=\"nofollow\">a simple .bat for starting the crawler and indexer</a>, but it is meant for Nutch 2.x, might not be applicable for Nutch 1.x.</p>\n", "question_id": 15188050, "creation_date": 1392736790, "is_accepted": false, "score": 0, "last_activity_date": 1392738736, "answer_id": 21857945}, {"question_id": 15188050, "owner": {"user_id": 2332454, "link": "http://stackoverflow.com/users/2332454/vetus", "user_type": "registered", "reputation": 1}, "body": "<p>You have to change the project dependences hadoop-core and hadoop-tools. I'm using 0.20.2 version and works fine. </p>\n", "creation_date": 1393854598, "is_accepted": false, "score": 0, "last_activity_date": 1393854598, "answer_id": 22148516}], "question_id": 15188050, "tags": ["windows", "solr", "hadoop", "cygwin", "nutch"], "answer_count": 5, "link": "http://stackoverflow.com/questions/15188050/nutch-in-windows-failed-to-set-permissions-of-path", "last_activity_date": 1411668441, "accepted_answer_id": 21637481, "body": "<p>I'm trying to user Solr with Nutch on a Windows Machine and I'm getting the following error:</p>\n\n<pre><code>Exception in thread \"main\" java.io.IOException: Failed to set permissions of path: c:\\temp\\mapred\\staging\\admin-1654213299\\.staging to 0700\n</code></pre>\n\n<p>From a lot of threads I learned, that hadoop wich seems to be used by nutch does some chmod magic that will work on unix machines, but not on windows.</p>\n\n<p>This problem exists for more than a year now. I found one thread, where the code line is shown and a fix proposed. Am I really them only one who has this problem? Are all others creating a custom build in order to run nutch on windows? Or is there some option to disable the hadoop stuff or another solution? Maybe another crawler than nutch?</p>\n\n<p>Thanks a lot.\nBoris</p>\n\n<p>Here's the stack trace of what I'm doing....</p>\n\n<pre><code>    admin@WIN-G1BPD00JH42 /cygdrive/c/solr/apache-nutch-1.6\n    $ bin/nutch crawl urls -dir crawl -depth 3 -topN 5 -solr http://localhost:8080/solr-4.1.0\n    cygpath: can't convert empty path\n    crawl started in: crawl\n    rootUrlDir = urls\n    threads = 10\n    depth = 3\n    solrUrl=http://localhost:8080/solr-4.1.0\n    topN = 5\n    Injector: starting at 2013-03-03 17:43:15\n    Injector: crawlDb: crawl/crawldb\n    Injector: urlDir: urls\n    Injector: Converting injected urls to crawl db entries.\n    Exception in thread \"main\" java.io.IOException: Failed to set permissions of path:         c:\\temp\\mapred\\staging\\admin-1654213299\\.staging to 0700\n        at org.apache.hadoop.fs.FileUtil.checkReturnValue(FileUtil.java:689)\n        at org.apache.hadoop.fs.FileUtil.setPermission(FileUtil.java:662)\n        at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:509)\n        at org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:344)\n        at org.apache.hadoop.fs.FilterFileSystem.mkdirs(FilterFileSystem.java:189)\n        at org.apache.hadoop.mapreduce.JobSubmissionFiles.getStagingDir(JobSubmissionFiles.java:116)\n        at org.apache.hadoop.mapred.JobClient$2.run(JobClient.java:856)\n        at org.apache.hadoop.mapred.JobClient$2.run(JobClient.java:850)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Unknown Source)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1121)\n        at org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:850)\n        at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:824)\n        at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:1261)\n        at org.apache.nutch.crawl.Injector.inject(Injector.java:281)\n        at org.apache.nutch.crawl.Crawl.run(Crawl.java:127)\n        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n        at org.apache.nutch.crawl.Crawl.main(Crawl.java:55)\n</code></pre>\n", "creation_date": 1362329620, "score": 7},
{"title": "Maven plugin for eclipse not found", "view_count": 23, "is_answered": false, "answers": [{"question_id": 26037247, "owner": {"user_id": 1671737, "link": "http://stackoverflow.com/users/1671737/joakim-z", "user_type": "registered", "reputation": 76}, "body": "<p>Have you added <a href=\"http://download.eclipse.org/technology/m2e/releases/\" rel=\"nofollow\">http://download.eclipse.org/technology/m2e/releases/</a> as an update site inside eclipse? Doing that should enable you to download it.</p>\n", "creation_date": 1411644277, "is_accepted": false, "score": 0, "last_activity_date": 1411644277, "answer_id": 26037405}], "question_id": 26037247, "tags": ["linux", "eclipse", "maven", "ubuntu-12.04", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/26037247/maven-plugin-for-eclipse-not-found", "last_activity_date": 1411644277, "owner": {"age": 25, "answer_count": 126, "creation_date": 1396591478, "user_id": 3496666, "accept_rate": 72, "view_count": 618, "location": "Chennai, India", "reputation": 1801}, "body": "<p>Trying to run nutch on eclipse. I following <a href=\"http://wiki.apache.org/nutch/RunNutchInEclipse\" rel=\"nofollow\">this guide</a> but i could not install maven plugin for eclipse. <a href=\"http://download.eclipse.org/technology/m2e/releases/\" rel=\"nofollow\">Maven plugin for eclipse</a> seems not available. How can i install m2e plugin? </p>\n", "creation_date": 1411643828, "score": 0},
{"title": "Ubuntu 14.04: install nutch 1.9 and Solr to", "view_count": 736, "is_answered": false, "answers": [{"question_id": 26030973, "owner": {"user_id": 3055197, "link": "http://stackoverflow.com/users/3055197/talat", "user_type": "registered", "reputation": 51}, "body": "<p>You can use every Nutch version with any solr version. Nutch connect solr with solrj. At the default Nutch 1.9 use solrj 3.4. Nutch 2.x use solrj 4.6.0 If you use solr 3.4 and higher version, It is compatible. IMHO if you use different version Solr you should change solrj version and build it. </p>\n", "creation_date": 1411622936, "is_accepted": false, "score": 0, "last_activity_date": 1411622936, "answer_id": 26031206}], "question_id": 26030973, "tags": ["ubuntu", "solr", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/26030973/ubuntu-14-04-install-nutch-1-9-and-solr-to", "last_activity_date": 1411622936, "owner": {"user_id": 4077714, "view_count": 1, "answer_count": 0, "creation_date": 1411621541, "reputation": 1}, "body": "<p>I installed Ubuntu 14.04 (32-bit) on a Oracle Virtualbox.\nI am looking for the compatible version of Solr and Nutch for this VM.\nAnd instructions on installing both Solr and Nutch.\nThanks.</p>\n", "creation_date": 1411621846, "score": 0},
{"title": "How to view hbase data stored in hbase by nutch", "view_count": 236, "owner": {"user_id": 3454410, "answer_count": 20, "creation_date": 1395646154, "accept_rate": 48, "view_count": 174, "reputation": 832}, "is_answered": true, "answers": [{"question_id": 25973031, "owner": {"user_id": 3055197, "link": "http://stackoverflow.com/users/3055197/talat", "user_type": "registered", "reputation": 51}, "body": "<p>Web pages and documents are stored as a row. If you know your document link, you can get it on hbase shell or other tool (hive pig etc.) Nutch stores URL as reverse URL.</p>\n\n<p>E.g. \"<a href=\"http://bar.foo.com:8983/to/index.html?a=b\" rel=\"nofollow\">http://bar.foo.com:8983/to/index.html?a=b</a>\" becomes \"com.foo.bar:8983:http/to/index.html?a=b\".</p>\n\n<p>Information about Hbase Shell <a href=\"http://wiki.apache.org/hadoop/Hbase/Shell\" rel=\"nofollow\">http://wiki.apache.org/hadoop/Hbase/Shell</a></p>\n", "creation_date": 1411573912, "is_accepted": true, "score": 0, "last_activity_date": 1411573912, "answer_id": 26021310}], "question_id": 25973031, "tags": ["hadoop", "hbase", "hdfs", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/25973031/how-to-view-hbase-data-stored-in-hbase-by-nutch", "last_activity_date": 1411573912, "accepted_answer_id": 26021310, "body": "<p>I am using apache nutch to store data in hbase that is using hdfs as its file system. I want to know which documents it stored in hbase etc. How to read that data from hbase e.g. Some pdf is stored in hbase. I want to read it. how I will do it.</p>\n\n<p>Please guide me?</p>\n", "creation_date": 1411385346, "score": 0},
{"title": "Nutch error &quot;Limit reached, skipping further inlinks for&quot;", "view_count": 94, "is_answered": true, "answers": [{"question_id": 26012474, "owner": {"user_id": 3055197, "link": "http://stackoverflow.com/users/3055197/talat", "user_type": "registered", "reputation": 51}, "body": "<p>This is not an error. Actually this means finds more inlinks than default setting (db.max.inlinks),only the first N inlinks will be stored, and the rest will be discarded.At the default db.max.inlinks is set 10000. </p>\n\n<p>IMHO if you want to crawl more outlinks pages. You should increase db.max.outlinks.per.page settings. At the defualt it is set 100 per page.  </p>\n", "creation_date": 1411556515, "is_accepted": false, "score": 1, "last_activity_date": 1411556515, "answer_id": 26015258}], "question_id": 26012474, "tags": ["apache", "web-crawler", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/26012474/nutch-error-limit-reached-skipping-further-inlinks-for", "last_activity_date": 1411556515, "owner": {"user_id": 3454410, "answer_count": 20, "creation_date": 1395646154, "accept_rate": 48, "view_count": 174, "reputation": 832}, "body": "<p>My nutch version is 2.2.1 and it is working well for few days but now it is not going to crawl anything any gives following error like.</p>\n\n<p>Limit reached, skipping further inlinks for de.ard.www:http/<br>\nLimit reached, skipping further inlinks for de.rbb-online.mediathek:http/</p>\n\n<p>Limit reached, skipping further inlinks for de.rbb-online.www:http/</p>\n\n<p>How to get rid of it?</p>\n", "creation_date": 1411548544, "score": 0},
{"title": "Apache nutch 1.9 database", "view_count": 229, "owner": {"age": 26, "answer_count": 2, "creation_date": 1380910276, "user_id": 2847689, "accept_rate": 85, "view_count": 686, "reputation": 749}, "is_answered": true, "answers": [{"question_id": 25990801, "owner": {"user_id": 1699191, "link": "http://stackoverflow.com/users/1699191/ameertawfik", "user_type": "registered", "reputation": 618}, "body": "<p>If you can see your data already indexed in Solr, then you do not need to retrieve any thing from Nutch. What you need right now is the right Solr client to interact with Solr. The client will query Solr and parse responses.</p>\n\n<p>Since you are going to use Java, you should use <a href=\"http://wiki.apache.org/solr/Solrj\" rel=\"nofollow\">SolrJ</a>.</p>\n", "creation_date": 1411498147, "is_accepted": true, "score": 1, "last_activity_date": 1411498147, "answer_id": 26002565}], "question_id": 25990801, "tags": ["database", "apache", "solr", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/25990801/apache-nutch-1-9-database", "last_activity_date": 1411498147, "accepted_answer_id": 26002565, "body": "<p>I have set up nutch 1.9 with solr properly. Now I would like to retrieve this data via java into a program, to analyse and display the data. At them moment I can query the data with solr. However, I cannot find any further information about the underlying database which is used by nutch and how to retrive data.</p>\n\n<p>Any recommendations, how that can be done?</p>\n\n<p>I appreciate your answer!</p>\n", "creation_date": 1411462340, "score": 0},
{"title": "Implementing search engine and integrating it to Liferay", "view_count": 376, "is_answered": false, "question_id": 25993656, "tags": ["solr", "liferay", "web-crawler", "search-engine", "nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/25993656/implementing-search-engine-and-integrating-it-to-liferay", "last_activity_date": 1411470888, "owner": {"age": 28, "answer_count": 16, "creation_date": 1398342846, "user_id": 3568831, "accept_rate": 71, "view_count": 51, "location": "Cameroon", "reputation": 137}, "body": "<p>I'm on a project that require implementation of a portal and a search engine that should be integrated with the portal. \nI choosed using <strong>Liferay</strong> for the portal but for the search engine there is lack of idea.\nI studied how seach engine works and i figured out <strong>three module on a search engine</strong></p>\n\n<ol>\n<li>The crawler</li>\n<li>The indexer</li>\n<li>The query engine.</li>\n</ol>\n\n<p>I noticed somme existing solutions for the module: <strong>Apache Nutch for the crawler</strong>, <strong>Apache Solr for the Query engine</strong>. so some research brought me in an article of that forum dealing in coupling Solr with Nuch. the problem is don't know if Solr has a Query Engine for the web user interface side (form). else are Solr and Nutch sufficient tho implement the three module of a search engine? please if anybody has details on the architecture and interactions that should be in place you are welcome.</p>\n", "creation_date": 1411470888, "score": 0},
{"title": "Nutch deployment on hadoop will not index to solr", "view_count": 518, "is_answered": false, "answers": [{"question_id": 23815577, "owner": {"user_id": 73117, "link": "http://stackoverflow.com/users/73117/romain", "user_type": "registered", "reputation": 5120}, "body": "<p>Did you put the Nutch jar (and dependencies if needed) in a 'lib' directory in the HDFS workspace of the workflow?</p>\n", "creation_date": 1400826031, "is_accepted": false, "score": 0, "last_activity_date": 1400826031, "answer_id": 23822256}, {"question_id": 23815577, "owner": {"user_id": 261984, "accept_rate": 60, "link": "http://stackoverflow.com/users/261984/eddy", "user_type": "registered", "reputation": 518}, "body": "<p>Ah, I'm beginning to loathe the packaging of Nutch!</p>\n\n<p>Try extracting the classes/plugins folder from the job archive, copy it to HDFS (something like  hdfs dfs -put -r plugins lib) and then add the HDFS path of the plugins folder to the \"files\" list of the indexing step.</p>\n\n<p>Best,\nEdoardo  </p>\n", "creation_date": 1411462728, "is_accepted": false, "score": 0, "last_activity_date": 1411462728, "answer_id": 25990943}], "question_id": 23815577, "tags": ["apache", "solr", "nutch", "oozie", "hue"], "answer_count": 2, "link": "http://stackoverflow.com/questions/23815577/nutch-deployment-on-hadoop-will-not-index-to-solr", "last_activity_date": 1411462728, "owner": {"user_id": 1841456, "answer_count": 35, "creation_date": 1353490674, "accept_rate": 64, "view_count": 130, "location": "San Jose, CA, United States", "reputation": 570}, "body": "<p>I have oozie workflow that does a nutch crawl I designed using hue. </p>\n\n<p>All steps in the process work, except for indexing to solr.</p>\n\n<p>The oozie action that defines the solrindex is as follows</p>\n\n<p>`</p>\n\n<pre><code>&lt;start to=\"solr-test\"/&gt;\n    &lt;action name=\"solr-test\"&gt;\n        &lt;java&gt;\n            &lt;job-tracker&gt;${jobTracker}&lt;/job-tracker&gt;\n            &lt;name-node&gt;${nameNode}&lt;/name-node&gt;\n            &lt;main-class&gt;org.apache.nutch.indexer.IndexingJob&lt;/main-class&gt;\n            &lt;java-opts&gt;solr.server.url=http://ip-redacted:8983/solr/raw&lt;/java-opts&gt;\n            &lt;arg&gt;hdfs://ip-redacted:8020/user/admin/c&lt;/arg&gt;\n            &lt;arg&gt;-dir&lt;/arg&gt;\n            &lt;arg&gt;hdfs://ip-redacted:8020/user/admin/s000&lt;/arg&gt;\n        &lt;/java&gt;\n        &lt;ok to=\"end\"/&gt;\n        &lt;error to=\"kill\"/&gt;\n    &lt;/action&gt;\n    &lt;kill name=\"kill\"&gt;\n        &lt;message&gt;Action failed, error message[${wf:errorMessage(wf:lastErrorNode())}]&lt;/message&gt;\n    &lt;/kill&gt;\n    &lt;end name=\"end\"/&gt;\n</code></pre>\n\n<p>`</p>\n\n<p>When I run the action I get the following error message </p>\n\n<pre><code>Main class [org.apache.oozie.action.hadoop.JavaMain], exit code [-1]\n</code></pre>\n\n<p>The locations <code>hdfs://ip-redacted:8020/user/admin/c</code> and \n<code>hdfs://ip-redacted:8020/user/admin/s000</code> are locations that contain the crawldb and the segments respectively.</p>\n\n<p>The stderr of the job says ::</p>\n\n<pre><code>`Log Length: 122\nIntercepting System.exit(-1)\nFailing Oozie Launcher, Main class [org.apache.oozie.action.hadoop.JavaMain], exit code [-1]`\n</code></pre>\n\n<p>The syslog says::</p>\n\n<pre><code>`ERROR [main] org.apache.nutch.indexer.IndexingJob: Indexer: java.lang.RuntimeException: org.apache.nutch.indexer.IndexWriter not found.\nat org.apache.nutch.indexer.IndexWriters.&lt;init&gt;(IndexWriters.java:51)\nat org.apache.nutch.indexer.IndexingJob.index(IndexingJob.java:100)\nat org.apache.nutch.indexer.IndexingJob.run(IndexingJob.java:185)\nat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)\nat org.apache.nutch.indexer.IndexingJob.main(IndexingJob.java:195)\nat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\nat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\nat java.lang.reflect.Method.invoke(Method.java:606)\nat org.apache.oozie.action.hadoop.JavaMain.run(JavaMain.java:55)\nat org.apache.oozie.action.hadoop.LauncherMain.run(LauncherMain.java:38)\nat org.apache.oozie.action.hadoop.JavaMain.main(JavaMain.java:36)\nat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\nat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\nat java.lang.reflect.Method.invoke(Method.java:606)\nat org.apache.oozie.action.hadoop.LauncherMapper.map(LauncherMapper.java:225)\nat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)\nat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:430)\nat org.apache.hadoop.mapred.MapTask.run(MapTask.java:342)\nat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)\nat java.security.AccessController.doPrivileged(Native Method)\nat javax.security.auth.Subject.doAs(Subject.java:415)\nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1557)\nat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)`\n</code></pre>\n\n<p>have verified that the class exists in the apache-nutch-1.7.jar file.</p>\n\n<p>And if I request hadoop to run as a map-reduce job in the command shell as follows:: </p>\n\n<pre><code>`hadoop jar apache-nutch-1.7.jar org.apache.nutch.indexer.IndexingJob -D solr.server.url=http://ip-redacted:8983/solr/raw hdfs://ip-redacted:8020/user/admin/c -dir hdfs://ip-redacted:8020/user/admin/s000`\n</code></pre>\n\n<p>It works!! But, when I do it as a oozie job, created through Hue, it fails...</p>\n\n<p>Also, other actions, like inject, generate, fetch, parse work fine in Hue. It's only solrindex step that fails and I don't know what to do to fix it. Any input on this will be great!</p>\n", "creation_date": 1400786883, "score": 0},
{"title": "Insufficient space for shared memory file when i try to run nutch generate command", "view_count": 21734, "owner": {"user_id": 328836, "answer_count": 4, "creation_date": 1272541802, "accept_rate": 62, "view_count": 86, "location": "Goa India", "reputation": 500}, "is_answered": true, "answers": [{"last_edit_date": 1358015872, "owner": {"user_id": 1150329, "accept_rate": 89, "link": "http://stackoverflow.com/users/1150329/tejas-patil", "user_type": "registered", "reputation": 4793}, "body": "<p>I think that the temporary location that was used has got full. Try using some other location. Also, check the #inodes free in each partition and clear up some space.</p>\n\n<p><strong>EDIT:</strong>\nThere is no need to change the /tmp at OS level. We want nutch and hadoop to use some other location for storing temp files. Look at this to do that :\n<a href=\"http://stackoverflow.com/questions/2354525/what-should-be-hadoop-tmp-dir\">What should be hadoop.tmp.dir?</a></p>\n", "question_id": 14290504, "creation_date": 1357969201, "is_accepted": true, "score": 5, "last_activity_date": 1358015872, "answer_id": 14290628}, {"last_edit_date": 1411248696, "owner": {"user_id": 1642266, "link": "http://stackoverflow.com/users/1642266/kingz", "user_type": "registered", "reputation": 1744}, "body": "<p>Yeah this is really an issue with the space available on the volume your /tmp is mounted on. If you are running this on EC2, or any cloud platform, attach a new volume and mount your /tmp on that. If running locally, no other option besides cleaning up to make more room. </p>\n\n<p>Try commands like: df -h to see the % used and available space on each volume mounted on your instance. You will see something like:</p>\n\n<pre><code>Filesystem            Size  Used Avail Use% Mounted on\n/dev/xvda1            7.9G  7.9G     0 100% /\ntmpfs                  30G     0   30G   0% /dev/shm\n/dev/xvda3             35G  1.9G   31G   6% /var\n/dev/xvda4             50G   44G  3.8G  92% /opt\n/dev/xvdb             827G  116G  669G  15% /data/1\n/dev/xvdc             827G  152G  634G  20% /data/2\n/dev/xvdd             827G  149G  637G  19% /data/3\n/dev/xvde             827G  150G  636G  20% /data/4\ncm_processes           30G   22M   30G   1% /var/run/cloudera-scm-agent/process\n</code></pre>\n\n<p>You will begin to see this error when the disk space is full as shown in this dump.</p>\n", "question_id": 14290504, "creation_date": 1411248152, "is_accepted": false, "score": 1, "last_activity_date": 1411248696, "answer_id": 25953429}], "question_id": 14290504, "tags": ["java", "jvm", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/14290504/insufficient-space-for-shared-memory-file-when-i-try-to-run-nutch-generate-comma", "last_activity_date": 1411248696, "accepted_answer_id": 14290628, "body": "<p>I have been running nutch crawling commands for the passed 3 weeks and now i get the below error when i try to run any nutch command</p>\n\n<p>Java HotSpot(TM) 64-Bit Server VM warning: Insufficient space for shared memory file:\n   /tmp/hsperfdata_user/27050\nTry using the -Djava.io.tmpdir= option to select an alternate temp location.</p>\n\n<p>Error: Could not find or load main class <em>_</em>.tmp.hsperfdata_user.27055</p>\n\n<p>How do i solve this issue??</p>\n", "creation_date": 1357967951, "score": 4},
{"title": "How to add some additional fields into solr when indexing from nutch?", "view_count": 488, "owner": {"age": 25, "answer_count": 126, "creation_date": 1396591478, "user_id": 3496666, "accept_rate": 72, "view_count": 618, "location": "Chennai, India", "reputation": 1801}, "is_answered": true, "answers": [{"question_id": 25945964, "owner": {"user_id": 1699191, "link": "http://stackoverflow.com/users/1699191/ameertawfik", "user_type": "registered", "reputation": 618}, "body": "<p>If the value of the additional fields does not change, then you can use the Nutch's index-static plugin. It allows you to add a number of fields with their contents. You first need to enable it in nutch-site.xml. You then add the list of fields as shown below:</p>\n\n<pre><code>&lt;property&gt;\n &lt;name&gt;index.static&lt;/name&gt;\n &lt;value&gt;indexed_by:solr,crawled_by:nutch-1.8,crawl_name:nutch&lt;/value&gt;\n &lt;description&gt;\n  Used by plugin index-static to adds fields with static data at indexing time. \n   You can specify a comma-separated list of fieldname:fieldcontent per Nutch job.\n  Each fieldcontent can have multiple values separated by space, e.g.,\n   field1:value1.1 value1.2 value1.3,field2:value2.1 value2.2 ...\n   It can be useful when collections can't be created by URL patterns, \n  like in subcollection, but on a job-basis.\n  &lt;/description&gt;\n&lt;/property&gt;\n</code></pre>\n\n<p>If the value of these fields is not static and independent of indexed documents, then you will need to write a IndexingFilter plugin to do that. Have a look at the index-static plugin to know how implement yours.</p>\n", "creation_date": 1411248657, "is_accepted": true, "score": 0, "last_activity_date": 1411248657, "answer_id": 25953488}], "question_id": 25945964, "tags": ["solr", "cygwin", "web-crawler", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/25945964/how-to-add-some-additional-fields-into-solr-when-indexing-from-nutch", "last_activity_date": 1411248657, "accepted_answer_id": 25953488, "body": "<p>I am using nutch 1.9 using cygwin and solr 4.8.0. I can index the crawled data into solr using below code. </p>\n\n<p><strong>bin/crawl urls/ crawlresult/ http://localhost:8983/solr/ 1</strong></p>\n\n<p>But i want to add some additional fields while indexing such as indexed_by, crawled_by, crawl_name, etc. <br/>\nI need help on this.</p>\n\n<p>Thanks in Advance.</p>\n", "creation_date": 1411194606, "score": 0},
{"title": "Can not post to solr during solr and nutch integration", "view_count": 1410, "is_answered": false, "answers": [{"question_id": 18700371, "owner": {"user_id": 2767188, "link": "http://stackoverflow.com/users/2767188/sumitkumar-jadhav", "user_type": "registered", "reputation": 3}, "body": "<pre><code>$ bin/nutch solrindex http://localhost:8983/solr/ crawl/crawldb crawl/linkdb crawl/segments/*\n</code></pre>\n\n<p>SolrIndexer: starting at 2013-09-11 08:23:14\nSolrIndexer: finished at 2013-09-11 08:23:28, elapsed: 00:00:13</p>\n\n<p>when i tried this it worked perfectly...</p>\n", "creation_date": 1378872117, "is_accepted": false, "score": 0, "last_activity_date": 1378872117, "answer_id": 18732450}, {"question_id": 18700371, "owner": {"user_id": 3496666, "accept_rate": 72, "link": "http://stackoverflow.com/users/3496666/kumar", "user_type": "registered", "reputation": 1801}, "body": "<p>I had the same issue then i added some property for plugin, it solved my issue.\nCheck the below link where i answered for the similar question.</p>\n\n<p><a href=\"http://stackoverflow.com/a/25945844/3496666\">http://stackoverflow.com/a/25945844/3496666</a></p>\n", "creation_date": 1411193894, "is_accepted": false, "score": 0, "last_activity_date": 1411193894, "answer_id": 25945883}], "question_id": 18700371, "tags": ["solr", "integration", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/18700371/can-not-post-to-solr-during-solr-and-nutch-integration", "last_activity_date": 1411193894, "owner": {"age": 25, "answer_count": 0, "creation_date": 1316874826, "user_id": 962698, "accept_rate": 0, "view_count": 18, "location": "nadiad", "reputation": 16}, "body": "<p>I am using solr-4.4.0 and nutch-1.7.\nI am newbie in solr and nutch.\nI am trying to integrate solr and nutch using following blog::</p>\n\n<blockquote>\n  <p><a href=\"http://www.building-blocks.com/thinking/building-a-search-engine-with-nutch-and-solr-in-10-minutes/\" rel=\"nofollow\">http://www.building-blocks.com/thinking/building-a-search-engine-with-nutch-and-solr-in-10-minutes/</a></p>\n</blockquote>\n\n<p>Started solr successfully::</p>\n\n<pre><code>manish@ubuntu:~$ cd /usr/local/solr/example/\nmanish@ubuntu:/usr/local/solr/example$ java -jar start.jar\n</code></pre>\n\n<p>As well as indexed urls succesfully::</p>\n\n<pre><code>manish@ubuntu:/usr/local/nutch/bin/nutch crawl urls -dir crawl -depth 3 -topN 50\n</code></pre>\n\n<p>Sample output::</p>\n\n<pre><code>fetching http://www.tatvic.com/tatvic-excel-plugin-webinar (queue crawl delay=5000ms)\n-activeThreads=10, spinWaiting=9, fetchQueues.totalSize=36\n-activeThreads=10, spinWaiting=10, fetchQueues.totalSize=36\n-activeThreads=10, spinWaiting=10, fetchQueues.totalSize=36\n-activeThreads=10, spinWaiting=10, fetchQueues.totalSize=36\n-activeThreads=10, spinWaiting=10, fetchQueues.totalSize=36\n-activeThreads=10, spinWaiting=10, fetchQueues.totalSize=36\n.\n.\n.\n.\n.\n..\n\nParsed (1ms):http://www.tatvic.com/anomaly-detection/\nParsed (1ms):http://www.tatvic.com/blog/dynamic-funnel-in-google-analytics/\nParsed (1ms):http://www.tatvic.com/blog/google-analytics-custom-reports/\nParsed (0ms):http://www.tatvic.com/collaborative-filtering/\nParsed (1ms):http://www.tatvic.com/contact-thanks/\n..\n.\n.\n.\n.\nLinkDb: linkdb: crawl/linkdb\nLinkDb: URL normalize: true\nLinkDb: URL filter: true\nLinkDb: internal links will be ignored.\nLinkDb: adding segment: file:/usr/local/nutch/crawl/segments/20130909064920\nLinkDb: adding segment: file:/usr/local/nutch/crawl/segments/20130909065449\nLinkDb: adding segment: file:/usr/local/nutch/crawl/segments/20130909064936\nLinkDb: adding segment: file:/usr/local/nutch/crawl/segments/20130909043135\nLinkDb: adding segment: file:/usr/local/nutch/crawl/segments/20130909035154\nLinkDb: merging with existing linkdb: crawl/linkdb\nLinkDb: finished at 2013-09-09 07:01:53, elapsed: 00:00:04\ncrawl finished: crawl\n</code></pre>\n\n<p>But,while posting to solr i got the following::</p>\n\n<pre><code>manish@ubuntu:/usr/local/nutch$ bin/nutch solrindex http://127.0.0.1:8983/solr/ crawl/crawldb -linkdb crawl/linkdb crawl/segments/*\nIndexer: starting at 2013-09-09 07:05:05\nIndexer: deleting gone documents: false\nIndexer: URL filtering: false\nIndexer: URL normalizing: false\nNo IndexWriters activated - check your configuration\n\nIndexer: finished at 2013-09-09 07:05:12, elapsed: 00:00:07\n</code></pre>\n\n<p>which does it really means???Is it not doing posting at all??</p>\n\n<p>In browser it does not show me anything::</p>\n\n<pre><code> http://127.0.0.1:8983/solr/select?q=tatvic\n</code></pre>\n\n<p>browser sample::</p>\n\n<pre><code> &lt;response&gt;&lt;lst name=\"responseHeader\"&gt;&lt;int name=\"status\"&gt;0&lt;/int&gt;&lt;int name=\"QTime\"&gt;111&lt;/int&gt;&lt;lst name=\"params\"&gt;&lt;str name=\"q\"&gt;tatvic&lt;/str&gt;&lt;/lst&gt;&lt;/lst&gt;&lt;result name=\"response\" numFound=\"0\" start=\"0\"/&gt;&lt;/response&gt;\n</code></pre>\n\n<p>I tried lot more times!!</p>\n\n<p>I think it might be a issue.</p>\n\n<p>Can you please help me to figure this out??</p>\n", "creation_date": 1378736246, "score": 0},
{"title": "Running Apache Nutch in windows 7", "view_count": 686, "is_answered": true, "answers": [{"question_id": 19117968, "owner": {"user_id": 3496666, "accept_rate": 72, "link": "http://stackoverflow.com/users/3496666/kumar", "user_type": "registered", "reputation": 1801}, "body": "<p>I had the same issue on this before 2 days. Here's the solution that i followed</p>\n\n<ol>\n<li>Download <a href=\"http://mvnrepository.com/artifact/org.apache.hadoop/hadoop-core/0.20.2\" rel=\"nofollow\">http://mvnrepository.com/artifact/org.apache.hadoop/hadoop-core/0.20.2</a></li>\n<li>Replace (nutch-directory)/lib/hadoop-core-1.2.0.jar with the downloaded file renaming it with the same name.</li>\n</ol>\n\n<p>That's it. </p>\n", "creation_date": 1411130300, "is_accepted": false, "score": 1, "last_activity_date": 1411130300, "answer_id": 25934135}], "question_id": 19117968, "tags": ["nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/19117968/running-apache-nutch-in-windows-7", "last_activity_date": 1411130300, "owner": {"user_id": 2835125, "view_count": 0, "answer_count": 0, "creation_date": 1380635186, "reputation": 6}, "body": "<p>i am trying to run Nutch with Cygwin. I am having problems with crawling the content</p>\n\n<p>My comment is</p>\n\n<p>$ bin/nutch crawl urls -dir crawl -depth 3 -topN 5</p>\n\n<p>Response is</p>\n\n<p>**cygpath: can't convert empty path</p>\n\n<p><strong>InjectorJob: Using class org.apache.gora.memory.store.MemStore as the Gora storage class.\nException in thread \"main\" java.io.IOException: Failed to set permissions of path: \\tmp\\hadoop-user\\mapred\\staging\\user1249593824.staging to 0700**</strong></p>\n\n<pre><code>    at org.apache.hadoop.fs.FileUtil.checkReturnValue(FileUtil.java:691)\n    at org.apache.hadoop.fs.FileUtil.setPermission(FileUtil.java:664)\n    at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:514)\n    at org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:349)\n    at org.apache.hadoop.fs.FilterFileSystem.mkdirs(FilterFileSystem.java:193)\n    at org.apache.hadoop.mapreduce.JobSubmissionFiles.getStagingDir(JobSubmissionFiles.java:126)\n    at org.apache.hadoop.mapred.JobClient$2.run(JobClient.java:942)\n    at org.apache.hadoop.mapred.JobClient$2.run(JobClient.java:936)\n    at java.security.AccessController.doPrivileged(Native Method)\n    at javax.security.auth.Subject.doAs(Subject.java:415)\n    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)\n    at org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:936)\n    at org.apache.hadoop.mapreduce.Job.submit(Job.java:550)\n    at org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:580)\n    at org.apache.nutch.util.NutchJob.waitForCompletion(NutchJob.java:50)\n    at org.apache.nutch.crawl.InjectorJob.run(InjectorJob.java:233)\n    at org.apache.nutch.crawl.Crawler.runTool(Crawler.java:68)\n    at org.apache.nutch.crawl.Crawler.run(Crawler.java:136)\n    at org.apache.nutch.crawl.Crawler.run(Crawler.java:250)\n    at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n    at org.apache.nutch.crawl.Crawler.main(Crawler.java:257)\n</code></pre>\n\n<p>Help me on this. </p>\n", "creation_date": 1380635379, "score": 1},
{"title": "org.apache.nutch.crawl.Crawl missing in nutch 1.9 on hadoop 1.2.1", "view_count": 779, "is_answered": true, "answers": [{"question_id": 25726147, "owner": {"user_id": 3055197, "link": "http://stackoverflow.com/users/3055197/talat", "user_type": "registered", "reputation": 51}, "body": "<p>Crawl.java removed at 1.8 version. You can use crawl shell script for all crawling. </p>\n\n<p>Deprecated class o.a.n.crawl.Crawler is still in code base <a href=\"https://issues.apache.org/jira/browse/NUTCH-1621\" rel=\"nofollow\">https://issues.apache.org/jira/browse/NUTCH-1621</a></p>\n", "creation_date": 1410768914, "is_accepted": false, "score": 1, "last_activity_date": 1410768914, "answer_id": 25843524}], "question_id": 25726147, "tags": ["hadoop", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/25726147/org-apache-nutch-crawl-crawl-missing-in-nutch-1-9-on-hadoop-1-2-1", "last_activity_date": 1410768914, "owner": {"user_id": 2445001, "answer_count": 1, "creation_date": 1370173486, "accept_rate": 29, "view_count": 16, "reputation": 133}, "body": "<p>I have installed fully distributed Hadoop 1.2.1. I was trying to integrated nutch with steps below:</p>\n\n<ol>\n<li>Download apache-nutch-1.9-src.zip</li>\n<li>Add value http.agent.name into nutch-site.xml</li>\n<li>Copy <code>hadoop-env.sh</code>, <code>core-site.xml</code>, <code>hdfs-site.xml</code>, <code>mapred-site.xml</code>,\n<code>masters</code>, <code>slaves</code> into $NUTCH_HOME/conf</li>\n<li>compile using <code>ant runtime</code></li>\n<li>create <code>urls/seed.txt</code> and put on hadoop dfs</li>\n<li>edit $NUTCH_HOME/conf/regex-urlfilter.txt</li>\n</ol>\n\n<p>Test crawl using command: \n<p> <code>bin/hadoop -jar nutch-1.9.job org.apache.nutch.crawl.Crawl urls -dir urls -depth 1 -topN 5</code>\n<p> and get this error:</p>\n\n<blockquote>\n  <p>Exception in thread \"main\" java.lang.ClassNotFoundException: <strong>org.apache.nutch.crawl.Crawl</strong>\n    at java.net.URLClassLoader$1.run(URLClassLoader.java:366)\n    at java.net.URLClassLoader$1.run(URLClassLoader.java:355)\n    at java.security.AccessController.doPrivileged(Native Method)\n    at java.net.URLClassLoader.findClass(URLClassLoader.java:354)\n    at java.lang.ClassLoader.loadClass(ClassLoader.java:425)\n    at java.lang.ClassLoader.loadClass(ClassLoader.java:358)\n    at java.lang.Class.forName0(Native Method)\n    at java.lang.Class.forName(Class.java:270)\n    at org.apache.hadoop.util.RunJar.main(RunJar.java:153)</p>\n</blockquote>\n\n<p>I tried extract nutch-1.9.job and I didn't find out class Crawl in org/apache/nutch/crawl.\n<p> Do I need to config something? </p>\n", "creation_date": 1410185165, "score": 3},
{"title": "Nutch and Http POST authentication?", "view_count": 1032, "owner": {"user_id": 1324850, "answer_count": 0, "creation_date": 1334083902, "accept_rate": 80, "view_count": 166, "location": "San Francisco, CA", "reputation": 391}, "is_answered": true, "answers": [{"question_id": 11353259, "owner": {"user_id": 1526859, "accept_rate": 82, "link": "http://stackoverflow.com/users/1526859/alexander-chepurnoy", "user_type": "registered", "reputation": 474}, "body": "<ol>\n<li>Make a file with data: regex for URLs requiring auth / URL to submit form / form data </li>\n<li>Make own http protocol plugin modifying standard protocol-httpclient plugin. If URL to make http request is requiring auth and no auth made yet, so go to form and send it.</li>\n</ol>\n\n<p>Here's the simplest solution. The problem is, there is no one simple solution for big amount of websites. There are problems with cookie expiring / using of Javascript during login / etc. Search through Nutch's JIRA, there were many discussions about that.</p>\n", "creation_date": 1342355774, "is_accepted": true, "score": 1, "last_activity_date": 1342355774, "answer_id": 11491948}, {"last_edit_date": 1410764259, "owner": {"user_id": 2086509, "accept_rate": 73, "link": "http://stackoverflow.com/users/2086509/jayesh-bhoyar", "user_type": "registered", "reputation": 594}, "body": "<p>Here is the answer that you guys are looking for:</p>\n\n<p><a href=\"http://lifelongprogrammer.blogspot.com/2014/02/part1-using-apache-http-client-to-do-http-post-form-authentication.html\" rel=\"nofollow\">http://lifelongprogrammer.blogspot.com/2014/02/part1-using-apache-http-client-to-do-http-post-form-authentication.html</a></p>\n\n<p>and</p>\n\n<p><a href=\"https://issues.apache.org/jira/browse/NUTCH-827\" rel=\"nofollow\">https://issues.apache.org/jira/browse/NUTCH-827</a></p>\n\n<p>These two links have complete and sample code. If you follow each steps correctly, then you will be able to achieve Form Based Authentication in Nutch.</p>\n", "question_id": 11353259, "creation_date": 1398972754, "is_accepted": false, "score": -1, "last_activity_date": 1410764259, "answer_id": 23415119}], "question_id": 11353259, "tags": ["post", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/11353259/nutch-and-http-post-authentication", "last_activity_date": 1410764259, "accepted_answer_id": 11491948, "body": "<p>I'm stuck at the point where I need to crawl websites that have a form post. \nNutch does not support this. \nHow do I get around this so I can crawl these websites using Nutch? Is there a better solution?</p>\n", "creation_date": 1341526387, "score": 1},
{"title": "what should i configure to stop nutch re-index or fetch again. It should index only once for a url", "view_count": 271, "is_answered": true, "answers": [{"last_edit_date": 1410763323, "owner": {"user_id": 2086509, "accept_rate": 73, "link": "http://stackoverflow.com/users/2086509/jayesh-bhoyar", "user_type": "registered", "reputation": 594}, "body": "<p>Here are the processes that nutch performs:</p>\n\n<ol>\n<li><p>Fetches the seed URL</p></li>\n<li><p>Generates URL from that Seed URL</p></li>\n<li><p>Creates a linkDB</p></li>\n<li><p>Fetches the content of only those link which are new to Nutch or checksum is changed (i.e. modified URL)</p></li>\n<li><p>It will fetch the content and parse the content for only those links which are new or modified. </p></li>\n<li><p>For other URLs, it just generates the URL and keeps it in the link DB.</p></li>\n</ol>\n\n<p>Example:</p>\n\n<ol>\n<li><p>In first crawl Nutch fetched, generates and crawled content from 10 URLs.</p></li>\n<li><p>Now say 3 links are added in my website</p></li>\n<li><p>In second fetch /re-indexing, Nutch will visit all the 13 URLs, and generate the URL and keep it in the linkdb, but only fetch the content for newly-added 3 URLs and fetch the content and parse it.</p></li>\n</ol>\n", "question_id": 25398340, "creation_date": 1408875142, "is_accepted": false, "score": 1, "last_activity_date": 1410763323, "answer_id": 25470589}], "question_id": 25398340, "tags": ["web-crawler", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/25398340/what-should-i-configure-to-stop-nutch-re-index-or-fetch-again-it-should-index-o", "last_activity_date": 1410763323, "owner": {"user_id": 1057685, "answer_count": 7, "creation_date": 1321873922, "accept_rate": 73, "view_count": 25, "location": "Bangalore, India", "reputation": 43}, "body": "<p>Can any one point me right documentation or hack to stop nutch re-index or fetch same content.\nI want to crawl only once for a given url</p>\n", "creation_date": 1408517159, "score": 0},
{"title": "how to bypass robots.txt with apache nutch 2.2.1", "view_count": 1900, "owner": {"age": 28, "answer_count": 5, "creation_date": 1371305078, "user_id": 2488981, "accept_rate": 72, "view_count": 119, "reputation": 547}, "is_answered": true, "answers": [{"last_edit_date": 1410762031, "owner": {"user_id": 2086509, "accept_rate": 73, "link": "http://stackoverflow.com/users/2086509/jayesh-bhoyar", "user_type": "registered", "reputation": 594}, "body": "<ol>\n<li><p>At first, we should respect the robots.txt file if you are crawling any external sites.\nOtherwise you are at risk - your IP banned or worse can be any legal case.</p></li>\n<li><p>If your site is internal and not expose to external world, then you should change the robots.txt file to allow your crawler.</p></li>\n<li><p>If your site is exposed to the Internet and if data is confidential, then you can try out the following option. Because here you cannot take a risk of modifying the robots.txt file since external crawler can use your crawler name and crawl the site.</p>\n\n<p>In Fetcher.java file:</p>\n\n<pre><code>if (!rules.isAllowed(fit.u.toString())) { }\n</code></pre>\n\n<p>This is the block that is responsible for blocking the URLs.  You can play around this code block to resolve your issue.</p></li>\n</ol>\n", "question_id": 24058899, "creation_date": 1402331420, "is_accepted": true, "score": 2, "last_activity_date": 1410762031, "answer_id": 24124623}], "question_id": 24058899, "tags": ["java", "nutch", "robots.txt", "web-crawler"], "answer_count": 1, "link": "http://stackoverflow.com/questions/24058899/how-to-bypass-robots-txt-with-apache-nutch-2-2-1", "last_activity_date": 1410762031, "accepted_answer_id": 24124623, "body": "<p>Can anyone please tell me if there is any way for apache nutch to ignore or bypass robots.txt while crawling. I am using nutch 2.2.1. I found that \"RobotRulesParser.java\"(full path:-src/plugin/lib-http/src/java/org/apache/nutch/protocol/http/api/ \nRobotRulesParser.java) is responsible for the reading and parsing the robots.txt. Is there any way to modify this file to ignore robots.txt and go on with crawling?</p>\n\n<p>Or is there any other way to achieve the same?</p>\n", "creation_date": 1401967341, "score": 0},
{"title": "Nutch in Hadoop 2.x", "view_count": 876, "owner": {"user_id": 1654582, "view_count": 12, "answer_count": 3, "creation_date": 1347017440, "reputation": 53}, "is_answered": true, "answers": [{"last_edit_date": 1410756077, "owner": {"user_id": 1177597, "link": "http://stackoverflow.com/users/1177597/viacheslav-dobromyslov", "user_type": "registered", "reputation": 1859}, "body": "<p>At the moment it's impossible to integrate Nutch 2.2.1 (Gora 0.3) with HBase 0.98.x.\nSee: <a href=\"https://issues.apache.org/jira/browse/GORA-304\" rel=\"nofollow\">https://issues.apache.org/jira/browse/GORA-304</a></p>\n\n<p>Official Nutch tutorial recommends only 0.90.x HBase branch:\n<a href=\"http://wiki.apache.org/nutch/Nutch2Tutorial\" rel=\"nofollow\">http://wiki.apache.org/nutch/Nutch2Tutorial</a></p>\n\n<p>Also you can download HBase 0.94.24-hadoop-2.5.0 version which I created and tested today:\n<a href=\"https://github.com/dobromyslov/hbase/releases/tag/0.94.24-hadoop-2.5.0\" rel=\"nofollow\">https://github.com/dobromyslov/hbase/releases/tag/0.94.24-hadoop-2.5.0</a></p>\n\n<p>Take a note that Nutch 2.2.1 does not support HBase 0.94.x and you have to get the latest Nutch 2.x from Git branch: <a href=\"https://github.com/apache/nutch/tree/2.x\" rel=\"nofollow\">https://github.com/apache/nutch/tree/2.x</a></p>\n", "question_id": 23436168, "creation_date": 1410659526, "is_accepted": true, "score": 2, "last_activity_date": 1410756077, "answer_id": 25829395}], "question_id": 23436168, "tags": ["hadoop", "hbase", "cluster-computing", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/23436168/nutch-in-hadoop-2-x", "last_activity_date": 1410756077, "accepted_answer_id": 25829395, "body": "<p>I have a three-node cluster running Hadoop 2.2.0 and HBase 0.98.1 and I need to use a Nutch 2.2.1 crawler on top of that. But it only supports Hadoop versions from 1.x branch. By now I am able to submit a Nutch job to my cluster, but it fails with java.lang.NumberFormatException.\nSo my question is pretty simple: how do I make Nutch work in my environment?</p>\n", "creation_date": 1399061387, "score": 1},
{"title": "How to explore hadoop file with .job extention", "view_count": 22, "is_answered": false, "answers": [{"question_id": 25671357, "owner": {"user_id": 1337352, "accept_rate": 32, "link": "http://stackoverflow.com/users/1337352/babu", "user_type": "registered", "reputation": 623}, "body": "<p>Well I finaly found that *.job extension is similar to *.jar extention and so it can be simple unziped.</p>\n\n<pre><code>unzip myjob.job\n</code></pre>\n", "creation_date": 1410691661, "is_accepted": false, "score": 0, "last_activity_date": 1410691661, "answer_id": 25832453}], "question_id": 25671357, "tags": ["hadoop", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/25671357/how-to-explore-hadoop-file-with-job-extention", "last_activity_date": 1410691661, "owner": {"user_id": 1337352, "answer_count": 21, "creation_date": 1334613000, "accept_rate": 32, "view_count": 155, "location": "Prague, Czech Republic", "reputation": 623}, "body": "<p>I have apache-nutch-2.3-SNAPSHOT.job file and I would like to explore what is packed inside. What tools should I use?</p>\n", "creation_date": 1409850827, "score": 0},
{"title": "Compatibility issue between Hbase 0.94.2 and apache nutch dependency", "view_count": 413, "is_answered": true, "answers": [{"question_id": 23991018, "owner": {"user_id": 1177597, "link": "http://stackoverflow.com/users/1177597/viacheslav-dobromyslov", "user_type": "registered", "reputation": 1859}, "body": "<p>You installed Apache Nutch 2.2.1 it uses Apache Gora 0.3 which only supports old Apache HBase 0.90.x as it stated in the <a href=\"https://wiki.apache.org/nutch/Nutch2Tutorial\" rel=\"nofollow\">official docs</a>.</p>\n\n<p>Anyway you still can use Nutch 2.2.x using the following workaround:</p>\n\n<ol>\n<li><p>Clone, configure and build fresh <a href=\"https://github.com/apache/nutch/tree/2.x\" rel=\"nofollow\">Nutch from the official Git branch 2.x</a> as it migrated to Gora 0.4 which compatible with Apache HBase 0.94.x.</p></li>\n<li><p>Clone and build <a href=\"https://github.com/dobromyslov/hbase/releases/tag/0.94.24-hadoop-2.5.0\" rel=\"nofollow\">my version of Apache HBase 0.94.24-hadoop-2.5.0</a> to use it with the latest Apache Hadoop 2.5.0.</p></li>\n</ol>\n\n<p>Similar issue was created for <a href=\"https://issues.apache.org/jira/browse/GORA-304\" rel=\"nofollow\">Apache Gora 0.3</a> project. They don't plan to upgrade Apache HBase dependency to the fresh one in the nearest future.</p>\n\n<p>You can also read <a href=\"http://hbase.apache.org/book/configuration.html\" rel=\"nofollow\">compatibility documentation for Apache HBase</a> to figure out how to build your own version for any Hadoop release.</p>\n\n<p>Apache Nutch was tested and works well with the following stack:</p>\n\n<ul>\n<li>Apache Nutch from 2.x git branch which uses Gora 0.4;</li>\n<li>Apache Hbase 0.94.24-hadoop-2.5.0;</li>\n<li>Apache Hadoop 2.5.0.</li>\n</ul>\n", "creation_date": 1410680077, "is_accepted": false, "score": 1, "last_activity_date": 1410680077, "answer_id": 25831086}], "question_id": 23991018, "tags": ["apache", "hadoop", "hbase", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/23991018/compatibility-issue-between-hbase-0-94-2-and-apache-nutch-dependency", "last_activity_date": 1410680077, "owner": {"age": 28, "answer_count": 5, "creation_date": 1371305078, "user_id": 2488981, "accept_rate": 72, "view_count": 119, "reputation": 547}, "body": "<p>I am trying to install apache nutch 2.2.1 and have successfully build it after making the required changes in the configuration files by following <a href=\"http://www.blogjava.net/paulwong/archive/2013/08/31/403513.html\" rel=\"nofollow\">http://www.blogjava.net/paulwong/archive/2013/08/31/403513.html</a> tutorial. But even after building it I am not able to crawl anything and after hours of inspection I realized that the hbase version on my company cluster is Hbase- 0.94.2 whereas the installation dependency for  apache nutch 2.2.1 is HBase 0.90.4. As hbase-0.90.4.jar is not compatible with Hbase- 0.94.2 I am getting the following error when I try to inject the url into nutch.  Kindly help me in changing the dependency of the apache nutch or fixing the error.</p>\n\n<h1>I am posting  the error below.</h1>\n\n<p>Caused by: java.lang.RuntimeException: java.lang.IllegalArgumentException: Not a host:port pair: \u00ef\u00bf\u00bd-11562@bt13acl1node26.comp.com\u00ef\u00bf\u00bd$3\u00ef\u00bf\u00bd\u00bf\u00bdbt13acl1node26.comp.com,60000,1401268790838\n        at org.apache.gora.hbase.store.HBaseStore.initialize(HBaseStore.java:127)\n        at org.apache.gora.store.DataStoreFactory.initializeDataStore(DataStoreFactory.java:102)\n        at org.apache.gora.store.DataStoreFactory.createDataStore(DataStoreFactory.java:161)\n        ... 12 more</p>\n\n<p>Caused by: java.lang.IllegalArgumentException: Not a host:port pair: \u00ef\u00bf\u00bd-11562@bt13acl1node26.comp.com\u00ef\u00bf\u00bd$3\u00ef\u00bf\u00bd\u00bf\u00bdbt13acl1node26.comp.com,60000,1401268790838\n        at org.apache.hadoop.hbase.HServerAddress.(HServerAddress.java:60)\n        at org.apache.hadoop.hbase.MasterAddressTracker.getMasterAddress(MasterAddressTracker.java:63)\n        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.getMaster(HConnectionManager.java:354)\n        at org.apache.hadoop.hbase.client.HBaseAdmin.(HBaseAdmin.java:94)\n        at org.apache.gora.hbase.store.HBaseStore.initialize(HBaseStore.java:109)\n        ... 14 more</p>\n", "creation_date": 1401700436, "score": 1},
{"title": "java.lang.ClassNotFoundException: org.apache.gora.hbase.store.HBaseStore", "view_count": 1777, "is_answered": true, "answers": [{"last_edit_date": 1368113823, "owner": {"user_id": 582789, "link": "http://stackoverflow.com/users/582789/alfonso-nishikawa", "user_type": "registered", "reputation": 1263}, "body": "<p>It is not the NoClassDefFoundError from the tutorial.\nWhat you miss is <code>gora-hbase-0.2.1.jar</code>.\nDid you forget to configure gora-hbase dependency in ivy?</p>\n", "question_id": 16401667, "creation_date": 1368113500, "is_accepted": false, "score": 1, "last_activity_date": 1368113823, "answer_id": 16465507}, {"question_id": 16401667, "owner": {"user_id": 1177597, "link": "http://stackoverflow.com/users/1177597/viacheslav-dobromyslov", "user_type": "registered", "reputation": 1859}, "body": "<p>This is caused by wrong build configuration. To fix this just open <code>/ivy/ivy.xml</code> and uncomment these lines:</p>\n\n<pre><code>&lt;dependency org=\"org.apache.gora\" name=\"gora-core\" rev=\"0.4\" conf=\"*-&gt;default\"/&gt;\n&lt;dependency org=\"org.apache.gora\" name=\"gora-hbase\" rev=\"0.4\" conf=\"*-&gt;default\" /&gt;\n</code></pre>\n\n<p>And rebuild your nutch with:</p>\n\n<pre><code>ant clean\nant runtime\n</code></pre>\n\n<p>If it doesn't help then Nutch 2 tutorial says:</p>\n\n<blockquote>\n  <p>It's possible to encounter the following exception:\n  java.lang.NoClassDefFoundError:\n  org/apache/hadoop/hbase/HBaseConfiguration; this is caused by the fact\n  that sometimes the hbase TEST jar is deployed in the lib dir. To\n  resolve this just copy the lib over from your installed HBase dir into\n  the build lib dir. (This issue is currently in progress).</p>\n</blockquote>\n", "creation_date": 1410678361, "is_accepted": false, "score": 1, "last_activity_date": 1410678361, "answer_id": 25830910}], "question_id": 16401667, "tags": ["hbase", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/16401667/java-lang-classnotfoundexception-org-apache-gora-hbase-store-hbasestore", "last_activity_date": 1410678361, "owner": {"user_id": 2212133, "view_count": 5, "answer_count": 1, "creation_date": 1364310439, "reputation": 6}, "body": "<p>I try to run Nutch 2.0 with HBase as a storage backend for Gora.\nI following this Tutorial: <code>http://wiki.apache.org/nutch/Nutch2Tutorial</code> and got java.lang.NoClassDefFoundError; this tutorial also covers this error by \"copy the lib over from your installed HBase dir into the build lib dir\" but I don't know which exactly hbase TEST jar need to copy.\nIf have anyone success run Nutch 2.0 with HBase, please tell me which jar file I need to copy.\nThanks you very much.\n(Sorry about my English writing skill)</p>\n", "creation_date": 1367852853, "score": 1},
{"title": "nutch inject with hbase NoSuchMethodError", "view_count": 1374, "is_answered": true, "answers": [{"question_id": 12377941, "owner": {"user_id": 322076, "accept_rate": 27, "link": "http://stackoverflow.com/users/322076/neville-chinan", "user_type": "registered", "reputation": 154}, "body": "<p>It works with hbase-0.90.5 I guess there is a problem or some other configuration to be added when using nutch 2 and hbase-0.94.*</p>\n", "creation_date": 1347449699, "is_accepted": false, "score": 1, "last_activity_date": 1347449699, "answer_id": 12387259}, {"question_id": 12377941, "owner": {"user_id": 1677276, "link": "http://stackoverflow.com/users/1677276/shawn", "user_type": "registered", "reputation": 4}, "body": "<p>When I run nutch2.0 with 0.94.1 in command, it's nosuchmethod, and in eclipse it's java.lang.NoClassDefFoundError: org/apache/hadoop/hbase/HBaseConfiguration.\nalthogh I copy jar or config classpath. it's not work.\nnow I will change to 90.5, I hope it going to work.</p>\n", "creation_date": 1348024351, "is_accepted": false, "score": 0, "last_activity_date": 1348024351, "answer_id": 12488054}, {"question_id": 12377941, "owner": {"user_id": 1177597, "link": "http://stackoverflow.com/users/1177597/viacheslav-dobromyslov", "user_type": "registered", "reputation": 1859}, "body": "<p>This is a common compatibility issue these days. We just need to wait when Apache Gora adopts Apache HBase 0.98.x or above. Meanwhile there exists a workaround described below.</p>\n\n<p>Apache Nutch 2.2.1 uses Apache Gora 0.3 which only supports old Apache HBase 0.90.x.\nTo use Nutch 2.2.x at the moment you have to:</p>\n\n<ol>\n<li><p>Clone, configure and build fresh <a href=\"https://github.com/apache/nutch/tree/2.x\" rel=\"nofollow\">Nutch from the official Git branch 2.x</a>.</p></li>\n<li><p>Clone and build <a href=\"https://github.com/dobromyslov/hbase/releases/tag/0.94.24-hadoop-2.5.0\" rel=\"nofollow\">my version of Apache HBase 0.94.24-hadoop-2.5.0</a>.</p></li>\n</ol>\n\n<p>You can also read <a href=\"http://hbase.apache.org/book/configuration.html\" rel=\"nofollow\">compatibility documentation for Apache HBase</a> to figure out how to build your own version for any Hadoop release.</p>\n\n<p>And to be completely informed take a look at the similar <a href=\"https://issues.apache.org/jira/browse/GORA-304\" rel=\"nofollow\">issue with Apache Gora 0.3</a>.</p>\n\n<p>My tested working stack is:</p>\n\n<ul>\n<li>Apache Nutch from 2.x git branch which uses Gora 0.4;</li>\n<li>Apache Hbase 0.94.24-hadoop-2.5.0;</li>\n<li>Apache Hadoop 2.5.0.</li>\n</ul>\n", "creation_date": 1410678027, "is_accepted": false, "score": 1, "last_activity_date": 1410678027, "answer_id": 25830864}], "question_id": 12377941, "tags": ["hbase", "nutch"], "answer_count": 3, "link": "http://stackoverflow.com/questions/12377941/nutch-inject-with-hbase-nosuchmethoderror", "last_activity_date": 1410678027, "owner": {"age": 34, "answer_count": 8, "creation_date": 1271838283, "user_id": 322076, "accept_rate": 27, "view_count": 28, "location": "London, United Kingdom", "reputation": 154}, "body": "<p>When I try and run nutch I am presented with the following error.  I have hbase-0.94.0 installed and running, hadoop is running also with no problems.  in ${NUTCH_HOME}/runtime/local/lib hbase-0.94.0.jar is present.  I get the following error, it does seem to be a problem with a dependency but I am quite stuck.\nThanks</p>\n\n<pre><code>./nutch inject /tmp/seed.txt\nInjectorJob: starting\nInjectorJob: urlDir: /tmp/seed.txt\n2012-09-11 22:02:14.097 java[7636:1903] Unable to load realm info from SCDynamicStore\nException in thread \"main\" java.lang.NoSuchMethodError: org.apache.hadoop.hbase.HColumnDescriptor.setMaxVersions(I)V\n    at org.apache.gora.hbase.store.HBaseMapping$HBaseMappingBuilder.addFamilyProps(HBaseMapping.java:114)\n    at org.apache.gora.hbase.store.HBaseStore.readMapping(HBaseStore.java:545)\n    at org.apache.gora.hbase.store.HBaseStore.initialize(HBaseStore.java:113)\n    at org.apache.gora.store.DataStoreFactory.initializeDataStore(DataStoreFactory.java:102)\n    at org.apache.gora.store.DataStoreFactory.createDataStore(DataStoreFactory.java:161)\n    at org.apache.gora.store.DataStoreFactory.createDataStore(DataStoreFactory.java:135)\n    at org.apache.nutch.storage.StorageUtils.createWebStore(StorageUtils.java:69)\n    at org.apache.nutch.crawl.InjectorJob.run(InjectorJob.java:243)\n    at org.apache.nutch.crawl.InjectorJob.inject(InjectorJob.java:268)\n    at org.apache.nutch.crawl.InjectorJob.run(InjectorJob.java:288)\n    at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n    at org.apache.nutch.crawl.InjectorJob.main(InjectorJob.java:298)\n</code></pre>\n", "creation_date": 1347397667, "score": 2},
{"title": "Configuring Hbase standalone mode with Apache Nutch : java.lang.IllegalArgumentException: Not a host:port pair", "view_count": 2958, "is_answered": true, "answers": [{"question_id": 13946725, "owner": {"user_id": 1274085, "link": "http://stackoverflow.com/users/1274085/hari", "user_type": "registered", "reputation": 166}, "body": "<p>Nutch 2.1 ships with hbase-0.90.4. Replace the jar file in /build/lib with the version of hbase installed on your machine. That should fix the issue.</p>\n", "creation_date": 1356632405, "is_accepted": false, "score": 3, "last_activity_date": 1356632405, "answer_id": 14059297}, {"question_id": 13946725, "owner": {"user_id": 1177597, "link": "http://stackoverflow.com/users/1177597/viacheslav-dobromyslov", "user_type": "registered", "reputation": 1859}, "body": "<p>This is a common compatibility issue.</p>\n\n<p>Apache Nutch 2.2.1 uses Apache Gora 0.3 which only supports old Apache HBase 0.90.x.\nTo use Nutch 2.2.x at the moment you have to:</p>\n\n<ol>\n<li><p>Clone, configure and build fresh <a href=\"https://github.com/apache/nutch/tree/2.x\" rel=\"nofollow\">Nutch from the official Git branch 2.x</a>.</p></li>\n<li><p>Clone and build <a href=\"https://github.com/dobromyslov/hbase/releases/tag/0.94.24-hadoop-2.5.0\" rel=\"nofollow\">my version of Apache HBase 0.94.24-hadoop-2.5.0</a>.</p></li>\n</ol>\n\n<p>You can also read <a href=\"http://hbase.apache.org/book/configuration.html\" rel=\"nofollow\">compatibility documentation for Apache HBase</a> to figure out how to build your own version for any Hadoop release.</p>\n\n<p>And to be completely informed take a look at the similar <a href=\"https://issues.apache.org/jira/browse/GORA-304\" rel=\"nofollow\">issue with Apache Gora 0.3</a>.</p>\n\n<p>My tested working stack is:</p>\n\n<ul>\n<li>Apache Nutch from 2.x git branch which uses Gora 0.4;</li>\n<li>Apache Hbase 0.94.24-hadoop-2.5.0;</li>\n<li>Apache Hadoop 2.5.0.</li>\n</ul>\n", "creation_date": 1410677762, "is_accepted": false, "score": 0, "last_activity_date": 1410677762, "answer_id": 25830850}], "question_id": 13946725, "tags": ["hbase", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/13946725/configuring-hbase-standalone-mode-with-apache-nutch-java-lang-illegalargumente", "last_activity_date": 1410677762, "owner": {"user_id": 1764882, "answer_count": 0, "creation_date": 1350897702, "accept_rate": 6, "view_count": 36, "reputation": 110}, "body": "<p>I got the following error while I running the below command in apache Nutch 2.1 and hbase 0.94.3. </p>\n\n<blockquote>\n  <p>Exception in thread \"main\" org.apache.gora.util.GoraException: java.lang.IllegalArgumentException: Not a host:port pair:</p>\n</blockquote>\n\n<p>Below I have paste my hbase-site.xml configuration  </p>\n\n<pre><code>&lt;code&gt;\n   &lt;property&gt;\n        &lt;name&gt;hbase.rootdir&lt;/name&gt;\n        &lt;value&gt;file:///home/data/&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;hbase.zookeeper.property.dataDir&lt;/name&gt;\n    &lt;value&gt;/home/data/zookeeper&lt;/value&gt;\n  &lt;/property&gt;\n&lt;/code&gt;\n</code></pre>\n\n<p>Can you please suggest me to proceed further? </p>\n", "creation_date": 1355898628, "score": 1},
{"title": "Nutch 1.4 and Solr 3.6 - Nutch not crawling 301/302 redirects", "view_count": 280, "is_answered": false, "answers": [{"question_id": 25810424, "owner": {"user_id": 18941, "accept_rate": 71, "link": "http://stackoverflow.com/users/18941/thesoftwarejedi", "user_type": "registered", "reputation": 20767}, "body": "<p>Try running <a href=\"https://www.wireshark.org/download.html\" rel=\"nofollow\">Wireshark</a> on the webserver to see exactly what is being served, and on the machine Nutch is on to see what's being requested.  If they're on the same server, great.  Try that and add HTTP to your filter box after the capture.</p>\n", "creation_date": 1410565566, "is_accepted": false, "score": 0, "last_activity_date": 1410565566, "answer_id": 25818460}], "question_id": 25810424, "tags": ["solr", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/25810424/nutch-1-4-and-solr-3-6-nutch-not-crawling-301-302-redirects", "last_activity_date": 1410565566, "owner": {"age": 34, "answer_count": 3, "creation_date": 1236098475, "user_id": 73291, "view_count": 31, "location": "St Louis, MO", "reputation": 109}, "body": "<p>I am having an issue where the initial page is crawled by the redirect is not being crawled or indexed.</p>\n\n<p>I have the http.redirect.max property set to 5, I have attempted values 0, 1, and 3.</p>\n\n<pre><code>&lt;property&gt;\n  &lt;name&gt;http.redirect.max&lt;/name&gt;\n  &lt;value&gt;5&lt;/value&gt;\n  &lt;description&gt;The maximum number of redirects the fetcher will follow when\n  trying to fetch a page. If set to negative or 0, fetcher won't immediately\n  follow redirected URLs, instead it will record them for later fetching.\n  &lt;/description&gt;\n&lt;/property&gt;\n</code></pre>\n\n<p>I have also attempted to clear out a majority of what is in the regex-urlfilter.txt and crawl-urlfilter.txt. Other than the website being crawled this is the only other params in these files.</p>\n\n<pre><code># skip file: ftp: and mailto: urls\n-^(file|ftp|mailto):\n\n# skip image and other suffixes we can't yet parse\n-\\.(gif|GIF|jpg|JPG|png|PNG|ico|ICO|css|sit|eps|wmf|zip|ppt|mpg|xls|gz|rpm|tgz|mov|MOV|exe|jpeg|JPEG|bmp|BMP|PDF|pdf|js|JS|swf|SWF|ashx|css|CSS|wmv|WMV)$\n</code></pre>\n\n<p>Also it seems like Nutch is crawling and pushing only pages that have querystring parameters. </p>\n\n<p>When looking at the output.</p>\n\n<pre><code>http://example.com/build    Version: 7\nStatus: 4 (db_redir_temp)\nFetch time: Fri Sep 12 00:32:33 EDT 2014\nModified time: Wed Dec 31 19:00:00 EST 1969\nRetries since fetch: 0\nRetry interval: 2700 seconds (0 days)\nScore: 0.04620983\nSignature: null\nMetadata: _pst_: temp_moved(13), lastModified=0: http://example.com/build/\n</code></pre>\n\n<p>There is a default IIS redirect occuring throwing a 302 to add the trailing slash. I have made sure this slash is already added on all pages. So unsure why this is being redirected.</p>\n\n<p>Just a bit more information, here are some parameters I have tried.</p>\n\n<pre><code>depth=5 (tried 1-10)\nthreads=30 (tried 1 - 30)\nadddays=7 (tried 0, 7)\ntopN=500 (tried 500, 1000)\n</code></pre>\n", "creation_date": 1410531491, "score": 2},
{"title": "How to read Nutch content from Java/Scala?", "view_count": 797, "owner": {"user_id": 924313, "answer_count": 45, "creation_date": 1314913712, "accept_rate": 97, "view_count": 1259, "location": "Zagreb, Croatia", "reputation": 8029}, "is_answered": true, "answers": [{"last_edit_date": 1405341028, "owner": {"user_id": 1629362, "accept_rate": 67, "link": "http://stackoverflow.com/users/1629362/bitkot", "user_type": "registered", "reputation": 2987}, "body": "<p><strong>Scala:</strong></p>\n\n<pre><code>val conf = NutchConfiguration.create()\nval fs = FileSystem.get(conf)\nval file = new Path(\".../part-00000/data\")\nval reader = new SequenceFile.Reader(fs, file, conf)\n\nval webdata = Stream.continually {\n  val key = new Text()\n  val content = new Content()\n  reader.next(key, content)\n  (key, content)\n}\n\nprintln(webdata.head)\n</code></pre>\n\n<p><strong>Java:</strong></p>\n\n<pre><code>public class ContentReader {\n    public static void main(String[] args) throws IOException { \n        Configuration conf = NutchConfiguration.create();       \n        Options opts = new Options();       \n        GenericOptionsParser parser = new GenericOptionsParser(conf, opts, args);       \n        String[] remainingArgs = parser.getRemainingArgs();     \n        FileSystem fs = FileSystem.get(conf);\n        String segment = remainingArgs[0];\n        Path file = new Path(segment, Content.DIR_NAME + \"/part-00000/data\");\n        SequenceFile.Reader reader = new SequenceFile.Reader(fs, file, conf);\n        Text key = new Text();\n        Content content = new Content();\n        // Loop through sequence files\n        while (reader.next(key, content)) {\n            try {\n                System.out.write(content.getContent(), 0,\n                        content.getContent().length);\n            } catch (Exception e) {\n            }\n        }\n    }\n}\n</code></pre>\n\n<p>Alternatively, you can use <code>org.apache.nutch.segment.SegmentReader</code> (<a href=\"http://www.coderanch.com/t/601082/hadoop/databases/Extract-data-Hadoop-File-system\" rel=\"nofollow\">example</a>).</p>\n", "question_id": 24699305, "creation_date": 1405333311, "is_accepted": true, "score": 1, "last_activity_date": 1405341028, "answer_id": 24734361}], "question_id": 24699305, "tags": ["java", "hadoop", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/24699305/how-to-read-nutch-content-from-java-scala", "last_activity_date": 1410475745, "accepted_answer_id": 24734361, "body": "<p>I'm using Nutch to crawl some websites (as a process that runs separate of everything else), while I want to use a Java (Scala) program to analyse the HTML data of websites using Jsoup.</p>\n\n<p>I got Nutch to work by following the <a href=\"http://wiki.apache.org/nutch/NutchTutorial\" rel=\"nofollow\">tutorial</a> (without the script, only executing the individual instructions worked), and I think it's saving the websites' HTML in the <code>crawl/segments/&lt;time&gt;/content/part-00000</code> directory.</p>\n\n<p>The problem is that I cannot figure out how to actually read the website data (URLs and HTML) in a Java/Scala program. I read this <a href=\"https://wiki.apache.org/nutch/Getting_Started\" rel=\"nofollow\">document</a>, but find it a bit overwhelming since I've never used Hadoop.</p>\n\n<p>I tried to adapt the example code to my environment, and this is what I arrived at (mostly by guesswprk):</p>\n\n<pre><code>  val reader = new MapFile.Reader(FileSystem.getLocal(new Configuration()), \".../apache-nutch-1.8/crawl/segments/20140711115438/content/part-00000\", new Configuration())\n  var key = null\n  var value = null\n  reader.next(key, value) // test for a single value\n  println(key)\n  println(value)\n</code></pre>\n\n<p>However, I am getting this exception when I run it:</p>\n\n<pre><code>Exception in thread \"main\" java.lang.NullPointerException\n    at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:1873)\n    at org.apache.hadoop.io.MapFile$Reader.next(MapFile.java:517)\n</code></pre>\n\n<p>I am not sure how to work with a <code>MapFile.Reader</code>, specifically, what constructor parameters I am supposed to pass to it. What Configuration objects am I supposed to pass in? Is that the correct FileSystem? And is that the data file I'm interested in?</p>\n", "creation_date": 1405086450, "score": 0},
{"title": "How to make apache solr URL POST Supported", "view_count": 157, "is_answered": false, "question_id": 25784309, "tags": ["solr", "nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/25784309/how-to-make-apache-solr-url-post-supported", "last_activity_date": 1410429115, "owner": {"user_id": 552521, "answer_count": 91, "creation_date": 1293117735, "accept_rate": 77, "view_count": 804, "location": "Dublin, Ireland", "reputation": 2952}, "body": "<p>I am trying to integrate apache nucth and Solr and when nutch tries to dump the output to solr , it throws </p>\n\n<pre><code>HTTP method POST is not supported by this URL\n</code></pre>\n\n<p>I checked configurations but couldn't find the right point to make solr url POST supported , and I don't like to use Tomcat to drive solr , so how to make solr url POST supported</p>\n", "creation_date": 1410429115, "score": 0},
{"title": "Cygwin JVM error while trying to launch Nutch Win-7", "view_count": 201, "owner": {"user_id": 552521, "answer_count": 91, "creation_date": 1293117735, "accept_rate": 77, "view_count": 804, "location": "Dublin, Ireland", "reputation": 2952}, "is_answered": true, "answers": [{"question_id": 25749337, "owner": {"user_id": 552521, "accept_rate": 77, "link": "http://stackoverflow.com/users/552521/abhishek-choudhary", "user_type": "registered", "reputation": 2952}, "body": "<p>Ok I realized that </p>\n\n<pre><code>Could not reserve enough space for object heap\n</code></pre>\n\n<p>does mean that more memory requirement is the trouble as some windows applications were pulling more memory , so I decrease the memory and problem solved.</p>\n", "creation_date": 1410353382, "is_accepted": true, "score": 1, "last_activity_date": 1410353382, "answer_id": 25765909}], "question_id": 25749337, "tags": ["cygwin", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/25749337/cygwin-jvm-error-while-trying-to-launch-nutch-win-7", "last_activity_date": 1410353382, "accepted_answer_id": 25765909, "body": "<p>I am trying to launch apache-nutch from win, so used cygwin but whenever I run the command </p>\n\n<pre><code>bin/crawl urls -dir crawl -depth 3 -topN 5\n</code></pre>\n\n<p>it throws the error-</p>\n\n<pre><code>Error occurred during initialization of VM\nCould not reserve enough space for object heap\nError: Could not create the Java Virtual Machine.\nError: A fatal exception has occurred. Program will exit.\n</code></pre>\n\n<p>I already changed the JAVA_HEAP_MAX in bin\\nutch with higher value and tried but nothing worked.</p>\n", "creation_date": 1410278845, "score": 2},
{"title": "query nutch 2 table result from cassandra 2 dose not look right", "view_count": 235, "owner": {"user_id": 665216, "answer_count": 5, "creation_date": 1300401517, "accept_rate": 56, "view_count": 51, "reputation": 591}, "is_answered": true, "answers": [{"question_id": 23327152, "owner": {"user_id": 2379376, "link": "http://stackoverflow.com/users/2379376/daniel", "user_type": "registered", "reputation": 51}, "body": "<p>Nutch stores its data in the f column family as <em>BytesType</em>. The column names are stored as <em>UTF8Type</em>.\nIf you want to get the data as a String you have to convert it first. A row is completely stored in the ByteBuffer. In your example you convert the whole byte buffer to String what gives you the whole row. When you select one row, you get the current position an limit of that row. So you have to read from begin:buffer current pointer position to buffer limit. For example to get the websites content in the \"cnt\" field:</p>\n\n<pre><code>// This is the byte buffer you get from selecting column \"cnt\"\nByteBuffer buffer;\nint length = buffer.limit() - buffer.position();\n\nbyte[] cellValue = new byte[length];\n\nbuffer.get(cellValue, 0, length);\n\nreturn new String(cellValue, Charset.forName(\"UTF-8\"));\n</code></pre>\n", "creation_date": 1410290728, "is_accepted": true, "score": 0, "last_activity_date": 1410290728, "answer_id": 25752136}], "question_id": 23327152, "tags": ["java", "cassandra", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/23327152/query-nutch-2-table-result-from-cassandra-2-dose-not-look-right", "last_activity_date": 1410290728, "accepted_answer_id": 25752136, "body": "<p>I am using Nutch 2.2.1 and Cassandra 2 to craw pages. for a test i just inject one url to Cassandra and explore the database.</p>\n\n<p>Using CQL i can query the table in the webpage keyspace</p>\n\n<p>cqlsh:simplex> select * from webpage.f;</p>\n\n<pre><code> key                                  | column1 | value\n--------------------------------------+---------+--------------------\n 0x6564752e6373752e7777773a687474702f |  0x6669 |         0x00278d00\n 0x6564752e6373752e7777773a687474702f |    0x73 |         0x3f800000\n 0x6564752e6373752e7777773a687474702f |  0x7473 | 0x00000145a266703e\n</code></pre>\n\n<p>which is fine if i convert those hex bytes to string. the key will be the reverted url.</p>\n\n<p>then i write java code to read the table f using datastax java driver 2 (<a href=\"http://www.datastax.com/documentation/developer/java-driver/2.0/java-driver/whatsNew2.html\" rel=\"nofollow\">http://www.datastax.com/documentation/developer/java-driver/2.0/java-driver/whatsNew2.html</a>)</p>\n\n<p>i followed the sample code </p>\n\n<pre><code>Cluster cluster = Cluster.builder().addContactPoint(\"10.20.104.181\").build();\nSession session = cluster.connect();\nResultSet results = session.execute(\"SELECT * FROM webpage.f\");\nfor (Row row : results) {\n\n            System.out.println(\"Key\");\n        System.out.println(toStrFromByteBuffer(row.getBytes(\"key\")));\n        System.out.println(\"column1\");\n        System.out.println(toStrFromByteBuffer(row.getBytes(\"column1\")));\n        System.out.println(\"value\");\n        System.out.println(toStrFromByteBuffer(row.getBytes(\"value\")));\n\n}\ncluster.close();\n\n\n\npublic static String toStrFromByteBuffer(ByteBuffer buffer)\n    {\n        byte[] ar=buffer.array();\n        System.out.println(ar.length);\n        return new String(ar,Charset.forName(\"UTF-8\"));\n    }\n</code></pre>\n\n<p>the result is below. You can see row.getBytes(\"key\") returns a whole row data not a specific column value. </p>\n\n<p>Could some master help on this?</p>\n\n<p><img src=\"http://i.stack.imgur.com/4ks5J.gif\" alt=\"enter image description here\"></p>\n", "creation_date": 1398622676, "score": 2},
{"title": "Nutch + Solr - Indexer causes java.lang.OutOfMemoryError: Java heap space", "view_count": 381, "is_answered": false, "answers": [{"question_id": 25708897, "owner": {"user_id": 1281305, "accept_rate": 88, "link": "http://stackoverflow.com/users/1281305/johnny-greenwood", "user_type": "registered", "reputation": 494}, "body": "<p>I have solved it with configuration below (mapred-site.xml file):</p>\n\n<pre><code>&lt;property&gt;\n  &lt;name&gt;mapred.jobtracker.retirejob.interval&lt;/name&gt;\n  &lt;value&gt;3600000&lt;/value&gt;\n&lt;/property&gt;\n\n&lt;property&gt;\n  &lt;name&gt;mapred.job.tracker.retiredjobs.cache.size&lt;/name&gt;\n  &lt;value&gt;100&lt;/value&gt;\n&lt;/property&gt;\n\n&lt;property&gt;\n&lt;name&gt;mapred.child.java.opts&lt;/name&gt;\n&lt;value&gt;-Xmx4000m -XX:+UseConcMarkSweepGC&lt;/value&gt;\n&lt;/property&gt;\n\n&lt;property&gt;\n&lt;name&gt;mapred.child.ulimit&lt;/name&gt;\n&lt;value&gt;6000000&lt;/value&gt;\n&lt;/property&gt;\n\n&lt;property&gt;\n  &lt;name&gt;mapred.jobtracker.completeuserjobs.maximum&lt;/name&gt;\n  &lt;value&gt;5&lt;/value&gt;\n&lt;/property&gt;\n\n&lt;property&gt;\n  &lt;name&gt;mapred.job.tracker.handler.count&lt;/name&gt;\n  &lt;value&gt;5&lt;/value&gt;\n&lt;/property&gt;\n</code></pre>\n", "creation_date": 1410245847, "is_accepted": false, "score": 0, "last_activity_date": 1410245847, "answer_id": 25738395}], "question_id": 25708897, "tags": ["java", "tomcat", "hadoop", "solr", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/25708897/nutch-solr-indexer-causes-java-lang-outofmemoryerror-java-heap-space", "last_activity_date": 1410246157, "owner": {"age": 27, "answer_count": 17, "creation_date": 1332257106, "user_id": 1281305, "accept_rate": 88, "view_count": 129, "location": "Prague, Czech Republic", "reputation": 494}, "body": "<p>I have configured my 2 servers to run in distributed mode (with Hadoop) and my configuration for crawling process is Nutch 2.2.1 - HBase (as a storage) and Solr. Solr is run by Tomcat. The problem is everytime I try to do the last step - I mean when I want to index data from HBase into Solr. After then this <strong>[1]</strong> error occures. I tried to add CATALINA_OPTS (or JAVA_OPTS) like this: </p>\n\n<blockquote>\n  <p>CATALINA_OPTS=\"$JAVA_OPTS -XX:+UseConcMarkSweepGC -Xms1g -Xmx6000m\n  -XX:MinHeapFreeRatio=10 -XX:MaxHeapFreeRatio=30 -XX:MaxPermSize=512m -XX:+CMSClassUnloadingEnabled\"</p>\n</blockquote>\n\n<p>to Tomcat's catalina.sh script and run server with this script but it didn't help. I also add these <strong>[2]</strong> properties to nutch-site.xml file but it ended up with <code>OutOfMemory</code> again. Can you help me please? </p>\n\n<p><strong>[1]</strong></p>\n\n<pre><code>2014-09-06 22:52:50,683 FATAL org.apache.hadoop.mapred.Child: Error running child : java.lang.OutOfMemoryError: Java heap space \n    at java.util.Arrays.copyOf(Arrays.java:2367) \n    at java.lang.AbstractStringBuilder.expandCapacity(AbstractStringBuilder.java:130) \n    at java.lang.AbstractStringBuilder.ensureCapacityInternal(AbstractStringBuilder.java:114) \n    at java.lang.AbstractStringBuilder.append(AbstractStringBuilder.java:587) \n    at java.lang.StringBuffer.append(StringBuffer.java:332) \n    at java.io.StringWriter.write(StringWriter.java:77) \n    at org.apache.solr.common.util.XML.escape(XML.java:204) \n    at org.apache.solr.common.util.XML.escapeCharData(XML.java:77) \n    at org.apache.solr.common.util.XML.writeXML(XML.java:147) \n    at org.apache.solr.client.solrj.util.ClientUtils.writeVal(ClientUtils.java:161) \n    at org.apache.solr.client.solrj.util.ClientUtils.writeXML(ClientUtils.java:129) \n    at org.apache.solr.client.solrj.request.UpdateRequest.writeXML(UpdateRequest.java:355) \n    at org.apache.solr.client.solrj.request.UpdateRequest.getXML(UpdateRequest.java:271) \n    at org.apache.solr.client.solrj.request.RequestWriter.getContentStream(RequestWriter.java:66) \n    at org.apache.solr.client.solrj.request.RequestWriter$LazyContentStream.getDelegate(RequestWriter.java:94) \n    at org.apache.solr.client.solrj.request.RequestWriter$LazyContentStream.getName(RequestWriter.java:104) \n    at org.apache.solr.client.solrj.impl.HttpSolrServer.request(HttpSolrServer.java:247) \n    at org.apache.solr.client.solrj.impl.HttpSolrServer.request(HttpSolrServer.java:197) \n    at org.apache.solr.client.solrj.request.AbstractUpdateRequest.process(AbstractUpdateRequest.java:117) \n    at org.apache.solr.client.solrj.SolrServer.add(SolrServer.java:68) \n    at org.apache.solr.client.solrj.SolrServer.add(SolrServer.java:54) \n    at org.apache.nutch.indexwriter.solr.SolrIndexWriter.close(SolrIndexWriter.java:96) \n    at org.apache.nutch.indexer.IndexWriters.close(IndexWriters.java:117) \n    at org.apache.nutch.indexer.IndexerOutputFormat$1.close(IndexerOutputFormat.java:54) \n    at org.apache.hadoop.mapred.MapTask$NewDirectOutputCollector.close(MapTask.java:650) \n    at org.apache.hadoop.mapred.MapTask.closeQuietly(MapTask.java:1793) \n    at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:779) \n    at org.apache.hadoop.mapred.MapTask.run(MapTask.java:364) \n    at org.apache.hadoop.mapred.Child$4.run(Child.java:255) \n    at java.security.AccessController.doPrivileged(Native Method) \n    at javax.security.auth.Subject.doAs(Subject.java:415) \n    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190) \n</code></pre>\n\n<p><strong>[2]</strong></p>\n\n<pre><code>&lt;property&gt;\n  &lt;name&gt;http.content.limit&lt;/name&gt;\n  &lt;value&gt;150000000&lt;/value&gt;\n&lt;/property&gt;\n\n&lt;property&gt;\n   &lt;name&gt;indexer.max.tokens&lt;/name&gt;\n   &lt;value&gt;100000&lt;/value&gt;\n&lt;/property&gt;\n\n&lt;property&gt;\n  &lt;name&gt;http.timeout&lt;/name&gt;\n  &lt;value&gt;50000&lt;/value&gt;\n&lt;/property&gt;\n\n&lt;property&gt;\n  &lt;name&gt;solr.commit.size&lt;/name&gt;\n  &lt;value&gt;100&lt;/value&gt;\n&lt;/property&gt;\n</code></pre>\n", "creation_date": 1410082557, "score": 0},
{"title": "How to Create a nutch plugin that returns raw html to the parser", "view_count": 749, "is_answered": false, "answers": [{"question_id": 22639257, "owner": {"user_id": 300327, "accept_rate": 40, "link": "http://stackoverflow.com/users/300327/kartik", "user_type": "registered", "reputation": 1084}, "body": "<p>You may need to implement HTMLParser. In your getFields implementation,</p>\n\n<pre><code> private static final Collection&lt;WebPage.Field&gt; FIELDS = new HashSet&lt;WebPage.Field&gt;();\n  static {\n    FIELDS.add(WebPage.Field.CONTENT);\n    FIELDS.add(WebPage.Field.OUTLINKS);\n  }\n  public Collection&lt;Field&gt; getFields() {\n    return FIELDS;\n  }\n</code></pre>\n", "creation_date": 1409728523, "is_accepted": false, "score": 0, "last_activity_date": 1409728523, "answer_id": 25638418}], "question_id": 22639257, "tags": ["java", "solr", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/22639257/how-to-create-a-nutch-plugin-that-returns-raw-html-to-the-parser", "last_activity_date": 1409728523, "owner": {"user_id": 3460377, "answer_count": 3, "creation_date": 1395760847, "accept_rate": 17, "view_count": 11, "reputation": 1}, "body": "<p>I am trying to create a plugin for nutch. I am using nutch 1.7 and solr. I used a lot of different tutorials. I want to realize a plugin that returns raw html data. i used the standard wiki of nutch and the following tutorial:<a href=\"http://sujitpal.blogspot.nl/2009/07/nutch-custom-plugin-to-parse-and-add.html\" rel=\"nofollow\">http://sujitpal.blogspot.nl/2009/07/nutch-custom-plugin-to-parse-and-add.html</a></p>\n\n<p>I created two files getDivinfohtml.java and getDivinfo.java.</p>\n\n<p>getDivinfohtml.java needs to read the content and then return the complete source code. or atleast a part of the source code</p>\n\n<pre><code> package org.apache.nutch.indexer;\n public class getDivInfohtml implements HtmlParseFilter\n {\nprivate static final Log LOG = LogFactory.getLog(getDivInfohtml.class);\nprivate Configuration conf;\n    public static final String TAG_KEY = \"source\";\n    // Logger logger = Logger.getLogger(\"mylog\");\n    // FileHandler fh;\n    //FileSystem fs = FileSystem.get(conf);\n    //Path file = new Path(segment, Content.DIR_NAME + \"/part-00000/data\");\n    //SequenceFile.Reader reader = new SequenceFile.Reader(fs, file, conf);\n    //Text key = new Text();\n    // Content content = new Content();\n    // fh = new FileHandler(\"/root/JulienKulkerNutch/mylogfile.log\");\n// logger.addHandler(fh);\n// SimpleFormatter formatter = new SimpleFormatter();\n//fh.setFormatter(formatter);\n\n\npublic ParseResult filter(Content content, ParseResult parseResult, HTMLMetaTags metaTags, DocumentFragment doc)\n{\n    try\n    {\n        LOG.info(\"Parsing Url:\" + content.getUrl());\n        LOG.info(\"Julien: \"+content.toString().substring(content.toString().indexOf(\"&lt;!DOCTYPE html\")));\n\n        Parse parse = parseResult.get(content.getUrl());\n        Metadata metadata = parse.getData().getParseMeta();\n        String fullContent = metadata.get(\"fullcontent\");\n\n        Document document = Jsoup.parse(fullContent);\n        Element contentwrapper = document.select(\"div#jobBodyContent\").first();\n        String source = contentwrapper.text();\n        metadata.add(\"SOURCE\", source);\n\n        return parseResult;\n\n    }\n    catch(Exception e)\n    {\n        LOG.info(e);\n    }\n\n    return parseResult;\n}\n\n\npublic Configuration getConf()\n{\n    return conf;\n}\n\npublic void setConf(Configuration conf)\n{\n    this.conf = conf;\n}\n</code></pre>\n\n<p>}</p>\n\n<p>It reads the compelete content right now and then extract the text in jobBodyContent. </p>\n\n<p>Then we have the parser that needs to put the data into the fields </p>\n\n<p>getDivinfo(parser)</p>\n\n<pre><code>public NutchDocument filter(NutchDocument doc, Parse parse, Text url, CrawlDatum datum, Inlinks inlinks)\n{\n    // LOG.info(\"Julien is sukkel\");\n    try\n    {\n        fh = new FileHandler(\"/root/JulienKulkerNutch/mylogfile2.log\");\n        SimpleFormatter formatter = new SimpleFormatter();\n        fh.setFormatter(formatter);\n        logger.info(\"Julien is sukkel\");\n        Metadata metadata = parse.getData().getParseMeta();\n        logger.info(\"julien is gek:\");\n        String fullContent = metadata.get(\"SOURCE\");\n        logger.info(\"Output:\" + metadata);\n        logger.info(fullContent);\n        String fullSource = parse.getData().getParseMeta().getValues(getDivInfohtml.TAG_KEY);\n        logger.info(fullSource);\n        doc.add(\"divcontent\", fullContent);\n\n    }\n    catch(Exception e)\n    {\n        //LOG.info(e);\n    }\n\n    return doc;\n}\n</code></pre>\n\n<p>the erros is in getDivinfo: String fullSource = parse.getData().getParseMeta().getValues(getDivInfohtml.TAG_KEY);</p>\n\n<p>[javac] /root/JulienKulkerNutch/apache-nutch-1.8/src/plugin/myDivSelector/src/java/org/apache/nutch/indexer/getDivInfo.java:58: error: cannot find symbol\n    [javac]             String fullSource = parse.getData().getParseMeta().getValues(getDivInfohtml.TAG_KEY);</p>\n", "creation_date": 1395761610, "score": 0},
{"title": "Solr indexer...SolrException:Bad Request", "view_count": 2245, "is_answered": false, "answers": [{"last_edit_date": 1409692457, "owner": {"user_id": 1113772, "accept_rate": 100, "link": "http://stackoverflow.com/users/1113772/paul-sweatte", "user_type": "registered", "reputation": 14979}, "body": "<p>Use the Nutch config(nutch-default.xml) to set the path to the temporary directory:</p>\n\n<pre><code>&lt;property&gt;\n &lt;name&gt;mapred.temp.dir&lt;/name&gt;\n &lt;value&gt;/tmp&lt;/value&gt;\n &lt;description&gt;A shared directory for temporary files.&lt;/description&gt;\n&lt;/property&gt;\n</code></pre>\n\n<p><strong>References</strong></p>\n\n<ul>\n<li><p><a href=\"http://wiki.apache.org/nutch/NutchConfigurationFiles\" rel=\"nofollow\">Nutch Configuration Files</a></p></li>\n<li><p><a href=\"http://hadoop.apache.org/docs/r1.0.4/mapred-default.html\" rel=\"nofollow\">MapReduce Default Configuration</a></p></li>\n<li><p><a href=\"http://wiki.apache.org/nutch/GettingNutchRunningWithDebian#Configure_Tomcat.27s_File_and_Webapp_Paths\" rel=\"nofollow\">Configure Tomcat's File and Webapp Paths</a></p></li>\n<li><p><a href=\"https://issues.apache.org/jira/browse/NUTCH-159\" rel=\"nofollow\">Specify temp/working directory for crawl</a></p></li>\n<li><p><a href=\"http://stackoverflow.com/questions/1924136/1113772\">Environment variable to control java.io.tmpdir?</a></p></li>\n</ul>\n", "question_id": 14814315, "creation_date": 1409678189, "is_accepted": false, "score": 0, "last_activity_date": 1409692457, "answer_id": 25628878}], "question_id": 14814315, "tags": ["solr", "nutch", "solrj", "websolr"], "answer_count": 1, "link": "http://stackoverflow.com/questions/14814315/solr-indexer-solrexceptionbad-request", "last_activity_date": 1409692457, "owner": {"user_id": 1731155, "answer_count": 2, "creation_date": 1349772488, "accept_rate": 0, "view_count": 21, "reputation": 291}, "body": "<p>When I start solrindex, I have the exception\n    <code>org.apache.solr.common.SolrException: Bad Request</code>\nthis is the hadoop.log file content</p>\n\n<pre><code>...\n2013-02-11 17:01:22,079 INFO  anchor.AnchorIndexingFilter - Anchor deduplication is: off\n2013-02-11 17:01:22,079 INFO  indexer.IndexingFilters - Adding org.apache.nutch.indexer.anchor.AnchorIndexingFilter\n2013-02-11 17:01:22,222 INFO  solr.SolrMappingReader - source: content dest: content\n2013-02-11 17:01:22,222 INFO  solr.SolrMappingReader - source: title dest: title\n2013-02-11 17:01:22,222 INFO  solr.SolrMappingReader - source: host dest: host\n2013-02-11 17:01:22,222 INFO  solr.SolrMappingReader - source: segment dest: segment\n2013-02-11 17:01:22,222 INFO  solr.SolrMappingReader - source: boost dest: boost\n2013-02-11 17:01:22,222 INFO  solr.SolrMappingReader - source: digest dest: digest\n2013-02-11 17:01:22,222 INFO  solr.SolrMappingReader - source: tstamp dest: tstamp\n2013-02-11 17:01:22,223 INFO  solr.SolrMappingReader - source: url dest: id\n2013-02-11 17:01:22,223 INFO  solr.SolrMappingReader - source: url dest: url\n2013-02-11 17:01:23,940 INFO  solr.SolrWriter - Indexing 250 documents\n2013-02-11 17:01:23,952 INFO  solr.SolrWriter - Deleting 0 documents\n2013-02-11 17:01:24,780 WARN  mapred.LocalJobRunner - job_local_0001\norg.apache.solr.common.SolrException: Bad Request\n\nBad Request\n\n request: http://localhost:8080/solr/update?wt=javabin&amp;version=2\nat org.apache.solr.client.solrj.impl.CommonsHttpSolrServer.request(CommonsHttpSolrServer.java:427)\nat org.apache.solr.client.solrj.impl.CommonsHttpSolrServer.request(CommonsHttpSolrServer.java:249)\nat org.apache.solr.client.solrj.request.AbstractUpdateRequest.process(AbstractUpdateRequest.java:105)\nat org.apache.nutch.indexer.solr.SolrWriter.write(SolrWriter.java:124)\nat org.apache.nutch.indexer.IndexerOutputFormat$1.write(IndexerOutputFormat.java:55)\nat org.apache.nutch.indexer.IndexerOutputFormat$1.write(IndexerOutputFormat.java:44)\nat     org.apache.hadoop.mapred.ReduceTask$OldTrackingRecordWriter.write(ReduceTask.java:457)\nat org.apache.hadoop.mapred.ReduceTask$3.collect(ReduceTask.java:497)\nat org.apache.nutch.indexer.IndexerMapReduce.reduce(IndexerMapReduce.java:304)\nat org.apache.nutch.indexer.IndexerMapReduce.reduce(IndexerMapReduce.java:53)\nat org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:519)\nat org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:420)\nat org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:260)\n2013-02-11 17:01:25,330 ERROR solr.SolrIndexer - java.io.IOException: Job failed!\n</code></pre>\n\n<p>I think that i have some problem with my schema.xml\nthis is it</p>\n\n<pre><code>&lt;?xml version=\"1.0\" encoding=\"UTF-8\" ?&gt;\n&lt;!--\n    Licensed to the Apache Software Foundation (ASF) under one or\n    more contributor license agreements. See the NOTICE file\n    distributed with this work for additional information regarding\n    copyright ownership. The ASF licenses this file to You under the\n    Apache License, Version 2.0 (the \"License\"); you may not use\n    this file except in compliance with the License. You may obtain\n    a copy of the License at\n    http://www.apache.org/licenses/LICENSE-2.0 Unless required by\n    applicable law or agreed to in writing, software distributed\n    under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n    See the License for the specific language governing permissions\n    and limitations under the License.\n--&gt;\n&lt;!--\n    Description: This document contains Solr 3.1 schema definition to\n    be used with Solr integration currently build into Nutch. See\n    https://issues.apache.org/jira/browse/NUTCH-442\n    https://issues.apache.org/jira/browse/NUTCH-699\n    https://issues.apache.org/jira/browse/NUTCH-994\n    https://issues.apache.org/jira/browse/NUTCH-997\n    https://issues.apache.org/jira/browse/NUTCH-1058\n    https://issues.apache.org/jira/browse/NUTCH-1232\n    and\n    http://svn.apache.org/viewvc/lucene/dev/branches/branch_3x/solr/\n    example/solr/conf/schema.xml?view=markup\n    for more info.\n--&gt;\n&lt;schema name=\"nutch\" version=\"1.5\"&gt;\n&lt;types&gt;\n    &lt;fieldType name=\"string\" class=\"solr.StrField\" sortMissingLast=\"true\"\n        omitNorms=\"true\"/&gt; \n    &lt;fieldType name=\"long\" class=\"solr.TrieLongField\" precisionStep=\"0\"\n        omitNorms=\"true\" positionIncrementGap=\"0\"/&gt;\n    &lt;fieldType name=\"float\" class=\"solr.TrieFloatField\" precisionStep=\"0\"\n        omitNorms=\"true\" positionIncrementGap=\"0\"/&gt;\n    &lt;fieldType name=\"date\" class=\"solr.TrieDateField\" precisionStep=\"0\"\n        omitNorms=\"true\" positionIncrementGap=\"0\"/&gt;\n\n    &lt;fieldType name=\"text\" class=\"solr.TextField\"\n        positionIncrementGap=\"100\"&gt;\n        &lt;analyzer&gt;\n            &lt;tokenizer class=\"solr.WhitespaceTokenizerFactory\"/&gt;\n            &lt;filter class=\"solr.StopFilterFactory\"\n                ignoreCase=\"true\" words=\"stopwords.txt\"/&gt;\n            &lt;filter class=\"solr.WordDelimiterFilterFactory\"\n                generateWordParts=\"1\" generateNumberParts=\"1\"\n                catenateWords=\"1\" catenateNumbers=\"1\" catenateAll=\"0\"\n                splitOnCaseChange=\"1\"/&gt;\n            &lt;filter class=\"solr.LowerCaseFilterFactory\"/&gt;\n            &lt;filter class=\"solr.EnglishPorterFilterFactory\"\n                protected=\"protwords.txt\"/&gt;\n            &lt;filter class=\"solr.RemoveDuplicatesTokenFilterFactory\"/&gt;\n        &lt;/analyzer&gt;\n    &lt;/fieldType&gt;\n    &lt;fieldType name=\"url\" class=\"solr.TextField\"\n        positionIncrementGap=\"100\"&gt;\n        &lt;analyzer&gt;\n            &lt;tokenizer class=\"solr.StandardTokenizerFactory\"/&gt;\n            &lt;filter class=\"solr.LowerCaseFilterFactory\"/&gt;\n            &lt;filter class=\"solr.WordDelimiterFilterFactory\"\n                generateWordParts=\"1\" generateNumberParts=\"1\"/&gt;\n        &lt;/analyzer&gt;\n    &lt;/fieldType&gt;\n&lt;/types&gt;\n&lt;fields&gt;\n    &lt;field name=\"id\" type=\"string\" stored=\"true\" indexed=\"true\"/&gt;\n&lt;!--    &lt;field name=\"site\" type=\"string\" stored=\"false\" indexed=\"true\"/&gt;  --&gt;\n    &lt;!-- core fields --&gt;\n    &lt;field name=\"segment\" type=\"string\" stored=\"true\" indexed=\"false\"/&gt;\n    &lt;field name=\"digest\" type=\"string\" stored=\"true\" indexed=\"false\"/&gt;\n    &lt;field name=\"boost\" type=\"float\" stored=\"true\" indexed=\"false\"/&gt;\n\n    &lt;!-- fields for index-basic plugin --&gt;\n    &lt;field name=\"host\" type=\"string\" stored=\"false\" indexed=\"true\"/&gt;\n    &lt;field name=\"url\" type=\"url\" stored=\"true\" indexed=\"true\"\n        required=\"true\"/&gt;\n    &lt;field name=\"content\" type=\"text\" stored=\"false\" indexed=\"true\"/&gt;\n    &lt;field name=\"title\" type=\"text\" stored=\"true\" indexed=\"true\"/&gt;\n    &lt;field name=\"cache\" type=\"string\" stored=\"true\" indexed=\"false\"/&gt;\n    &lt;field name=\"tstamp\" type=\"date\" stored=\"true\" indexed=\"false\"/&gt;\n\n    &lt;!-- fields for index-anchor plugin --&gt;\n    &lt;field name=\"anchor\" type=\"string\" stored=\"true\" indexed=\"true\"\n        multiValued=\"true\"/&gt;\n\n    &lt;!-- fields for index-more plugin --&gt;\n    &lt;field name=\"type\" type=\"string\" stored=\"true\" indexed=\"true\"\n        multiValued=\"true\"/&gt;\n    &lt;field name=\"contentLength\" type=\"long\" stored=\"true\"\n        indexed=\"false\"/&gt;\n    &lt;field name=\"lastModified\" type=\"date\" stored=\"true\"\n        indexed=\"false\"/&gt;\n    &lt;field name=\"date\" type=\"date\" stored=\"true\" indexed=\"true\"/&gt;\n\n    &lt;!-- fields for languageidentifier plugin --&gt;\n    &lt;field name=\"lang\" type=\"string\" stored=\"true\" indexed=\"true\"/&gt;\n\n    &lt;!-- fields for subcollection plugin --&gt;\n    &lt;field name=\"subcollection\" type=\"string\" stored=\"true\"\n        indexed=\"true\" multiValued=\"true\"/&gt;\n\n    &lt;!-- fields for feed plugin (tag is also used by microformats-reltag)--&gt;\n    &lt;field name=\"author\" type=\"string\" stored=\"true\" indexed=\"true\"/&gt;\n    &lt;field name=\"tag\" type=\"string\" stored=\"true\" indexed=\"true\" multiValued=\"true\"/&gt;\n    &lt;field name=\"feed\" type=\"string\" stored=\"true\" indexed=\"true\"/&gt;\n    &lt;field name=\"publishedDate\" type=\"date\" stored=\"true\"\n        indexed=\"true\"/&gt;\n    &lt;field name=\"updatedDate\" type=\"date\" stored=\"true\"\n        indexed=\"true\"/&gt;\n\n    &lt;!-- fields for creativecommons plugin --&gt;\n    &lt;field name=\"cc\" type=\"string\" stored=\"true\" indexed=\"true\"\n        multiValued=\"true\"/&gt;\n\n&lt;!--    &lt;field name=\"Metatags\" type=\"string\" stored=\"true\" indexed=\"true\" multiValued=\"true\"/&gt; --&gt;\n\n&lt;/fields&gt;\n&lt;uniqueKey&gt;id&lt;/uniqueKey&gt;\n&lt;defaultSearchField&gt;content&lt;/defaultSearchField&gt;\n&lt;solrQueryParser defaultOperator=\"OR\"/&gt;\n</code></pre>\n\n<p></p>\n\n<p>I coped schema.xml from nutch/conf folder to the solr/conf, and now I'm  use nutch 1.6 solr 3.6.2 and apache tomcat 6.0.36...i think i will help to understand th e problem.\nI have not any errors in apahce log file...\nolso i change solr-solrj-3.4.jar file in nutch/lib folder with solr-solrj-3.6.2.jar from solr/dist/ folder...</p>\n\n<p>i can show my solrindex-mapping.xml</p>\n\n<pre><code>&lt;mapping&gt;\n&lt;!-- Simple mapping of fields created by Nutch IndexingFilters\n     to fields defined (and expected) in Solr schema.xml.\n\n         Any fields in NutchDocument that match a name defined\n         in field/@source will be renamed to the corresponding\n         field/@dest.\n         Additionally, if a field name (before mapping) matches\n         a copyField/@source then its values will be copied to \n         the corresponding copyField/@dest.\n\n         uniqueKey has the same meaning as in Solr schema.xml\n         and defaults to \"id\" if not defined.\n     --&gt;\n&lt;fields&gt;\n    &lt;field dest=\"content\" source=\"content\"/&gt;\n    &lt;field dest=\"title\" source=\"title\"/&gt;\n    &lt;field dest=\"host\" source=\"host\"/&gt;\n    &lt;field dest=\"segment\" source=\"segment\"/&gt;\n    &lt;field dest=\"boost\" source=\"boost\"/&gt;\n    &lt;field dest=\"digest\" source=\"digest\"/&gt;\n    &lt;field dest=\"tstamp\" source=\"tstamp\"/&gt;\n    &lt;field dest=\"id\" source=\"url\"/&gt;\n    &lt;copyField source=\"url\" dest=\"url\"/&gt;\n&lt;/fields&gt;\n&lt;uniqueKey&gt;id&lt;/uniqueKey&gt;\n&lt;/mapping&gt;\n</code></pre>\n\n<p>Thanks for any suggestions.</p>\n", "creation_date": 1360593594, "score": 0},
{"title": "how to fetch all the outlinks refrenced on a particular page with page using nutch&#39;s parser job", "view_count": 109, "is_answered": false, "question_id": 25607211, "tags": ["hbase", "nutch", "gora"], "answer_count": 0, "link": "http://stackoverflow.com/questions/25607211/how-to-fetch-all-the-outlinks-refrenced-on-a-particular-page-with-page-using-nut", "last_activity_date": 1409579098, "owner": {"user_id": 3832389, "answer_count": 11, "creation_date": 1405173693, "accept_rate": 45, "view_count": 63, "reputation": 339}, "body": "<p>i am using the nutch2.2 and hbase 0.94 and gora 0.4 and when i am executing the steps as follows </p>\n\n<pre><code>1.nutch inject seed.txt\n2.nutch generate -batchId 231\n3.nutch fetch 231\n4.nutch parse 231\n5.nutch updatedb 231\n</code></pre>\n\n<p>i'll get the html content of a particular page say([<a href=\"http://www.flipkart.com/mens-clothing/t-shirts?otracker=hp_nmenu_sub_men_0_T-Shirts]\" rel=\"nofollow\">http://www.flipkart.com/mens-clothing/t-shirts?otracker=hp_nmenu_sub_men_0_T-Shirts]</a>) but when i am executing the step 4</p>\n\n<pre><code>nutch parse 231\n</code></pre>\n\n<p>and see my <code>webpage</code> table created in hbase there is a <code>ol(outlink)</code> column family but it is empty</p>\n\n<p>if anyone can help it will be good for me if i get all the outlink.</p>\n\n<p>Thanks in advance</p>\n", "creation_date": 1409579098, "score": 2},
{"title": "What database options are available for Nutch 2.1?", "view_count": 991, "owner": {"age": 55, "answer_count": 293, "creation_date": 1283911805, "user_id": 441979, "accept_rate": 100, "view_count": 797, "location": "Reston, VA", "reputation": 5115}, "is_answered": true, "answers": [{"question_id": 13483970, "owner": {"user_id": 441979, "accept_rate": 100, "link": "http://stackoverflow.com/users/441979/alex-blakemore", "user_type": "registered", "reputation": 5115}, "body": "<p>The database connection info for Nutch 2.1 is specified in the conf/gora.properties file (should have seen that)</p>\n", "creation_date": 1353531889, "is_accepted": false, "score": 3, "last_activity_date": 1353531889, "answer_id": 13502054}, {"question_id": 13483970, "owner": {"user_id": 1274085, "link": "http://stackoverflow.com/users/1274085/hari", "user_type": "registered", "reputation": 166}, "body": "<p>I've tried with MYSQL and HBASE.</p>\n\n<p>For MYSQL, this link helps iron out most of the quirks: <a href=\"http://nlp.solutions.asia/?p=180\" rel=\"nofollow\">http://nlp.solutions.asia/?p=180</a></p>\n\n<p>For HBASE, versions above 0.90.x cause problems (Invalid Host Value pair). I've been able to get it working with 0.90.5</p>\n", "creation_date": 1353652279, "is_accepted": true, "score": 4, "last_activity_date": 1353652279, "answer_id": 13524128}], "question_id": 13483970, "tags": ["nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/13483970/what-database-options-are-available-for-nutch-2-1", "last_activity_date": 1409337527, "accepted_answer_id": 13524128, "body": "<p>I'm trying to test out Nutch 2.1 on a single Windows machine. The following command dies:</p>\n\n<pre><code>nutch crawl seeds -dir crawl -solr http://somehost:8983/solr -depth 2 -topN 2\n</code></pre>\n\n<p>...with a traceback of several exceptions:</p>\n\n<ul>\n<li><code>java.net.ConnectionException: Connection refused</code></li>\n<li><code>GoraException</code></li>\n<li><code>SQLTransientConnectionException</code></li>\n<li><code>org.hsqldb.HsqlException</code></li>\n</ul>\n\n<p>This is the same problem as this post: <a href=\"http://stackoverflow.com/questions/12581492/connection-refused-error-when-running-nutch-2\">connection refused error when running Nutch 2</a></p>\n\n<p>It looks like Nutch 2 wants some kind of database already installed, but there's no mention of that in the (sparse) documentation that I can see.</p>\n\n<p>The production environment will eventually be a linux/Hadoop cluster, but for the moment I'm just trying to get a simple local system to work out of the box.</p>\n\n<p>So what options are there for a simple Nutch database? How do you tell Nutch and Gora about the database? HBase might be a good answer as soon as we have our Hadoop cluster up and running. However; in the meantime is there a simple, even slow, database that will work for initial exploration on a single system?</p>\n", "creation_date": 1353453948, "score": 1},
{"title": "How can I find how nutch reached a link/url?", "view_count": 94, "is_answered": false, "question_id": 25557495, "tags": ["solr", "web-crawler", "nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/25557495/how-can-i-find-how-nutch-reached-a-link-url", "last_activity_date": 1409257833, "owner": {"age": 32, "answer_count": 3, "creation_date": 1313596715, "user_id": 899003, "accept_rate": 91, "view_count": 40, "reputation": 87}, "body": "<p>We have an odd situation with a Nutch crawl we're doing, wherein at some point Nutch reaches an erroneous webpage for which, really, the server should be sending a 404. For whatever reason the server isn't. </p>\n\n<p>When Nutch hits this \"bad\" URL, a page generates with all relative paths based off an erroneous. So, if the root page is \"<a href=\"http://example.com/bad\" rel=\"nofollow\">http://example.com/bad</a>\", this page will have hundreds of links for things like \"example.com/bad/data/1\" and \"example.com/bad/calendar/2012\" and \"example.com/bad/data/1/calendar/2012\".</p>\n\n<p>As such, Nutch will continue to crawl forever.</p>\n\n<p>I'd like to trace back whatever page erroneously first links to this \"bad\" page. It seems like this should maybe be possible using either the <code>bin/nutch readlinkdb</code> or the <code>bin/nutch readdb</code> command. I have tried specifying an error url (via '--url') with each with no relevant URLs found. Doing a full dump does in fact net data that doesn't include any erroneous URLs. But Nutch definitely does inject erroneous URLs into Solr.</p>\n\n<p>Why would this be, and how can I trace Nutch's path to the erroneous URLs?</p>\n", "creation_date": 1409257833, "score": 1},
{"title": "apache nutch 2.2.1 with hbase ERROR", "view_count": 454, "is_answered": false, "answers": [{"last_edit_date": 1409218756, "owner": {"user_id": 582789, "link": "http://stackoverflow.com/users/582789/alfonso-nishikawa", "user_type": "registered", "reputation": 1263}, "body": "<p>This is the line failing:</p>\n\n<pre><code>batchId = new Utf8(conf.get(GeneratorJob.BATCH_ID));\n</code></pre>\n\n<p>How are you running your job? If I am not wrong, <code>crawl</code> command is deprecated, and now <code>generate</code> needs a batch id; at least, it happened to me time ago. Now with a development branch seems to work fine even if you don't set the batch id...</p>\n\n<hr>\n\n<p>From <a href=\"http://wiki.apache.org/nutch/Nutch2Tutorial\" rel=\"nofollow\">http://wiki.apache.org/nutch/Nutch2Tutorial</a>: </p>\n\n<blockquote>\n  <p>N.B. The crawl command in the bin/nutch script is deprecated. You\n  should use individual commands or alternatively use the bin/crawl\n  script... which effectively chains together individual commands.</p>\n</blockquote>\n", "question_id": 25237924, "creation_date": 1409217822, "is_accepted": false, "score": 0, "last_activity_date": 1409218756, "answer_id": 25545256}], "question_id": 25237924, "tags": ["apache", "solr", "hbase", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/25237924/apache-nutch-2-2-1-with-hbase-error", "last_activity_date": 1409218756, "owner": {"user_id": 3928644, "view_count": 0, "answer_count": 0, "creation_date": 1407741411, "reputation": 1}, "body": "<p>I use Nutch 2.2.1, 4.3.0 and HBase 0.90.4 SOLR. </p>\n\n<p>I get the following error. </p>\n\n<pre><code>InjectorJob: Using class org.apache.gora.hbase.store.HBaseStore as the Gora storage class.\nInjectorJob: total number of urls rejected by filters: 0\nInjectorJob: total number of urls injected after normalization and filtering: 1\nException in thread \"main\" java.lang.RuntimeException: job failed: name=generate: null, jobid=job_local1662982347_0002\n    at org.apache.nutch.util.NutchJob.waitForCompletion(NutchJob.java:54)\n    at org.apache.nutch.crawl.GeneratorJob.run(GeneratorJob.java:199)\n    at org.apache.nutch.crawl.Crawler.runTool(Crawler.java:68)\n    at org.apache.nutch.crawl.Crawler.run(Crawler.java:152)\n    at org.apache.nutch.crawl.Crawler.run(Crawler.java:250)\n    at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n    at org.apache.nutch.crawl.Crawler.main(Crawler.java:257)\n</code></pre>\n\n<p>In the Hadoop logs is the following. </p>\n\n<pre><code>2014-08-11 09:13:43,246 INFO  crawl.InjectorJob - InjectorJob: Using class org.apache.gora.hbase.store.HBaseStore as the Gora storage class.\n2014-08-11 09:13:43,293 WARN  util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n2014-08-11 09:13:43,372 WARN  snappy.LoadSnappy - Snappy native library not loaded\n2014-08-11 09:13:44,017 INFO  mapreduce.GoraRecordWriter - gora.buffer.write.limit = 10000\n2014-08-11 09:13:44,245 INFO  regex.RegexURLNormalizer - can't find rules for scope 'inject', using default\n2014-08-11 09:13:44,381 WARN  mapred.FileOutputCommitter - Output path is null in cleanup\n2014-08-11 09:13:44,686 INFO  crawl.InjectorJob - InjectorJob: total number of urls rejected by filters: 0\n2014-08-11 09:13:44,686 INFO  crawl.InjectorJob - InjectorJob: total number of urls injected after normalization and filtering: 1\n2014-08-11 09:13:44,695 INFO  crawl.FetchScheduleFactory - Using FetchSchedule impl: org.apache.nutch.crawl.DefaultFetchSchedule\n2014-08-11 09:13:44,696 INFO  crawl.AbstractFetchSchedule - defaultInterval=2592000\n2014-08-11 09:13:44,696 INFO  crawl.AbstractFetchSchedule - maxInterval=7776000\n2014-08-11 09:13:45,392 INFO  mapreduce.GoraRecordReader - gora.buffer.read.limit = 10000\n2014-08-11 09:13:45,501 INFO  crawl.FetchScheduleFactory - Using FetchSchedule impl: org.apache.nutch.crawl.DefaultFetchSchedule\n2014-08-11 09:13:45,501 INFO  crawl.AbstractFetchSchedule - defaultInterval=2592000\n2014-08-11 09:13:45,501 INFO  crawl.AbstractFetchSchedule - maxInterval=7776000\n2014-08-11 09:13:45,547 INFO  regex.RegexURLNormalizer - can't find rules for scope 'generate_host_count', using default\n2014-08-11 09:13:45,654 INFO  mapreduce.GoraRecordWriter - gora.buffer.write.limit = 10000\n2014-08-11 09:13:45,670 WARN  mapred.FileOutputCommitter - Output path is null in cleanup\n2014-08-11 09:13:45,671 WARN  mapred.LocalJobRunner - job_local1662982347_0002\njava.lang.NullPointerException\n    at org.apache.avro.util.Utf8.&lt;init&gt;(Utf8.java:37)\n    at org.apache.nutch.crawl.GeneratorReducer.setup(GeneratorReducer.java:100)\n    at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:174)\n    at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:649)\n    at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:418)\n    at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:398)\n</code></pre>\n\n<p>Unfortunately I have no idea what I'm doing wrong. </p>\n\n<p>I have all the things implemented in the book \"Web crawling and data mining with Apache Nutch\". </p>\n\n<p>Unfortunately coming back error. Currently I am unfortunately Clueless.</p>\n", "creation_date": 1407742194, "score": 0},
{"title": "How to integrate apache-nutch-1.9 and Hadoop 2.3.0-cdh5.1.0?", "view_count": 378, "is_answered": false, "question_id": 25502207, "tags": ["hadoop", "nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/25502207/how-to-integrate-apache-nutch-1-9-and-hadoop-2-3-0-cdh5-1-0", "last_activity_date": 1409045218, "owner": {"user_id": 3967840, "view_count": 1, "answer_count": 0, "creation_date": 1408708743, "reputation": 6}, "body": "<p>I'm very new to <code>nutch</code> and was trying to integrate <code>nutch 1.9</code> with <code>Hadoop 2.3.0-cdh5.1.0</code> and getting exceptions like below: </p>\n\n<pre><code>Injector: java.lang.UnsupportedOperationException: Not implemented by the DistributedFileSystem FileSystem implementation\n        at org.apache.hadoop.fs.FileSystem.getScheme(FileSystem.java:214)\n        at org.apache.hadoop.fs.FileSystem.loadFileSystems(FileSystem.java:2365)\n        at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:2375)\n        at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2392)\n        at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:89)\n        at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2431)\n        at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2413)\n        at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:368)\n        at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:167)\n        at org.apache.nutch.crawl.Injector.inject(Injector.java:297)\n        at org.apache.nutch.crawl.Injector.run(Injector.java:380)\n        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)\n        at org.apache.nutch.crawl.Injector.main(Injector.java:370)\n</code></pre>\n\n<p>I have few questions about this:\nHow do i solve this issue. Also I had question in mind if I can really integrate the version of <code>hadoop</code> which I am using to <code>nutch 1.9</code>?</p>\n\n<p>Thanks,\nSandeep</p>\n", "creation_date": 1409044745, "score": 1},
{"title": "Nutch 1.7 JAVA_HOME not set Error", "view_count": 1352, "is_answered": true, "answers": [{"question_id": 25289173, "owner": {"user_id": 994000, "link": "http://stackoverflow.com/users/994000/betolink", "user_type": "registered", "reputation": 83}, "body": "<p>First try: readlink -f $(which java)</p>\n\n<p>That will tell you exactly where your JAVA_HOME is, you should see something like:</p>\n\n<pre><code>  /usr/lib/jvm/java-7-openjdk-amd64/jre/bin/java\n</code></pre>\n\n<p>Then try using this value to set your JAVA_HOME just before you call the crawl script i.e.</p>\n\n<pre><code>export JAVA_HOME=/usr/lib/jvm/java-7-openjdk-amd64/jre/ \nbin/nutch crawl urls -dir crawl -depth 3 -topN 5\n</code></pre>\n\n<p>note that the value should point to the JRE directory inside a valid JDK location.</p>\n\n<p>p.s. You are missing the Solr URL parameter (in case you want to index the crawled documents of course)</p>\n", "creation_date": 1408046181, "is_accepted": false, "score": 1, "last_activity_date": 1408046181, "answer_id": 25316549}], "question_id": 25289173, "tags": ["apache", "nutch", "web-crawler", "java-home"], "answer_count": 1, "link": "http://stackoverflow.com/questions/25289173/nutch-1-7-java-home-not-set-error", "last_activity_date": 1408046181, "owner": {"age": 26, "answer_count": 20, "creation_date": 1339467515, "user_id": 1450201, "accept_rate": 72, "view_count": 464, "location": "Istanbul, Turkey", "reputation": 756}, "body": "<p>I'm experimenting <strong>Apache Nutch 1.7</strong> and <strong>Solr</strong> on <strong>Ubuntu 14.04 x64 (AMD) LTS</strong> and when i try to run Nutch, it gives me this error message:</p>\n\n<blockquote>\n  <p>Error: JAVA_HOME is not set.</p>\n</blockquote>\n\n<p>But when i type <strong>echo $JAVA_HOME</strong> command on terminal, it gives me this path:\n<strong>/usr/lib/jvm/java-7-openjdk-amd64</strong></p>\n\n<p>Below you can see what i've done step by step. How can i fix this?</p>\n\n<p><strong>*ps</strong>: Ubuntu is a virtual machine which runs on Mac with Oracle VirtualBox</p>\n\n<ol>\n<li>Intalling java on terminal with <strong>sudo apt-get -y install openjdk-7-jdk</strong></li>\n<li>Checking java installation by <strong>java -version</strong> command</li>\n<li><p>Setting JAVA_HOME with:</p></li>\n<li><p><strong>sudo nano /etc/environment</strong></p></li>\n<li><p>Then typing following line at the bottom of file: <strong>JAVA_HOME=\"/usr/lib/jvm/java-7-openjdk-amd64\"</strong></p></li>\n<li><p>kntrl+X shortcut for Saving changes.</p></li>\n<li><p>Then this command: <strong>source /etc/environment</strong></p></li>\n<li><p>Now JAVA_HOME must be set. I checked it by following command and it gives me the path. <strong>echo $JAVA_HOME</strong> and output is same as above.</p></li>\n<li><p>Then i installed Solr by <strong>sudo apt-get -y install solr-tomcat</strong></p></li>\n<li><p>I controlled installation by typing this address in a browser: <strong><code>http://localhost:8080/solr</code></strong> and it shows me initial page of solr</p></li>\n<li><p>I downloaded Apache Nutch 1.7 from <a href=\"http://nutch.apache.org\" rel=\"nofollow\">http://nutch.apache.org</a> and file was named as apache-nutch-1.7.-bin.tar.gz</p></li>\n<li><p>Then extract it: <strong>tar -zxvf apache-nutch-1.7-bin.tar.gz</strong></p></li>\n<li><p>I verfied Nutch's installation by simply this:\n<strong>cd apache-nutch-1.7</strong> then \n<strong>bin/nutch</strong>\nAnd the output is like <strong>Usage: nutch COMMAND where......</strong></p></li>\n<li><p>Then i edit my <strong>conf/nutch-site.xml</strong> file as in here: <a href=\"http://www.opensourceconnections.com/blog/2014/05/24/crawling-with-nutch/\" rel=\"nofollow\"><strong><em>Link</em></strong></a> (You need to look under this title: \"<strong>3) Set Up Your Nutch-Site.Xml</strong>\" )\nThings i did different from that last reference are; MyBot  and MyBot,* fields. Instead of <em>MyBot</em> i wrote <em>mySpider</em></p></li>\n<li><p>Then i get in conf directory of nutch with Terminal. Here's what i did after:\n<strong>mkdir -p urls</strong> , \n<strong>cd urls</strong> , \n<strong>touch seed.txt</strong> , \n<strong>nano seed.txt</strong></p></li>\n<li><p>i only wrote this url in the file as it's suggested in official tutorial of nutch:\n<a href=\"http://nutch.apache.org\" rel=\"nofollow\">http://nutch.apache.org</a></p></li>\n</ol>\n\n<p>17After i saved my changed in seed.txt file. I edit the conf/regex-urlfilter.txt file. I delete these two lines:</p>\n\n<blockquote>\n  <h1>accept anything else</h1>\n  \n  <p>+.</p>\n</blockquote>\n\n<p>Then i wrote this instead of them:</p>\n\n<blockquote>\n  <p><code>+^http://([a-z0-9]*\\.)*nutch.apache.org/</code></p>\n</blockquote>\n\n<p>After that,</p>\n\n<p>I used this command as it's suggested in tutorial:\n<strong>bin/nutch crawl urls -dir crawl -depth 3 -topN 5</strong></p>\n\n<p><strong>After this command i see this error message:\nError: JAVA_HOME is not set.</strong></p>\n\n<p>I also found this article but it didn't solve my problem either:\n<a href=\"http://stackoverflow.com/questions/24771239/nutch-getting-error-java-home-is-not-set-when-trying-to-crawl\">Nutch - Getting Error: JAVA_HOME is not set. when trying to crawl</a></p>\n", "creation_date": 1407940913, "score": 0},
{"title": "An alternative web crawler to Nutch", "view_count": 7243, "is_answered": true, "answers": [{"question_id": 4269632, "owner": {"user_id": 397474, "accept_rate": 100, "link": "http://stackoverflow.com/users/397474/nate-c", "user_type": "registered", "reputation": 5157}, "body": "<p><a href=\"http://scrapy.org/\">Scrapy</a> is a python library that crawls web sites. It is fairly small (compared to Nutch) and designed for limited site crawls. It has a Django type MVC style that I found pretty easy to customize.</p>\n", "creation_date": 1290621423, "is_accepted": false, "score": 5, "last_activity_date": 1290621423, "answer_id": 4269982}, {"question_id": 4269632, "owner": {"user_id": 183100, "link": "http://stackoverflow.com/users/183100/pascal-dimassimo", "user_type": "registered", "reputation": 5766}, "body": "<p>For the crawling part, I really like <a href=\"http://anemone.rubyforge.org/\">anemone</a> and <a href=\"http://code.google.com/p/crawler4j/\">crawler4j</a>. They both allow you to add your custom logic for links selection and page handling. For each page that you decide to keep, you can easily add the call to Solr. </p>\n", "creation_date": 1298817326, "is_accepted": false, "score": 5, "last_activity_date": 1298817326, "answer_id": 5133772}, {"question_id": 4269632, "owner": {"user_id": 254385, "link": "http://stackoverflow.com/users/254385/arachnode-net", "user_type": "registered", "reputation": 795}, "body": "<p>In, C#, but a lot simpler and you can communicate directly with the author.  (me)</p>\n\n<p>I used to use Nutch and you are correct; it is a bear to work with.</p>\n\n<p><a href=\"http://arachnode.net\" rel=\"nofollow\">http://arachnode.net</a></p>\n", "creation_date": 1362342801, "is_accepted": false, "score": 3, "last_activity_date": 1362342801, "answer_id": 15190525}, {"question_id": 4269632, "owner": {"user_id": 447344, "accept_rate": 50, "link": "http://stackoverflow.com/users/447344/thomas-decaux", "user_type": "registered", "reputation": 6657}, "body": "<p>It depends on how many web sites and so URLs you think crawl. Apache Nutch stores page documents on Apache HBase (which relies on Apache Hadoop), it's solid but very hard to setup and administrate. </p>\n\n<p>A crawler is just a page fetch (like a CURL) and a list of links who will fill your URLs list, and so on. So, if you intent to crawl less than 1 000 000 pages, I am sure you can write a crawler on your own (especially if you have a few web sites), use a simple MySQL database and ElasticSearch.</p>\n\n<p>Of course a crawler could be more sophisticated, you could remove from your HTML document the HEAD part, and keep only the real \"content\" of the page. Also you could have a \"rank\" scoring (maybe by mixing site social reputation, pagerank etc...).</p>\n", "creation_date": 1399019899, "is_accepted": false, "score": 2, "last_activity_date": 1399019899, "answer_id": 23424014}, {"question_id": 4269632, "owner": {"user_id": 3812643, "accept_rate": 67, "link": "http://stackoverflow.com/users/3812643/jinhong-lu", "user_type": "registered", "reputation": 83}, "body": "<p>I do believe the nutch is the best choice for you application, but if you want, there is a simple tool: <a href=\"https://webarchive.jira.com/wiki/display/Heritrix/Heritrix\" rel=\"nofollow\">Heritrix</a>.\nBesides that, I recommand js for the front-end language, because solr returns json which is easily handled by js.</p>\n", "creation_date": 1407913232, "is_accepted": false, "score": 0, "last_activity_date": 1407913232, "answer_id": 25279981}], "question_id": 4269632, "tags": ["search-engine", "web-crawler", "nutch"], "answer_count": 5, "link": "http://stackoverflow.com/questions/4269632/an-alternative-web-crawler-to-nutch", "last_activity_date": 1407913232, "owner": {"age": 35, "answer_count": 13, "creation_date": 1270842308, "user_id": 313127, "accept_rate": 75, "view_count": 424, "location": "Algiers, Algeria", "reputation": 2433}, "body": "<p>I'm trying to build a specialised search engine web site that indexes a limited number of web sites. The solution I came up with is:</p>\n\n<ul>\n<li>using Nutch as the web crawler,</li>\n<li>using Solr as the search engine,</li>\n<li>the front-end and the site logic is coded with Wicket.</li>\n</ul>\n\n<p>The problem is that I find Nutch quite complex and it's a big piece of software to customise, despite the fact that a detailed documentation (books, recent tutorials.. etc) does just not exist.</p>\n\n<p>Questions now:</p>\n\n<ol>\n<li>Any constructive criticism about the hole idea of the site?</li>\n<li>Is there a good yet simple alternative to Nutch (as the crawling part of the site)?</li>\n</ol>\n\n<p>Thanks </p>\n", "creation_date": 1290619446, "score": 14},
{"title": "Using Nutch crawler with Solr", "view_count": 10310, "is_answered": true, "answers": [{"question_id": 211411, "owner": {"user_id": 21239, "accept_rate": 81, "link": "http://stackoverflow.com/users/21239/mauricio-scheffer", "user_type": "registered", "reputation": 81026}, "body": "<p>It's still an <a href=\"https://issues.apache.org/jira/browse/NUTCH-442\" rel=\"nofollow\">open issue</a>. If you're feeling adventurous you could try applying those patches yourself, although it looks like <a href=\"http://www.mail-archive.com/nutch-user@lucene.apache.org/msg10872.html\" rel=\"nofollow\">it's not so simple</a></p>\n", "creation_date": 1229649236, "is_accepted": false, "score": 1, "last_activity_date": 1229649236, "answer_id": 379855}, {"question_id": 211411, "owner": {"user_id": 1431230, "link": "http://stackoverflow.com/users/1431230/bdaniels", "user_type": "registered", "reputation": 183}, "body": "<p>If you're willing to upgrade to nutch 1.0 you can use the solrindex as described in this article by Lucid Imagination: <a href=\"http://www.lucidimagination.com/blog/2009/03/09/nutch-solr/\">http://www.lucidimagination.com/blog/2009/03/09/nutch-solr/</a>.</p>\n", "creation_date": 1249758522, "is_accepted": false, "score": 6, "last_activity_date": 1249758522, "answer_id": 1249651}, {"question_id": 211411, "owner": {"user_id": 3812643, "accept_rate": 67, "link": "http://stackoverflow.com/users/3812643/jinhong-lu", "user_type": "registered", "reputation": 83}, "community_owned_date": 1407912854, "creation_date": 1407912854, "is_accepted": false, "body": "<p>nutch 2.x is designed to use solr as default. You can follow the steps in <a href=\"http://wiki.apache.org/nutch/Nutch2Tutorial\" rel=\"nofollow\">http://wiki.apache.org/nutch/Nutch2Tutorial</a>,  or a better instruction in the book \"Web Crawling and Data Mining with Apache Nutch\".</p>\n", "score": 1, "last_activity_date": 1407912854, "answer_id": 25279861}], "question_id": 211411, "tags": ["lucene", "solr", "nutch"], "answer_count": 3, "link": "http://stackoverflow.com/questions/211411/using-nutch-crawler-with-solr", "last_activity_date": 1407912854, "owner": {"age": 38, "answer_count": 29, "creation_date": 1217828332, "user_id": 253, "accept_rate": 85, "view_count": 403, "location": "London, United Kingdom", "reputation": 1832}, "body": "<p>Am I able to integrate Apache Nutch crawler with the Solr Index server?</p>\n\n<p><strong>Edit:</strong></p>\n\n<p>One of our devs came up with a solution from these posts</p>\n\n<ol>\n<li><a href=\"http://wiki.apache.org/nutch/RunningNutchAndSolr\">Running Nutch and Solr</a></li>\n<li><a href=\"http://www.mail-archive.com/nutch-commits@lucene.apache.org/msg02227.html\">Update for Running Nutch and Solr</a></li>\n</ol>\n\n<p><strong>Answer</strong></p>\n\n<p>Yes</p>\n", "creation_date": 1224232359, "score": 12},
{"title": "Crawling websites which ask for authentication", "view_count": 1235, "is_answered": true, "answers": [{"question_id": 25183951, "owner": {"user_id": 2086509, "accept_rate": 73, "link": "http://stackoverflow.com/users/2086509/jayesh-bhoyar", "user_type": "registered", "reputation": 594}, "body": "<p>Few points which will help you in resolving this issue:</p>\n\n<p>1) Yes you have chosen wrong website to crawl and index try some different websites.</p>\n\n<p>2) Nutch only support <strong><em>NTLM, Basic or Digest authentication</em></strong>. It do not support the <strong>Form Based Authentication</strong>. The sites that you are trying use have Form based Authentication.</p>\n\n<p>3) To implement Form Based Authentication you will have to customize your Nutch code.</p>\n\n<p>I am sure following 2 links will help you in making some progress in this issue that you are facing:</p>\n\n<p><a href=\"http://technical-fundas.blogspot.in/2014/05/nutch-solr-formed-based-authentication.html\" rel=\"nofollow\">http://technical-fundas.blogspot.in/2014/05/nutch-solr-formed-based-authentication.html</a></p>\n\n<p><a href=\"http://technical-fundas.blogspot.in/2014/06/how-to-configure-nutch-in-eclipse-for.html\" rel=\"nofollow\">http://technical-fundas.blogspot.in/2014/06/how-to-configure-nutch-in-eclipse-for.html</a></p>\n", "creation_date": 1407495103, "is_accepted": false, "score": 2, "last_activity_date": 1407495103, "answer_id": 25201825}], "question_id": 25183951, "tags": ["nutch", "web-crawler"], "answer_count": 1, "link": "http://stackoverflow.com/questions/25183951/crawling-websites-which-ask-for-authentication", "last_activity_date": 1407495103, "owner": {"user_id": 3322698, "answer_count": 0, "creation_date": 1392715716, "accept_rate": 29, "view_count": 35, "reputation": 70}, "body": "<p>I followed this <a href=\"https://wiki.apache.org/nutch/HttpAuthenticationSchemes\" rel=\"nofollow\">https://wiki.apache.org/nutch/HttpAuthenticationSchemes</a> link for crawling few websites by providing username and password</p>\n\n<p>Work around:I have set the auth-configuration in httpclient-auth.xml file:</p>\n\n<pre><code>&lt;auth-configuration&gt;\n&lt;credentials username=\"xyz\" password=\"xyz\"&gt;\n&lt;default realm=\"domain\" /&gt;\n&lt;authscope host=\"www.gmail.com\" port=\"80\"/&gt;\n&lt;/credentials&gt;\n&lt;/auth-configuration&gt;\n</code></pre>\n\n<p>ii)Define httpclient property in both nutch-site.xml and nutch-default.xml</p>\n\n<pre><code>&lt;property&gt;\n&lt;name&gt;plugin.includes&lt;/name&gt;\n&lt;value&gt;protocol-httpclient|urlfilter-regex|parse-(text|html|js)|index-(basic|anchor)|query-(basic|site|url)|response-(json|xml)|summary-basic|scoring-opic|urlnormalizer-(pass|regex|basic)&lt;/value&gt;\n&lt;/property&gt;\n</code></pre>\n\n<p>iii) Also have defined the auth configuration file in nutch-site.xml.</p>\n\n<pre><code>&lt;property&gt;\n&lt;name&gt;http.auth.file&lt;/name&gt;\n&lt;value&gt;httpclient-auth.xml&lt;/value&gt;\n&lt;description&gt;Authentication configuration file for 'protocol-httpclient' plugin.\n&lt;/description&gt;\n</code></pre>\n\n<p></p>\n\n<p>I'm not able to crawl it and getting no error..</p>\n\n<p>Requirements: I want to crawl websites like gmail.com or yahoomail.com or anything which asks for authentication.</p>\n\n<p>Where am i going wrong, am i choosing wrong websites for crawling</p>\n\n<p>( if yes please provide me the websites which asks for authentication I'll register for it)</p>\n\n<p>(if no how can i crawl my gmail or facebook accounts)</p>\n", "creation_date": 1407418407, "score": 0},
{"title": "solr does not index all url&#39;s crawled by nutch", "view_count": 244, "is_answered": true, "answers": [{"question_id": 25021025, "owner": {"user_id": 3794838, "link": "http://stackoverflow.com/users/3794838/deepa", "user_type": "registered", "reputation": 18}, "body": "<p>As the above mentioned url's are redirecting urls's I have added the property of \nhttp.redirect.max to -1. I have made sure the redirected url's are not being filtered by the regex-urlfilters.txt. It work's now.</p>\n", "creation_date": 1407368758, "is_accepted": false, "score": 1, "last_activity_date": 1407368758, "answer_id": 25172096}], "question_id": 25021025, "tags": ["solr", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/25021025/solr-does-not-index-all-urls-crawled-by-nutch", "last_activity_date": 1407368758, "owner": {"user_id": 3794838, "view_count": 5, "answer_count": 1, "creation_date": 1404235706, "reputation": 18}, "body": "<p>I am using nutch2.2.1 to crawl Eventbrite.com, but not all url's I received from nutch are not being indexed by solr. \nEg:<code>http://www.eventbrite.com/e/10335408485?aff=es2&amp;rank=1</code>\n<code>http://www.eventbrite.com/e/11176375841?aff=es2&amp;rank=6</code>\n <code>http://www.eventbrite.com/e/11276808237?aff=es2&amp;rank=4</code>\n <code>http://www.eventbrite.com/e/11454156691?aff=es2&amp;rank=5</code>\n <code>http://www.eventbrite.com/e/11595013999?aff=es2&amp;rank=8</code>\n <code>http://www.eventbrite.com/e/11898132635?aff=es2&amp;rank=9</code></p>\n\n<p>I have received these url, when I crawl , but I am not seeing them in solr admin page. I  have modified the nutch-site.xml file to allow the special characters in url. As I am storing the content of the url's in hbase. I could see that html content of the other urls are being retrieved. But not from these pages. Do I need to modify any other configuration files?</p>\n", "creation_date": 1406653443, "score": 1},
{"title": "how to crawl a website by specifying depth", "view_count": 2116, "owner": {"user_id": 3832389, "answer_count": 11, "creation_date": 1405173693, "accept_rate": 45, "view_count": 63, "reputation": 339}, "is_answered": true, "answers": [{"last_edit_date": 1407223462, "owner": {"user_id": 2086509, "accept_rate": 73, "link": "http://stackoverflow.com/users/2086509/jayesh-bhoyar", "user_type": "registered", "reputation": 594}, "body": "<p>My question is what do you want to do by crawling the page and not indexing it in SOLR?</p>\n\n<p>Answer to your question:</p>\n\n<p>If you want to use Nutch Crawler and you don want to index it into SOLR then remove following piece of code from crawl script.</p>\n\n<p><a href=\"http://technical-fundas.blogspot.com/2014/07/crawl-your-website-using-nutch-crawler.html\" rel=\"nofollow\">http://technical-fundas.blogspot.com/2014/07/crawl-your-website-using-nutch-crawler.html</a></p>\n\n<p>Answer to you other question:</p>\n\n<p>How to get the HTML content for all the links that has been crawled by Nutch(check this link):</p>\n\n<p><a href=\"http://stackoverflow.com/questions/5123757/how-to-get-the-html-content-from-nutch\">How to get the html content from nutch</a></p>\n\n<p>This will definitely resolve your issue. </p>\n", "question_id": 25073899, "creation_date": 1406889912, "is_accepted": true, "score": 1, "last_activity_date": 1407223462, "answer_id": 25078512}], "question_id": 25073899, "tags": ["solr", "nutch", "web-crawler"], "answer_count": 1, "link": "http://stackoverflow.com/questions/25073899/how-to-crawl-a-website-by-specifying-depth", "last_activity_date": 1407223462, "accepted_answer_id": 25078512, "body": "<p>I am using nutch 2.x. So I am trying to use nutch command with depth option as</p>\n\n<p>$: nutch inject ./urls/seed.txt -depth 5 </p>\n\n<p>after executing this command getting message like </p>\n\n<p>Unrecognized arg -depth</p>\n\n<p>so when i got failed in this i tried to use nutch crawl as</p>\n\n<p>$: nutch crawl ./urls/seed.txt -depth 5</p>\n\n<p>getting error like </p>\n\n<p>Command crawl is deprecated, please use bin/crawl instead</p>\n\n<p>So i tried to use crawl command to crawl urls in seed.txt with the depth option in that case it is asking for solr but i am not using solr</p>\n\n<p>so my question is how to crawl a website by specifying depth</p>\n", "creation_date": 1406873443, "score": 3},
{"title": "Pom.xml Error Inside Apache Nutch Source Code / Eclipse Kepler", "view_count": 737, "is_answered": false, "answers": [{"question_id": 19016914, "owner": {"user_id": 3779978, "link": "http://stackoverflow.com/users/3779978/leopold", "user_type": "registered", "reputation": 53}, "body": "<p>there is a artifact online in <a href=\"http://mvnrepository.com/artifact/javax.jms/jms/1.1\" rel=\"nofollow\">http://mvnrepository.com/artifact/javax.jms/jms/1.1</a> from (Oct 07, 2013). It should work than. </p>\n\n<pre><code>&lt;dependency&gt;\n    &lt;groupId&gt;javax.jms&lt;/groupId&gt;\n    &lt;artifactId&gt;jms&lt;/artifactId&gt;\n    &lt;version&gt;1.1&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre>\n", "creation_date": 1406806366, "is_accepted": false, "score": 0, "last_activity_date": 1406806366, "answer_id": 25057846}], "question_id": 19016914, "tags": ["java", "maven", "pom.xml", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/19016914/pom-xml-error-inside-apache-nutch-source-code-eclipse-kepler", "last_activity_date": 1406806366, "owner": {"user_id": 2762832, "answer_count": 12, "creation_date": 1378760991, "accept_rate": 37, "view_count": 177, "reputation": 339}, "body": "<p>I downloaded Apache Nutch project and everything seems to be fine, except for this one annoying pom.xml error that never seems to go away. I have eclipse kepler and converted the apache nutch project to a maven project and then updated the dependencies. However this error does not seem to go away at all. All inputs/feedbacks are welcome &amp; highly appreciated.</p>\n\n<pre><code>Multiple annotations found at this line:\n        - Missing artifact javax.jms:jms:jar:1.1\n        - ArtifactTransferException: Failure to transfer com.sun.jdmk:jmxtools:jar:1.2.1 from https://maven-\n         repository.dev.java.net/nonav/repository was cached in the local repository, resolution will not be reattempted until \n         the update interval of java.net has elapsed or updates are forced. Original error: Could not transfer artifact \n         com.sun.jdmk:jmxtools:jar:1.2.1 from/to java.net (https://maven-repository.dev.java.net/nonav/repository): No \n         connector available to access repository java.net (https://maven-repository.dev.java.net/nonav/repository) of type \n         legacy using the available factories AsyncRepositoryConnectorFactory, WagonRepositoryConnectorFactory\n        - ArtifactTransferException: Failure to transfer javax.jms:jms:jar:1.1 from https://maven-repository.dev.java.net/\n         nonav/repository was cached in the local repository, resolution will not be reattempted until the update interval of \n         java.net has elapsed or updates are forced. Original error: Could not transfer artifact javax.jms:jms:jar:1.1 from/to \n         java.net (https://maven-repository.dev.java.net/nonav/repository): No connector available to access repository \n         java.net (https://maven-repository.dev.java.net/nonav/repository) of type legacy using the available factories \n         AsyncRepositoryConnectorFactory, WagonRepositoryConnectorFactory\n        - ArtifactTransferException: Failure to transfer com.sun.jmx:jmxri:jar:1.2.1 from https://maven-\n         repository.dev.java.net/nonav/repository was cached in the local repository, resolution will not be reattempted until \n         the update interval of java.net has elapsed or updates are forced. Original error: Could not transfer artifact \n         com.sun.jmx:jmxri:jar:1.2.1 from/to java.net (https://maven-repository.dev.java.net/nonav/repository): No \n         connector available to access repository java.net (https://maven-repository.dev.java.net/nonav/repository) of type \n         legacy using the available factories AsyncRepositoryConnectorFactory, WagonRepositoryConnectorFactory\n        - Missing artifact com.sun.jdmk:jmxtools:jar:1.2.1\n        - Missing artifact com.sun.jmx:jmxri:jar:1.2.1\n</code></pre>\n", "creation_date": 1380152014, "score": 0},
{"title": "nutch Unable to successfully parse content", "view_count": 3572, "is_answered": true, "answers": [{"last_edit_date": 1326796709, "owner": {"user_id": 525590, "accept_rate": 75, "link": "http://stackoverflow.com/users/525590/jpee", "user_type": "registered", "reputation": 111}, "body": "<p>Which plugins have you configured? If you are using tika, then tika has a mapping from mime-type like xhtml/xml to a parser. If there is no entry in the configfile, nothing happens.</p>\n\n<p>You could disable tika and only use the parse-html plugin. </p>\n\n<p>I tested your site with our default plugin config.</p>\n\n<pre><code>protocol-http|urlfilter-regex|parse-(html)|index-(basic|anchor)\n|query-    (basic|site|url)|response-(json|xml)\n|summary-basic|scoring-opic|urlnormalizer-     \n(pass|regex|basic)\n</code></pre>\n\n<p>And got your page parsed.</p>\n\n<pre><code>Parsed (32ms):http://sujitpal.blogspot.com/\n</code></pre>\n\n<p>Grettings\nJPee</p>\n", "question_id": 8784656, "creation_date": 1326795903, "is_accepted": false, "score": 1, "last_activity_date": 1326796709, "answer_id": 8892983}], "question_id": 8784656, "tags": ["solr", "web-crawler", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/8784656/nutch-unable-to-successfully-parse-content", "last_activity_date": 1406521963, "owner": {"user_id": 764618, "answer_count": 6, "creation_date": 1306052225, "accept_rate": 42, "view_count": 41, "reputation": 93}, "body": "<p>I try to crawl using nutch 1.4 , but I'm facing error in parsing, this is the log file:</p>\n\n<pre><code>2012-01-09 09:12:02,696 INFO  parse.ParseSegment - ParseSegment: starting at          2012-01-09 09:12:02 \n2012-01-09 09:12:02,697 INFO  parse.ParseSegment - ParseSegment: segment: crawl/segments/20120109091153\n2012-01-09 09:12:03,416 WARN  parse.ParseUtil - Unable to successfully parse content http://sujitpal.blogspot.com/ of type application/xhtml+xml\n2012-01-09 09:12:03,417 INFO  parse.ParseSegment - Parsing: http:// sujitpal.blogspot.com/\n2012-01-09 09:12:03,418 WARN  parse.ParseSegment - Error parsing: http://sujitpal.blogspot.com/: failed(2,200): org.apache.nutch.parse.ParseException: Unable to successfully parse content\n2012-01-09 09:12:03,419 INFO  crawl.SignatureFactory - Using Signature impl: org.apache.nutch.crawl.MD5Signature\n</code></pre>\n\n<p>by checking config/nutch-site.xml I found html|text|xhtml|xml are included in the plugin.includes preperty</p>\n\n<pre><code>&lt;property&gt;\n&lt;name&gt;plugin.includes&lt;/name&gt;\n&lt;value&gt;myplugins|protocol-httpclient|query-(basic|site|url)|summary- \nbasic|urlfilter-    \nregex|parse-(xml|xhtml|html|tika|text|js)|index-(basic|anchor)|scoring-  \nopic|urlnormalizer-(pass|regex|basic)|query-(basic|site|url)|response-(json|xml)\n&lt;/value&gt;\n&lt;description&gt;Regular expression naming plugin directory names to\ninclude.  Any plugin not matching this expression is excluded.\nIn any case you need at least include the nutch-extensionpoints plugin. By\ndefault Nutch includes crawling just HTML and plain text via HTTP,\nand basic indexing and search plugins. In order to use HTTPS please enable \nprotocol-httpclient, but be aware of possible intermittent problems with the \nunderlying commons-httpclient library.\n&lt;/description&gt;\n&lt;/property&gt;\n</code></pre>\n\n<p>Why can't it parse xhtml/xml or even text/xml?</p>\n", "creation_date": 1326090419, "score": 0},
{"title": "Could not change Nutch http proxy scheme to basic", "view_count": 288, "is_answered": false, "answers": [{"question_id": 24902307, "owner": {"user_id": 2086509, "accept_rate": 73, "link": "http://stackoverflow.com/users/2086509/jayesh-bhoyar", "user_type": "registered", "reputation": 594}, "body": "<p>This is because you have not selected the proper scheme while configuring the httpclient-auth.xml file.</p>\n\n<p>Here is the configuration that I have used and is working perfectly fine for me.</p>\n\n<pre><code>&lt;auth-configuration&gt;\n        &lt;credentials username=\"&lt;username&gt;\" password=\"&lt;password&gt;\"&gt;\n                 &lt;authscope host=\"&lt;hostname&gt;\" port=\"80\" scheme=\"basic\" realm=\"&lt;realm_value&gt;\"/&gt;\n        &lt;/credentials&gt;\n&lt;/auth-configuration&gt;\n</code></pre>\n\n<p>Let me know how it goes at your end after trying this solution...</p>\n", "creation_date": 1406209084, "is_accepted": false, "score": 0, "last_activity_date": 1406209084, "answer_id": 24935335}], "question_id": 24902307, "tags": ["proxy", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/24902307/could-not-change-nutch-http-proxy-scheme-to-basic", "last_activity_date": 1406209084, "owner": {"user_id": 3835707, "view_count": 1, "answer_count": 1, "creation_date": 1405317647, "reputation": 11}, "body": "<p>No matter what I typed, it always goes to NTLM. I want my scheme as basic.\nWith my proxy, id and pwd, curl works.\nHowever Nutch does not work.\nI think it could be the matter of encoding(capital letter, etc.), so want to figure it out.\nHowever, as it always sets scheme as NTLM, it hides id and pwd in log.</p>\n\n<pre><code>---httpclient-auth.xml---\n&lt;auth-configuration&gt;\n        &lt;credentials username=\"id\" password=\"pw\"&gt;\n                &lt;default/&gt;\n        &lt;/credentials&gt;\n&lt;/auth-configuration&gt;\n\n---nutch-site.xml---\n&lt;?xml version=\"1.0\"?&gt;\n&lt;?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?&gt;\n\n&lt;!-- Put site-specific property overrides in this file. --&gt;\n\n&lt;configuration&gt;\n    &lt;property&gt;\n        &lt;name&gt;http.agent.name&lt;/name&gt;\n        &lt;value&gt;My Nutch Spider&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;property&gt;\n        &lt;name&gt;http.robots.agents&lt;/name&gt;\n        &lt;value&gt;My Nutch Spider,*&lt;/value&gt;\n    &lt;/property&gt;\n\n        &lt;property&gt;\n                &lt;name&gt;plugin.includes&lt;/name&gt;\n                &lt;value&gt;protocol-httpclient|urlfilter-regex|parse-(html|tika)|index-(basic|anchor)|scoring-opic|urlnormalizer-(pass|regex|basic)&lt;/value&gt;\n        &lt;/property&gt;\n\n    &lt;property&gt;\n        &lt;name&gt;http.proxy.host&lt;/name&gt;\n        &lt;value&gt;proxyhost&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;property&gt;\n        &lt;name&gt;http.proxy.port&lt;/name&gt;\n        &lt;value&gt;9000&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;property&gt;\n        &lt;name&gt;http.proxy.username&lt;/name&gt;\n        &lt;value&gt;id&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;property&gt;\n        &lt;name&gt;http.proxy.password&lt;/name&gt;\n        &lt;value&gt;pw&lt;/value&gt;\n    &lt;/property&gt;\n&lt;/configuration&gt;\n\n---hadoop.log---\nINFO  httpclient.Http - http.proxy.host = proxyhost\nINFO  httpclient.Http - http.proxy.port = 9000\nINFO  httpclient.Http - http.timeout = 10000\nINFO  httpclient.Http - http.content.limit = 65536\nINFO  httpclient.Http - http.agent = My Nutch Spider/Nutch-1.4\nINFO  httpclient.Http - http.accept.language = en-us,en-    gb,en;q=0.7,*;q=0.3\nINFO  auth.AuthChallengeProcessor - ntlm authentication scheme     selected\nINFO  httpclient.HttpMethodDirector - Failure authenticating     with NTLM &lt;any realm&gt;@proxyhost:9000\nINFO  auth.AuthChallengeProcessor - ntlm authentication scheme     selected\nINFO  fetcher.Fetcher - -activeThreads=1, spinWaiting=0,     fetchQueues.totalSize=0\nINFO  httpclient.HttpMethodDirector - Failure authenticating     with NTLM &lt;any realm&gt;@proxyhost:9000\nINFO  fetcher.Fetcher - fetch of http://url/ failed     with: Http code=407, url=http://url/\n</code></pre>\n", "creation_date": 1406092147, "score": 0},
{"title": "Disk Space getting filled up due to jobcache in tmp directory of nutch linux instance", "view_count": 653, "is_answered": true, "answers": [{"question_id": 24925361, "owner": {"user_id": 2046453, "accept_rate": 58, "link": "http://stackoverflow.com/users/2046453/iqstatic", "user_type": "registered", "reputation": 1404}, "body": "<p>The directory name you mentioned is <code>/tmp/hadoop-root/mapred/local/taskTracker/root/jobcache/</code>.\nThis directory is used by the TaskTracker (slave) daemons to localize job files when the tasks\nare run on the slaves. When a job completes, the directories under the jobCache must get automatically cleaned up.\nThis email chain <a href=\"http://mail-archives.apache.org/mod_mbox/hadoop-user/201301.mbox/%3C26850_1357828735_0MGE0023YZCTOO30_99DD75DC8938B743BBBC2CA54F7224A706D2E1AF@NYSGMBXB06.a.wcmc-ad.net%3E\" rel=\"nofollow\">http://mail-archives.apache.org/mod_mbox/hadoop-user/201301.mbox/%3C26850_1357828735_0MGE0023YZCTOO30_99DD75DC8938B743BBBC2CA54F7224A706D2E1AF@NYSGMBXB06.a.wcmc-ad.net%3E</a> discussed a similar problem.</p>\n", "creation_date": 1406193342, "is_accepted": false, "score": 1, "last_activity_date": 1406193342, "answer_id": 24929693}], "question_id": 24925361, "tags": ["linux", "hadoop", "solr", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/24925361/disk-space-getting-filled-up-due-to-jobcache-in-tmp-directory-of-nutch-linux-ins", "last_activity_date": 1406193342, "owner": {"user_id": 2197873, "view_count": 3, "answer_count": 1, "creation_date": 1363928555, "reputation": 6}, "body": "<p>I am a newbie. We have setup solr environment and we see that in nutch we are facing an issue. Disk space is being 100% utilized. When we debug it we see that the jobcache in the below location is utilizing more space (70% appx.).</p>\n\n<p>\"/tmp/hadoop-root/mapred/local/taskTracker/root/jobcache/\".</p>\n\n<p>I have searched many forums to understand what exactly does this jobcache folder contains.</p>\n\n<p>Can anyone help me in understanding what does this jobcache folder contains and how can I restrict this tmp folder to not to utilize the space.</p>\n\n<p>What effect will it have if I remove the jobcache folder and again create it by using mkdir command?</p>\n\n<p>Thanks in advance.</p>\n", "creation_date": 1406177545, "score": 1},
{"title": "Nutch says No URLs to fetch - check your seed list and URL filters", "view_count": 864, "is_answered": true, "answers": [{"last_edit_date": 1406040986, "owner": {"user_id": 1953475, "accept_rate": 72, "link": "http://stackoverflow.com/users/1953475/b-mr-w", "user_type": "registered", "reputation": 5595}, "body": "<p>I was looking at your regex-filter and I spotted a few glitches that you might think about giving a try. Since it won't fit well into the comment, I will post it here anyway even it might not be the complete answer. </p>\n\n<ol>\n<li>Your customized regular expression <code>+^http://([a-z0-9\\-A-Z]*\\.)*nutch.apache.org/([a-z0-9\\-A-Z]*\\/)*</code> might be the problem. Nutch's regex-urlfilter sometimes can get really confusing and I would highly recommend you start with something that works for everyone, maybe <code>+^http://([a-z0-9]*\\.)*nutch.apache.org/</code> from <a href=\"http://wiki.apache.org/nutch/NutchTutorial\" rel=\"nofollow\">Wiki</a> just to get started. </li>\n<li>After the two steps above, and you are sure Nutch is working, then you can tweak the regex. </li>\n</ol>\n\n<p>To test the regex, I found two ways to do it:</p>\n\n<ol>\n<li>feed a list of URLs to be the seed. And inject them to a new database and see who has been injected or rejected. This doesn't really any coding.</li>\n<li>You can set up <a href=\"http://wiki.apache.org/nutch/RunNutchInEclipse\" rel=\"nofollow\">Nutch in Eclipse</a> and call the corresponding class to test it. </li>\n</ol>\n\n<p><img src=\"http://i.stack.imgur.com/W3o3J.jpg\" alt=\"enter image description here\"></p>\n", "question_id": 24816472, "creation_date": 1405806915, "is_accepted": false, "score": 1, "last_activity_date": 1406040986, "answer_id": 24845354}], "question_id": 24816472, "tags": ["nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/24816472/nutch-says-no-urls-to-fetch-check-your-seed-list-and-url-filters", "last_activity_date": 1406040986, "owner": {"user_id": 3835707, "view_count": 1, "answer_count": 1, "creation_date": 1405317647, "reputation": 11}, "body": "<p>~/runtime/local/bin/urls/seed.txt >></p>\n\n<pre><code>http://nutch.apache.org/\n</code></pre>\n\n<p>~/runtime/local/conf/nutch-site.xml >></p>\n\n<pre><code>&lt;?xml version=\"1.0\"?&gt;\n&lt;?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?&gt;\n\n&lt;!-- Put site-specific property overrides in this file. --&gt;\n\n&lt;configuration&gt;\n    &lt;property&gt;\n            &lt;name&gt;http.agent.name&lt;/name&gt;\n            &lt;value&gt;My Nutch Spider&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;property&gt;\n            &lt;name&gt;http.timeout&lt;/name&gt;\n            &lt;value&gt;99999999&lt;/value&gt;\n            &lt;description&gt;&lt;/description&gt;\n    &lt;/property&gt;\n\n    &lt;property&gt;\n            &lt;value&gt;protocol-file|protocol-httpclient|urlfilter-regex|parse-(html|tika)|index-(basic|anchor)|\n            scoring-opic|urlnormalizer-(pass|regex|basic)|index-more\n            &lt;/value&gt;\n            &lt;description&gt;Regular expression naming plugin directory names to\n            include.  Any plugin not matching this expression is excluded.\n            In any case you need at least include the nutch-extensionpoints plugin.\n            &lt;/description&gt;\n    &lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre>\n\n<p>~/runtime/local/conf/regex-urlfilter.txt >></p>\n\n<pre><code># skip http: ftp: and mailto: urls\n-^(http|ftp|mailto):\n\n# skip image and other suffixes we can't yet parse\n# for a more extensive coverage use the urlfilter-suffix plugin\n-\\.(gif|GIF|jpg|JPG|png|PNG|ico|ICO|css|CSS|sit|SIT|eps|EPS|wmf|WMF|zip|ZIP|ppt|PPT|mpg|MPG|xls|XLS|gz|GZ|rpm|RPM|tgz|TGZ|mov|MOV|exe|EXE|jpeg|JPEG|bmp|BMP|js|JS)$\n\n# skip URLs containing certain characters as probable queries, etc.\n-[?*!@=]\n\n# skip URLs with slash-delimited segment that repeats 3+ times, to break loops\n-.*(/[^/]+)/[^/]+\\1/[^/]+\\1/\n\n# accept anything else\n+^http://([a-z0-9\\-A-Z]*\\.)*nutch.apache.org/([a-z0-9\\-A-Z]*\\/)*\n</code></pre>\n\n<p>If I crawl, it says like this.</p>\n\n<pre><code>/home/apache-nutch-1.4-bin/runtime/local/bin\n$ ./nutch crawl urls -dir newCrawl/ -depth 3 -topN 3\ncygpath: can't convert empty path\nsolrUrl is not set, indexing will be skipped...\ncrawl started in: newCrawl\nrootUrlDir = urls\nthreads = 10\ndepth = 3\nsolrUrl=null\ntopN = 3\nInjector: starting at 2014-07-18 11:35:36\nInjector: crawlDb: newCrawl/crawldb\nInjector: urlDir: urls\nInjector: Converting injected urls to crawl db entries.\nInjector: Merging injected urls into crawl db.\nInjector: finished at 2014-07-18 11:35:39, elapsed: 00:00:02\nGenerator: starting at 2014-07-18 11:35:39\nGenerator: Selecting best-scoring urls due for fetch.\nGenerator: filtering: true\nGenerator: normalizing: true\nGenerator: topN: 3\nGenerator: jobtracker is 'local', generating exactly one partition.\nGenerator: 0 records selected for fetching, exiting ...\nStopping at depth=0 - no more URLs to fetch.\nNo URLs to fetch - check your seed list and URL filters.\ncrawl finished: newCrawl\n</code></pre>\n\n<p>No matter what the web adresses are, it always says no urls to fetch.\nI am struggling with this problem for 3 days. Please Help!!!!</p>\n", "creation_date": 1405651117, "score": 1},
{"title": "Working with apache nutch 2.2.1", "view_count": 302, "is_answered": false, "answers": [{"question_id": 24824844, "owner": {"user_id": 582789, "link": "http://stackoverflow.com/users/582789/alfonso-nishikawa", "user_type": "registered", "reputation": 1263}, "body": "<p>Thomas.</p>\n\n<p>What version of Gora are you using? SqlStore was only enabled on Gora 0.1, and now is a missing feature to be implemented. Sorry for the bad news :( I think it the sql module was deleted because of some license issues... I think.</p>\n\n<p>Sorry for the bad news :( some day the module surely will be back.</p>\n", "creation_date": 1405701730, "is_accepted": false, "score": 0, "last_activity_date": 1405701730, "answer_id": 24830007}], "question_id": 24824844, "tags": ["java", "apache", "nutch", "web-crawler", "gora"], "answer_count": 1, "link": "http://stackoverflow.com/questions/24824844/working-with-apache-nutch-2-2-1", "last_activity_date": 1405701730, "owner": {"user_id": 3852917, "view_count": 0, "answer_count": 0, "creation_date": 1405685535, "reputation": 6}, "body": "<p>I'm trying to start my first crawl, I've already configured the database settings and I execute the following command:  <code>bin/nutch inject urls</code></p>\n\n<p>And the bug result is the following:</p>\n\n<pre><code>InjectorJob: starting at 2014-07-18 08:13:34\nInjectorJob: Injecting urlDir: urls\nInjectorJob: Using class org.apache.gora.sql.store.SqlStore as the Gora storage class.\nInjectorJob: java.lang.RuntimeException: job failed: name=inject urls, jobid=job_local1172062909_0001\n        at org.apache.nutch.util.NutchJob.waitForCompletion(NutchJob.java:54)\n        at org.apache.nutch.crawl.InjectorJob.run(InjectorJob.java:233)\n        at org.apache.nutch.crawl.InjectorJob.inject(InjectorJob.java:251)\n        at org.apache.nutch.crawl.InjectorJob.run(InjectorJob.java:273)\n        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n        at org.apache.nutch.crawl.InjectorJob.main(InjectorJob.java:282)\n</code></pre>\n\n<p>Can someone help me?</p>\n", "creation_date": 1405685849, "score": 1},
{"title": "Error when crawling with Nutch - Input path does not exist: hdfs://.../urls/seed.txt", "view_count": 490, "is_answered": false, "question_id": 24789447, "tags": ["hadoop", "nutch", "emr", "web-crawler"], "answer_count": 0, "link": "http://stackoverflow.com/questions/24789447/error-when-crawling-with-nutch-input-path-does-not-exist-hdfs-urls-seed", "last_activity_date": 1405654206, "owner": {"age": 34, "answer_count": 5, "creation_date": 1381086063, "user_id": 2852435, "view_count": 10, "location": "United States", "reputation": 35}, "body": "<p>I have installed Apache Nutch and running a crawl using:</p>\n\n<pre><code>bin/crawl ./urls/seed.txt crawl http://localhost:8983/solr/ 5\n</code></pre>\n\n<p>Works fine from runtime/local. When I run that same command from runtime/deploy I get:</p>\n\n<pre><code>14/07/16 19:43:35 INFO crawl.InjectorJob: InjectorJob: starting at 2014-07-16 19:43:35\n14/07/16 19:43:35 INFO crawl.InjectorJob: InjectorJob: Injecting urlDir: urls/seed.txt\n14/07/16 19:43:37 INFO connection.CassandraHostRetryService: Downed Host Retry service started with queue size -1 and retry delay 10s\n14/07/16 19:43:37 INFO service.JmxMonitor: Registering JMX me.prettyprint.cassandra.service_Test Cluster:ServiceType=hector,MonitorType=hector\n14/07/16 19:43:37 INFO crawl.InjectorJob: InjectorJob: Using class org.apache.gora.cassandra.store.CassandraStore as the Gora storage class.\n14/07/16 19:43:37 INFO mapred.JobClient: Default number of map tasks: null\n14/07/16 19:43:37 INFO mapred.JobClient: Setting default number of map tasks based on cluster size to : 12\n14/07/16 19:43:37 INFO mapred.JobClient: Default number of reduce tasks: 0\n14/07/16 19:43:38 INFO security.ShellBasedUnixGroupsMapping: add hadoop to shell userGroupsCache\n14/07/16 19:43:38 INFO mapred.JobClient: Setting group to hadoop\n14/07/16 19:43:39 INFO mapred.JobClient: Cleaning up the staging area hdfs://172.31.13.61:9000/mnt/var/lib/hadoop/tmp/mapred/staging/hadoop/.staging/job_201407161337_0024\n14/07/16 19:43:39 ERROR security.UserGroupInformation: PriviledgedActionException as:hadoop cause:org.apache.hadoop.mapreduce.lib.input.InvalidInputException: Input path does not exist: hdfs://172.31.13.61:9000/user/hadoop/urls/seed.txt\n14/07/16 19:43:39 ERROR crawl.InjectorJob: InjectorJob: org.apache.hadoop.mapreduce.lib.input.InvalidInputException: Input path does not exist: hdfs://172.31.13.61:9000/user/hadoop/urls/seed.txt\n    at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.listStatus(FileInputFormat.java:235)\n    at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.getSplits(FileInputFormat.java:252)\n    at org.apache.hadoop.mapred.JobClient.writeNewSplits(JobClient.java:1016)\n    at org.apache.hadoop.mapred.JobClient.writeSplits(JobClient.java:1033)\n    at org.apache.hadoop.mapred.JobClient.access$700(JobClient.java:174)\n    at org.apache.hadoop.mapred.JobClient$2.run(JobClient.java:951)\n    at org.apache.hadoop.mapred.JobClient$2.run(JobClient.java:904)\n    at java.security.AccessController.doPrivileged(Native Method)\n    at javax.security.auth.Subject.doAs(Subject.java:415)\n    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1140)\n    at org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:904)\n    at org.apache.hadoop.mapreduce.Job.submit(Job.java:501)\n    at org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:531)\n    at org.apache.nutch.util.NutchJob.waitForCompletion(NutchJob.java:50)\n    at org.apache.nutch.crawl.InjectorJob.run(InjectorJob.java:233)\n    at org.apache.nutch.crawl.InjectorJob.inject(InjectorJob.java:251)\n    at org.apache.nutch.crawl.InjectorJob.run(InjectorJob.java:273)\n    at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n    at org.apache.nutch.crawl.InjectorJob.main(InjectorJob.java:282)\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n    at java.lang.reflect.Method.invoke(Method.java:606)\n    at org.apache.hadoop.util.RunJar.main(RunJar.java:187)\n</code></pre>\n\n<p>It is not locating the file seed.txt and yes it does exist in $HOME/urls/seed.txt. I am using AWS EMR and Cassandra. Any help would be greatly appreciated.</p>\n", "creation_date": 1405540308, "score": 1},
{"title": "Nutch - Getting Error: JAVA_HOME is not set. when trying to crawl", "view_count": 771, "is_answered": false, "answers": [{"last_edit_date": 1405536355, "owner": {"user_id": 2852435, "link": "http://stackoverflow.com/users/2852435/clay-allen", "user_type": "registered", "reputation": 35}, "body": "<p>The problem is I was running</p>\n\n<pre><code>sudo bin/crawl crawl urls -dir crawl -depth 3 -topN 5\n</code></pre>\n\n<p>I used</p>\n\n<pre><code>bin/crawl ./urls/seed.txt TestCrawl http://localhost:8983/solr/ 5\n</code></pre>\n\n<p>And all is well, just a malformed command.. i.e. 'crawl' is deprecated as stated here: <a href=\"http://wiki.apache.org/nutch/NutchTutorial\" rel=\"nofollow\">Apache Nutch Tutorial</a></p>\n", "question_id": 24771239, "creation_date": 1405532793, "is_accepted": false, "score": 0, "last_activity_date": 1405536355, "answer_id": 24787390}], "question_id": 24771239, "tags": ["java", "hadoop", "cassandra", "nutch", "emr"], "answer_count": 1, "link": "http://stackoverflow.com/questions/24771239/nutch-getting-error-java-home-is-not-set-when-trying-to-crawl", "last_activity_date": 1405536355, "owner": {"age": 34, "answer_count": 5, "creation_date": 1381086063, "user_id": 2852435, "view_count": 10, "location": "United States", "reputation": 35}, "body": "<p>First and foremost I'm a Nutch/Hadoop newbie. I have installed Cassandra. I have installed Nutch on the Master node of my EMR cluster. When I attempt to execute a crawl using the following command:</p>\n\n<pre><code>sudo bin/crawl crawl urls -dir crawl -depth 3 -topN 5\n</code></pre>\n\n<p>I get </p>\n\n<pre><code>Error: JAVA_HOME is not set.\n</code></pre>\n\n<p>If I run the command without 'sudo' I get:</p>\n\n<pre><code>    Injector: starting at 2014-07-16 02:12:24\nInjector: crawlDb: urls/crawldb\nInjector: urlDir: crawl\nInjector: Converting injected urls to crawl db entries.\nInjector: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/home/hadoop/apache-nutch-1.8/runtime/local/crawl\n    at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:197)\n    at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:208)\n    at org.apache.hadoop.mapred.JobClient.writeOldSplits(JobClient.java:1081)\n    at org.apache.hadoop.mapred.JobClient.writeSplits(JobClient.java:1073)\n    at org.apache.hadoop.mapred.JobClient.access$700(JobClient.java:179)\n    at org.apache.hadoop.mapred.JobClient$2.run(JobClient.java:983)\n    at org.apache.hadoop.mapred.JobClient$2.run(JobClient.java:936)\n    at java.security.AccessController.doPrivileged(Native Method)\n    at javax.security.auth.Subject.doAs(Subject.java:415)\n    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)\n    at org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:936)\n    at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:910)\n    at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:1353)\n    at org.apache.nutch.crawl.Injector.inject(Injector.java:279)\n    at org.apache.nutch.crawl.Injector.run(Injector.java:316)\n    at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n    at org.apache.nutch.crawl.Injector.main(Injector.java:306)\n</code></pre>\n\n<p>I cannot figure this out. I've seen the other forum here: <a href=\"http://stackoverflow.com/questions/17374028/cant-get-apache-nutch-to-crawl-permissions-and-java-home-suspected\">Similar Topic</a></p>\n\n<p>and followed it to no avail. I have added </p>\n\n<pre><code>export JAVA_HOME=/usr/lib/jvm/java-7-oracle\n</code></pre>\n\n<p>and</p>\n\n<pre><code>export PATH=$PATH:${JAVA_HOME}/bin\n</code></pre>\n\n<p>to my ~/.bashrc and I am using Linux..</p>\n\n<p>Any help will be appreciated!!</p>\n", "creation_date": 1405477558, "score": 0},
{"title": "Solr indexing failed", "view_count": 1967, "owner": {"age": 27, "answer_count": 6, "creation_date": 1360846715, "user_id": 2072104, "view_count": 45, "location": "Minsk, Belarus", "reputation": 391}, "is_answered": true, "answers": [{"last_edit_date": 1362217646, "owner": {"user_id": 2072104, "link": "http://stackoverflow.com/users/2072104/slawter", "user_type": "registered", "reputation": 391}, "body": "<p>When you got any exception, you should open logs and check that exceptions in log. In my case I modify <code>schema.xml</code> and insert some new fields with <code>type=\"text\"</code>, but in my <code>schema.xml</code> was another type which called <code>text_general</code>, it was easy fix after logs reading.</p>\n", "question_id": 14875521, "creation_date": 1361108358, "is_accepted": true, "score": 1, "last_activity_date": 1362217646, "answer_id": 14921675}], "question_id": 14875521, "tags": ["solr", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/14875521/solr-indexing-failed", "last_activity_date": 1405535562, "accepted_answer_id": 14921675, "body": "<p>I did all like in <a href=\"http://wiki.apache.org/nutch/NutchTutorial#A4._Setup_Solr_for_search\" rel=\"nofollow\">this tutorial</a>, but there is some truble. When I try call <code>./nutch solrindex http://127.0.0.1:8080/solr/ crawl/crawldb -linkdb crawl/linkdb crawl/segments/*</code> command after nutch crawling I get next exception</p>\n\n<pre><code>&gt; WARNING: job_local_0001 org.apache.solr.common.SolrException: Bad\n&gt; Request\n&gt; \n&gt; Bad Request\n&gt; \n&gt; request: http://127.0.0.1:8080/solr/update?wt=javabin&amp;version=2\n&gt;         at org.apache.solr.client.solrj.impl.CommonsHttpSolrServer.request(CommonsHttpSolrServer.java:430)\n&gt;         at org.apache.solr.client.solrj.impl.CommonsHttpSolrServer.request(CommonsHttpSolrServer.java:244)\n&gt;         at org.apache.solr.client.solrj.request.AbstractUpdateRequest.process(AbstractUpdateRequest.java:105)\n&gt;         at org.apache.nutch.indexer.solr.SolrWriter.close(SolrWriter.java:142)\n&gt;         at org.apache.nutch.indexer.IndexerOutputFormat$1.close(IndexerOutputFormat.java:48)\n&gt;         at org.apache.hadoop.mapred.ReduceTask$OldTrackingRecordWriter.close(ReduceTask.java:466)\n&gt;         at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:530)\n&gt;         at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:420)\n&gt;         at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:260)\n</code></pre>\n\n<p>What should I do to resolve this problem?</p>\n\n<p>P.s. Solr is working. I used solr 4.1 and nutch 1.6.</p>\n", "creation_date": 1360847097, "score": 2},
{"title": "How to lookup HBase REST API (Stargate) if the row-keys are reversed urls", "view_count": 2520, "owner": {"user_id": 3836275, "view_count": 0, "answer_count": 1, "creation_date": 1405329895, "reputation": 16}, "is_answered": true, "answers": [{"question_id": 24734045, "owner": {"user_id": 3836275, "link": "http://stackoverflow.com/users/3836275/codermd", "user_type": "registered", "reputation": 16}, "body": "<p>Needed to URL encode the forward slash as well. The following works.</p>\n\n<pre><code>curl http://localhost:8900/webpage/com.usatoday.www%3Ahttp%2F\n</code></pre>\n", "creation_date": 1405515383, "is_accepted": true, "score": 1, "last_activity_date": 1405515383, "answer_id": 24781391}], "question_id": 24734045, "tags": ["rest", "hadoop", "hbase", "nutch", "stargate"], "answer_count": 1, "link": "http://stackoverflow.com/questions/24734045/how-to-lookup-hbase-rest-api-stargate-if-the-row-keys-are-reversed-urls", "last_activity_date": 1405515383, "accepted_answer_id": 24781391, "body": "<p>I am using nutch2.2.1 + hbase0.90.4, and wanting to access the data via the <a href=\"https://wiki.apache.org/hadoop/Hbase/Stargate\" rel=\"nofollow\">HBase REST API Stargate</a>. If I seed nutch with a url (eg. www.usatoday.com), the reversed url becomes the HBase row-key in the designated table ('webpage'). I can lookup the data via the hbase shell as follows: </p>\n\n<pre><code>hbase(main):001:0&gt; get 'webpage', 'com.usatoday.www:http/'\nCOLUMN                                         CELL                                                                                                                                 \n f:fi                                          timestamp=1404762373394,value=\\x00'\\x8D\\x00                                                                                         \n f:ts                                          timestamp=1404762373394, value=\\x00\\x00\\x01G\\x12\\\\xB5\\xB3                                                                            \n mk:_injmrk_                                   timestamp=1404762373394, value=y                                                                                                     \n mk:dist                                       timestamp=1404762373394, value=0                                                                                                     \n mtdt:_csh_                                    timestamp=1404762373394, value=?\\x80\\x00\\x00                                                                                         \n s:s                                           timestamp=1404762373394, value=?\\x80\\x00\\x00       \n</code></pre>\n\n<p>However, I am having trouble using the REST API. Presumably I need to do some pretty simple URL encoding to suppress the colon before 'http' that is making trouble for me?</p>\n\n<p>For eg., I get a HTTP 404 when I try</p>\n\n<pre><code>curl http://localhost:8900/webpage/com.usatoday.www:http/\n</code></pre>\n\n<p>also when I try</p>\n\n<pre><code>curl http://localhost:8900/webpage/com.usatoday.www%3Ahttp/\n</code></pre>\n\n<p>I know that the REST API is working fine as I can create a row called 'row3' into a table called 'test' and lookup</p>\n\n<pre><code>curl http://localhost:8900/test/row3  \n</code></pre>\n\n<p>to see the following expected result: </p>\n\n<pre><code>&lt;?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"yes\"?&gt;&lt;CellSet&gt;&lt;Row key=\"cm93Mw==\"&gt;&lt;Cell timestamp=\"1404761922130\" column=\"Y2Y6Yw==\"&gt;dGhpcyBpcyBzb3J0YSB3b3JraW5nIG5vdw==&lt;/Cell&gt;&lt;/Row&gt;&lt;/CellSet&gt;\n</code></pre>\n\n<p>Thanks for the help!</p>\n", "creation_date": 1405332329, "score": 1},
{"title": "Solr - cannot get stemming and elevation to work together", "view_count": 27, "is_answered": false, "answers": [{"question_id": 24775089, "owner": {"user_id": 907642, "link": "http://stackoverflow.com/users/907642/okke-klein", "user_type": "registered", "reputation": 2221}, "body": "<p>If you want elevation of documents with tokens that have been analyzed you should use the same field for both </p>\n\n<pre><code>&lt;searchComponent name=\"elevator\" class=\"solr.QueryElevationComponent\"&gt;\n&lt;!-- pick a fieldType to analyze queries --&gt;\n&lt;str name=\"queryFieldType\"&gt;text&lt;/str&gt;\n&lt;str name=\"config-file\"&gt;elevate.xml&lt;/str&gt;\n&lt;/searchComponent&gt;\n</code></pre>\n", "creation_date": 1405512815, "is_accepted": false, "score": 0, "last_activity_date": 1405512815, "answer_id": 24780523}], "question_id": 24775089, "tags": ["xml", "solr", "xsd", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/24775089/solr-cannot-get-stemming-and-elevation-to-work-together", "last_activity_date": 1405512815, "owner": {"age": 39, "answer_count": 1, "creation_date": 1382707008, "user_id": 2920135, "view_count": 1, "location": "Worcestershire, United Kingdom", "reputation": 13}, "body": "<p>Solr 4.6 running on CentOS 6.5, run by a Solr newbie (me!). We use Nutch to crawl our sites, then pass the info to Solr.</p>\n\n<p>Elevate was working fine until we added the stemming feature.</p>\n\n<ul>\n<li>Our Solr config file: <a href=\"http://www.benbrown.org/solrconfig.xml\" rel=\"nofollow\">solrconfig.xml</a></li>\n<li>Our Solr schema file: <a href=\"http://www.benbrown.org/schema.xml\" rel=\"nofollow\">schema.xml</a></li>\n<li><p>Our search string is:</p>\n\n<p>[solrserver]/elevate?q=[search term]&amp;fl=content%2Ctitle%2Curl&amp;wt=json&amp;indent=true&amp;defType=edismax&amp;qf=content%2Ctitle&amp;stopwords=true&amp;lowercaseOperators=true</p></li>\n</ul>\n\n<p>What am I doing wrong?</p>\n\n<p>Thanks for any help.</p>\n\n<p>Ben</p>\n", "creation_date": 1405496939, "score": 0},
{"title": "after running nutch with hadoop error job failed: name=generate: null", "view_count": 135, "is_answered": false, "question_id": 24678837, "tags": ["java", "ubuntu", "hadoop", "nosql", "nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/24678837/after-running-nutch-with-hadoop-error-job-failed-name-generate-null", "last_activity_date": 1405429701, "owner": {"user_id": 998039, "view_count": 4, "answer_count": 0, "creation_date": 1318785539, "reputation": 13}, "body": "<p>first i installed hadoop on 2 nodes and was worked perfectly . after that i install nutch on the server with this configuration :\nnutch-ste.xml :\n<img src=\"http://i.stack.imgur.com/snBSE.jpg\" alt=\"enter image description here\"></p>\n\n<p>but after executing this command :</p>\n\n<pre><code>bin/hadoop jar ~/Desktop/apache-nutch-2.2.1.job org.apache.nutch.crawl.crawler urls -depth 3 -topN 5\n</code></pre>\n\n<p>this error appears :\n<img src=\"http://i.stack.imgur.com/AM6rN.jpg\" alt=\"enter image description here\">\nplease help me . what should i do ?</p>\n\n<p><strong>EDIT</strong></p>\n\n<p>I think the main error is here . but i don't know what should i do to solve this error.\n<img src=\"http://i.stack.imgur.com/pvmMA.png\" alt=\"enter image description here\"></p>\n", "creation_date": 1405001493, "score": 0},
{"title": "Nutch 2.x not crawling websites like flipkart and jabong", "view_count": 528, "owner": {"user_id": 3832389, "answer_count": 11, "creation_date": 1405173693, "accept_rate": 45, "view_count": 63, "reputation": 339}, "is_answered": true, "answers": [{"question_id": 24720760, "owner": {"user_id": 3282759, "accept_rate": 75, "link": "http://stackoverflow.com/users/3282759/kuntal-g", "user_type": "registered", "reputation": 1154}, "body": "<p>The regex-urlfilter blocks urls that have querystring parameters:</p>\n\n<h1>skip URLs containing certain characters as probable queries, etc.</h1>\n\n<p>-[?*!@=]</p>\n\n<p>Modify that file so that urls with querystring parameters are crawled:</p>\n\n<h1>skip URLs containing certain characters as probable queries, etc.</h1>\n\n<p>-[*!@]</p>\n\n<p>Nutch probably lacks the support for crawling Ajax page. See <a href=\"http://lucene.472066.n3.nabble.com/How-to-crowl-AJAX-populated-pages-td3783398.html\" rel=\"nofollow\">this</a> </p>\n\n<p>you can probably look at\n<a href=\"https://issues.apache.org/jira/browse/NUTCH-1323\" rel=\"nofollow\">https://issues.apache.org/jira/browse/NUTCH-1323</a></p>\n", "creation_date": 1405240594, "is_accepted": true, "score": 3, "last_activity_date": 1405240594, "answer_id": 24720905}], "question_id": 24720760, "tags": ["hbase", "hdfs", "nutch", "web-crawler"], "answer_count": 1, "link": "http://stackoverflow.com/questions/24720760/nutch-2-x-not-crawling-websites-like-flipkart-and-jabong", "last_activity_date": 1405240594, "accepted_answer_id": 24720905, "body": "<p>I've done some experiments with nutch to crawl websites which were not having any ajax calls and I got all the data.</p>\n\n<p>I executed following steps to get the data.</p>\n\n<ol>\n<li>user@localhost:~/sample/nutch/runtime/local/bin$ ./nutch inject /path/to/the/seed.txt</li>\n<li>$: ./nutch generate -batchId 321</li>\n<li>$: ./nutch fetch 321</li>\n<li>$: ./nutch parse 321</li>\n<li>$: ./nutch updatedb</li>\n</ol>\n\n<p>I have hbase as the storage which stores files on hdfs. If I execute these 5 steps it gives me all the data if the url is <a href=\"http://www.naaptol.com/brands/nokia/mobile-phones.html\">http://www.naaptol.com/brands/nokia/mobile-phones.html</a> but if I change it to <a href=\"http://www.flipkart.com/mens-footwear/shoes/sports-shoes/pr?sid=osp,cil,nit,1cu&amp;otracker=hp_nmenu_sub_men_0_Sports%20Shoes\">http://www.flipkart.com/mens-footwear/shoes/sports-shoes/pr?sid=osp,cil,nit,1cu&amp;otracker=hp_nmenu_sub_men_0_Sports%20Shoes</a> it gives me nothing</p>\n\n<p>My nutch-site.xml file:</p>\n\n<pre><code>&lt;?xml version=\"1.0\"?&gt;\n&lt;?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?&gt;\n\n&lt;!-- Put site-specific property overrides in this file. --&gt;\n\n&lt;configuration&gt;\n        &lt;property&gt;\n                &lt;name&gt;storage.data.store.class&lt;/name&gt;\n                &lt;value&gt;org.apache.gora.hbase.store.HBaseStore&lt;/value&gt;\n                &lt;description&gt;Default class for storing data&lt;/description&gt;\n        &lt;/property&gt;\n        &lt;property&gt;\n                &lt;name&gt;http.agent.name&lt;/name&gt;\n                &lt;value&gt;com.datametica.agent&lt;/value&gt;\n                &lt;description&gt;this is just an agent name&lt;/description&gt;\n        &lt;/property&gt;\n        &lt;property&gt;\n                &lt;name&gt;http.robots.agents&lt;/name&gt;\n                &lt;value&gt;datametica_robot&lt;/value&gt;\n                &lt;description&gt;this is just a robot&lt;/description&gt;\n        &lt;/property&gt;\n        &lt;property&gt;\n                &lt;name&gt;plugin.folders&lt;/name&gt;\n                &lt;value&gt;/home/sachin/source_codes/svn/nutch/nutch_2.x/build/plugins&lt;/value&gt;\n        &lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre>\n", "creation_date": 1405239331, "score": 4},
{"title": "nutch redirection handling issue", "view_count": 583, "is_answered": true, "answers": [{"question_id": 17592948, "owner": {"user_id": 1953475, "accept_rate": 72, "link": "http://stackoverflow.com/users/1953475/b-mr-w", "user_type": "registered", "reputation": 5595}, "body": "<p>Again, in the omnipotent <a href=\"http://svn.apache.org/viewvc/nutch/trunk/conf/nutch-default.xml?view=markup\" rel=\"nofollow\">nutch-default.xml</a>, there is an attribute that controls the way how Nutch handles redirection. </p>\n\n<pre><code>&lt;property&gt;\n  &lt;name&gt;http.redirect.max&lt;/name&gt;\n  &lt;value&gt;0&lt;/value&gt;\n  &lt;description&gt;The maximum number of redirects the fetcher will follow when\n  trying to fetch a page. If set to negative or 0, fetcher won't immediately\n  follow redirected URLs, instead it will record them for later fetching.\n  &lt;/description&gt;\n&lt;/property&gt;\n</code></pre>\n\n<p>As the description has mentioned, <code>fetcher won't immediately follow redirected URLs and record them for later fetching</code>. I still have not figured out how to force the URLs in <strong><code>db_redir_temp</code></strong> to be fetched. However, if you change the configuration right at the beginning, I assume your probably might go away. </p>\n", "creation_date": 1405174975, "is_accepted": false, "score": 1, "last_activity_date": 1405174975, "answer_id": 24713844}], "question_id": 17592948, "tags": ["nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/17592948/nutch-redirection-handling-issue", "last_activity_date": 1405174975, "owner": {"user_id": 2526784, "answer_count": 0, "creation_date": 1372313344, "accept_rate": 64, "view_count": 53, "reputation": 77}, "body": "<p>I am bit new to nutch . Thing is I am crawling a url which redirects to another url .Now when analysing my crawl results I get content of first url along with status code : temp redirected to (second url name) . Now my question is that why I am not getting content and details of that second url .Is that redirected url getting crawled or not? Please help.</p>\n", "creation_date": 1373544367, "score": 1},
{"title": "How to configure Nutch to expand zip files and send the contents to Solr", "view_count": 87, "is_answered": false, "question_id": 23463749, "tags": ["solr", "nutch", "apache-tika"], "answer_count": 0, "link": "http://stackoverflow.com/questions/23463749/how-to-configure-nutch-to-expand-zip-files-and-send-the-contents-to-solr", "last_activity_date": 1405094036, "owner": {"user_id": 765287, "answer_count": 50, "creation_date": 1306112708, "accept_rate": 87, "view_count": 271, "location": "Montreal, Canada", "reputation": 2470}, "body": "<p>I'm using Nutch (2.2.1) to crawl and index a set of web pages. These pages contain many .zip files, and each .zip contains many documents.  I'll be searching the crawled data with Solr (4.7), and, within Solr, I'd like each document (within each zip) to have its own record.</p>\n\n<p>Can anyone suggest a good way to set this up?</p>\n\n<p>Is it possible to decompress .zip files in Nutch, and to get Nutch send multiple records to Solr, one for each file inside the .zip?  If so, how?  Would I need to write a plugin, or can this be done through configuration options alone?</p>\n\n<p>On the other hand, would it make more sense to expand and index the zip files outside of Nutch, using a separate app?</p>\n\n<p>Any advice would be much appreciated.</p>\n\n<p>Thanks!</p>\n", "creation_date": 1399255315, "score": 1},
{"title": "nutch regex, how to implement crawling strategy", "view_count": 99, "is_answered": true, "answers": [{"question_id": 24690628, "owner": {"user_id": 3827084, "link": "http://stackoverflow.com/users/3827084/user3827084", "user_type": "registered", "reputation": 16}, "body": "<p>I solved it on my own. Here my solution to it: \nregex for just the start page \n+^.<em>[.]de/$ \nregex for directory 1 \n+.</em>/directoryname1/.* \nregex for directory 2 \n+.<em>/directoryname2/.</em></p>\n", "creation_date": 1405085813, "is_accepted": false, "score": 1, "last_activity_date": 1405085813, "answer_id": 24699090}], "question_id": 24690628, "tags": ["regex", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/24690628/nutch-regex-how-to-implement-crawling-strategy", "last_activity_date": 1405085813, "owner": {"user_id": 3827084, "view_count": 0, "answer_count": 1, "creation_date": 1405024532, "reputation": 16}, "body": "<p>I try to establish the follwoing crawling behaviour in a nutch 1.8 in environment in the regex-urlfilter.txt file:</p>\n\n<p>First:\nCrawl Startpage (www.domainname.com) of the Site defined in the seed.txt file.\nSecond:\nAdditionally only crawl pages of two specific directories \"directoryname1\" (www.domainname.com/directoryname1/...) and \"directoryname2\" (www.domainname.com/directoryname2/...) linked from the start page and disregard everything else.</p>\n\n<p>So far the filters I tried were either too general and the crawler crawled the start page and all other directories (not only directory 1 and 2), or were too strict, so that the crawler did not start at all (as the seed-URL did not match the regex of the urlfilter for the directory).</p>\n\n<p>Thanks for your help chris </p>\n", "creation_date": 1405056551, "score": 1},
{"title": "Can NUTCH be used to fetch data from e-commerce sites?", "view_count": 383, "is_answered": true, "answers": [{"question_id": 24264720, "owner": {"user_id": 2178867, "link": "http://stackoverflow.com/users/2178867/cristi2091", "user_type": "registered", "reputation": 101}, "body": "<p>What Nutch does is crawl some websites and index their webpages.</p>\n\n<p>What you need is a scraper, which is used to extract specific information from some webpages. I don't know what tools are available out there for scraping but it should be pretty easy to write one of your own using some scripting language like Perl or Python.</p>\n\n<p>However if you want to use Nutch for scraping you can build your own plugin to extract additional data. You can find more about plugins on this link: <a href=\"https://wiki.apache.org/nutch/PluginCentral\" rel=\"nofollow\">https://wiki.apache.org/nutch/PluginCentral</a>.</p>\n\n<p>You should notice that scraping is not always legal, some sites specifically prohibit any automated download of content from their pages, you should check that site's Term of Use before trying to scrape it.</p>\n", "creation_date": 1404797689, "is_accepted": false, "score": 1, "last_activity_date": 1404797689, "answer_id": 24624364}, {"question_id": 24264720, "owner": {"user_id": 766955, "accept_rate": 64, "link": "http://stackoverflow.com/users/766955/frederic-bazin", "user_type": "registered", "reputation": 1123}, "body": "<p>try scrapy it's a very powerful and well documented scraping framework.\nIt takes a few hours for experienced programmer to scrap e-commerce data.</p>\n", "creation_date": 1404826884, "is_accepted": false, "score": 1, "last_activity_date": 1404826884, "answer_id": 24633390}], "question_id": 24264720, "tags": ["database", "web-scraping", "nutch", "web-crawler"], "answer_count": 2, "link": "http://stackoverflow.com/questions/24264720/can-nutch-be-used-to-fetch-data-from-e-commerce-sites", "last_activity_date": 1404826884, "owner": {"user_id": 3748631, "view_count": 1, "answer_count": 0, "creation_date": 1403010127, "reputation": 1}, "body": "<p>I want to fetch data from sites.\nExample I want prices of different items from ebay.\nI want to store those products with their prices in my database.\nWill NUTCH be helpful here?\nIf not then what scraper/crawling i should prefer?</p>\n", "creation_date": 1403010563, "score": 0},
{"title": "solr overwrite on just some fields with duplicate uniquekeys", "view_count": 219, "is_answered": false, "question_id": 24608766, "tags": ["performance", "solr", "lucene", "nutch", "solrcloud"], "answer_count": 0, "link": "http://stackoverflow.com/questions/24608766/solr-overwrite-on-just-some-fields-with-duplicate-uniquekeys", "last_activity_date": 1404798524, "owner": {"user_id": 1727204, "answer_count": 15, "creation_date": 1349638210, "accept_rate": 43, "view_count": 102, "reputation": 524}, "body": "<p>I am using nutch for crawling some pages into solr index. For the purpose of specifying The documents that are read I added a boolean value called <code>\"read_flag\"</code> into <code>\"schema.xml\"</code> with default value of false. When an application user read a document it will send a solr update query to set the solr index and set the <code>read_flag</code> to <code>true</code>. On application side I user solr query on all documents that their <code>\"read_flag\"</code> is false to see all the documents that are not read yet. I also defined url to be uniquekey so Solr will overwrite new documents if it finds out that its url is duplicate. My problem is when the new document is sent to solr for indexing its read_flag would be false which is overwritten to solr existing document that might have <code>read_flag=true</code>! I am thinking of some solution but all of them have some performance cost.</p>\n\n<p>1) Use to different documents one for read_flag another for other parts. User solr join in query time. The problem is I am not able to use solrCloud and multi sharding in this way!</p>\n\n<p>2) Changing Nutch to send update query instead of add query for every new document. Probably the performance of indexing decreases in this method. Since this is not applicable for the documents that should be insert for the first time probably it is not feasible solution.</p>\n\n<p>3) Changing solr code to misses some fields for overwriting on duplicate unique-key. </p>\n\n<p>4) Using solrj inside nutch to read document <code>read_flag</code> value and send it for index with this <code>read_flag</code> value.</p>\n\n<p>My question would be what method is best suitable for my situation? and How can I do such method?</p>\n\n<p>Rgards.</p>\n", "creation_date": 1404729896, "score": 0},
{"title": "Exception while using Nutch with MySQL", "view_count": 516, "owner": {"user_id": 2682363, "answer_count": 20, "creation_date": 1376482207, "accept_rate": 100, "view_count": 90, "reputation": 713}, "is_answered": true, "answers": [{"question_id": 22420744, "owner": {"user_id": 2702504, "accept_rate": 69, "link": "http://stackoverflow.com/users/2702504/keerthivasan", "user_type": "registered", "reputation": 9065}, "body": "<p>Yes, the stacktrace says clearly that there is a syntax in your MySQL query for table creation.</p>\n\n<blockquote>\n  <p>Caused by: java.io.IOException: com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near '-depth_webpage (id VARCHAR(767) PRIMARY KEY,headers BLOB,text VARCHAR(32000),sta' at line 1</p>\n</blockquote>\n\n<p>Please make sure that the query executes successfully in standalone from MySQL workbench or phpmyadmin</p>\n", "creation_date": 1394868351, "is_accepted": true, "score": 0, "last_activity_date": 1394868351, "answer_id": 22420893}], "question_id": 22420744, "tags": ["java", "mysql", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/22420744/exception-while-using-nutch-with-mysql", "last_activity_date": 1404725339, "accepted_answer_id": 22420893, "body": "<p>I followed the setup from  <a href=\"http://nlp.solutions.asia/?p=180\" rel=\"nofollow\">Setting up Nutch 2.1 with MySQL </a> and i ran  </p>\n\n<pre><code>bin/crawl urls -depth 1 -topN 2\n</code></pre>\n\n<p>but it throughs an IOException shown as below</p>\n\n<pre><code>InjectorJob: starting at 2014-03-15 12:22:10 \nInjectorJob: Injecting urlDir: urls\nInjectorJob: org.apache.gora.util.GoraException: java.io.IOException:om.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near '-depth_webpage (id VARCHAR(767) PRIMARY KEY,headers BLOB,text VARCHAR(32000),sta' at line 1\n</code></pre>\n\n<p>This is how the Hadoop.log looks like </p>\n\n<pre><code>2014-03-15 12:25:44,275 INFO  crawl.InjectorJob - InjectorJob: starting at 2014-03-15 12:25:44\n2014-03-15 12:25:44,275 INFO  crawl.InjectorJob - InjectorJob: Injecting urlDir: urls\n2014-03-15 12:25:45,467 INFO  store.SqlStore - creating schema: -depth_webpage\n2014-03-15 12:25:45,517 ERROR crawl.InjectorJob - InjectorJob: org.apache.gora.util.GoraException: java.io.IOException: com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near '-depth_webpage (id VARCHAR(767) PRIMARY KEY,headers BLOB,text VARCHAR(32000),sta' at line 1\n    at org.apache.gora.store.DataStoreFactory.createDataStore(DataStoreFactory.java:167)\n    at org.apache.gora.store.DataStoreFactory.createDataStore(DataStoreFactory.java:135)\n    at org.apache.nutch.storage.StorageUtils.createWebStore(StorageUtils.java:75)\n    at org.apache.nutch.crawl.InjectorJob.run(InjectorJob.java:221)\n    at org.apache.nutch.crawl.InjectorJob.inject(InjectorJob.java:251)\n    at org.apache.nutch.crawl.InjectorJob.run(InjectorJob.java:273)\n    at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n    at org.apache.nutch.crawl.InjectorJob.main(InjectorJob.java:282)\nCaused by: java.io.IOException: com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near '-depth_webpage (id VARCHAR(767) PRIMARY KEY,headers BLOB,text VARCHAR(32000),sta' at line 1\n    at org.apache.gora.sql.store.SqlStore.createSchema(SqlStore.java:226)\n    at org.apache.gora.sql.store.SqlStore.initialize(SqlStore.java:172)\n    at org.apache.gora.store.DataStoreFactory.initializeDataStore(DataStoreFactory.java:102)\n    at org.apache.gora.store.DataStoreFactory.createDataStore(DataStoreFactory.java:161)\n    ... 7 more\n</code></pre>\n\n<p>Also nothing is entered into my SQL Database.</p>\n\n<p>Is this error connect to My table CREATION or something else </p>\n\n<p>I am using nutch 2.2.1</p>\n", "creation_date": 1394867384, "score": 1},
{"title": "NUTCH does not crawl a particular website", "view_count": 198, "is_answered": false, "answers": [{"question_id": 24452120, "owner": {"user_id": 2178867, "link": "http://stackoverflow.com/users/2178867/cristi2091", "user_type": "registered", "reputation": 101}, "body": "<p>This might be because the site's robots.txt file restricts your crawler's access to the site.</p>\n\n<p>By default nutch checks the robots.txt file, which is located in <a href=\"http://yourhostname.com/robots.txt\" rel=\"nofollow\">http://yourhostname.com/robots.txt</a>, and if it's not allowed to crawl that site it will not fetch any page.</p>\n", "creation_date": 1404720362, "is_accepted": false, "score": 0, "last_activity_date": 1404720362, "answer_id": 24605733}], "question_id": 24452120, "tags": ["nutch", "web-crawler"], "answer_count": 1, "link": "http://stackoverflow.com/questions/24452120/nutch-does-not-crawl-a-particular-website", "last_activity_date": 1404720362, "owner": {"user_id": 2116143, "answer_count": 0, "creation_date": 1361980865, "accept_rate": 0, "view_count": 3, "reputation": 16}, "body": "<p>I'm using Apache NUTCH version 2.2.1 to crawl some websites. Everything works fine except one website which is the <a href=\"http://eur-lex.europa.eu/homepage.html\" rel=\"nofollow\">http://eur-lex.europa.eu/homepage.html</a>  website.</p>\n\n<p>I tried with the Apache NUTCH version 1.8, I have the same behaviour, nothing is fetched.\nIt fetches and parses the entry page but after that it is as if it can not extract its links.</p>\n\n<p>I see always the following: </p>\n\n<pre><code>------------------------------\n-finishing thread FetcherThread5, activeThreads=4\n-finishing thread FetcherThread4, activeThreads=3\n-finishing thread FetcherThread3, activeThreads=2\n-finishing thread FetcherThread2, activeThreads=1\n0/1 spinwaiting/active, 0 pages, 0 errors, 0.0 0 pages/s, 0 0 kb/s, 0 URLs in 1 queues\n-finishing thread FetcherThread0, activeThreads=0\n\n-----------------\n</code></pre>\n\n<p>Any idea?</p>\n", "creation_date": 1403872915, "score": 0},
{"title": "Nutch 1.8 and Apache Solr 4.8 integration job fail", "view_count": 430, "is_answered": false, "question_id": 24519403, "tags": ["apache", "solr", "nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/24519403/nutch-1-8-and-apache-solr-4-8-integration-job-fail", "last_activity_date": 1404249435, "owner": {"user_id": 3795069, "view_count": 0, "answer_count": 0, "creation_date": 1404240001, "reputation": 6}, "body": "<p>I am trying to crawl the web using Nutch 1.8 and Solr 4.8 on Windows 7.</p>\n\n<pre><code>bin/crawl urls newsolr http://localhost:8983/solr/ 1 -depth 1\n</code></pre>\n\n<p>I keep getting the following error</p>\n\n<pre><code>Indexer: java.io.IOException: Job failed!\n    at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:1252)\n    at org.apache.nutch.indexer.IndexingJob.index(IndexingJob.java:114)\n    at org.apache.nutch.indexer.IndexingJob.run(IndexingJob.java:176)\n    at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n    at org.apache.nutch.indexer.IndexingJob.main(IndexingJob.java:186)\n</code></pre>\n\n<p>Here is the part of the log file:</p>\n\n<pre><code>2014-07-01 16:58:33,613 INFO  solr.SolrMappingReader - source: content dest: content\n2014-07-01 16:58:33,613 INFO  solr.SolrMappingReader - source: title dest: title\n2014-07-01 16:58:33,613 INFO  solr.SolrMappingReader - source: host dest: host\n2014-07-01 16:58:33,613 INFO  solr.SolrMappingReader - source: segment dest: segment\n2014-07-01 16:58:33,613 INFO  solr.SolrMappingReader - source: boost dest: boost\n2014-07-01 16:58:33,613 INFO  solr.SolrMappingReader - source: digest dest: digest\n2014-07-01 16:58:33,613 INFO  solr.SolrMappingReader - source: tstamp dest: tstamp\n2014-07-01 16:58:33,613 INFO  solr.SolrMappingReader - source: url dest: id\n2014-07-01 16:58:33,613 INFO  solr.SolrMappingReader - source: url dest: url\n2014-07-01 16:58:33,643 INFO  solr.SolrIndexWriter - Indexing 1 documents\n2014-07-01 16:58:33,773 WARN  mapred.LocalJobRunner - job_local_0001\norg.apache.solr.common.SolrException: Method Not Allowed\n\nMethod Not Allowed\n\nrequest: http://localhost:8983/solr/\n    at org.apache.solr.client.solrj.impl.CommonsHttpSolrServer.request(CommonsHttpSolrServer.java:430)\n    at org.apache.solr.client.solrj.impl.CommonsHttpSolrServer.request(CommonsHttpSolrServer.java:244)\n    at org.apache.solr.client.solrj.request.AbstractUpdateRequest.process(AbstractUpdateRequest.java:105)\n    at org.apache.nutch.indexwriter.solr.SolrIndexWriter.close(SolrIndexWriter.java:155)\n    at org.apache.nutch.indexer.IndexWriters.close(IndexWriters.java:118)\n    at org.apache.nutch.indexer.IndexerOutputFormat$1.close(IndexerOutputFormat.java:44)\n    at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:474)\n    at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:411)\n    at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:216)\n2014-07-01 16:58:34,628 ERROR indexer.IndexingJob - Indexer: java.io.IOException: Job failed!\n    at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:1252)\n    at org.apache.nutch.indexer.IndexingJob.index(IndexingJob.java:114)\n    at org.apache.nutch.indexer.IndexingJob.run(IndexingJob.java:176)\n    at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n    at org.apache.nutch.indexer.IndexingJob.main(IndexingJob.java:186)`\n</code></pre>\n\n<p>Finally, Solr's error log:</p>\n\n<pre><code>`org.apache.solr.common.SolrException: ERROR: [doc=http://.com/] unknown field 'tstamp' `\n</code></pre>\n\n<p>This is my first solr/nutch setup.  Any help is greatly appreciated.  Thanks in advanced!</p>\n", "creation_date": 1404249435, "score": 1},
{"title": "Sunspot and Nutch with Solr", "view_count": 26, "owner": {"age": 28, "answer_count": 3, "creation_date": 1370349743, "user_id": 2451767, "accept_rate": 88, "view_count": 19, "location": "United States", "reputation": 102}, "is_answered": true, "answers": [{"question_id": 24513535, "owner": {"user_id": 8136, "accept_rate": 83, "link": "http://stackoverflow.com/users/8136/mattmcknight", "user_type": "registered", "reputation": 5571}, "body": "<p>Sunspot is designed to index/search ActiveRecord models. It adds a <code>search</code> method to the models. </p>\n\n<p>If you are not using <code>ActiveRecord</code> or some other database persistence for these items in your Rails application, you should just use <a href=\"https://github.com/rsolr/rsolr\" rel=\"nofollow\">rsolr</a>, not sunspot.</p>\n", "creation_date": 1404231922, "is_accepted": true, "score": 0, "last_activity_date": 1404231922, "answer_id": 24515140}], "question_id": 24513535, "tags": ["search", "solr", "nutch", "sunspot"], "answer_count": 1, "link": "http://stackoverflow.com/questions/24513535/sunspot-and-nutch-with-solr", "last_activity_date": 1404231922, "accepted_answer_id": 24515140, "body": "<p>I'm a little confused about the use of sunspot with solr.\nI worked with solr and nutch, let's say that I have solr with all my data indexed by nutch and now I want to configure sunspot, only to search data in solr, not to index to it.</p>\n\n<p>I have been investigating but, the info I saw is always about configuring models first, to index to solr.</p>\n\n<p>Take a look below, how can I configure controllers in Ruby on Rails with sunspot to get these results from my solr?</p>\n\n<pre><code>{\n   \"content\": \"Varadero es la mejor playa de Cuba, recientemente se remodelo\",\n   \"title\": \"Varadero ampliada la Internet\",\n   \"segment\": \"20131114152100\",\n   \"boost\": 1,\n   \"digest\": \"e6cc9412d5066dae9e176fd7bc598913\",\n   \"tstamp\": \"2013-11-14T15:47:55.235Z\",\n   \"id\": \"http://blogs.uclv.edu.cu/blog/1039#main-content\",\n   \"url\": \"http://blogs.uclv.edu.cu/blog/1039#main-content\",\n   \"_version_\": 1451712964205740000\n}\n</code></pre>\n\n<p>Thanks in advance for your time</p>\n", "creation_date": 1404226964, "score": 0},
{"title": "Nutch 2.2.1 doesnt continue after Injector job", "view_count": 410, "owner": {"user_id": 2294535, "answer_count": 12, "creation_date": 1366279158, "accept_rate": 66, "view_count": 275, "location": "Iit Kharagpur, India", "reputation": 772}, "is_answered": true, "answers": [{"question_id": 22586950, "owner": {"user_id": 2294535, "accept_rate": 66, "link": "http://stackoverflow.com/users/2294535/amrith-krishna", "user_type": "registered", "reputation": 772}, "body": "<p>What was missing was I didnt add Proxy and port details in the nutch-site.xml, as I was accessing through proxy. setting up the same for Ant or JVM is not enough</p>\n", "creation_date": 1396921656, "is_accepted": true, "score": 0, "last_activity_date": 1396921656, "answer_id": 22926176}], "question_id": 22586950, "tags": ["java", "apache", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/22586950/nutch-2-2-1-doesnt-continue-after-injector-job", "last_activity_date": 1403791902, "accepted_answer_id": 22926176, "body": "<p>I am learning nutch and trying to carawl as per this <a href=\"https://wiki.apache.org/nutch/NutchTutorial\" rel=\"nofollow\">tutorial </a>.I am working on an ubuntu machinewith bash shell. But when I run the script, the execution happens, but nothing happens after ,</p>\n\n<pre><code>InjectorJob: starting at 2014-03-23 09:28:50\nInjectorJob: Injecting urlDir: urls/seed.txt\n</code></pre>\n\n<p>I have waited for hours, I tried running the same with <code>sudo</code>. The same issue occurs. I have tried with default urls given in the tutorial as well. What can be the probable errors?</p>\n", "creation_date": 1395547594, "score": 1},
{"title": "Apache nutch performance tuning for whole web crawling", "view_count": 595, "is_answered": false, "question_id": 24383212, "tags": ["java", "performance", "web-crawler", "nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/24383212/apache-nutch-performance-tuning-for-whole-web-crawling", "last_activity_date": 1403602649, "owner": {"user_id": 1727204, "answer_count": 15, "creation_date": 1349638210, "accept_rate": 43, "view_count": 102, "reputation": 524}, "body": "<p>I am going to use nutch for crawling around 300 web pages. The crawl works fine until around 6 mins! after it starts to becomes slower and slower until it drops to near zero performance. I check the log and it seems that the number of spinWaiting threads increases with passing the time. Would you please guide me through this problem?!</p>\n\n<p>Here is my nutch-site.xml config file:</p>\n\n<pre><code> &lt;property&gt;\n   &lt;name&gt;plugin.folders&lt;/name&gt;\n   &lt;value&gt;/home/nutch/workspace/trunk/src/plugin&lt;/value&gt;\n &lt;/property&gt;\n &lt;property&gt;\n  &lt;name&gt;http.agent.name&lt;/name&gt;\n  &lt;value&gt;nutch-test&lt;/value&gt;\n &lt;/property&gt;\n\n&lt;property&gt;\n  &lt;name&gt;parser.skip.truncated&lt;/name&gt;\n  &lt;value&gt;false&lt;/value&gt;\n  &lt;description&gt;Boolean value for whether we should skip parsing for truncated documents. By default this \n  property is activated due to extremely high levels of CPU which parsing can sometimes take.  \n  &lt;/description&gt;\n&lt;/property&gt;\n&lt;property&gt;\n  &lt;name&gt;db.ignore.external.links&lt;/name&gt;\n  &lt;value&gt;true&lt;/value&gt;\n  &lt;description&gt;If true, outlinks leading from a page to external hosts\n  will be ignored. This is an effective way to limit the crawl to include\n  only initially injected hosts, without creating complex URLFilters.\n  &lt;/description&gt;\n&lt;/property&gt;\n&lt;property&gt;\n  &lt;name&gt;fetcher.server.delay&lt;/name&gt;\n  &lt;value&gt;1.0&lt;/value&gt;\n  &lt;description&gt;The number of seconds the fetcher will delay between \n   successive requests to the same server.&lt;/description&gt;\n&lt;/property&gt;\n&lt;property&gt;\n  &lt;name&gt;http.max.delays&lt;/name&gt;\n  &lt;value&gt;2&lt;/value&gt;\n  &lt;description&gt;The number of times a thread will delay when trying to\n  fetch a page.  Each time it finds that a host is busy, it will wait\n  fetcher.server.delay.  After http.max.delays attepts, it will give\n  up on the page for now.&lt;/description&gt;\n&lt;/property&gt;\n&lt;property&gt;\n  &lt;name&gt;fetcher.server.min.delay&lt;/name&gt;\n  &lt;value&gt;0.5&lt;/value&gt;\n  &lt;description&gt;The minimum number of seconds the fetcher will delay between \n  successive requests to the same server. This value is applicable ONLY\n  if fetcher.threads.per.host is greater than 1 (i.e. the host blocking\n  is turned off).&lt;/description&gt;\n&lt;/property&gt;\n&lt;property&gt;\n  &lt;name&gt;fetcher.threads.per.host&lt;/name&gt;\n  &lt;value&gt;3&lt;/value&gt;\n&lt;/property&gt;\n&lt;property&gt;\n  &lt;name&gt;fetcher.threads.fetch&lt;/name&gt;\n  &lt;value&gt;100&lt;/value&gt;\n  &lt;description&gt;The number of FetcherThreads the fetcher should use.\n  This is also determines the maximum number of requests that are\n  made at once (each FetcherThread handles one connection). The total\n  number of threads running in distributed mode will be the number of\n  fetcher threads * number of nodes as fetcher has one map task per node.\n  &lt;/description&gt;\n&lt;/property&gt;\n&lt;property&gt;\n  &lt;name&gt;generate.max.count&lt;/name&gt;\n  &lt;value&gt;10000&lt;/value&gt;\n  &lt;description&gt;The maximum number of urls in a single\n  fetchlist.  -1 if unlimited. The urls are counted according\n  to the value of the parameter generator.count.mode.\n  &lt;/description&gt;\n&lt;/property&gt;\n&lt;property&gt;\n &lt;name&gt;fetcher.max.crawl.delay&lt;/name&gt;\n &lt;value&gt;10&lt;/value&gt;\n &lt;description&gt;\n If the Crawl-Delay in robots.txt is set to greater than this value (in\n seconds) then the fetcher will skip this page, generating an error report.\n If set to -1 the fetcher will never skip such pages and will wait the\n amount of time retrieved from robots.txt Crawl-Delay, however long that\n might be.\n &lt;/description&gt;\n&lt;/property&gt; \n&lt;property&gt;\n  &lt;name&gt;generate.max.per.host&lt;/name&gt;\n  &lt;value&gt;3&lt;/value&gt;\n&lt;/property&gt;\n</code></pre>\n\n<p>Best regards.</p>\n", "creation_date": 1403602649, "score": 0},
{"title": "Crawling different sites with nutch 1.8", "view_count": 74, "is_answered": false, "answers": [{"question_id": 24281458, "owner": {"user_id": 432844, "link": "http://stackoverflow.com/users/432844/julien-nioche", "user_type": "registered", "reputation": 939}, "body": "<p>Not clear why you are getting this. Are you writing a HTMLParseFilter? What you could do is to exit the parse method if the current document's URL does not match a given pattern or alternatively pass some metadata from the seeds which you could use to determine which HTMLParseFilter implementation to use.</p>\n\n<p>BTW you'd get a more relevant audience by posting on the Nutch user list (see <a href=\"http://nutch.apache.org/mailing_lists.html\" rel=\"nofollow\">http://nutch.apache.org/mailing_lists.html</a>)</p>\n", "creation_date": 1403106330, "is_accepted": false, "score": 0, "last_activity_date": 1403106330, "answer_id": 24289853}], "question_id": 24281458, "tags": ["nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/24281458/crawling-different-sites-with-nutch-1-8", "last_activity_date": 1403165162, "owner": {"user_id": 3751696, "view_count": 1, "answer_count": 0, "creation_date": 1403081322, "reputation": 1}, "body": "<p>i am using nutch 1.8 to crawling information from sites who has different patterns from same field. I was writing plugins for each of that sites the , but when i start nutch, just first plugin is matching with all sites, others as they are not exists.</p>\n\n<p>If the first plugin is not matched with site, skip to next one and check them, etc until you find the right plugin for site?</p>\n", "creation_date": 1403082476, "score": 0},
{"title": "Getting parent title when crawling child pages using nutch", "view_count": 106, "is_answered": true, "answers": [{"question_id": 24227196, "owner": {"user_id": 432844, "link": "http://stackoverflow.com/users/432844/julien-nioche", "user_type": "registered", "reputation": 939}, "body": "<p>You could write a HTMLParseFilter and add a custom metadata (<a href=\"https://issues.apache.org/jira/browse/NUTCH-1622\" rel=\"nofollow\">see JIRA</a>) to the outlinks with the title of the main page as value.</p>\n\n<p>BTW you'd get a more relevant audience by posting on the <a href=\"http://nutch.apache.org/mailing_lists.html\" rel=\"nofollow\">Nutch user list</a></p>\n", "creation_date": 1403106618, "is_accepted": false, "score": 1, "last_activity_date": 1403106618, "answer_id": 24289954}], "question_id": 24227196, "tags": ["nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/24227196/getting-parent-title-when-crawling-child-pages-using-nutch", "last_activity_date": 1403106618, "owner": {"age": 33, "answer_count": 0, "creation_date": 1402813841, "user_id": 3741756, "view_count": 1, "location": "Hyderabad, India", "reputation": 1}, "body": "<p>When crawling a website say www.example.com has a page x.html which has outlinks of 4 childs ,if process the the links i am getting the x.html title for all the 4 child pages. </p>\n\n<p>In which case, we add parent title to child in nutch?</p>\n", "creation_date": 1402814594, "score": 0},
{"title": "Extracting text from a web page with Nutch", "view_count": 657, "is_answered": false, "question_id": 13850164, "tags": ["java", "web-scraping", "nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/13850164/extracting-text-from-a-web-page-with-nutch", "last_activity_date": 1403010832, "owner": {"user_id": 172620, "answer_count": 5, "creation_date": 1252796039, "accept_rate": 90, "view_count": 97, "location": "San Francisco, CA", "reputation": 391}, "body": "<p>I'm brand new to Nutch, so please bear with me on this one. </p>\n\n<p>My goal is to simply extract some content from a web page and be able to retrieve the resultant information. For example, let's say I've crawled some pages on an e-commerce site and intend to store product information (i.e., name, category, price, etc). </p>\n\n<p>To reiterate, suppose my seed.txt file contains www.site.com and I initialize a crawl. Assume my HtmlParseFilters are set up to correctly parse product information from www.site.com, and that this crawl will parse both www.site.com and www.site.com/link. If www.site.com contains product A with ID 1 and www.site.com/link contains product B with ID 2, I'm expecting I'll be able to use some sort of predefined utility to give me those results alone: </p>\n\n<p><code>\nA 1</p>\n\n<p>B 2 \n</code></p>\n\n<p>At the moment, I'm finding myself modifying org.apache.nutch.segment.SegmentReader and the toString() methods of org.apache.nutch.parse.ParseResult AND org.apache.nutch.metadata.MetaData to isolate my extracted results, which makes me feel like I'm missing something... I don't want to see the outlinks, recno, URL, parse metadata, Playback or any other crawl metadata; I just want the results of my HTML filtration, as described above. </p>\n\n<p>In short, I'm really trying to understand how to use nutch to scrape a group of sites and give me ONLY what I have extracted. </p>\n\n<p>Don't hesitate to let me know if you need any clarification.</p>\n", "creation_date": 1355353160, "score": 2},
{"title": "Recrawling strategy of google crawler", "view_count": 119, "is_answered": true, "answers": [{"question_id": 23982278, "owner": {"user_id": 2915763, "link": "http://stackoverflow.com/users/2915763/ramanan-r", "user_type": "registered", "reputation": 789}, "body": "<blockquote>\n  <p>We use a huge set of computers to fetch (or \"crawl\") billions of pages\n  on the web. Googlebot uses an algorithmic process: computer programs\n  determine which sites to crawl, how often, and how many pages to fetch\n  from each site.</p>\n  \n  <p>Googlebot's crawl process begins with a list of webpage URLs,\n  generated from previous crawl processes and augmented with Sitemap\n  data provided by webmasters. As Googlebot visits each of these\n  websites it detects links (SRC and HREF) on each page and adds them to\n  its list of pages to crawl. New sites, changes to existing sites, and\n  dead links are noted and used to update the Google index.</p>\n</blockquote>\n\n<p><a href=\"https://support.google.com/webmasters/answer/182072?hl=en\" rel=\"nofollow\">https://support.google.com/webmasters/answer/182072?hl=en</a></p>\n\n<p>First why does it has to finish its job in 10 minutes?</p>\n\n<p>As in the first paragraph, not all sites are recrawled at same interval. They have an algorithm to determine this.</p>\n\n<p>So googlebot will fetch every page again, but at very different intervals. Its option (2) in your question but with an added algorithm.</p>\n\n<p>They use hadoop infrastructure for scalability.</p>\n", "creation_date": 1402638180, "is_accepted": false, "score": 1, "last_activity_date": 1402638180, "answer_id": 24198465}], "question_id": 23982278, "tags": ["hadoop", "solr", "web-crawler", "nutch", "google-crawlers"], "answer_count": 1, "link": "http://stackoverflow.com/questions/23982278/recrawling-strategy-of-google-crawler", "last_activity_date": 1402638180, "owner": {"user_id": 1727204, "answer_count": 15, "creation_date": 1349638210, "accept_rate": 43, "view_count": 102, "reputation": 524}, "body": "<p>I was wondering how recrawling strategy for some huge search engines works. For example let considering google. We know that google is using dynamic interval for recrawling each web-site. Suppose according to google dynamic interval there is 100k number of sites that should be recrawl every 10 mins. So the crawling process of these 100000 sites should be done in less than 10 mins. Probably there are two possible situations:</p>\n\n<p>1) google bot will fetch the first page of each of these sites and then generate the list of URLs in this page. For each URL it will check whether this url is fetched before or not. If it is new it will fetch the new page. This process will continue until the end of crawl or specific deep threshold.</p>\n\n<p>2) google bot will fetch every page again (no matter it has updated or not)</p>\n\n<p>Suppose google using first strategy, then how a page with same url but updated content will be crawled and indexed? Suppose google using second one, then how it can recrawl all of these pages in less than 10 mins? what about other web pages? probably there are more than 6 billion web page available how recrawling all of these pages in timely manner is possible? I really think it is not possible with using some new technologies like nutch and solr on the hadoop infrastructure.</p>\n\n<p>Regards.</p>\n", "creation_date": 1401642150, "score": 0},
{"title": "nutch crawl does not use all entries in seed.txt", "view_count": 317, "is_answered": false, "answers": [{"question_id": 24084490, "owner": {"user_id": 2915763, "link": "http://stackoverflow.com/users/2915763/ramanan-r", "user_type": "registered", "reputation": 789}, "body": "<p>Configure this correctly:</p>\n\n<pre><code>bin/nutch crawl $URLS -dir $CRAWL_LOC -depth 10 -topN 1000\n</code></pre>\n\n<p>Depth: nutch will crawl upto this level in depth</p>\n\n<p>topN: in each level, nutch will crawl this number of url's</p>\n", "creation_date": 1402636942, "is_accepted": false, "score": 0, "last_activity_date": 1402636942, "answer_id": 24198227}], "question_id": 24084490, "tags": ["nutch", "web-crawler"], "answer_count": 1, "link": "http://stackoverflow.com/questions/24084490/nutch-crawl-does-not-use-all-entries-in-seed-txt", "last_activity_date": 1402636942, "owner": {"user_id": 3715456, "view_count": 2, "answer_count": 0, "creation_date": 1402064927, "reputation": 1}, "body": "<p>I am using apache-nutch-1.6 and I can successfuly crawl web sites.\nMy Problem is that not all entries in the seed.txt file are used. It depends on which sites are inside. So is there anywhere a Limit how much is crawled? No error message. Just if I delete one site an other sites is deeply crawled, whereever if the other one is there this one is crawled and from the other sites only the top sites I beleave....</p>\n", "creation_date": 1402065551, "score": 0},
{"title": "Cannot start HBase start_hbase.sh: command not found", "view_count": 1511, "owner": {"user_id": 2772527, "answer_count": 6, "creation_date": 1378987395, "accept_rate": 77, "view_count": 92, "reputation": 430}, "is_answered": true, "answers": [{"question_id": 24143023, "owner": {"user_id": 1613323, "link": "http://stackoverflow.com/users/1613323/kkmishra", "user_type": "registered", "reputation": 468}, "body": "<p>try with following command:</p>\n\n<pre><code>./start_hbase.sh\n</code></pre>\n\n<p>if its not runnable then try after making it runnable, to make runnable use following command:</p>\n\n<pre><code>chmod a+x start_hbase.sh\n</code></pre>\n", "creation_date": 1402414455, "is_accepted": true, "score": 3, "last_activity_date": 1402414455, "answer_id": 24145087}], "question_id": 24143023, "tags": ["solr", "cygwin", "hbase", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/24143023/cannot-start-hbase-start-hbase-sh-command-not-found", "last_activity_date": 1402414455, "accepted_answer_id": 24145087, "body": "<p>Trying, in vain so far, to make Nutch + Solr work. I'm having very hard time understanding how to go about this thing with <code>nutch</code> and <code>solr</code>. I have followed all the tutorials I could find on the internet, most of them for older versions, but I still could not make any of them work. At this moment I'm follwoing this <a href=\"https://github.com/renepickhardt/metalcon/wiki/simpleNutchSolrSetup\" rel=\"nofollow\">guide</a></p>\n\n<p>I have unpacked <strong>nutch 2.2.1</strong>, <strong>sorl 4.3.1</strong>, <strong>hbase 0.90.4</strong> to directory on my xampp local server (none of the tutorials said where I should unpack them to, so I assumed that on local server).</p>\n\n<p>I'm using <code>Cygwin</code> on windows 7. JAVA_HOME is pointing to <code>/cygdrive/c/PROGRA~1/java/jdk1.8.0_05</code></p>\n\n<p>I stuck at <code>Configure HBase</code> step. As the tutorial dictates I have configured <code>/hbase-0.90.4/conf/hbase-site.xml</code> as follows:</p>\n\n<pre><code>&lt;property&gt;\n   &lt;name&gt;hbase.rootdir&lt;/name&gt;\n   &lt;value&gt;file:///C:/xampp/htdocs/trynutch/hbase&lt;/value&gt;\n&lt;/property&gt;\n&lt;property&gt;\n   &lt;name&gt;hbase.zookeeper.property.dataDir&lt;/name&gt;\n   &lt;value&gt;C:/xampp/htdocs/trynutch/zookeeper&lt;/value&gt;\n&lt;/property&gt;\n</code></pre>\n\n<p>As per tutorial after this I should be able to run the following command:</p>\n\n<pre><code>$ ./trynutch/hbase/bin/start_hbase.sh\n</code></pre>\n\n<p>When I run it in cygwin terminal, it gives an error:</p>\n\n<pre><code>DM@comp ~\n$ cd C:/xampp/htdocs/trynutch/hbase-0.90.4/bin\n\nDM@comp /cygdrive/c/xampp/htdocs/trynutch/hbase-0.90.4/bin\n$ start_hbase.sh\n-bash: start_hbase.sh: command not found\n</code></pre>\n\n<p>I'd appreciate any information.</p>\n", "creation_date": 1402408898, "score": 0},
{"title": "How to create custom index writer for Apache Nutch 2.x?", "view_count": 306, "is_answered": false, "answers": [{"question_id": 21670824, "owner": {"user_id": 2086509, "accept_rate": 73, "link": "http://stackoverflow.com/users/2086509/jayesh-bhoyar", "user_type": "registered", "reputation": 594}, "body": "<p>You will have to write your custom plugin and then configure the nutch to use your custom plugin.</p>\n\n<p>Following link will give you step by step procedure to achieve this task:</p>\n\n<p><strong><em><a href=\"http://wiki.apache.org/nutch/WritingPluginExample\" rel=\"nofollow\">http://wiki.apache.org/nutch/WritingPluginExample</a></em></strong></p>\n", "creation_date": 1402381130, "is_accepted": false, "score": 0, "last_activity_date": 1402381130, "answer_id": 24134145}], "question_id": 21670824, "tags": ["apache", "plugins", "configuration", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/21670824/how-to-create-custom-index-writer-for-apache-nutch-2-x", "last_activity_date": 1402381130, "owner": {"user_id": 1271400, "answer_count": 6, "creation_date": 1331811039, "accept_rate": 68, "view_count": 35, "reputation": 322}, "body": "<p>I'm looking a way to create custom index writer for Apache Nutch. Which config file should I edit and what kind of properties I should add there?</p>\n\n<p>I couldn't find anything useful from conf/nutch-default.xml</p>\n", "creation_date": 1392015473, "score": 2},
{"title": "Authentication and Connection refused error while crawling using nutch", "view_count": 1877, "owner": {"user_id": 3275347, "view_count": 10, "answer_count": 1, "creation_date": 1391606426, "reputation": 23}, "is_answered": true, "answers": [{"question_id": 23108625, "owner": {"user_id": 2086509, "accept_rate": 73, "link": "http://stackoverflow.com/users/2086509/jayesh-bhoyar", "user_type": "registered", "reputation": 594}, "body": "<p>1) From the logs it clearly says Authentication failure for NTLM on your particular site.</p>\n\n<p>Here you must first check username/password.</p>\n\n<p>Then Scheme of Auth Basic/NTLM/\nAnd then port on which you want to autheticate</p>\n\n<p>If you validate these 3 point and use correct values then your Authentication problem should get resolved...</p>\n", "creation_date": 1401872293, "is_accepted": true, "score": 1, "last_activity_date": 1401872293, "answer_id": 24033073}], "question_id": 23108625, "tags": ["nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/23108625/authentication-and-connection-refused-error-while-crawling-using-nutch", "last_activity_date": 1401872293, "accepted_answer_id": 24033073, "body": "<p>I am trying to crawl some urls using Nutch 1.7 but facing </p>\n\n<p>1)authentication issues and \n2)connetion refused exception.</p>\n\n<p>1)According to the logs I could see that it is trying to authenticate with NTLM \nbut after that it shows \"Redirect required\" and finally releasing the connection..(could be seen in logpart-1)</p>\n\n<p>According to the Nutch tutorial in \n<a href=\"http://wiki.apache.org/nutch/HttpAuthenticationSchemes#A_note_on_NTLM_domains\" rel=\"nofollow\">http://wiki.apache.org/nutch/HttpAuthenticationSchemes#A_note_on_NTLM_domains</a></p>\n\n<p>i) I have set the auth-configuration in httpclient-auth.xml  file:</p>\n\n<pre><code>&lt;auth-configuration&gt;\n&lt;credentials username=\"A1101029\" password=\"ABC123#$\"&gt;\n&lt;default realm=\"domain\" /&gt;\n&lt;authscope host=\"sp.xxx.com\" port=\"80\"/&gt;\n&lt;/credentials&gt;\n&lt;/auth-configuration&gt;\n</code></pre>\n\n<p>ii)Define httpclient  property in both nutch-site.xml and nutch-default.xml</p>\n\n<pre><code>&lt;property&gt;\n&lt;name&gt;plugin.includes&lt;/name&gt;\n&lt;value&gt;protocol-(httpclient|http)|urlfilter-  \nregex|parse-(text|html|tika)|index-(more|basic|anchor)|indexer-solr|scoring-  \nopic|urlnormalizer-(pass|regex|basic)&lt;/value&gt; \n&lt;/property \n</code></pre>\n\n<p>iii) Also have defined the auth configuration file in nutch-site.xml.</p>\n\n<pre><code>&lt;property&gt;\n&lt;name&gt;http.auth.file&lt;/name&gt;\n&lt;value&gt;httpclient-auth.xml&lt;/value&gt;\n&lt;description&gt;Authentication configuration file for 'protocol-httpclient' plugin.\n&lt;/description&gt;\n&lt;/property&gt;\n</code></pre>\n\n<p>But there is no success for me!!</p>\n\n<p>Am I not configuring the authentication in proper way or I am missing something..\nCan anyone please help me with the proper required authentication configuration in nutch.</p>\n\n<p>Attaching complete hadoop.log!!</p>\n\n<p>logpart-1:authentication</p>\n\n<pre><code>2014-04-16 05:11:23,712 DEBUG httpclient.HttpMethodDirector - Authorization required\n2014-04-16 05:11:23,712 DEBUG auth.AuthChallengeProcessor - Using authentication scheme: ntlm\n2014-04-16 05:11:23,731 DEBUG auth.AuthChallengeProcessor - Authorization challenge processed\n2014-04-16 05:11:23,733 DEBUG httpclient.HttpMethodDirector - Authentication scope: NTLM &lt;any realm&gt;@sp.zzz.com:80\n2014-04-16 05:11:23,732 DEBUG fetcher.Fetcher - FetcherThread spin-waiting ...\n2014-04-16 05:11:23,733 DEBUG fetcher.Fetcher - FetcherThread spin-waiting ...\n2014-04-16 05:11:23,733 DEBUG httpclient.HttpMethodDirector - Retry authentication\n2014-04-16 05:11:23,733 DEBUG fetcher.Fetcher - FetcherThread spin-waiting ...\n2014-04-16 05:11:23,734 DEBUG httpclient.HttpMethodBase - Resorting to protocol version default close connection policy\n2014-04-16 05:11:23,733 DEBUG cookie.CookieSpec - Unrecognized cookie attribute: name=HttpOnly, value=null\n2014-04-16 05:11:23,734 DEBUG httpclient.HttpMethodBase - Should NOT close connection, using HTTP/1.1\n2014-04-16 05:11:23,735 DEBUG httpclient.HttpMethodBase - Cookie accepted: \"PHPSESSID=9f9378mvh9e720f5o3l0ibc1o7\"\n2014-04-16 05:11:23,735 DEBUG httpclient.HttpMethodDirector - Authenticating with NTLM &lt;any realm&gt;@sp.zzz.com:80\n2014-04-16 05:11:23,735 DEBUG httpclient.HttpMethodDirector - Redirect required\n2014-04-16 05:11:23,735 DEBUG params.HttpMethodParams - Credential charset not configured, using HTTP element charset\n2014-04-16 05:11:23,735 DEBUG httpclient.HttpMethodBase - Should close connection in response to directive: close\n2014-04-16 05:11:23,735 DEBUG httpclient.HttpConnection - Releasing connection back to connection manager.\n2014-04-16 05:11:23,736 DEBUG httpclient.MultiThreadedHttpConnectionManager - Freeing connection, hostConfig=HostConfiguration[host=www.xxxportal.com]\n2014-04-16 05:11:23,736 DEBUG util.IdleConnectionHandler - Adding connection at: 1397643083736\n2014-04-16 05:11:23,736 DEBUG httpclient.MultiThreadedHttpConnectionManager - Notifying no-one, there are no waiting threads\n2014-04-16 05:11:23,737 DEBUG httpclient.HttpMethodBase - Adding Host request header\n2014-04-16 05:11:23,744 DEBUG httpclient.HttpMethodDirector - Authorization required\n2014-04-16 05:11:23,744 DEBUG auth.AuthChallengeProcessor - Using authentication scheme: ntlm\n2014-04-16 05:11:23,744 DEBUG auth.AuthChallengeProcessor - Authorization challenge processed\n2014-04-16 05:11:23,744 DEBUG httpclient.HttpMethodDirector - Authentication scope: NTLM &lt;any realm&gt;@sp.zzz.com:80\n2014-04-16 05:11:23,745 INFO  regex.RegexURLNormalizer - can't find rules for scope 'fetcher', using default\n2014-04-16 05:11:23,745 DEBUG httpclient.HttpMethodDirector - Credentials required\n2014-04-16 05:11:23,745 DEBUG httpclient.HttpMethodDirector - Credentials provider not available\n2014-04-16 05:11:23,745 INFO  httpclient.HttpMethodDirector - Failure authenticating with NTLM &lt;any realm&gt;@sp.zzz.com:80\n2014-04-16 05:11:23,745 DEBUG httpclient.HttpMethodBase - Resorting to protocol version default close connection policy\n2014-04-16 05:11:23,745 DEBUG httpclient.HttpMethodBase - Should NOT close connection, using HTTP/1.1\n2014-04-16 05:11:23,746 DEBUG httpclient.HttpConnection - Releasing connection back to connection manager.\n2014-04-16 05:11:23,746 DEBUG httpclient.MultiThreadedHttpConnectionManager - Freeing connection, hostConfig=HostConfiguration[host=sp.zzz.com]\n</code></pre>\n\n<p>2)For few of other links I am getting \n                \"I/O exception (java.net.ConnectException) caught when processing request: Connection refused: connect\"</p>\n\n<p>I am not behind any proxy and have turned off all the firewall settings in the system still\n no idea why I am getting connection refused exception.              </p>\n\n<p>Here also I am not able to find out the exact reason why I am getting connection refused exception.\n Please help me to understand the exact problem in this case a well.</p>\n\n<p>Attaching the complete hadoop.log!!</p>\n\n<p>logPart2-connection refused.</p>\n\n<pre><code>2014-04-16 05:11:26,443 INFO  fetcher.Fetcher - * queue: www.xxxportal.com\n2014-04-16 05:11:26,443 INFO  fetcher.Fetcher -   maxThreads    = 1\n2014-04-16 05:11:26,444 INFO  fetcher.Fetcher -   inProgress    = 0\n2014-04-16 05:11:26,444 INFO  fetcher.Fetcher -   crawlDelay    = 5000\n2014-04-16 05:11:26,444 INFO  fetcher.Fetcher -   minCrawlDelay = 0\n2014-04-16 05:11:26,444 INFO  fetcher.Fetcher -   nextFetchTime = 1397643088739\n2014-04-16 05:11:26,444 INFO  fetcher.Fetcher -   now           = 1397643086444\n2014-04-16 05:11:26,444 INFO  fetcher.Fetcher -   0. www.xxxportal.com/profiles/\n2014-04-16 05:11:26,445 INFO  fetcher.Fetcher -   1. www.xxxportal.com/wiki/index.php\n2014-04-16 05:11:26,445 INFO  fetcher.Fetcher -   2. www.xxxportal.com/sop/\n2014-04-16 05:11:26,560 DEBUG httpclient.HttpMethodDirector - Closing the connection.\n2014-04-16 05:11:26,560 INFO  httpclient.HttpMethodDirector - I/O exception (java.net.ConnectException) caught when processing request: Connection refused: connect\n2014-04-16 05:11:26,560 DEBUG httpclient.HttpMethodDirector - Connection refused: connect\njava.net.ConnectException: Connection refused: connect\n                at java.net.DualStackPlainSocketImpl.waitForConnect(Native Method)\n                at java.net.DualStackPlainSocketImpl.socketConnect(DualStackPlainSocketImpl.java:85)\n                at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:339)\n                at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:200)\n                at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:182)\n                at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)\n                at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)\n                at java.net.Socket.connect(Socket.java:579)\n                at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n                at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n                at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n                at java.lang.reflect.Method.invoke(Method.java:606)\n                at org.apache.commons.httpclient.protocol.ReflectionSocketFactory.createSocket(ReflectionSocketFactory.java:140)\n                at org.apache.commons.httpclient.protocol.DefaultProtocolSocketFactory.createSocket(DefaultProtocolSocketFactory.java:125)\n                at org.apache.commons.httpclient.HttpConnection.open(HttpConnection.java:707)\n                at org.apache.commons.httpclient.MultiThreadedHttpConnectionManager$HttpConnectionAdapter.open(MultiThreadedHttpConnectionManager.java:1361)\n                at org.apache.commons.httpclient.HttpMethodDirector.executeWithRetry(HttpMethodDirector.java:387)\n                at org.apache.commons.httpclient.HttpMethodDirector.executeMethod(HttpMethodDirector.java:171)\n                at org.apache.commons.httpclient.HttpClient.executeMethod(HttpClient.java:397)\n                at org.apache.commons.httpclient.HttpClient.executeMethod(HttpClient.java:323)\n                at org.apache.nutch.protocol.httpclient.HttpResponse.&lt;init&gt;(HttpResponse.java:94)\n                at org.apache.nutch.protocol.httpclient.Http.getResponse(Http.java:154)\n                at org.apache.nutch.protocol.http.api.HttpRobotRulesParser.getRobotRulesSet(HttpRobotRulesParser.java:75)\n                at org.apache.nutch.protocol.RobotRulesParser.getRobotRulesSet(RobotRulesParser.java:157)\n                at org.apache.nutch.protocol.http.api.HttpBase.getRobotRules(HttpBase.java:391)\n                at org.apache.nutch.fetcher.Fetcher$FetcherThread.run(Fetcher.java:676)\n2014-04-16 05:11:26,564 INFO  httpclient.HttpMethodDirector - Retrying request\n2014-04-16 05:11:26,565 DEBUG httpclient.HttpConnection - Open connection to www.zzzlearninglounge.com:80\n</code></pre>\n", "creation_date": 1397649073, "score": 1},
{"title": "Regular expression to filter URLs that have more than one ampersand &quot;&amp;&quot;", "view_count": 245, "owner": {"user_id": 1830069, "answer_count": 10, "creation_date": 1353080229, "accept_rate": 63, "view_count": 103, "reputation": 543}, "is_answered": true, "answers": [{"question_id": 24001821, "owner": {"user_id": 2212650, "link": "http://stackoverflow.com/users/2212650/adam-yost", "user_type": "registered", "reputation": 2472}, "body": "<p>since you seem to be matching URLs which meet each case, then filtering by those, it is fairly simple to match URLs which DO contain 2 &amp;s.</p>\n\n<pre><code>(&amp;.*&amp;)+\n</code></pre>\n\n<p>That will match anything containing a sequence with at least 2 ampersands</p>\n", "creation_date": 1401737499, "is_accepted": false, "score": 1, "last_activity_date": 1401737499, "answer_id": 24001857}, {"question_id": 24001821, "owner": {"user_id": 1078583, "accept_rate": 69, "link": "http://stackoverflow.com/users/1078583/zx81", "user_type": "registered", "reputation": 27170}, "body": "<p>Insert this negative lookahead after the head of string anchor <code>^</code></p>\n\n<pre><code>(?![^&amp;]*&amp;){2}\n</code></pre>\n\n<p><strong>Explain Regex</strong></p>\n\n<pre><code>(?!                      # look ahead to see if there is not (2\n                         # times):\n  [^&amp;]*                  #   any character except: '&amp;' (0 or more\n                         #   times (matching the most amount\n                         #   possible))\n  &amp;                      #   '&amp;'\n){2}                     # end of look-ahead\n</code></pre>\n", "creation_date": 1401737538, "is_accepted": false, "score": 3, "last_activity_date": 1401737538, "answer_id": 24001867}, {"question_id": 24001821, "owner": {"user_id": 1081110, "link": "http://stackoverflow.com/users/1081110/david-wallace", "user_type": "registered", "reputation": 38454}, "body": "<p>Your line in the Nutch config should be</p>\n\n<pre><code>-&amp;.*&amp;\n</code></pre>\n\n<p>This tells Nutch to skip anything with two or more <code>&amp;</code> characters, and any number of characters between.</p>\n\n<p>Whether it's a good idea to ignore such URLs depends on the purpose of your search engine, and the nature of the URLs within the domain that you're searching.  It's impossible to answer the last sentence of your question without understanding the problem domain.</p>\n", "creation_date": 1401737550, "is_accepted": true, "score": 3, "last_activity_date": 1401737550, "answer_id": 24001870}], "question_id": 24001821, "tags": ["java", "regex", "nutch"], "answer_count": 3, "link": "http://stackoverflow.com/questions/24001821/regular-expression-to-filter-urls-that-have-more-than-one-ampersand", "last_activity_date": 1401737550, "accepted_answer_id": 24001870, "body": "<p>I am using a web-crawler(called <strong>Nutch</strong>) which crawls the web when I feed in a bunch of urls into it. I set certain regular expression filters to control the crawler to specific domains and specific filters. </p>\n\n<pre><code># skip URLs containing a back slash\n\n-[\\\\]    \n\n# skip URLS containing more than 6 levels\n\n-^http://([a-zA-Z.-]+)/(?:[^/]+/){6,}.*$\n\n# crawl only domain abc\n\n+^http://www.abc.xx.yyy.zzz/pubs/([a-z]+)\n</code></pre>\n\n<p><strong>Issue</strong>: Within the specific domain, my crawler is crawling all search form urls with all the pagination and query parameters which I don't want. Examples are:</p>\n\n<pre><code>http://www.abc.xx.yyy.zzz/pubs/biblio_results.asp?Library=ABC&amp;SubjectScope=keyword&amp;SubjectMode=contains&amp;SubjectText=abc_archive&amp;URLs=yes&amp;Order=year&amp;SortOrder=DESC&amp;Abstracts=no\n</code></pre>\n\n<p>I am not sure how to set a regular expression to let my crawler ignore any such URL like the example above that has more than one \"&amp;\" sign in its URL path. </p>\n\n<p>On a side note, is it a good idea to ignore such URLs while building a search engine?</p>\n", "creation_date": 1401737325, "score": 0},
{"title": "Edit file placed in class folder", "view_count": 41, "is_answered": false, "answers": [{"last_edit_date": 1401567596, "owner": {"user_id": 2731457, "accept_rate": 81, "link": "http://stackoverflow.com/users/2731457/kyranstar", "user_type": "registered", "reputation": 976}, "body": "<p>This was actually a big problem for me, and I always wondered why there weren't more tutorials to do this.</p>\n\n<p>You need to use this method: <code>class.getResourceAsStream(\"/path/here\");</code> to get an <code>InputStream</code> to your file.</p>\n\n<p>So you can do this:</p>\n\n<pre><code>InputStream is = getClass().getResourceAsStream(\"/path/here\");\nBufferedReader br = new BufferedReader(new InputStreamReader(is, \"UTF-8\"));\n</code></pre>\n\n<p><strong>Edit:</strong></p>\n\n<p>Oh. So you want to EDIT the file in the Jar? Well, it is possible(I believe) by using a zip library to get into the Jar. But honestly this is a lot more difficult than it should be. In my opinion, you should just save the config file to a predetermined location (ex. \"C:\\Program Files (x86)\\appnamehere\\config.txt\" or something) and just access that path.</p>\n", "question_id": 23974152, "creation_date": 1401567202, "is_accepted": false, "score": 0, "last_activity_date": 1401567596, "answer_id": 23974227}], "question_id": 23974152, "tags": ["java", "regex", "eclipse", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/23974152/edit-file-placed-in-class-folder", "last_activity_date": 1401567596, "owner": {"user_id": 3425297, "answer_count": 4, "creation_date": 1394961655, "accept_rate": 20, "view_count": 2, "location": "Lublin, Poland", "reputation": 73}, "body": "<p>I have a question: How can I change/edit text file placed in class folder which is added to build path in my java project. </p>\n\n<p>I'm working with Apache Nutch and in my gui app user must be able to edit regex-urlfilter.txt file (add or delete filter). It is palaced in nutchConf directory which was added to Build Path.</p>\n\n<p>I'll be grateful for any answer.</p>\n\n<p>EDIT: And I want to get it to work after exporting to jar and run from terminal.</p>\n", "creation_date": 1401566328, "score": 0},
{"title": "Writing a regular expression for nutch&#39;s regex-urlfilter.txt file", "view_count": 579, "is_answered": false, "answers": [{"question_id": 23930859, "owner": {"user_id": 1881318, "link": "http://stackoverflow.com/users/1881318/tahagh", "user_type": "registered", "reputation": 619}, "body": "<p>Note that your url filters should also match with your seed URLs or else they will be filter out and hence nutch won't get any chance to parse them and extract the links you wanted. </p>\n\n<p>For example, if your seed file contains this url <a href=\"http://www.utiltrucks.com/home\" rel=\"nofollow\">http://www.utiltrucks.com/home</a> then you should also add an entry in your regex-urlfilter file like this:</p>\n\n<p>+<a href=\"http://www.utiltrucks.com/home\" rel=\"nofollow\">http://www.utiltrucks.com/home</a></p>\n\n<p>This should be also done for all pages that in the path from your seed urls to your target pages that you want to extract links from.</p>\n", "creation_date": 1401520943, "is_accepted": false, "score": 0, "last_activity_date": 1401520943, "answer_id": 23967530}], "question_id": 23930859, "tags": ["regex", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/23930859/writing-a-regular-expression-for-nutchs-regex-urlfilter-txt-file", "last_activity_date": 1401520943, "owner": {"user_id": 1654592, "view_count": 15, "answer_count": 0, "creation_date": 1347017678, "reputation": 23}, "body": "<p>I'm having some problems with regex-urlfilter.txt file.\nI want to crawl only links that have numbers before '.html', should be easy but I can't get it right...</p>\n\n<p>Here's an example:</p>\n\n<p><a href=\"http://www.utiltrucks.com/annonce-occasion-camion-poids-lourd/marque-renault/modele-midliner/ref-71015.html\" rel=\"nofollow\">http://www.utiltrucks.com/annonce-occasion-camion-poids-lourd/marque-renault/modele-midliner/ref-71015.html</a>\n<a href=\"http://www.utiltrucks.com/annonce-occasion-camion-poids-lourd/dpt-.html\" rel=\"nofollow\">http://www.utiltrucks.com/annonce-occasion-camion-poids-lourd/dpt-.html</a></p>\n\n<p>I want to catch the first link.</p>\n\n<p>I've tried with the following entry in regex-urlfilter: </p>\n\n<h1>accept anything else</h1>\n\n<p>+<a href=\"http://www.utiltrucks.com/annonce-occasion.+?%5b0-9%5d+.html\" rel=\"nofollow\">http://www.utiltrucks.com/annonce-occasion.+?[0-9]+.html</a></p>\n\n<p>I get a message:\n0 records selected for fetching, exiting ... </p>\n\n<p>Anybody got an idea how to pull this off?</p>\n", "creation_date": 1401358247, "score": 0},
{"title": "Apache Nutch 1.8 Contains no folder named &quot;runtime&quot;", "view_count": 107, "is_answered": false, "question_id": 23889640, "tags": ["apache", "nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/23889640/apache-nutch-1-8-contains-no-folder-named-runtime", "last_activity_date": 1401194293, "owner": {"age": 23, "answer_count": 3, "creation_date": 1379514529, "user_id": 2791893, "accept_rate": 75, "view_count": 13, "location": "Punjab", "reputation": 81}, "body": "<p>I am starting with the Search Engine and i have tried to use Apache nutch as a base for learning. I am watching a tutorial of apache nutch 1.4 (i know its older version ) in which i am asked to go into a folder named runtime but their is no folder in my version. Now my question is , has runtime folder changed with some other folders in newer version ? what i can do to as an alternative with that ? Please help me with that. </p>\n", "creation_date": 1401194293, "score": 1},
{"title": "Nutch not fetching any URL", "view_count": 72, "is_answered": false, "question_id": 23881736, "tags": ["hadoop", "solr", "nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/23881736/nutch-not-fetching-any-url", "last_activity_date": 1401169061, "owner": {"age": 22, "answer_count": 0, "creation_date": 1387817585, "user_id": 3130223, "view_count": 12, "location": "Chennai, India", "reputation": 12}, "body": "<p>I'm running Nutch 2 on a Hadoop cluster (2 nodes). I run the crawl command as</p>\n\n<pre><code>bin/crawl urls/seed.txt TestCrawl http://10.130.231.16:8983/solr/nutch 2\n</code></pre>\n\n<p>The screen shows that 765 URLs have been injected after filtering. The stats show that nothing has been fetched.</p>\n\n<pre><code>14/05/27 01:33:44 INFO crawl.WebTableReader: Statistics for WebTable: \n14/05/27 01:33:44 INFO crawl.WebTableReader: jobs:  {db_stats-job_201405261214_0047=     {jobID=job_201405261214_0047, jobName=db_stats, counters={File Input Format Counters =  {BYTES_READ=0}, Job Counters ={TOTAL_LAUNCHED_REDUCES=1, SLOTS_MILLIS_MAPS=10102,   FALLOW_SLOTS_MILLIS_REDUCES=0, FALLOW_SLOTS_MILLIS_MAPS=0, TOTAL_LAUNCHED_MAPS=1,   SLOTS_MILLIS_REDUCES=10187}, Map-Reduce Framework={MAP_OUTPUT_MATERIALIZED_BYTES=6,   MAP_INPUT_RECORDS=0, REDUCE_SHUFFLE_BYTES=6, SPILLED_RECORDS=0, MAP_OUTPUT_BYTES=0,   COMMITTED_HEAP_BYTES=231735296, CPU_MILLISECONDS=2570, SPLIT_RAW_BYTES=1017,   COMBINE_INPUT_RECORDS=0, REDUCE_INPUT_RECORDS=0, REDUCE_INPUT_GROUPS=0, COMBINE_OUTPUT_RECORDS=0, PHYSICAL_MEMORY_BYTES=313917440, REDUCE_OUTPUT_RECORDS=0, VIRTUAL_MEMORY_BYTES=2243407872, MAP_OUTPUT_RECORDS=0}, FileSystemCounters={FILE_BYTES_READ=6, HDFS_BYTES_READ=1017, FILE_BYTES_WRITTEN=156962, HDFS_BYTES_WRITTEN=86}, File Output Format Counters ={BYTES_WRITTEN=86}}}}\n14/05/27 01:33:44 INFO crawl.WebTableReader: TOTAL urls:    0\n</code></pre>\n\n<p>Why is this happening? My Regex-filter and Domain-filter are set to allow all domains (I'm trying to do whole web crawling).</p>\n", "creation_date": 1401169061, "score": 1},
{"title": "How to access crawled content from nutch for content categorisation", "view_count": 193, "is_answered": false, "answers": [{"question_id": 23719398, "owner": {"user_id": 973840, "accept_rate": 85, "link": "http://stackoverflow.com/users/973840/diaa", "user_type": "registered", "reputation": 385}, "body": "<p>The crawled content is stored in the data file in the segments directory for example:</p>\n\n<blockquote>\n  <p>segments\\2014...\\content\\part-00000\\data</p>\n</blockquote>\n\n<p>The file type is a sequence file. To read it you can use code from <a href=\"https://www.inkling.com/read/hadoop-definitive-guide-tom-white-3rd/chapter-4/reading-a-sequencefile\" rel=\"nofollow\">the hadoop book</a> or from <a href=\"http://stackoverflow.com/questions/16070587/reading-and-writing-sequencefile-using-hadoop-2-0-apis\">this answer</a> </p>\n", "creation_date": 1400574832, "is_accepted": false, "score": 0, "last_activity_date": 1400574832, "answer_id": 23754772}, {"question_id": 23719398, "owner": {"user_id": 2586595, "link": "http://stackoverflow.com/users/2586595/mohsen89z", "user_type": "registered", "reputation": 128}, "body": "<p>Why don't you use Solr for categorization?</p>\n\n<p>Just write your own plugin and categorize pages before sending them to Solr and store category value in Solr!</p>\n", "creation_date": 1401082830, "is_accepted": false, "score": 0, "last_activity_date": 1401082830, "answer_id": 23863389}], "question_id": 23719398, "tags": ["java", "hdfs", "nutch", "tagging"], "answer_count": 2, "link": "http://stackoverflow.com/questions/23719398/how-to-access-crawled-content-from-nutch-for-content-categorisation", "last_activity_date": 1401082830, "owner": {"age": 26, "answer_count": 2, "creation_date": 1347779481, "user_id": 1675314, "view_count": 14, "location": "Colombo, Sri Lanka", "reputation": 16}, "body": "<p>I am running nutch integrated with Solr for a search engine, the nutch crawl job happens on hadoop. My next requirement is to run a content categorisation job for this crawled content, how can I access the text content that is stored in HDFS for this tagging job, I am planning to run the tagging job with Java, how can I access this content through Java ?</p>\n", "creation_date": 1400396319, "score": 1},
{"title": "Searching a set of keywords in a large string", "view_count": 165, "owner": {"user_id": 1587370, "answer_count": 93, "creation_date": 1344511841, "accept_rate": 80, "view_count": 81, "reputation": 1453}, "is_answered": true, "answers": [{"question_id": 23800718, "owner": {"user_id": 428904, "accept_rate": 82, "link": "http://stackoverflow.com/users/428904/hirak", "user_type": "registered", "reputation": 2676}, "body": "<p>You can use lucene <a href=\"https://lucene.apache.org/core/3_6_0/api/all/org/apache/lucene/analysis/shingle/ShingleFilter.html\" rel=\"nofollow\">ShingleFilter</a></p>\n\n<p>You will find lots of example on the net, here is one <a href=\"http://www.massapi.com/class/sh/ShingleFilter.html\" rel=\"nofollow\">http://www.massapi.com/class/sh/ShingleFilter.html</a></p>\n", "creation_date": 1400744979, "is_accepted": false, "score": -1, "last_activity_date": 1400744979, "answer_id": 23800799}, {"question_id": 23800718, "owner": {"user_id": 3353500, "link": "http://stackoverflow.com/users/3353500/john-petrone", "user_type": "registered", "reputation": 13839}, "body": "<p>You've tagged your question with Elasticsearch - if you're open to using ES I think Percolation with highlighting may fit what you need. You could register each keyword as a separate query with the percolator and then run each document or string thru it. It will return a list of the queries that matched. You can also combine it with highlighting.</p>\n\n<p><a href=\"http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-percolate.html\" rel=\"nofollow\">http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-percolate.html</a></p>\n\n<p><a href=\"http://blog.qbox.io/elasticsesarch-percolator\" rel=\"nofollow\">http://blog.qbox.io/elasticsesarch-percolator</a></p>\n", "creation_date": 1400769637, "is_accepted": true, "score": 1, "last_activity_date": 1400769637, "answer_id": 23810000}], "question_id": 23800718, "tags": ["java", "solr", "lucene", "elasticsearch", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/23800718/searching-a-set-of-keywords-in-a-large-string", "last_activity_date": 1400769637, "accepted_answer_id": 23810000, "body": "<p>I have to do a search in a text file or a large string to check if the text contains a set of keywords (could be millions). If it contains the keywords I have to highlight whatever keywords got matched. What approach should be taken for this? Does lucene provide a solution for this?</p>\n", "creation_date": 1400744757, "score": 1},
{"title": "Add metadata to Crawldb dump", "view_count": 105, "is_answered": false, "question_id": 23802324, "tags": ["nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/23802324/add-metadata-to-crawldb-dump", "last_activity_date": 1400749732, "owner": {"user_id": 3664155, "view_count": 2, "answer_count": 0, "creation_date": 1400747988, "reputation": 21}, "body": "<p>I'm starting with Nutch (trunk version) and I'm spinning around the code without seeing something that seems obvious.</p>\n\n<p>I want to extract the resource of every URLs crawled ( eg: <a href=\"https://stackoverflow.com/questions/ask\">https://stackoverflow.com/questions/ask</a> ===> /question/ask ) expecting two results:\n1. Post the information as an additional field to a Solr instance. I have solved this problem writing an IndexingFilter plugin and works perfectly.\n2. Dumping this information as metadata when the next command it's thrown: bin/nutch readdb -dump crawldb</p>\n\n<p>And at this second point it's where I'm stucked. Reading documentation and other examples it seems I have to use the CrawlDatum but I don't know in what class I have to modify in order to show this information when a dump is made.\nMaybe someone knows where to touch in order to achieve this?</p>\n\n<p>Some help will be appreciated!</p>\n", "creation_date": 1400749549, "score": 1},
{"title": "Performance Benchmarking for Apache Nutch", "view_count": 983, "owner": {"user_id": 56150, "answer_count": 43, "creation_date": 1232187860, "accept_rate": 98, "view_count": 768, "reputation": 1728}, "is_answered": true, "answers": [{"question_id": 472136, "owner": {"user_id": 37379, "accept_rate": 89, "link": "http://stackoverflow.com/users/37379/sam", "user_type": "registered", "reputation": 6017}, "body": "<p>Perhaps the <a href=\"http://lucene.apache.org/nutch/mailing%5Flists.html\" rel=\"nofollow\">nutch mailing list</a> could help you with this.</p>\n", "creation_date": 1247164778, "is_accepted": true, "score": 0, "last_activity_date": 1247164778, "answer_id": 1105756}], "question_id": 472136, "tags": ["apache", "search-engine", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/472136/performance-benchmarking-for-apache-nutch", "last_activity_date": 1400688969, "accepted_answer_id": 1105756, "body": "<p>I want to know if there are any existing benchmarks and sizing information for an apache nutch based search engine deployment. I want to know for say 10 million searches a month what should be the hardware sizing that needs to deployed.</p>\n", "creation_date": 1232694175, "score": 1},
{"title": "How to get indivisual html file from &quot;segments&quot; which i got after crwal in nutch?", "view_count": 20, "is_answered": false, "answers": [{"question_id": 23718707, "owner": {"user_id": 973840, "accept_rate": 85, "link": "http://stackoverflow.com/users/973840/diaa", "user_type": "registered", "reputation": 385}, "body": "<p>Execute the following command:</p>\n\n<pre><code>bin/nutch readseg -dump &lt;segmentDir&gt; &lt;outputDir&gt;\n</code></pre>\n\n<p>and replace segmentDir and outputDir with your segment and output directories (without '&lt;')</p>\n", "creation_date": 1400575052, "is_accepted": false, "score": 0, "last_activity_date": 1400575052, "answer_id": 23754840}], "question_id": 23718707, "tags": ["java", "linux", "indexing", "web-crawler", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/23718707/how-to-get-indivisual-html-file-from-segments-which-i-got-after-crwal-in-nutch", "last_activity_date": 1400575052, "owner": {"user_id": 1977823, "view_count": 4, "answer_count": 0, "creation_date": 1358179892, "reputation": 1}, "body": "<p>I am new to nuth.I have used nutch to crawl some domain.Now i need to get all html file corresponding to the domain in a separate folder.I have got the output from crawler as crawler/linkdb,crawler/crawldb and crawler/segments.Now please help me to proceed through commandline.</p>\n", "creation_date": 1400389170, "score": 0},
{"title": "Customising nutch", "view_count": 70, "owner": {"user_id": 2484414, "answer_count": 2, "creation_date": 1371174989, "accept_rate": 67, "view_count": 28, "reputation": 53}, "is_answered": true, "answers": [{"question_id": 19469575, "owner": {"user_id": 2586595, "link": "http://stackoverflow.com/users/2586595/mohsen89z", "user_type": "registered", "reputation": 128}, "body": "<p>copy content of \"out\" folder into \"build\" folder!</p>\n", "creation_date": 1382252883, "is_accepted": false, "score": 0, "last_activity_date": 1382252883, "answer_id": 19474822}, {"last_edit_date": 1400566765, "owner": {"user_id": 2484414, "accept_rate": 67, "link": "http://stackoverflow.com/users/2484414/julyflowers", "user_type": "registered", "reputation": 53}, "body": "<p>Okay, for beginners :</p>\n\n<p>Download apache ant and install it in the home folder of <code>cygwin</code> (if that's what you are using to run nutch) </p>\n\n<p>Once you make changes to your code, execute the following commands in <code>cygwin</code>, after <code>cd-ing</code> to the project directory.</p>\n\n<p>1) <code>ant clean</code> </p>\n\n<p>2) <code>ant</code></p>\n\n<p>This will clean and build the project. Now run it, the changes in your code will be reflected.</p>\n", "question_id": 19469575, "creation_date": 1400565373, "is_accepted": true, "score": 1, "last_activity_date": 1400566765, "answer_id": 23752012}], "question_id": 19469575, "tags": ["java", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/19469575/customising-nutch", "last_activity_date": 1400566765, "accepted_answer_id": 23752012, "body": "<p>I am using cygwin to run nutch with the command</p>\n\n<pre><code>bin/nutch crawl urls -dir crawl -depth 3 -topN 50\n</code></pre>\n\n<p>Now, I am trying to make changes to crawl.java, but when I run again, the changes are not getting reflected in the output. Infact, even if main parts of the code are REMOVED, the command runs without errors and produces the same output. </p>\n\n<p>How to make the changes in code get reflected in the output? I am a beginner, so kindly help out.</p>\n", "creation_date": 1382207618, "score": 0},
{"title": "Integrating Apache Nutch with Cloudera Hbase and Solr", "view_count": 563, "is_answered": false, "question_id": 23722283, "tags": ["hadoop", "solr", "web-crawler", "nutch", "cloudera"], "answer_count": 0, "link": "http://stackoverflow.com/questions/23722283/integrating-apache-nutch-with-cloudera-hbase-and-solr", "last_activity_date": 1400418712, "owner": {"user_id": 1727204, "answer_count": 15, "creation_date": 1349638210, "accept_rate": 43, "view_count": 102, "reputation": 524}, "body": "<p>I am going to integrate Cloudera hadoop with Apache Nutch. Unfortunately when I try to crawl a website a below exception appear. I configure Nutch with pure Hbase and Solr without any problem but It seems that Cloudera made some changes inside Hbase that Gora module of Nutch can not understand.</p>\n\n<pre><code>./crawl urls/seed.txt testCrawl localhost:8983/solr/ 2\nInjectorJob: starting at 2014-05-18 17:29:33\nInjectorJob: Injecting urlDir: urls/seed.txt\nInjectorJob: org.apache.gora.util.GoraException: java.lang.RuntimeException: java.lang.NumberFormatException: For input string: \"60000\ufffd\ufffdu\ufffd\ufffdTm#PBUF\n\"\nlocalhost\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd(\"\nat org.apache.gora.store.DataStoreFactory.createDataStore(DataStoreFactory.java:167)\nat org.apache.gora.store.DataStoreFactory.createDataStore(DataStoreFactory.java:135)\nat org.apache.nutch.storage.StorageUtils.createWebStore(StorageUtils.java:75)\nat org.apache.nutch.crawl.InjectorJob.run(InjectorJob.java:221)\nat org.apache.nutch.crawl.InjectorJob.inject(InjectorJob.java:251)\nat org.apache.nutch.crawl.InjectorJob.run(InjectorJob.java:273)\nat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\nat org.apache.nutch.crawl.InjectorJob.main(InjectorJob.java:282)\nCaused by: java.lang.RuntimeException: java.lang.NumberFormatException: For input string: \"60000\ufffd\ufffdu\ufffd\ufffdTm#PBUF\n\"\nlocalhost\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd(\"\nat org.apache.gora.hbase.store.HBaseStore.initialize(HBaseStore.java:127)\nat org.apache.gora.store.DataStoreFactory.initializeDataStore(DataStoreFactory.java:102)\nat org.apache.gora.store.DataStoreFactory.createDataStore(DataStoreFactory.java:161)\n... 7 more\nCaused by: java.lang.NumberFormatException: For input string: \"60000\ufffd\ufffdu\ufffd\ufffdTm#PBUF\n\"\nlocalhost\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd(\"\nat java.lang.NumberFormatException.forInputString(NumberFormatException.java:48)\nat java.lang.Integer.parseInt(Integer.java:458)\nat java.lang.Integer.parseInt(Integer.java:499)\nat org.apache.hadoop.hbase.HServerAddress.&lt;init&gt;(HServerAddress.java:63)\nat org.apache.hadoop.hbase.MasterAddressTracker.getMasterAddress(MasterAddressTracker.java:63)\nat org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.getMaster(HConnectionManager.java:354)\nat org.apache.hadoop.hbase.client.HBaseAdmin.&lt;init&gt;(HBaseAdmin.java:94)\nat org.apache.gora.hbase.store.HBaseStore.initialize(HBaseStore.java:109)\n... 9 more\n</code></pre>\n\n<p>Best regards.</p>\n", "creation_date": 1400418712, "score": 1},
{"title": "nutch crawling stops after injector.", "view_count": 353, "owner": {"user_id": 3639189, "view_count": 3, "answer_count": 0, "creation_date": 1400126218, "reputation": 3}, "is_answered": true, "answers": [{"question_id": 23676969, "owner": {"user_id": 1539819, "link": "http://stackoverflow.com/users/1539819/sreemanth-pulagam", "user_type": "registered", "reputation": 613}, "body": "<p>Total number of urls injected is 0. That is nothing to crawl.</p>\n\n<pre><code>Injector: total number of urls rejected by filters: 1\nInjector: total number of urls injected after normalization and filtering: 0\n</code></pre>\n", "creation_date": 1400156211, "is_accepted": true, "score": 0, "last_activity_date": 1400156211, "answer_id": 23677971}], "question_id": 23676969, "tags": ["apache", "web", "generator", "nutch", "web-crawler"], "answer_count": 1, "link": "http://stackoverflow.com/questions/23676969/nutch-crawling-stops-after-injector", "last_activity_date": 1400156211, "accepted_answer_id": 23677971, "body": "<p>here is my cygwin screen looks...</p>\n\n<pre><code>cygpath: can't convert empty path\nInjector: starting at 2014-05-15 16:57:50\nInjector: crawlDb: -dir/crawldb\nInjector: urlDir: urls\nInjector: Converting injected urls to crawl db entries.\nPatch for HADOOP-7682: Instantiating workaround file system\nInjector: total number of urls rejected by filters: 1\nInjector: total number of urls injected after normalization and filtering: 0\nInjector: Merging injected urls into crawl db.\nInjector: overwrite: false\nInjector: update: false\nInjector: finished at 2014-05-15 16:57:52, elapsed: 00:00:02\n</code></pre>\n", "creation_date": 1400153423, "score": 0},
{"title": "How to deploy Apache Nutch -1.6 on tomcat?", "view_count": 274, "is_answered": false, "answers": [{"question_id": 17420530, "owner": {"user_id": 1539819, "link": "http://stackoverflow.com/users/1539819/sreemanth-pulagam", "user_type": "registered", "reputation": 613}, "body": "<p>Nutch 2.2.1 has REST based API to initiate jobs, retrieve data from db( configured store), stop job etc,</p>\n\n<ul>\n<li>Job manager API</li>\n<li>Db read API</li>\n<li>Configuration API</li>\n</ul>\n\n<p><a href=\"http://wiki.apache.org/nutch/Nutch2Tutorial\" rel=\"nofollow\">Nutch2 Tutorial wiki</a>  </p>\n\n<p><a href=\"http://wiki.apache.org/nutch/RunNutchInEclipse\" rel=\"nofollow\">Running Nutch in eclipse</a></p>\n\n<p>To start Nutch 2.2.1</p>\n\n<pre><code>./bin/nutch nutchserver 9000\n</code></pre>\n\n<p>After server successfully started we can access resources using rest requests</p>\n\n<ul>\n<li>Get detault configuration\n<br/> <code>http://localhost:9000/nutch/confs/default</code></li>\n<li><p>Create new crawl job on server\n<br/><code>http://localhost:9000/nutch/jobs</code> \n<br/>   <strong>ContentType:</strong> application/json\n<br/> <strong>http method type:</strong> PUT\n<br/><strong>Payload:</strong></p>\n\n<pre> <code> {\n    \"crawl\":\"123\",\n    \"type\":\"crawl\",\n    \"conf\":\"default\",\n    \"args\":\n        { \"class\":\"org.apache.nutch.crawl.Crawler\", \n          \"seed\":\"http://www.somesite.com\", \n           \"seedDir\":\"runtime/local/url/url.txt\", \"depth\":2 }\n\n  }</code></pre></li>\n</ul>\n", "creation_date": 1400155700, "is_accepted": false, "score": 0, "last_activity_date": 1400155700, "answer_id": 23677776}], "question_id": 17420530, "tags": ["tomcat", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/17420530/how-to-deploy-apache-nutch-1-6-on-tomcat", "last_activity_date": 1400155700, "owner": {"user_id": 1474659, "answer_count": 6, "creation_date": 1340364399, "accept_rate": 48, "view_count": 118, "reputation": 321}, "body": "<p>I am using nutch-1.6 for crawling by triggering commands from terminal. I have searched over the internet and found that earlier versions of nutch like 0.9,1.0 come with war file which can be deployed on tomcat server but i could not find any documentation for deploying nutch 1.6 on tomcat.</p>\n\n<p>Instead of triggering commands from terminal every time, I want to automate crawling system by deploying nutch on tomcat so that i can give command through a web page.</p>\n", "creation_date": 1372752538, "score": 1},
{"title": "Unable to verify crawled data stored in hbase", "view_count": 223, "is_answered": false, "answers": [{"question_id": 23564206, "owner": {"user_id": 1539819, "link": "http://stackoverflow.com/users/1539819/sreemanth-pulagam", "user_type": "registered", "reputation": 613}, "body": "<p>Instead of executing all those steps, can you use below command</p>\n\n<pre><code>./bin/crawl url/seed.txt shoppingcrawl http://localhost:8080/solr 2\n</code></pre>\n\n<p>If you are able to execute successfully, a table will be created in hbase , with name, <strong>shoppingcrawl</strong>_webpage. </p>\n\n<p>we can check by executing below command in hbase shell</p>\n\n<pre><code>hbase&gt; list\n</code></pre>\n\n<p>Then we can scan for specific table. In this case</p>\n\n<pre><code> hbase&gt; scan 'shoppingcrawl_webpage'\n</code></pre>\n", "creation_date": 1400153205, "is_accepted": false, "score": 0, "last_activity_date": 1400153205, "answer_id": 23676891}], "question_id": 23564206, "tags": ["solr", "hbase", "nutch", "web-crawler"], "answer_count": 1, "link": "http://stackoverflow.com/questions/23564206/unable-to-verify-crawled-data-stored-in-hbase", "last_activity_date": 1400153205, "owner": {"user_id": 3523860, "view_count": 3, "answer_count": 0, "creation_date": 1397221873, "reputation": 11}, "body": "<p>I have crawled website using 'nutch' with HBase as a storage back-end. I have referred this tutorial link- <code>http://wiki.apache.org/nutch/Nutch2Tutorial</code>.</p>\n\n<p>Nutch version is 2.2.1, HBase version 0.90.4 and Solr version 4.7.1</p>\n\n<p>Here are the steps I used-</p>\n\n<p>./runtime/local/bin/nutch inject urls</p>\n\n<p>./runtime/local/bin/nutch generate -topN 100 -adddays 30</p>\n\n<p>./runtime/local/bin/nutch fetch -all</p>\n\n<p>./runtime/local/bin/nutch fetch -all</p>\n\n<p>./runtime/local/bin/nutch updatedb</p>\n\n<pre><code>./runtime/local/bin/nutch solrindex http://localhost:8983/solr/ -all\n</code></pre>\n\n<p>My url/seed.txt file contains-\n<code>http://www.xyzshoppingsite.com/mobiles/</code></p>\n\n<p>And I have kept only below line in 'regex-urlfilter.txt' file (all other regex are commented).</p>\n\n<p><code>+^http://([a-z0-9]*\\.)*xyzshoppingsite.com/mobile/*</code></p>\n\n<p>At the end of the crawl, I can see a table \"webpage\" created in the HBase but I am unable to verify whether all and complete data have been crawled or not. \nWhen searched in Solr, it shows nothing, 0 result.</p>\n\n<p>My ultimate intention is to get the complete data present in all pages under  mobile in above URL. </p>\n\n<p>Could you please let me know,</p>\n\n<ul>\n<li><p>How to verify crawled data present in HBase?</p></li>\n<li><p>Solr log directory contains 0 files so I am unable to get a breakthrough. How to resolve this?</p></li>\n<li><p>Output of HBase command <code>scan \"webpage\"</code> shows only timestamp data and other data as </p>\n\n<p>'<code>value=\\x0A\\x0APlease Wait ... Redirecting to &lt;a href=\"/mobiles\"&gt;&lt;b&gt;http://www.xyzshoppingsite.com/mobiles&lt;/b&gt;&lt;/a&gt;Please Wait ... Redirecting to &lt;a href=\"/mobiles\"&gt;&lt;b&gt;http://www.xyzshoppingsite.com/mobiles&lt;/b&gt;&lt;/a&gt;</code>'</p></li>\n</ul>\n\n<p>Here, why is the data crawled like this and not the actual contents of page after redirection? </p>\n\n<p>Please help. Thanks in advance.</p>\n\n<p>Thanks and Regards!</p>\n", "creation_date": 1399637062, "score": 1},
{"title": "Nutch not working on linux environment", "view_count": 212, "is_answered": false, "answers": [{"question_id": 22325874, "owner": {"user_id": 1436703, "accept_rate": 60, "link": "http://stackoverflow.com/users/1436703/iceberg", "user_type": "registered", "reputation": 883}, "body": "<p>You can't run this command \"crawlresult\". You have to use a command from the command list of nutch script. if you crawl with nutch, you use this <a href=\"http://wiki.apache.org/nutch/NutchTutorial\" rel=\"nofollow\">tutorials</a> </p>\n", "creation_date": 1400152014, "is_accepted": false, "score": 0, "last_activity_date": 1400152014, "answer_id": 23676467}], "question_id": 22325874, "tags": ["linux", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/22325874/nutch-not-working-on-linux-environment", "last_activity_date": 1400152014, "owner": {"user_id": 1784775, "answer_count": 3, "creation_date": 1351579848, "accept_rate": 40, "view_count": 11, "reputation": 60}, "body": "<p>I've installed Nutch on linux system. When I go in directory 'bin' and run <code>./nutch</code>, it shows following  - </p>\n\n<pre><code>Usage: nutch COMMAND\nwhere COMMAND is one of:\n  crawl             one-step crawler for intranets (DEPRECATED - USE CRAWL SCRIPT INSTEAD)\n  readdb            read / dump crawl db\n  mergedb           merge crawldb-s, with optional filtering\n  readlinkdb        read / dump link db\n  inject            inject new urls into the database\n  generate          generate new segments to fetch from crawl db\n  freegen           generate new segments to fetch from text files\n  fetch             fetch a segment's pages\n  parse             parse a segment's pages\n  readseg           read / dump segment data\n  mergesegs         merge several segments, with optional filtering and slicing\n  updatedb          update crawl db from segments after fetching\n  invertlinks       create a linkdb from parsed segments\n  mergelinkdb       merge linkdb-s, with optional filtering\n  index             run the plugin-based indexer on parsed segments and linkdb\n  solrindex         run the solr indexer on parsed segments and linkdb\n  solrdedup         remove duplicates from solr\n  solrclean         remove HTTP 301 and 404 documents from solr\n  clean             remove HTTP 301 and 404 documents from indexing backends configured via plugins\n  parsechecker      check the parser for a given url\n  indexchecker      check the indexing filters for a given url\n  domainstats       calculate domain statistics from crawldb\n  webgraph          generate a web graph from existing segments\n  linkrank          run a link analysis program on the generated web graph\n  scoreupdater      updates the crawldb with linkrank scores\n  nodedumper        dumps the web graph's node scores\n  plugin            load a plugin and run one of its classes main()\n  junit             runs the given JUnit test\n or\n  CLASSNAME         run the class named CLASSNAME\nMost commands print help when invoked w/o parameters.\n</code></pre>\n\n<p>The above output shows that Nutch is correctly installed on this system. But next when I run <code>./nutch crawlresult urls -dir crawl -depth 3</code> I get following output - </p>\n\n<pre><code>./nutch: line 272: /usr/java/jdk1.7.0/bin/java: Success\n</code></pre>\n\n<p>whereas I was expecting the nutch to begin crawling and show the logs. Please tell me what is wrong?</p>\n", "creation_date": 1394541178, "score": 0},
{"title": "apache nutch dir is created, but result is empty", "view_count": 73, "is_answered": false, "question_id": 23669129, "tags": ["apache", "web", "solr", "web-crawler", "nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/23669129/apache-nutch-dir-is-created-but-result-is-empty", "last_activity_date": 1400127174, "owner": {"user_id": 3639189, "view_count": 3, "answer_count": 0, "creation_date": 1400126218, "reputation": 3}, "body": "<p>my output is as follows..no error and no result....\n$ ./crawl urls -dir crawl -depth 3 -topN 5\ncygpath: can't convert empty path\nInjector: starting at 2014-05-15 09:25:20\nInjector: crawlDb: -dir/crawldb\nInjector: urlDir: urls\nInjector: Converting injected urls to crawl db entries.\nPatch for HADOOP-7682: Instantiating workaround file system\nInjector: total number of urls rejected by filters: 1\nInjector: total number of urls injected after normalization and filtering: 0\nInjector: Merging injected urls into crawl db.\nInjector: overwrite: false\nInjector: update: false\nInjector: finished at 2014-05-15 09:25:23, elapsed: 00:00:03</p>\n", "creation_date": 1400127174, "score": 0},
{"title": "How to add HTTP Headers to the nutch index?", "view_count": 95, "is_answered": false, "question_id": 23620350, "tags": ["nutch", "websolr"], "answer_count": 0, "link": "http://stackoverflow.com/questions/23620350/how-to-add-http-headers-to-the-nutch-index", "last_activity_date": 1399935953, "owner": {"user_id": 1332380, "view_count": 4, "answer_count": 3, "creation_date": 1334346195, "reputation": 106}, "body": "<p>I'm attempting to use nutch with <a href=\"https://websolr.com\" rel=\"nofollow\">Websolr</a>. Their authentication method <a href=\"https://websolr.com/guides/websolr-advanced-auth\" rel=\"nofollow\">requires adding headers</a> to HTTP updates.</p>\n\n<p>To accomplish this I imagine I would have to write my own Nutch plugin and extend the <a href=\"http://svn.apache.org/viewvc/nutch/trunk/src/plugin/indexer-solr/src/java/org/apache/nutch/indexwriter/solr/SolrUtils.java?revision=1453776&amp;view=markup\" rel=\"nofollow\">getCommonsHttpSolrServer</a> method. Is there a way to do this without writing my own plugin?</p>\n", "creation_date": 1399935953, "score": 1},
{"title": "Which version of hadoop I need for running nutch?", "view_count": 181, "is_answered": false, "answers": [{"question_id": 23545432, "owner": {"user_id": 2225199, "link": "http://stackoverflow.com/users/2225199/nimeshjm", "user_type": "registered", "reputation": 1137}, "body": "<p>If you get Nutch 1.8, Hadoop will be bundled with the Nutch distribution. It has Hadoop 1.2.</p>\n", "creation_date": 1399841425, "is_accepted": false, "score": 0, "last_activity_date": 1399841425, "answer_id": 23597779}], "question_id": 23545432, "tags": ["hadoop", "version", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/23545432/which-version-of-hadoop-i-need-for-running-nutch", "last_activity_date": 1399841425, "owner": {"user_id": 1966107, "view_count": 4, "answer_count": 0, "creation_date": 1357808983, "reputation": 1}, "body": "<p>I'm pretty confused about what version and distribution of hadoop I can use with nutch 1 and nutch 2.\nDo I need \"plain\" Apache hadoop or can use e.g. Cloudera? In any case I would like to know dependencies of versions of nutch and hadoop.</p>\n", "creation_date": 1399561043, "score": 0},
{"title": "Nutch 1.2 No URLs to fetch", "view_count": 57, "is_answered": false, "question_id": 23555403, "tags": ["nutch", "web-crawler"], "answer_count": 0, "link": "http://stackoverflow.com/questions/23555403/nutch-1-2-no-urls-to-fetch", "last_activity_date": 1399599939, "owner": {"user_id": 1102835, "answer_count": 1, "creation_date": 1324074456, "accept_rate": 11, "view_count": 106, "reputation": 831}, "body": "<p>i have installed cygwin, copied nutch-1.2 inside cygwin directory.</p>\n\n<p>created nutch-1.2/urls/seed.txt with <a href=\"http://amac4.blogspot.co.uk\" rel=\"nofollow\">http://amac4.blogspot.co.uk</a></p>\n\n<p>regex-urifilter has </p>\n\n<pre><code># accept anything else\n+^http://amac4.blogspot.co.uk/\n</code></pre>\n\n<p>nutch-site has</p>\n\n<pre><code>    &lt;configuration&gt;\n    &lt;name&gt;http.agent.name&lt;/name&gt;\n    &lt;value&gt;My Nutch Spider&lt;/value&gt;\n    &lt;/configuration&gt;\n</code></pre>\n\n<p>what am i missing here.</p>\n", "creation_date": 1399599939, "score": 0},
{"title": "How to see data crawled by nutch using solr?", "view_count": 1053, "is_answered": false, "answers": [{"question_id": 23014882, "owner": {"user_id": 3533126, "accept_rate": 91, "link": "http://stackoverflow.com/users/3533126/lizh", "user_type": "registered", "reputation": 108}, "body": "<p>The simplest way to validate your data sounds like what you are trying to do: query the data and make sure it returns the expected results. \nSome help there:</p>\n\n<p>When you say you tried a basic query string, do you mean from the solr admin, or through the rest API?   If you are using the solr admin, you don't need to escape that first *. So q would be <em>:</em> directly.  In the Rest API, the * needs to be properly encoded. Something like this:</p>\n\n<pre><code>http://your_host_name:8888/solr/your_core_name/select?q=*%3A*&amp;wt=json&amp;indent=true\n</code></pre>\n\n<p>Another thing you can do is validate some of nutch's intermediary data is to dump the crawl or link dbs using the bin/nutch commands readdb, readlinkdb, mergedb.</p>\n", "creation_date": 1399541637, "is_accepted": false, "score": 0, "last_activity_date": 1399541637, "answer_id": 23537946}], "question_id": 23014882, "tags": ["apache", "solr", "lucene", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/23014882/how-to-see-data-crawled-by-nutch-using-solr", "last_activity_date": 1399541637, "owner": {"user_id": 3523860, "view_count": 3, "answer_count": 0, "creation_date": 1397221873, "reputation": 11}, "body": "<p>I am new to Nutch and Solr. So, I apologize in advance if I am asking basic  question.</p>\n\n<p>Details of environment:</p>\n\n<ol>\n<li>Virtual Box with Guest OS: Ubuntu 12.04.4, Host OS: Windows 8<br></li>\n<li>Nutch Release: Apache nutch 1.7 <br></li>\n<li>Solr Release: Apache Solr 3.6.2<br></li>\n<li>Referring to wiki.apache.org/nutch/NutchTutorial<br></li>\n</ol>\n\n<p>I initiated crawling with command-</p>\n\n<pre><code>bin/nutch crawl urls -solr http://&lt;code&gt;mylocalhost&lt;code&gt;:8983/solr/ -depth 3 -topN 5\n</code></pre>\n\n<p>This command succeeded with no errors.</p>\n\n<p>After that, I opened the solr admin page in browser and tried to search with a default query string: <code>\\*:*</code>.  However, this resulted in a page with the below content:</p>\n\n<pre><code>This XML file does not appear to have any style information associated with it. The document tree is shown below.\n&lt;response&gt;\n    &lt;lst name=\"responseHeader\"&gt;\n        &lt;int name=\"status\"&gt;0&lt;/int&gt;\n        &lt;int name=\"QTime\"&gt;0&lt;/int&gt;\n        &lt;lst name=\"params\"&gt;\n            &lt;str name=\"start\"&gt;0&lt;/str&gt;\n            &lt;str name=\"q\"&gt;*:*&lt;/str&gt;\n            &lt;str name=\"rows\"&gt;10&lt;/str&gt;\n            &lt;str name=\"indent\"&gt;on&lt;/str&gt;\n            &lt;str name=\"version\"&gt;2.2&lt;/str&gt;\n        &lt;/lst&gt;\n    &lt;/lst&gt;\n    &lt;result name=\"response\" numFound=\"0\" start=\"0\"/&gt;\n&lt;/response&gt;\n</code></pre>\n\n<p>When I tried to search for 'nutch' in solr, it resulted in an error: \"HTTP Error 400\". </p>\n\n<p>Could you please help me see data crawled by nutch so that I can validate it.</p>\n", "creation_date": 1397225066, "score": 1},
{"title": "Apache Nutch Error", "view_count": 451, "is_answered": false, "question_id": 23528351, "tags": ["apache", "solr", "hbase", "nutch", "web-crawler"], "answer_count": 0, "link": "http://stackoverflow.com/questions/23528351/apache-nutch-error", "last_activity_date": 1399496430, "owner": {"user_id": 3530034, "answer_count": 5, "creation_date": 1397427012, "accept_rate": 90, "view_count": 8, "location": "United Kingdom", "reputation": 48}, "body": "<p>I am completely new to Nutch Solr, and Hbase. I have installed the following</p>\n\n<p>apache-nutch-2.2.1, </p>\n\n<p>hbase-0.9.4, and </p>\n\n<p>solr-3.6.2. </p>\n\n<p>I am currently running OS X Mavericks and I have Java 1.7 installed</p>\n\n<p>Every other thing seems to work fine except when I try crawling, I get some few errors with actually no work done which is shown below:</p>\n\n<pre><code>mac:local engrsnmusa$ bin/crawl urls/seed.txt TestCrawl http://localhost:8983/solr/ 2\n</code></pre>\n\n<p>InjectorJob: starting at 2014-05-07 21:09:56</p>\n\n<p>InjectorJob: Injecting urlDir: urls/seed.txt</p>\n\n<p>2014-05-07 21:09:56.720 java[2865:1903] Unable to load realm info from SCDynamicStore</p>\n\n<p>InjectorJob: Using class org.apache.gora.memory.store.MemStore as the Gora storage class.</p>\n\n<p>InjectorJob: total number of urls rejected by filters: 0</p>\n\n<p>InjectorJob: total number of urls injected after normalization and filtering: 1</p>\n\n<p>Injector: finished at 2014-05-07 21:10:02, elapsed: 00:00:06</p>\n\n<p>Wed 7 May 2014 21:10:02 BST : Iteration 1 of 2</p>\n\n<p>Generating batchId</p>\n\n<p>Generating a new fetchlist</p>\n\n<p>GeneratorJob: starting at 2014-05-07 21:10:07</p>\n\n<p>GeneratorJob: Selecting best-scoring urls due for fetch.</p>\n\n<p>GeneratorJob: starting</p>\n\n<p>GeneratorJob: filtering: false</p>\n\n<p>GeneratorJob: normalizing: false</p>\n\n<p>GeneratorJob: topN: 50000</p>\n\n<p>2014-05-07 21:10:08.129 java[2874:1903] Unable to load realm info from SCDynamicStore</p>\n\n<p>GeneratorJob: finished at 2014-05-07 21:10:12, time elapsed: 00:00:04</p>\n\n<p>GeneratorJob: generated batch id: 1399493402-24778</p>\n\n<p>Fetching : </p>\n\n<p>FetcherJob: starting</p>\n\n<p>FetcherJob: batchId: 1399493402-24778</p>\n\n<p>Fetcher: Your 'http.agent.name' value should be listed first in 'http.robots.agents' property.</p>\n\n<p>FetcherJob: threads: 50</p>\n\n<p>FetcherJob: parsing: false</p>\n\n<p>FetcherJob: resuming: false</p>\n\n<p>FetcherJob : timelimit set for : 1399504217796</p>\n\n<p>2014-05-07 21:10:18.207 java[2881:1903] Unable to load realm info from SCDynamicStore</p>\n\n<p>Using queue mode : byHost</p>\n\n<p>Fetcher: threads: 50</p>\n\n<p>QueueFeeder finished: total 0 records. Hit by time limit :0</p>\n\n<p>-finishing thread FetcherThread0, activeThreads=0</p>\n\n<p>-finishing thread FetcherThread1, activeThreads=0</p>\n\n<p>-finishing thread FetcherThread2, activeThreads=0</p>\n\n<p>-finishing thread FetcherThread3, activeThreads=0</p>\n\n<p>-finishing thread FetcherThread4, activeThreads=0</p>\n\n<p>-finishing thread FetcherThread5, activeThreads=0</p>\n\n<p>-finishing thread FetcherThread6, activeThreads=0</p>\n\n<p>-finishing thread FetcherThread7, activeThreads=0</p>\n\n<p>-finishing thread FetcherThread8, activeThreads=0</p>\n\n<p>-finishing thread FetcherThread9, activeThreads=0</p>\n\n<p>-finishing thread FetcherThread10, activeThreads=0</p>\n\n<p>-finishing thread FetcherThread11, activeThreads=0</p>\n\n<p>-finishing thread FetcherThread12, activeThreads=0</p>\n\n<p>-finishing thread FetcherThread13, activeThreads=0</p>\n\n<p>-finishing thread FetcherThread14, activeThreads=0</p>\n\n<p>-finishing thread FetcherThread15, activeThreads=0</p>\n\n<p>-finishing thread FetcherThread16, activeThreads=0</p>\n\n<p>-finishing thread FetcherThread17, activeThreads=0</p>\n\n<p>-finishing thread FetcherThread18, activeThreads=0</p>\n\n<p>-finishing thread FetcherThread19, activeThreads=0</p>\n\n<p>-finishing thread FetcherThread20, activeThreads=0</p>\n\n<p>-finishing thread FetcherThread21, activeThreads=0</p>\n\n<p>-finishing thread FetcherThread22, activeThreads=0</p>\n\n<p>-finishing thread FetcherThread23, activeThreads=0</p>\n\n<p>-finishing thread FetcherThread24, activeThreads=0</p>\n\n<p>-finishing thread FetcherThread25, activeThreads=0</p>\n\n<p>-finishing thread FetcherThread26, activeThreads=0</p>\n\n<p>-finishing thread FetcherThread27, activeThreads=0</p>\n\n<p>-finishing thread FetcherThread28, activeThreads=0</p>\n\n<p>-finishing thread FetcherThread29, activeThreads=0</p>\n\n<p>-finishing thread FetcherThread30, activeThreads=0</p>\n\n<p>-finishing thread FetcherThread31, activeThreads=0</p>\n\n<p>-finishing thread FetcherThread32, activeThreads=0</p>\n\n<p>-finishing thread FetcherThread33, activeThreads=0</p>\n\n<p>-finishing thread FetcherThread34, activeThreads=0\n-finishing thread FetcherThread35, activeThreads=0</p>\n\n<p>-finishing thread FetcherThread36, activeThreads=0</p>\n\n<p>-finishing thread FetcherThread37, activeThreads=0</p>\n\n<p>-finishing thread FetcherThread38, activeThreads=0</p>\n\n<p>-finishing thread FetcherThread39, activeThreads=0</p>\n\n<p>-finishing thread FetcherThread40, activeThreads=0</p>\n\n<p>-finishing thread FetcherThread41, activeThreads=0</p>\n\n<p>-finishing thread FetcherThread42, activeThreads=0</p>\n\n<p>-finishing thread FetcherThread43, activeThreads=0</p>\n\n<p>-finishing thread FetcherThread44, activeThreads=0</p>\n\n<p>-finishing thread FetcherThread45, activeThreads=0</p>\n\n<p>-finishing thread FetcherThread46, activeThreads=0</p>\n\n<p>-finishing thread FetcherThread47, activeThreads=0</p>\n\n<p>-finishing thread FetcherThread48, activeThreads=0</p>\n\n<p>Fetcher: throughput threshold: -1</p>\n\n<p>Fetcher: throughput threshold sequence: 5</p>\n\n<p>-finishing thread FetcherThread49, activeThreads=0</p>\n\n<p>0/0 spinwaiting/active, 0 pages, 0 errors, 0.0 0 pages/s, 0 0 kb/s, 0 URLs in 0 queues</p>\n\n<p>-activeThreads=0</p>\n\n<p>FetcherJob: done</p>\n\n<p>Parsing : </p>\n\n<p>ParserJob: starting</p>\n\n<p>ParserJob: resuming:    false</p>\n\n<p>ParserJob: forced reparse:  false</p>\n\n<p>ParserJob: batchId: 1399493402-24778</p>\n\n<p>2014-05-07 21:10:36.065 java[2888:1903] Unable to load realm info from SCDynamicStore</p>\n\n<p>ParserJob: success</p>\n\n<p>CrawlDB update for TestCrawl</p>\n\n<p>DbUpdaterJob: starting</p>\n\n<p>2014-05-07 21:10:44.649 java[2895:1903] Unable to load realm info from SCDynamicStore</p>\n\n<p>DbUpdaterJob: done</p>\n\n<pre><code>Indexing TestCrawl on SOLR index -&gt; http://localhost:8983/solr/\n</code></pre>\n\n<p>SolrIndexerJob: starting</p>\n\n<p>2014-05-07 21:10:47.965 java[2902:1903] Unable to load realm info from SCDynamicStore</p>\n\n<p>SolrIndexerJob: done.</p>\n\n<pre><code>SOLR dedup -&gt; \"http://localhost:8983/solr/\"\n</code></pre>\n\n<p>2014-05-07 21:10:51.160 java[2909:1903] Unable to load realm info from SCDynamicStore</p>\n\n<p>Wed 7 May 2014 21:10:54 BST : Iteration 2 of 2</p>\n\n<p>Generating batchId</p>\n\n<p>Generating a new fetchlist</p>\n\n<p>GeneratorJob: starting at 2014-05-07 21:11:12</p>\n\n<p>GeneratorJob: Selecting best-scoring urls due for fetch.</p>\n\n<p>GeneratorJob: starting</p>\n\n<p>GeneratorJob: filtering: false</p>\n\n<p>GeneratorJob: normalizing: false</p>\n\n<p>GeneratorJob: topN: 50000</p>\n\n<p>2014-05-07 21:11:13.955 java[2919:1903] Unable to load realm info from SCDynamicStore</p>\n\n<p>GeneratorJob: finished at 2014-05-07 21:11:19, time elapsed: 00:00:06</p>\n\n<p>GeneratorJob: generated batch id: 1399493454-2657</p>\n\n<p>Fetching : </p>\n\n<p>FetcherJob: starting</p>\n\n<p>FetcherJob: batchId: 1399493454-2657</p>\n\n<p>Fetcher: Your 'http.agent.name' value should be listed first in 'http.robots.agents' property.</p>\n\n<p>FetcherJob: threads: 50</p>\n\n<p>FetcherJob: parsing: false</p>\n\n<p>FetcherJob: resuming: false</p>\n\n<p>FetcherJob : timelimit set for : 1399504284866</p>\n\n<p>2014-05-07 21:11:25.322 java[2930:1903] Unable to load realm info from SCDynamicStore</p>\n\n<p>Using queue mode : byHost</p>\n\n<p>Fetcher: threads: 50</p>\n\n<p>QueueFeeder finished: total 0 records. Hit by time limit :0</p>\n\n<p>-finishing thread FetcherThread0, activeThreads=0</p>\n\n<p>-finishing thread FetcherThread1, activeThreads=0</p>\n\n<p>-finishing thread FetcherThread2, activeThreads=0</p>\n\n<p>-finishing thread FetcherThread3, activeThreads=0</p>\n\n<p>-finishing thread FetcherThread4, activeThreads=0</p>\n\n<p>-finishing thread FetcherThread5, activeThreads=0</p>\n\n<p>-finishing thread FetcherThread6, activeThreads=0</p>\n\n<p>-finishing thread FetcherThread7, activeThreads=0</p>\n\n<p>-finishing thread FetcherThread8, activeThreads=0</p>\n\n<p>-finishing thread FetcherThread9, activeThreads=0</p>\n\n<p>-finishing thread FetcherThread10, activeThreads=0</p>\n\n<p>-finishing thread FetcherThread11, activeThreads=0</p>\n\n<p>-finishing thread FetcherThread12, activeThreads=0</p>\n\n<p>-finishing thread FetcherThread13, activeThreads=0</p>\n\n<p>-finishing thread FetcherThread14, activeThreads=0</p>\n\n<p>-finishing thread FetcherThread15, activeThreads=0</p>\n\n<p>-finishing thread FetcherThread16, activeThreads=0</p>\n\n<p>-finishing thread FetcherThread17, activeThreads=0</p>\n\n<p>-finishing thread FetcherThread18, activeThreads=0</p>\n\n<p>-finishing thread FetcherThread19, activeThreads=0</p>\n\n<p>-finishing thread FetcherThread20, activeThreads=0</p>\n\n<p>-finishing thread FetcherThread21, activeThreads=0</p>\n\n<p>-finishing thread FetcherThread22, activeThreads=0</p>\n\n<p>-finishing thread FetcherThread23, activeThreads=0</p>\n\n<p>-finishing thread FetcherThread24, activeThreads=0</p>\n\n<p>-finishing thread FetcherThread26, activeThreads=1</p>\n\n<p>-finishing thread FetcherThread25, activeThreads=0</p>\n\n<p>-finishing thread FetcherThread27, activeThreads=0</p>\n\n<p>-finishing thread FetcherThread28, activeThreads=0</p>\n\n<p>-finishing thread FetcherThread29, activeThreads=0</p>\n\n<p>-finishing thread FetcherThread31, activeThreads=0</p>\n\n<p>-finishing thread FetcherThread32, activeThreads=0</p>\n\n<p>-finishing thread FetcherThread33, activeThreads=0</p>\n\n<p>-finishing thread FetcherThread30, activeThreads=0</p>\n\n<p>-finishing thread FetcherThread34, activeThreads=0</p>\n\n<p>-finishing thread FetcherThread35, activeThreads=0</p>\n\n<p>-finishing thread FetcherThread36, activeThreads=0</p>\n\n<p>-finishing thread FetcherThread37, activeThreads=0</p>\n\n<p>-finishing thread FetcherThread38, activeThreads=0</p>\n\n<p>-finishing thread FetcherThread39, activeThreads=0</p>\n\n<p>-finishing thread FetcherThread40, activeThreads=0</p>\n\n<p>-finishing thread FetcherThread41, activeThreads=0</p>\n\n<p>-finishing thread FetcherThread42, activeThreads=0</p>\n\n<p>-finishing thread FetcherThread43, activeThreads=0</p>\n\n<p>-finishing thread FetcherThread44, activeThreads=0</p>\n\n<p>-finishing thread FetcherThread45, activeThreads=0</p>\n\n<p>-finishing thread FetcherThread46, activeThreads=0</p>\n\n<p>-finishing thread FetcherThread47, activeThreads=0</p>\n\n<p>-finishing thread FetcherThread48, activeThreads=0</p>\n\n<p>Fetcher: throughput threshold: -1</p>\n\n<p>Fetcher: throughput threshold sequence: 5</p>\n\n<p>-finishing thread FetcherThread49, activeThreads=0</p>\n\n<p>0/0 spinwaiting/active, 0 pages, 0 errors, 0.0 0 pages/s, 0 0 kb/s, 0 URLs in 0 queues</p>\n\n<p>-activeThreads=0</p>\n\n<p>FetcherJob: done</p>\n\n<p>Parsing : </p>\n\n<p>ParserJob: starting</p>\n\n<p>ParserJob: resuming:    false</p>\n\n<p>ParserJob: forced reparse:  false</p>\n\n<p>ParserJob: batchId: 1399493454-2657</p>\n\n<p>2014-05-07 21:11:41.723 java[2937:1903] Unable to load realm info from SCDynamicStore</p>\n\n<p>ParserJob: success</p>\n\n<p>CrawlDB update for TestCrawl</p>\n\n<p>DbUpdaterJob: starting</p>\n\n<p>2014-05-07 21:11:49.314 java[2944:1903] Unable to load realm info from SCDynamicStore</p>\n\n<p>DbUpdaterJob: done</p>\n\n<pre><code>Indexing TestCrawl on SOLR index -&gt; http://localhost:8983/solr/\n</code></pre>\n\n<p>SolrIndexerJob: starting</p>\n\n<p>2014-05-07 21:11:52.281 java[2951:1903] Unable to load realm info from SCDynamicStore</p>\n\n<p>SolrIndexerJob: done.</p>\n\n<pre><code>SOLR dedup -&gt; http://localhost:8983/solr/\n</code></pre>\n\n<p>2014-05-07 21:11:55.373 java[2958:1903] Unable to load realm info from SCDynamicStore</p>\n\n<p>mac:local engrsnmusa$ </p>\n\n<p>Thanks in advance for helping</p>\n", "creation_date": 1399496430, "score": 1},
{"title": "Nutch - deleting segments", "view_count": 116, "is_answered": false, "answers": [{"question_id": 23422174, "owner": {"user_id": 937918, "accept_rate": 23, "link": "http://stackoverflow.com/users/937918/chethan", "user_type": "registered", "reputation": 180}, "body": "<p>Thanks to the help on the Nutch mailing list, I found out that I can delete those segments.</p>\n", "creation_date": 1399354546, "is_accepted": false, "score": 0, "last_activity_date": 1399354546, "answer_id": 23486668}], "question_id": 23422174, "tags": ["solr", "web-crawler", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/23422174/nutch-deleting-segments", "last_activity_date": 1399354546, "owner": {"user_id": 937918, "answer_count": 15, "creation_date": 1315635668, "accept_rate": 23, "view_count": 44, "location": "Bangalore", "reputation": 180}, "body": "<p>I have a Nutch crawl with 4 segments which are fully indexed using the <code>bin/nutch solrindex</code> command. Now I'm all out of storage on the box, so can I delete the 4 segments and retain only the crawldb and continue crawling from where I left it?</p>\n\n<p>Since all the segments are merged and indexed to Solr I don't see a problem in deleting the segments, or am I wrong there?</p>\n", "creation_date": 1399012453, "score": 0},
{"title": "nutch running on EMR", "view_count": 276, "is_answered": false, "answers": [{"question_id": 17777525, "owner": {"user_id": 937918, "accept_rate": 23, "link": "http://stackoverflow.com/users/937918/chethan", "user_type": "registered", "reputation": 180}, "body": "<p>In case you're still looking for an answer: </p>\n\n<p>When you build Nutch, you will see a job jar in the deploy directory, upload this to S3 and reference that as you're custom Jar while setting up the EMR job flow. </p>\n\n<p>You can then add steps and mention the main class for example: <code>org.apache.nutch.crawl.Crawl</code> and the arguments that you would want. This doesn't change from the way it works in the <code>local</code> mode. For example: <code>urls -dir myCrawl -threads 10 -depth 5 -topN 1000</code>.</p>\n\n<p>You can get to know what main class to use by looking at the <code>bin/nutch</code> script if you intend to use something other than Crawl.java.</p>\n", "creation_date": 1399287760, "is_accepted": false, "score": 0, "last_activity_date": 1399287760, "answer_id": 23470892}], "question_id": 17777525, "tags": ["nutch", "amazon-emr"], "answer_count": 1, "link": "http://stackoverflow.com/questions/17777525/nutch-running-on-emr", "last_activity_date": 1399287760, "owner": {"user_id": 91324, "answer_count": 10, "creation_date": 1239830824, "accept_rate": 32, "view_count": 128, "reputation": 153}, "body": "<p>Can somebody please guide me into the right direction.  I'm trying to get Nutch working on Amazon EMR.  So far, I can get nutch running locally and getting it launched using the shell scripts that come with it.</p>\n\n<p>However, on Amazon, I need to specify a JAR location and options.  I can get the jar by compiling it myself.  However, I don't know where to start as far as the startup options are concerned.</p>\n\n<p>Additionally, what is the main difference between the 1.x and Nutch 2.0.  Is one recommended on EMR over the other?</p>\n", "creation_date": 1374445033, "score": 1},
{"title": "Nutch : Anchor text of current URL", "view_count": 298, "owner": {"user_id": 937918, "answer_count": 15, "creation_date": 1315635668, "accept_rate": 23, "view_count": 44, "location": "Bangalore", "reputation": 180}, "is_answered": true, "answers": [{"question_id": 12774660, "owner": {"user_id": 937918, "accept_rate": 23, "link": "http://stackoverflow.com/users/937918/chethan", "user_type": "registered", "reputation": 180}, "body": "<p>The anchor text is found in the inlinks, but for this to be populated, the <code>db.ignore.internal.links</code> has to be set to <code>false</code>.</p>\n", "creation_date": 1399012866, "is_accepted": true, "score": 0, "last_activity_date": 1399012866, "answer_id": 23422262}], "question_id": 12774660, "tags": ["nutch", "web-crawler"], "answer_count": 1, "link": "http://stackoverflow.com/questions/12774660/nutch-anchor-text-of-current-url", "last_activity_date": 1399012866, "accepted_answer_id": 23422262, "body": "<p>In an indexing filter, is there a way to figure out the Anchor text from which the current URL/document originated from? I tried the inlinks but that seems to be null.</p>\n\n<pre><code>public NutchDocument filter(NutchDocument doc, Parse parse, Text url, CrawlDatum datum,          Inlinks inlinks) IndexingException {\n\n    //Need to know the anchor text from which the current document originated from at this  point\n\n}\n</code></pre>\n\n<p>If the current URL is say <a href=\"http://foo.com/pagex\" rel=\"nofollow\">http://foo.com/pagex</a> the the link to pagex must've been found at <a href=\"http://foo.com\" rel=\"nofollow\">http://foo.com</a>. I need to know the anchor text of this link.</p>\n", "creation_date": 1349663552, "score": 1},
{"title": "Nutch-SOLR Formed Based Authentication", "view_count": 63, "is_answered": false, "question_id": 23414061, "tags": ["solr", "forms-authentication", "web-crawler", "nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/23414061/nutch-solr-formed-based-authentication", "last_activity_date": 1398971635, "owner": {"user_id": 3593773, "view_count": 2, "answer_count": 1, "creation_date": 1398968354, "reputation": 33}, "body": "<p>I was going through various links from this forum and other forum. However I was unable to get the exact and good code for this.</p>\n\n<p>Finally I got it and hence wanted to share with you guys.</p>\n\n<p>I could not post the answer so you can see answer given by Jayesh Bhoyar is the correct answer for this question</p>\n", "creation_date": 1398968942, "score": 1},
{"title": "how reading nutch generated content data on the segment folder using java", "view_count": 1471, "owner": {"user_id": 191300, "answer_count": 70, "creation_date": 1255709181, "accept_rate": 55, "view_count": 102, "reputation": 2168}, "is_answered": true, "answers": [{"question_id": 7506890, "owner": {"user_id": 191300, "accept_rate": 55, "link": "http://stackoverflow.com/users/191300/surajz", "user_type": "registered", "reputation": 2168}, "body": "<pre><code>org.apache.nutch.segment.SegmentReader \n</code></pre>\n\n<p>has a map reduce implementation that reads content data in the segment directory.</p>\n", "creation_date": 1316662786, "is_accepted": true, "score": 0, "last_activity_date": 1316662786, "answer_id": 7509293}, {"last_edit_date": 1398950933, "owner": {"user_id": 1013444, "accept_rate": 85, "link": "http://stackoverflow.com/users/1013444/kitwalker", "user_type": "registered", "reputation": 517}, "body": "<pre><code>import java.io.IOException;\n\nimport org.apache.commons.cli.Options;\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.fs.FileSystem;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.SequenceFile;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.util.GenericOptionsParser;\nimport org.apache.nutch.protocol.Content;\nimport org.apache.nutch.util.NutchConfiguration;\n\npublic class ContentReader {\n    public static void main(String[] args) throws IOException {\n        // Setup the parser\n        Configuration conf = NutchConfiguration.create();\n        Options opts = new Options();\n        GenericOptionsParser parser = new GenericOptionsParser(conf, opts, args);\n        String[] remainingArgs = parser.getRemainingArgs();\n        FileSystem fs = FileSystem.get(conf);\n        String segment = remainingArgs[0];\n        Path file = new Path(segment, Content.DIR_NAME + \"/part-00000/data\");\n        SequenceFile.Reader reader = new SequenceFile.Reader(fs, file, conf);\n        Text key = new Text();\n        Content content = new Content();\n        // Loop through sequence files\n        while (reader.next(key, content)) {\n            try {\n                System.out.write(content.getContent(), 0,\n                        content.getContent().length);\n            } catch (Exception e) {\n            }\n        }\n    }\n}\n</code></pre>\n", "question_id": 7506890, "creation_date": 1364905267, "is_accepted": false, "score": 5, "last_activity_date": 1398950933, "answer_id": 15764001}], "question_id": 7506890, "tags": ["nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/7506890/how-reading-nutch-generated-content-data-on-the-segment-folder-using-java", "last_activity_date": 1398950933, "accepted_answer_id": 7509293, "body": "<p>I am trying to read the content data inside the segment folder. I think the content data file is written in a custom <a href=\"http://wiki.apache.org/nutch/NutchFileFormats\" rel=\"nofollow\">format</a></p>\n\n<p>I experimented with nutch's <a href=\"http://javasourcecode.org/html/open-source/nutch/nutch-1.3/org/apache/nutch/protocol/Content.java.html\" rel=\"nofollow\">Content</a> class, but it does not recognize the format.</p>\n", "creation_date": 1316640515, "score": 1},
{"title": "How to configure Nutch in Eclipse for SOLR", "view_count": 938, "owner": {"age": 33, "answer_count": 48, "creation_date": 1361267142, "user_id": 2086509, "accept_rate": 73, "view_count": 138, "location": "Pune, India", "reputation": 594}, "is_answered": true, "answers": [{"question_id": 23394470, "owner": {"user_id": 2086509, "accept_rate": 73, "link": "http://stackoverflow.com/users/2086509/jayesh-bhoyar", "user_type": "registered", "reputation": 594}, "community_owned_date": 1400264758, "creation_date": 1398878977, "is_accepted": true, "body": "<p><strong>Checkout and Build Nutch:</strong></p>\n\n<p>1.Get the latest source code from SVN using terminal. </p>\n\n<p>For Nutch 1.x (ie.trunk) run this:<br>\nsvn co <a href=\"https://svn.apache.org/repos/asf/nutch/trunk\" rel=\"nofollow\">https://svn.apache.org/repos/asf/nutch/trunk</a></p>\n\n<p>2.Add \u201chttp.agent.name\u201d and \u201chttp.robots.agents\u201d with appropiate values in \u201cconf/nutch-site.xml\u201d. </p>\n\n<p>Here you have to rename the nutch-site.xml.template file to nutch-site.xml and make the changes accordingly.</p>\n\n<p>See conf/nutch-default.xml for the description of these properties. </p>\n\n<p>3.Also, add \u201cplugin.folders\u201d and set it to {PATH_TO_NUTCH_CHECKOUT}/build/plugins. eg. If Nutch is present at \"/home/Desktop/2.x\", </p>\n\n<p>set the property to:  </p>\n\n<pre><code>&lt;property&gt;\n   &lt;name&gt;plugin.folders&lt;/name&gt;\n   &lt;value&gt;/home/Desktop/2.x/build/plugins&lt;/value&gt;\n&lt;/property&gt;\n</code></pre>\n\n<p>There is no /build/plugins folder currently present. But when you run the \"ant eclipse\" command you will get the \"/build/plugins\" in your {PATH_TO_NUTCH_CHECKOUT}.</p>\n\n<p>Thats why it is written as set the absolute path as {PATH_TO_NUTCH_CHECKOUT}/build/plugins.</p>\n\n<p>Do not give relative path here as it wont.</p>\n\n<p>4.Run this command:<br>\nant eclipse</p>\n\n<p><strong>5.Load project in Eclipse</strong></p>\n\n<p>5.1.In Eclipse, click on \u201cFile\u201d -> \u201cImport...\u201d </p>\n\n<p>5.2.Select \u201cExisting Projects into Workspace\u201d  </p>\n\n<p>5.3.In the next window, set the root directory to the location where you took the checkout of nutch 2.x (or trunk). Click \u201cFinish\u201d. </p>\n\n<p>5.4.You will now see a new project named 2.x (or trunk) being added in the workspace.\nWait for a moment until Eclipse refreshes its SVN cache and builds its workspace. You can see the status at the bottom right corner of Eclipse. </p>\n\n<p>5.5.In Package Explorer, right click on the project \u201c2.x\u201d (or trunk), select \u201cBuild Path\u201d -> \u201cConfigure Build Path\u201d </p>\n\n<p>5.6.In the \u201cOrder and Export\u201d tab, scroll down and select \u201c2.x/conf\u201d (or trunk/conf). Click on \u201cTop\u201d button. Sadly, Eclipse will again build the workspace but this time it won\u2019t take take much.  </p>\n\n<p><strong>6.Need to Download following jar files :</strong></p>\n\n<p><a href=\"http://mvnrepository.com/artifact/org.elasticsearch/elasticsearch/0.90.1\" rel=\"nofollow\">http://mvnrepository.com/artifact/org.elasticsearch/elasticsearch/0.90.1</a></p>\n\n<p>Configure the above jar file in eclipse.</p>\n\n<p>7.One error you will get for \u201cElasticsearchException\u201d. Change it to \u201cElasticSearchException\u201d (S Capital)</p>\n\n<p>8.Now you are ready to run the nutch code in eclipse:</p>\n\n<p>8.1.Lets start off with the inject operation. </p>\n\n<p>8.2.Right click on the project in \u201cPackage Explorer\u201d -> select \u201cRun As\u201d -> select \u201cRun Configurations\u201d. </p>\n\n<p>8.3.Create a new configuration. Name it as \"inject\". </p>\n\n<p>For 1.x ie trunk : Set the main class as: org.apache.nutch.crawl.Injector </p>\n\n<p>For 2.x : Set the main class as: org.apache.nutch.crawl.InjectorJob </p>\n\n<p>8.4.  In the arguments tab, for program arguments, provide the path of the input directory which has seed urls. </p>\n\n<p>8.5.  Set VM Arguments to \u201c-Dhadoop.log.dir=logs -Dhadoop.log.file=hadoop.log\u201d </p>\n\n<p>8.6.  Click \"Apply\" and then click \"Run\". </p>\n\n<p>8.7.  If everything was set perfectly, then you should see inject operation progressing on console. </p>\n\n<p><strong>Class in Nutch 1.x (i.e.trunk)</strong> </p>\n\n<p>inject :- org.apache.nutch.crawl.Injector  </p>\n\n<p>generate :-  org.apache.nutch.crawl.Generator  </p>\n\n<p>fetch :-  org.apache.nutch.fetcher.Fetcher  </p>\n\n<p>parse :-  org.apache.nutch.parse.ParseSegment  </p>\n\n<p>updatedb :-  org.apache.nutch.crawl.CrawlDb </p>\n\n<p><strong>Class in Nutch 2.x</strong> </p>\n\n<p>inject :- org.apache.nutch.crawl.InjectorJob </p>\n\n<p>generate :-  org.apache.nutch.crawl.GeneratorJob </p>\n\n<p>fetch :-  org.apache.nutch.fetcher.FetcherJob </p>\n\n<p>parse :-  org.apache.nutch.parse.ParserJob </p>\n\n<p>updatedb :- org.apache.nutch.crawl.DbUpdaterJob </p>\n\n<p>HOPE THIS HELPS!!!!</p>\n", "score": 3, "last_activity_date": 1398878977, "answer_id": 23394471}], "question_id": 23394470, "tags": ["solr", "web-crawler", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/23394470/how-to-configure-nutch-in-eclipse-for-solr", "last_activity_date": 1398878977, "accepted_answer_id": 23394471, "body": "<p>I was looking for configuring the Nutch source code in Eclipse for SOLR.</p>\n\n<p>So here are the steps that I have followed and I am able to configure it successfully.</p>\n\n<p>Regards,</p>\n\n<p>Jayesh Bhoyar</p>\n", "creation_date": 1398878977, "score": 0},
{"title": "nutch 1.8 solr crawl not starting", "view_count": 617, "is_answered": false, "question_id": 23390216, "tags": ["apache", "tomcat", "solr", "nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/23390216/nutch-1-8-solr-crawl-not-starting", "last_activity_date": 1398866427, "owner": {"user_id": 2078716, "answer_count": 0, "creation_date": 1361029541, "accept_rate": 60, "view_count": 5, "reputation": 32}, "body": "<p>I'm searching for an answer for hours, and can't understand what the problem is.\nI have apache-nutch-1.8, solr-4.7.1 running on apache-tomcat 7.\nWhen running the following command from cygwin:</p>\n\n<pre><code>bin/crawl bin/urls -solr http://localhost:8080/solr/ -depth 10 -topN 10\n</code></pre>\n\n<p>I get the following:</p>\n\n<pre><code>cygpath: can't convert empty path\nInjector: starting at 2014-04-30 16:48:58\nInjector: crawlDb: -solr/crawldb\nInjector: urlDir: bin/urls\nInjector: Converting injected urls to crawl db entries.\nInjector: total number of urls rejected by filters: 0\nInjector: total number of urls injected after normalization and filtering: 517\nInjector: Merging injected urls into crawl db.\nInjector: overwrite: false\nInjector: update: false\nInjector: finished at 2014-04-30 16:49:02, elapsed: 00:00:03\n</code></pre>\n\n<p>and that's it. To the best of my understanding this should be right but followed by starting the crawl.\nI did all instructions in a few tutorials, and have a nutch.txt file in my urls folder, within it some 517 sites to crawl.\nI also added the sites as +^http://.... to the regex-urlfilter.txt in NUTCH_HOME/conf.\nI should say that I get the same result when stopping tomcat service all together.\nCan anyone help me understand what the problem is?\nThanks!</p>\n", "creation_date": 1398866427, "score": 1},
{"title": "Running Nutch in Eclipse - Missing Build Folder", "view_count": 527, "is_answered": true, "answers": [{"question_id": 20733408, "owner": {"user_type": "does_not_exist"}, "body": "<p>Here is the answer to your question:</p>\n\n<p>1) Yes. You are correct you have to rename the nutch-site.xml.template file to nutch-site.xml and make the changes accordingly.</p>\n\n<p>2) There is no <strong>/build/plugins</strong> folder currently present. But when you run the <strong>\"ant eclipse\"</strong> command you will get the <strong>\"/build/plugins\"</strong> in your <strong>{PATH_TO_NUTCH_CHECKOUT}</strong> \nThats why it is written as set the absolute path as <strong>{PATH_TO_NUTCH_CHECKOUT}/build/plugins</strong>\nDo not give relative path here as it wont.</p>\n\n<p>Hope this helps you!!!</p>\n", "creation_date": 1398760807, "is_accepted": false, "score": 2, "last_activity_date": 1398760807, "answer_id": 23359665}], "question_id": 20733408, "tags": ["java", "eclipse", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/20733408/running-nutch-in-eclipse-missing-build-folder", "last_activity_date": 1398760807, "owner": {"age": 27, "answer_count": 159, "creation_date": 1357505010, "user_id": 1953475, "accept_rate": 72, "view_count": 540, "location": "Denver, CO", "reputation": 5595}, "body": "<p>Hi I was following <a href=\"http://wiki.apache.org/nutch/RunNutchInEclipse\" rel=\"nofollow\">this tutorial</a> trying to run Nutch in Eclipse and run Nutch step by step.</p>\n\n<p>I finished this step (Nutch 1.X) without problem:</p>\n\n<pre><code> svn co https://svn.apache.org/repos/asf/nutch/trunk\n cd trunk\n</code></pre>\n\n<p>Since I was working on <code>1.X</code>, so I skipped to step#5. However, it mentioned:</p>\n\n<p><code>Add \u201chttp.agent.name\u201d and \u201chttp.robots.agents\u201d with appropiate values in \u201cconf/nutch-site.xml\u201d. See conf/nutch-default.xml for the description of these properties. Also, add \u201cplugin.folders\u201d and set it to {PATH_TO_NUTCH_CHECKOUT}/build/plugins. eg. If Nutch is present at \"/home/tejas/Desktop/2.x\", set the property to:</code></p>\n\n<pre><code> &lt;property&gt;\n   &lt;name&gt;plugin.folders&lt;/name&gt;\n   &lt;value&gt;/home/tejas/Desktop/2.x/build/plugins&lt;/value&gt;\n &lt;/property&gt;\n</code></pre>\n\n<p>So here is my question:</p>\n\n<p>(1). There is no nutch-site.xml file in the <code>trunk/conf</code> folder as default, however, there is a nutch-site.xml.template file which I renamed to nutch-site.xml to use.</p>\n\n<p>(2). <code>{PATH_TO_NUTCH_CHECKOUT}/build/plugins</code>, I really don't know where this <code>build</code>folder is located. \nThis is what I have done:</p>\n\n<pre><code>trunk$ find . | grep build\n./.svn/prop-base/build.xml.svn-base\n./.svn/text-base/build.xml.svn-base\n./build.xml\n./src/plugin/.svn/prop-base/build-plugin.xml.svn-base\n./src/plugin/.svn/prop-base/build.xml.svn-base\n...\n</code></pre>\n\n<p>Clearly there is no folder called 'build', neither 'plugins'.</p>\n\n<p>Then should I just take the <code>./src/plugin</code> as the <code>./build/plugins</code> folder and move on? I don't have that much experience building from source so any advise is appreciated!</p>\n", "creation_date": 1387741942, "score": 0},
{"title": "how to configure nutch 1.8 in windows error: nutch : command not found", "view_count": 619, "is_answered": false, "answers": [{"question_id": 23334244, "owner": {"user_id": 1881318, "link": "http://stackoverflow.com/users/1881318/tahagh", "user_type": "registered", "reputation": 619}, "body": "<p>Nutch scripts are written for linux environments. </p>\n\n<p>You might use this (although it seems it needs a lot more work to be completed):</p>\n\n<p><a href=\"https://github.com/veggen/nutch-windows-script\" rel=\"nofollow\">https://github.com/veggen/nutch-windows-script</a></p>\n\n<p>Or setup Cygwin as suggested here:</p>\n\n<p><a href=\"http://wiki.apache.org/nutch/GettingNutchRunningWithWindows\" rel=\"nofollow\">http://wiki.apache.org/nutch/GettingNutchRunningWithWindows</a></p>\n", "creation_date": 1398751649, "is_accepted": false, "score": 0, "last_activity_date": 1398751649, "answer_id": 23356784}], "question_id": 23334244, "tags": ["apache", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/23334244/how-to-configure-nutch-1-8-in-windows-error-nutch-command-not-found", "last_activity_date": 1398751649, "owner": {"user_id": 3548746, "view_count": 6, "answer_count": 0, "creation_date": 1397821223, "reputation": 6}, "body": "<p>I am trying to configure nutch in windows 7 and i have followed the follwing steps</p>\n\n<p>I have download and unziped the apache nutch 1.8,\nI have specified the agent name in  conf/nutch-site.xml like</p>\n\n<pre><code>&lt;configuration&gt;\n&lt;property&gt;\n &lt;name&gt;http.agent.name&lt;/name&gt;\n &lt;value&gt;My Nutch Spider&lt;/value&gt;\n&lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre>\n\n<p>and in apache home follwing command i did -></p>\n\n<p>mkdir -p urls</p>\n\n<p>cd urls</p>\n\n<p>touch seed.txt      --> to create a text file seed.txt under urls/ with the following content  (one URL per line for each site you want Nutch to crawl). </p>\n\n<p>nutch.apache.org/</p>\n\n<p>in conf/regex-urlfilter.txt edit with--> +^([a-z0-9]*.)*nutch.apache.org/ </p>\n\n<p>but in bin when i am doing </p>\n\n<p>bin/nutch crawl urls -dir crawl -depth 3 -topN 5</p>\n\n<p>error occurred -> bash: nutch: command not found</p>\n\n<p>why ?</p>\n", "creation_date": 1398667553, "score": 1},
{"title": "Apache Nutch: LinkConent inlink and from url", "view_count": 78, "is_answered": false, "question_id": 23208876, "tags": ["apache", "web-crawler", "nutch", "pagerank"], "answer_count": 0, "link": "http://stackoverflow.com/questions/23208876/apache-nutch-linkconent-inlink-and-from-url", "last_activity_date": 1398183756, "owner": {"user_id": 1933764, "answer_count": 0, "creation_date": 1356674584, "accept_rate": 74, "view_count": 53, "reputation": 325}, "body": "<p>I am using apache nutch to crawl some websites upto 6 levels deep. I am dumping the link content to my current working directory. The link content contains data in the following format:</p>\n\n<pre><code>www.abc.com/help Inlink:\n  fromUrl: www.abc.com anchor: Help\n  fromUrl: www.xyz.com anchor: abc help\n</code></pre>\n\n<p>My question with respect to nutch is, if nutch is able to generate the above data, should'nt the same lincontent file contain www.abc.com and its Inlink: information (similarly information about www.xyz.com) considering it has information about abc.com/help, it would have analyzed from www.abc.com and www.xyz.com. However I dont find the fromUrls having their inlink information in some cases. Why would this be? Am i missing something here?</p>\n", "creation_date": 1398130511, "score": 1},
{"title": "Getting error while running Nutch on OSX mavrics", "view_count": 195, "owner": {"user_id": 2674407, "view_count": 3, "answer_count": 6, "creation_date": 1376299559, "reputation": 38}, "is_answered": true, "answers": [{"question_id": 21592974, "owner": {"user_id": 1965370, "link": "http://stackoverflow.com/users/1965370/do-do", "user_type": "registered", "reputation": 389}, "body": "<p>If you are using Nutch 2.x .Command /bin/nutch crawl is deprecated. You should use /bin/crawl instead.</p>\n", "creation_date": 1392030961, "is_accepted": false, "score": 0, "last_activity_date": 1392030961, "answer_id": 21675666}, {"question_id": 21592974, "owner": {"user_id": 2674407, "link": "http://stackoverflow.com/users/2674407/user2674407", "user_type": "registered", "reputation": 38}, "body": "<p>Solved the problem...</p>\n\n<p>Folder name had a ' ' in it. that caused nutch crawl to not work.</p>\n\n<p>After removing the space character from the folder name, it works fine now. </p>\n", "creation_date": 1392228372, "is_accepted": true, "score": 0, "last_activity_date": 1392228372, "answer_id": 21736028}], "question_id": 21592974, "tags": ["osx-mavericks", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/21592974/getting-error-while-running-nutch-on-osx-mavrics", "last_activity_date": 1398158604, "accepted_answer_id": 21736028, "body": "<p>I run the following command and I get an error of class not found. I tried run the following </p>\n\n<p>$ bin/nutch crawl urls -dir crawl -depth 3 -topN 5<br/>\nbin/nutch: line 106: [: too many arguments<br/>\nError: Could not find or load main class Course.Web</p>\n\n<p><a href=\"http://stackoverflow.com/questions/16521582/apache-nutch-command-unable-to-execute\">Apache Nutch Command Unable to Execute</a>\nI tried both the approaches from the above link but none of them worked. </p>\n\n<p>I am using Nutch 1.7</p>\n", "creation_date": 1391654418, "score": 0},
{"title": "Nutch Crawling not working for particular URL", "view_count": 1662, "owner": {"age": 27, "answer_count": 125, "creation_date": 1313859912, "user_id": 903907, "accept_rate": 50, "view_count": 306, "location": "Chennai, India", "reputation": 2981}, "is_answered": true, "answers": [{"question_id": 16766169, "owner": {"user_id": 2082437, "accept_rate": 60, "link": "http://stackoverflow.com/users/2082437/abhinav", "user_type": "registered", "reputation": 767}, "body": "<p>Nutch crawler obeys robots.txt and if you see robots.txt located on <a href=\"http://www.google.co.in/robots.txt\" rel=\"nofollow\">http://www.google.co.in/robots.txt</a> you will find that /search is disallowed to crawl.</p>\n", "creation_date": 1369640676, "is_accepted": true, "score": 1, "last_activity_date": 1369640676, "answer_id": 16768563}], "question_id": 16766169, "tags": ["java", "web-crawler", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/16766169/nutch-crawling-not-working-for-particular-url", "last_activity_date": 1398096329, "accepted_answer_id": 16768563, "body": "<p>I am using apache nutch for crawling. When i crawled the page <code>http://www.google.co.in</code>. It crawls the page correctly and produce the results. But when i add one parameter in that url it does not fetch any results for the url <code>http://www.google.co.in/search?q=bill+gates</code>.</p>\n\n<pre><code>solrUrl is not set, indexing will be skipped...\ncrawl started in: crawl\nrootUrlDir = urls\nthreads = 10\ndepth = 3\nsolrUrl=null\ntopN = 100\nInjector: starting at 2013-05-27 08:01:57\nInjector: crawlDb: crawl/crawldb\nInjector: urlDir: urls\nInjector: Converting injected urls to crawl db entries.\nInjector: total number of urls rejected by filters: 0\nInjector: total number of urls injected after normalization and filtering: 1\nInjector: Merging injected urls into crawl db.\nInjector: finished at 2013-05-27 08:02:11, elapsed: 00:00:14\nGenerator: starting at 2013-05-27 08:02:11\nGenerator: Selecting best-scoring urls due for fetch.\nGenerator: filtering: true\nGenerator: normalizing: true\nGenerator: topN: 100\nGenerator: jobtracker is 'local', generating exactly one partition.\nGenerator: Partitioning selected urls for politeness.\nGenerator: segment: crawl/segments/20130527080219\nGenerator: finished at 2013-05-27 08:02:26, elapsed: 00:00:15\nFetcher: Your 'http.agent.name' value should be listed first in 'http.robots.agents' property.\nFetcher: starting at 2013-05-27 08:02:26\nFetcher: segment: crawl/segments/20130527080219\nUsing queue mode : byHost\nFetcher: threads: 10\nFetcher: time-out divisor: 2\nQueueFeeder finished: total 1 records + hit by time limit :0\nUsing queue mode : byHost\nUsing queue mode : byHost\nUsing queue mode : byHost\nfetching http://www.google.co.in/search?q=bill+gates\nUsing queue mode : byHost\nUsing queue mode : byHost\nUsing queue mode : byHost\nUsing queue mode : byHost\nUsing queue mode : byHost\nUsing queue mode : byHost\nUsing queue mode : byHost\nFetcher: throughput threshold: -1\nFetcher: throughput threshold retries: 5\n-finishing thread FetcherThread, activeThreads=8\n-finishing thread FetcherThread, activeThreads=7\n-finishing thread FetcherThread, activeThreads=1\n-finishing thread FetcherThread, activeThreads=2\n-finishing thread FetcherThread, activeThreads=3\n-finishing thread FetcherThread, activeThreads=4\n-finishing thread FetcherThread, activeThreads=5\n-finishing thread FetcherThread, activeThreads=6\n-finishing thread FetcherThread, activeThreads=1\n-finishing thread FetcherThread, activeThreads=0\n-activeThreads=0, spinWaiting=0, fetchQueues.totalSize=0\n-activeThreads=0\nFetcher: finished at 2013-05-27 08:02:33, elapsed: 00:00:07\nParseSegment: starting at 2013-05-27 08:02:33\nParseSegment: segment: crawl/segments/20130527080219\nParseSegment: finished at 2013-05-27 08:02:40, elapsed: 00:00:07\nCrawlDb update: starting at 2013-05-27 08:02:40\nCrawlDb update: db: crawl/crawldb\nCrawlDb update: segments: [crawl/segments/20130527080219]\nCrawlDb update: additions allowed: true\nCrawlDb update: URL normalizing: true\nCrawlDb update: URL filtering: true\nCrawlDb update: 404 purging: false\nCrawlDb update: Merging segment data into db.\nCrawlDb update: finished at 2013-05-27 08:02:54, elapsed: 00:00:13\nGenerator: starting at 2013-05-27 08:02:54\nGenerator: Selecting best-scoring urls due for fetch.\nGenerator: filtering: true\nGenerator: normalizing: true\nGenerator: topN: 100\nGenerator: jobtracker is 'local', generating exactly one partition.\nGenerator: 0 records selected for fetching, exiting ...\nStopping at depth=1 - no more URLs to fetch.\nLinkDb: starting at 2013-05-27 08:03:01\nLinkDb: linkdb: crawl/linkdb\nLinkDb: URL normalize: true\nLinkDb: URL filter: true\nLinkDb: internal links will be ignored.\nLinkDb: adding segment: file:/home/muthu/workspace/webcrawler/crawl/segments/20130527080219\nLinkDb: finished at 2013-05-27 08:03:08, elapsed: 00:00:07\ncrawl finished: crawl\n</code></pre>\n\n<p>I already add the code </p>\n\n<pre><code># skip URLs containing certain characters as probable queries, etc.\n-.*[?*!@=].*\n</code></pre>\n\n<p>Why this happens? can fetch the urls if i add parameter? Thanks in advance for your Help.</p>\n", "creation_date": 1369627789, "score": 0},
{"title": "Documentation for bin/nutch elasticindex", "view_count": 294, "owner": {"user_id": 832056, "answer_count": 32, "creation_date": 1309971839, "accept_rate": 84, "view_count": 163, "reputation": 859}, "is_answered": true, "answers": [{"question_id": 17523403, "owner": {"user_id": 832056, "accept_rate": 84, "link": "http://stackoverflow.com/users/832056/pqn", "user_type": "registered", "reputation": 859}, "body": "<p>I've modified <code>bin/crawl</code> to remove the <code>bin/nutch solrdedup</code> command, and replaced all mentions of <code>solrindex</code> with <code>elasticindex</code>.</p>\n", "creation_date": 1373278285, "is_accepted": true, "score": 0, "last_activity_date": 1373278285, "answer_id": 17524068}, {"question_id": 17523403, "owner": {"user_id": 3426717, "link": "http://stackoverflow.com/users/3426717/andrei", "user_type": "registered", "reputation": 31}, "body": "<p>I don't think it's possible to make Nutch 2.2.x work with Elasticsearch. But I don't see the added benefit of 2.2.x compared with 1.8. The only thing is that Nutch 2.2.x uses Gora to save the crawled pages in a database of your choice. Since you are using Elasticsearch to index the results I assume you don't need the database.\nI made Nutch 1.8 with Elasticsearch 0.90.11 and you can find the bundle on my GitHub account:\n<a href=\"https://github.com/andreivisan/NutchElasticsearch\" rel=\"nofollow\">https://github.com/andreivisan/NutchElasticsearch</a></p>\n", "creation_date": 1397343647, "is_accepted": false, "score": 0, "last_activity_date": 1397343647, "answer_id": 23037159}], "question_id": 17523403, "tags": ["search", "solr", "elasticsearch", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/17523403/documentation-for-bin-nutch-elasticindex", "last_activity_date": 1397987883, "accepted_answer_id": 17524068, "body": "<p>There is a lot of documentation and examples using the <code>bin/nutch solrindex</code> command, but the <code>bin/nutch elasticindex</code> command is lacking in coverage. I am struggling to combine an instance of Nutch 2.2.1 with Elasticsearch 0.90.2. I've tried to use <a href=\"https://github.com/marevol/elasticsearch-mocksolrplugin/tree/develop\" rel=\"nofollow\">this plugin</a> to disguise Elasticsearch as a Solr instance, but any <code>bin/crawl</code> jobs crash from internal server error. What I am looking for is an example of <code>bin/crawl</code> modified to use Elasticsearch or a detailed description of the <code>bin/nutch elasticindex</code> command (the nutch wiki doesn't have a page for it). Can I simply replace every occurrence of the phrase <code>solrindex</code> with <code>elasticindex</code> freely?</p>\n", "creation_date": 1373276326, "score": 0},
{"title": "How to use solr&#39;s DIH deltaimport to import mysql data which exported by nutch", "view_count": 148, "is_answered": false, "answers": [{"question_id": 20435825, "owner": {"user_id": 1805262, "link": "http://stackoverflow.com/users/1805262/aaronlau", "user_type": "registered", "reputation": 21}, "body": "<p>Trying to Use ScriptTransformer to transform each ununiform data before index,I hope this URL will be usefull for you\uff1a<a href=\"http://wiki.apache.org/solr/DataImportHandler#ScriptTransformer\" rel=\"nofollow\">http://wiki.apache.org/solr/DataImportHandler#ScriptTransformer</a></p>\n", "creation_date": 1386674360, "is_accepted": false, "score": 0, "last_activity_date": 1386674360, "answer_id": 20493058}], "question_id": 20435825, "tags": ["mysql", "solr", "web-crawler", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/20435825/how-to-use-solrs-dih-deltaimport-to-import-mysql-data-which-exported-by-nutch", "last_activity_date": 1397664901, "owner": {"user_id": 3076384, "view_count": 0, "answer_count": 0, "creation_date": 1386373985, "reputation": 1}, "body": "<p>I want to use solr's DIH deltaimport to import mysql data which exported by nutch, but the mysql data from nutch don't have timestamp field.\nThe reason I try to import from Mysql is that I want to combine the crawl data to other url management table.\nThe \"modifiedTime\" field in the crawl data is bigInt not timeStamp and is always Null.\nI examined about this question and I thought that the index filter plugin could solve this problem. Is it right?</p>\n\n<p>nutch 2.1\nSolr 3.6</p>\n", "creation_date": 1386375076, "score": 0},
{"title": "Create only linkdb in apache nutch", "view_count": 164, "is_answered": false, "answers": [{"last_edit_date": 1388227542, "owner": {"user_id": 215336, "accept_rate": 22, "link": "http://stackoverflow.com/users/215336/sriwantha-attanayake", "user_type": "registered", "reputation": 4043}, "body": "<p>The details of limiting the crawl content is given in <a href=\"http://facstaff.unca.edu/mcmcclur/class/Seminar/Pagerank/nutch/nutch.html\" rel=\"nofollow\">http://facstaff.unca.edu/mcmcclur/class/Seminar/Pagerank/nutch/nutch.html</a></p>\n\n<p>You can use the following configuration property in nutch site config</p>\n\n<pre><code>&lt;property&gt;\n  &lt;name&gt;fetcher.store.content&lt;/name&gt;\n  &lt;value&gt;false&lt;/value&gt;\n  &lt;description&gt;If true, fetcher will store content.&lt;/description&gt;\n&lt;/property&gt;\n</code></pre>\n\n<p>Please note that if you set this in the initial crawl, no content will be generated hence no links and fetcher will fail half way saying that it doesndoes not have segment content. </p>\n", "question_id": 20810307, "creation_date": 1388202214, "is_accepted": false, "score": 0, "last_activity_date": 1388227542, "answer_id": 20810768}], "question_id": 20810307, "tags": ["web-crawler", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/20810307/create-only-linkdb-in-apache-nutch", "last_activity_date": 1397582240, "owner": {"user_id": 215336, "answer_count": 51, "creation_date": 1258713507, "accept_rate": 22, "view_count": 662, "location": "Cambridge, United Kingdom", "reputation": 4043}, "body": "<p>I am using apache nutch version 1.7 to crawl the internet. Everything works fine. However, I am interested in inlinks and outlinks as all what I do is link analysis. So I am <strong>not</strong> interested in content, parse text etc. How can I instruct nutch only to build the linkdb? but not others (crawldb or segmentdb) as I am not interested in html content. Is there an option to automatically purge crawldb and segmentdb while crawling). I am doing a large scale crawling in the internet and need to be very space efficient.  </p>\n", "creation_date": 1388196687, "score": 0},
{"title": "How to block crawlers such as spyder/Nutch-2 from visiting a specific page?", "view_count": 305, "is_answered": true, "answers": [{"last_edit_date": 1397568128, "owner": {"user_id": 2381623, "link": "http://stackoverflow.com/users/2381623/dieter-kempeneers", "user_type": "registered", "reputation": 43}, "body": "<p>You can indeed use a <code>.htaccess</code>. <code>robots.txt</code> is another option but some crawlers will ignore this. \nYou can also block specific user agent strings. (They differ from crawler to crawler)</p>\n\n<p>robots.txt:</p>\n\n<pre><code>User-agent: *\nDisallow: /\n</code></pre>\n\n<p>This example tells all robots to stay out of the website:\nYou can block specific directories</p>\n\n<pre><code>Disallow: /demo/\n</code></pre>\n\n<p><a href=\"http://www.robotstxt.org/\" rel=\"nofollow\">More information about robots.txt</a></p>\n", "question_id": 23084666, "creation_date": 1397567635, "is_accepted": false, "score": 2, "last_activity_date": 1397568128, "answer_id": 23084823}, {"last_edit_date": 1397567968, "owner": {"user_id": 1796105, "accept_rate": 93, "link": "http://stackoverflow.com/users/1796105/h%c3%bcseyin-babal", "user_type": "registered", "reputation": 10858}, "body": "<p>You can forbid specific crawlers by doing thatfollowing;</p>\n\n<pre><code>RewriteEngine On\nRewriteCond %{HTTP_USER_AGENT} (spyder/Nutch-2) [NC]\n#For multi block\n#RewriteCond %{HTTP_USER_AGENT} (spyder/Nutch-2|baidu|google|...) [NC]\nRewriteRule .* - [R=403,L]\n</code></pre>\n\n<p>That crawler, can change agent name, so this may not be the solution. You need to block that crawler by looking at ip address in need;</p>\n\n<pre><code>Order Deny,Allow\nDeny from x.x.x.x\n</code></pre>\n\n<p>However, that bot can also change his ip address. This means, you need to track your access logs. And decide which agents to be blocked and add them to list manually</p>\n", "question_id": 23084666, "creation_date": 1397567642, "is_accepted": false, "score": 2, "last_activity_date": 1397567968, "answer_id": 23084833}, {"question_id": 23084666, "owner": {"user_id": 574632, "accept_rate": 100, "link": "http://stackoverflow.com/users/574632/steve", "user_type": "registered", "reputation": 15894}, "body": "<p>You can ban the particular IP address with .htaccess file:</p>\n\n<pre><code>Order Deny,Allow\nDeny from xxx.xx.xx.xx\n</code></pre>\n\n<p>where xxx represents IP address</p>\n", "creation_date": 1397567801, "is_accepted": false, "score": 0, "last_activity_date": 1397567801, "answer_id": 23084905}, {"question_id": 23084666, "owner": {"user_id": 205300, "accept_rate": 92, "link": "http://stackoverflow.com/users/205300/frank-luke", "user_type": "registered", "reputation": 771}, "body": "<p>Close. It would be better to use a <a href=\"http://www.thesitewizard.com/archive/robotstxt.shtml\" rel=\"nofollow\">robots.txt</a> file. The page linked goes through why you would want to set one up and how to do so. In summary:</p>\n\n<ol>\n<li>It avoids wasting server resources as the spiders and bots run the scripts on the page.</li>\n<li>It can save bandwidth.</li>\n<li>It removes clutter from the webstats.</li>\n<li>You can fine-tune it to exclude only certain robots.</li>\n</ol>\n\n<p>One caveat I should mention. Some spiders are coded to disregard the robots.txt file and will even examine it to see what you don't want them to visit. However, spiders from legit sources will obey the robots.txt directives.</p>\n", "creation_date": 1397567845, "is_accepted": false, "score": 0, "last_activity_date": 1397567845, "answer_id": 23084916}, {"question_id": 23084666, "owner": {"user_id": 1401720, "accept_rate": 100, "link": "http://stackoverflow.com/users/1401720/pitchinnate", "user_type": "registered", "reputation": 6418}, "body": "<p>You could use .htaccess or another option would be to use php code. At the top of the php code simply put something like this:</p>\n\n<pre><code>if(strpos($_SERVER['HTTP_USER_AGENT'],'spyder/Nutch-2') !== false) {\n    die();\n}\n//rest of code here\n</code></pre>\n", "creation_date": 1397568026, "is_accepted": false, "score": 0, "last_activity_date": 1397568026, "answer_id": 23084982}], "question_id": 23084666, "tags": ["php", "apache", ".htaccess", "web-crawler", "nutch"], "answer_count": 5, "link": "http://stackoverflow.com/questions/23084666/how-to-block-crawlers-such-as-spyder-nutch-2-from-visiting-a-specific-page", "last_activity_date": 1397578600, "owner": {"age": 41, "answer_count": 114, "creation_date": 1234728453, "user_id": 66708, "accept_rate": 86, "view_count": 1004, "location": "Rio De Janeiro, Brazil", "reputation": 7894}, "body": "<p>I have a Windows client application that consumes a php page hosted in a shared commercial webserver. </p>\n\n<p>In this <strong>php</strong> page I am returning an encrypted json. Also in this page I have a piece of code to keep track of which IPs are visiting this php page, and I have noticed that there is a <code>spyder/Nutch-2</code> crawler visiting this page.</p>\n\n<p>I am wandering how is possible that a crawler could find a page that is not published in any search engines. I there a way to block crawlers from visiting this specific page? </p>\n\n<p>Shall I use <code>.htaccess</code> file to configure it?</p>\n", "creation_date": 1397567231, "score": 1},
{"title": "nutch content.getContent() can not return entire results", "view_count": 75, "is_answered": false, "answers": [{"last_edit_date": 1397539057, "owner": {"user_id": 3534445, "link": "http://stackoverflow.com/users/3534445/anoop-kulkarni", "user_type": "registered", "reputation": 1}, "body": "<p>Your problem may actually be that Nutch isn't crawling your PDF or is truncating it because of its size. Make sure you have set the <code>file.content.limit</code> and <code>http.content.limit</code> properties in <code>nutch-site.xml</code> to appropriate values. You can set it to <code>-1</code> to accept any file size.</p>\n", "question_id": 23035512, "creation_date": 1397537259, "is_accepted": false, "score": 0, "last_activity_date": 1397539057, "answer_id": 23074878}], "question_id": 23035512, "tags": ["java", "pdf", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/23035512/nutch-content-getcontent-can-not-return-entire-results", "last_activity_date": 1397539057, "owner": {"user_id": 3527619, "view_count": 3, "answer_count": 0, "creation_date": 1397332245, "reputation": 6}, "body": "<p>I am new to Apache Nutch.</p>\n\n<p>I am using Nutch to crawl some websites to get all web contents (including HTML files, PDF, images).</p>\n\n<p>I have written a small java program to extract Nutch results from SEGEMENTs which looks like :</p>\n\n<pre><code>byte[] fileContents = content.getContent();\n</code></pre>\n\n<p>problem here is, <code>getContent()</code> returns byte array. Hence if size of PDF content is more than 70KB (approx 70,000 bytes) ; array returned by <code>getContent()</code> can not hold entire file contents ans I do not get correct PDFs.</p>\n\n<p>Is there any alternate way to handle large contents</p>\n\n<p>I have read something about <code>content.read()</code> / <code>content.write()</code> but could not find documentation.\nAny help on this will be appreciated.</p>\n", "creation_date": 1397333133, "score": 1},
{"title": "Nutch URL regex normalization of params and session IDs", "view_count": 974, "owner": {"user_id": 937918, "answer_count": 15, "creation_date": 1315635668, "accept_rate": 23, "view_count": 44, "location": "Bangalore", "reputation": 180}, "is_answered": true, "answers": [{"last_edit_date": 1338578796, "owner": {"user_id": 28760, "accept_rate": 95, "link": "http://stackoverflow.com/users/28760/lirik", "user_type": "registered", "reputation": 23398}, "body": "<p><a href=\"http://stackoverflow.com/questions/7045716/nutch-1-2-why-wont-nutch-crawl-url-with-query-strings\">Nutch removes the query strings</a>, so it doesn't have a problem with parsing URLs that have them. The reason query strings are most likely removed is because some websites add tracking information in the query string and it can potentially force \"dumber\" crawlers to go into an \"infinite loop\" by repeatedly queuing identical links whose only difference is the tracking info in the query string.</p>\n\n<h2>Update:</h2>\n\n<p>It turns out that Nutch allows you to <a href=\"http://osdir.com/ml/nutch-user.lucene.apache.org/2010-01/msg00060.html\" rel=\"nofollow\">turn on crawling with query strings</a> by commenting out the appropriate line in crawl_urlfilter.txt and regex-urlfilter.txt to enable crawling of urls that contain a '?' in them. </p>\n\n<p>However, as I mentioned in the comment below: the query strings can contain tracking information and that can potentially cause problems with a crawler's URL-seen test. The first problem is that it would make the URL-seen database too big since there would be a lot of duplicates whose only difference is the tracking information in the query string. The second problem is that it's going to make it much slower to run the URL-seen test, because the database is too big!</p>\n\n<p>So keep that in mind when changing the options on whether or not query strings should be allowed.</p>\n", "question_id": 10852951, "creation_date": 1338571434, "is_accepted": true, "score": 1, "last_activity_date": 1338578796, "answer_id": 10854779}], "question_id": 10852951, "tags": ["solr", "web-crawler", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/10852951/nutch-url-regex-normalization-of-params-and-session-ids", "last_activity_date": 1397508674, "accepted_answer_id": 10854779, "body": "<p>Using Nutch are there any risks involved (like loops) in crawling URLs that have params like </p>\n\n<pre><code>http://something.com?page=index\n</code></pre>\n\n<p>The url-regexfilter ignores such URLs. If I remove this filter, am I potentially overlooking something that could cause trouble?</p>\n", "creation_date": 1338563759, "score": 0},
{"title": "Nutch 2.X - Prefered urls to fetch", "view_count": 80, "is_answered": false, "answers": [{"question_id": 19068540, "owner": {"user_id": 2586595, "link": "http://stackoverflow.com/users/2586595/mohsen89z", "user_type": "registered", "reputation": 128}, "body": "<p>have you checked <code>TopN</code> value?\nOr is Nutch still crawling? because indexing and sending data to solr is done at the end of process!</p>\n", "creation_date": 1380632750, "is_accepted": false, "score": 0, "last_activity_date": 1380632750, "answer_id": 19116998}], "question_id": 19068540, "tags": ["solr", "web-crawler", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/19068540/nutch-2-x-prefered-urls-to-fetch", "last_activity_date": 1397508536, "owner": {"age": 27, "answer_count": 17, "creation_date": 1332257106, "user_id": 1281305, "accept_rate": 88, "view_count": 129, "location": "Prague, Czech Republic", "reputation": 494}, "body": "<p>I have this situation: There are over 160 URLs in my seed. I started my crawling one week ago. Now I have a lot of pages crawled in my storage but I can see in my Solr index that some URLs from seed are not crawled at all (the URLs do not have some restrictions from a robots.txt) or only in very small number. Is it possible tell Nutch to prefer some URLs? </p>\n", "creation_date": 1380382151, "score": 1},
{"title": "Nutch sitemap command won&#39;t work", "view_count": 155, "owner": {"user_id": 2674407, "view_count": 3, "answer_count": 6, "creation_date": 1376299559, "reputation": 38}, "is_answered": true, "answers": [{"question_id": 21861761, "owner": {"user_id": 2674407, "link": "http://stackoverflow.com/users/2674407/user2674407", "user_type": "registered", "reputation": 38}, "body": "<p>I guess by looking at <a href=\"https://issues.apache.org/jira/browse/NUTCH-1465\" rel=\"nofollow\">https://issues.apache.org/jira/browse/NUTCH-1465</a>\nit seems that its not supported in the current version</p>\n\n<p>It might be released in 1.8 version.. I am currently using 1.7 </p>\n", "creation_date": 1392746963, "is_accepted": true, "score": 0, "last_activity_date": 1392746963, "answer_id": 21861976}], "question_id": 21861761, "tags": ["web-crawler", "sitemap", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/21861761/nutch-sitemap-command-wont-work", "last_activity_date": 1397507651, "accepted_answer_id": 21861976, "body": "<p>I tried to use this command $ bin/nutch sitemap</p>\n\n<p>but it gives me the following error </p>\n\n<p>Error: Could not find or load main class sitemap</p>\n\n<p>It's mentioned on the website <a href=\"http://wiki.apache.org/nutch/SitemapFeature\" rel=\"nofollow\">http://wiki.apache.org/nutch/SitemapFeature</a>\nbut it does not work for me. I am using Nutch 1.7</p>\n\n<p>Any suggestions?</p>\n", "creation_date": 1392746334, "score": 0},
{"title": "Apache Solr 4 - after 1st commit the index does not grow", "view_count": 630, "owner": {"age": 27, "answer_count": 17, "creation_date": 1332257106, "user_id": 1281305, "accept_rate": 88, "view_count": 129, "location": "Prague, Czech Republic", "reputation": 494}, "is_answered": true, "answers": [{"question_id": 17641584, "owner": {"user_id": 137650, "link": "http://stackoverflow.com/users/137650/matslindh", "user_type": "registered", "reputation": 13901}, "body": "<p>Have you tried with a lower value before autocommitting? Try to commit for every 100 documents to avoid having too much information in memory.</p>\n", "creation_date": 1384645941, "is_accepted": true, "score": 1, "last_activity_date": 1384645941, "answer_id": 20025420}], "question_id": 17641584, "tags": ["solr", "web-crawler", "hbase", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/17641584/apache-solr-4-after-1st-commit-the-index-does-not-grow", "last_activity_date": 1397507116, "accepted_answer_id": 20025420, "body": "<p>I have written my own plugin for Apache Nutch 2.2.1 to crawl images, videos and podcasts from selected sites (I have 180 urls in my seed). I put this metadata to a hBase store and now I want to save it to the index (Solr). I have a lot of metadatas to save (webpages + images + videos + podcast). </p>\n\n<p>I am using Nutch script bin/crawl for the whole process (inject, generate, fetch, parse... and finally solrindex and dedup) but I have one problem. When I run this script for a first time, there are stored approximately 6000 documents (Lets say it is 3700 docs for images, 1700 for wegpages and the rest of docs are for videos and podcasts) to the index. It is ok...</p>\n\n<p>but...</p>\n\n<p>When I run the script for a second time, third time and so on... the index does not increase the number of documents (there are still 6000 documents) but a count of rows stored in hBase table grows (there is 97383 rows now)...</p>\n\n<p>Do you now where is the problem please? I am fighting with this problem really long time and I dont know... If it could be helpful, this is my configuration of solrconfix.xml <a href=\"http://pastebin.com/uxMW2nuq\" rel=\"nofollow\">http://pastebin.com/uxMW2nuq</a> and this is my nutch-site.xml <a href=\"http://pastebin.com/4bj1wdmT\" rel=\"nofollow\">http://pastebin.com/4bj1wdmT</a> </p>\n\n<p>When I look into the log, there is: </p>\n\n<pre><code>SEVERE: auto commit error...:java.lang.IllegalStateException: this writer hit an OutOfMemoryError; cannot commit \n        at org.apache.lucene.index.IndexWriter.prepareCommitInternal(IndexWriter.java:2668) \n        at org.apache.lucene.index.IndexWriter.commitInternal(IndexWriter.java:2834) \n        at org.apache.lucene.index.IndexWriter.commit(IndexWriter.java:2814) \n        at org.apache.solr.update.DirectUpdateHandler2.commit(DirectUpdateHandler2.java:529) \n        at org.apache.solr.update.CommitTracker.run(CommitTracker.java:216) \n        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471) \n        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334) \n        at java.util.concurrent.FutureTask.run(FutureTask.java:166) \n        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:178) \n        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:292) \n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) \n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) \n        at java.lang.Thread.run(Thread.java:722)\n</code></pre>\n", "creation_date": 1373821337, "score": 1},
{"title": "How can i crawl page but without fetching video/image content in nutch 2.1?", "view_count": 422, "owner": {"age": 27, "answer_count": 28, "creation_date": 1340810733, "user_id": 1486136, "accept_rate": 89, "view_count": 130, "location": "Bitola, Macedonia (FYROM)", "reputation": 2255}, "is_answered": true, "answers": [{"question_id": 14262630, "owner": {"user_id": 950178, "link": "http://stackoverflow.com/users/950178/jayendra", "user_type": "registered", "reputation": 34896}, "body": "<p>Check regex-urlfilter.txt file.  </p>\n\n<p>You can include the extensions of the file extensions which you dont want to index. e.g.</p>\n\n<pre><code># skip image and other suffixes we can't yet parse\n# for a more extensive coverage use the urlfilter-suffix plugin\n-\\.(gif|GIF|jpg|JPG|png|PNG|ico|ICO|css|CSS|sit|SIT|eps|EPS|wmf|WMF|zip|ZIP|ppt|PPT|mpg|MPG|xls|XLS|gz|GZ|rpm|RPM|tgz|TGZ|mov|MOV|exe|EXE|jpeg|JPEG|bmp|BMP|js|JS)$\n</code></pre>\n", "creation_date": 1357876621, "is_accepted": true, "score": 1, "last_activity_date": 1357876621, "answer_id": 14271428}], "question_id": 14262630, "tags": ["solr", "lucene", "web-crawler", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/14262630/how-can-i-crawl-page-but-without-fetching-video-image-content-in-nutch-2-1", "last_activity_date": 1397507084, "accepted_answer_id": 14271428, "body": "<p>I want to crawl a page and I need to take only the HTML itself, avoiding all images/videos etc... \nIs it possible to do this?\nThanks in advance.</p>\n", "creation_date": 1357835380, "score": 2},
{"title": "Nutch - crawl domain first", "view_count": 300, "owner": {"user_id": 1349953, "answer_count": 3, "creation_date": 1335121399, "accept_rate": 100, "view_count": 14, "reputation": 22}, "is_answered": true, "answers": [{"question_id": 15597488, "owner": {"user_id": 2225199, "link": "http://stackoverflow.com/users/2225199/nimeshjm", "user_type": "registered", "reputation": 1137}, "body": "<p>You can get the list of crawled urls using this command:</p>\n\n<pre><code>bin/nutch readdb crawl/crawldb -dump file\n</code></pre>\n\n<p>You can then manually edit the urls/seed.txt file with the output from that command.</p>\n", "creation_date": 1364592310, "is_accepted": true, "score": 2, "last_activity_date": 1364592310, "answer_id": 15711631}], "question_id": 15597488, "tags": ["url", "web-crawler", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/15597488/nutch-crawl-domain-first", "last_activity_date": 1397507076, "accepted_answer_id": 15711631, "body": "<p>I am new to Nutch and have very and I try to make it do some specific crawling, i.e. I want it to first go e.g 3 levels deep withing one specific domain(e.g. wikipedia) - that part can be achieved by modifying regex-urlfilter file.</p>\n\n<p>But then I want it to start crawling all external links that it fetched before but only with 1 level depth. </p>\n\n<p>So, my question is, is there any way to get list of crawled links from first run so that they could be used as seeds for second crawling?</p>\n", "creation_date": 1364121980, "score": 0},
{"title": "apache nutch does not add sublinks to main site", "view_count": 103, "is_answered": false, "answers": [{"question_id": 16282100, "owner": {"user_id": 2225199, "link": "http://stackoverflow.com/users/2225199/nimeshjm", "user_type": "registered", "reputation": 1137}, "body": "<p>Try changing you regex filter to:</p>\n\n<pre><code>+^http://([a-z0-9]*.)transmetod.ru/\n</code></pre>\n\n<p>Also, when you first run Nutch, it will crawl the urls you put in your seed file.\nThe next time your run the crawl, using the same crawl folder, It should pick up the outlinks of the first page and crawl them.</p>\n", "creation_date": 1368561856, "is_accepted": false, "score": 0, "last_activity_date": 1368561856, "answer_id": 16551894}], "question_id": 16282100, "tags": ["database", "web-crawler", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/16282100/apache-nutch-does-not-add-sublinks-to-main-site", "last_activity_date": 1397507066, "owner": {"age": 30, "answer_count": 8, "creation_date": 1298044563, "user_id": 623417, "accept_rate": 53, "view_count": 15, "location": "Russia", "reputation": 126}, "body": "<p>Could anyone please give a guidence on how to properly configure apache nutch in order to get some amount of records in the database as a result of crawling a web site. I would very appreciate that!</p>\n\n<p>Here details:</p>\n\n<p>I've got the following line in my <code>bin/urls/seed.txt</code> file:</p>\n\n<pre><code>http://transmetod.ru/\n</code></pre>\n\n<p>The following is the line from regex-urlfilter.txt file (all other regexps are commented) :</p>\n\n<pre><code>+^http://([a-z0-9]*\\.)*transmetod.ru/([a-z0-9]*\\.)*\n</code></pre>\n\n<p>Basically I expect lots of records in the database to appear as a result of crawling, but the only thing a got there is just a single record with base url ( with out any other records with additional sublinks in the url )</p>\n\n<p>This is a command line I use to run <em>apache-nutch-2.1</em> project:</p>\n\n<pre><code>./nutch crawl urls -depth 3 -topN 10000\n</code></pre>\n\n<p>Can anyone point me out to mistake I've made or gust give some piece of advice ?</p>\n\n<p>P.S.: basically, when I built project and ran it without any changes, I didn't get a bunch of records as well... (if I remmember things right)</p>\n", "creation_date": 1367248080, "score": 0},
{"title": "Nutch: Job Failed", "view_count": 582, "owner": {"user_id": 2652226, "answer_count": 3, "creation_date": 1375688142, "accept_rate": 75, "view_count": 7, "reputation": 3}, "is_answered": true, "answers": [{"question_id": 22804590, "owner": {"user_id": 2652226, "accept_rate": 75, "link": "http://stackoverflow.com/users/2652226/lussi", "user_type": "registered", "reputation": 3}, "body": "<p>was using bin/nutch inject bin/crawl/crawldb bin/urls command to inject</p>\n\n<p>instead of bin/nutch inject crawl/crawldb bin/urls</p>\n\n<p>Which solves the error.</p>\n\n<p>and for fetching urls i have done changes to regex-urlfilter.txt file, now am able to fetch the urls.</p>\n", "creation_date": 1396498164, "is_accepted": true, "score": 0, "last_activity_date": 1396498164, "answer_id": 22827679}], "question_id": 22804590, "tags": ["ruby-on-rails", "solr", "web-crawler", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/22804590/nutch-job-failed", "last_activity_date": 1397504547, "accepted_answer_id": 22827679, "body": "<p>i have problem while running nutch for inject\nfollowing is the command i am running</p>\n\n<p>bin/nutch inject bin/crawl/crawldb bin/urls</p>\n\n<p>after running above command, gets following error</p>\n\n<pre><code>Injector: starting at 2014-04-02 13:02:29\nInjector: crawlDb: bin/crawl/crawldb\nInjector: urlDir: bin/urls/seed.txt\nInjector: Converting injected urls to crawl db entries.\nInjector: total number of urls rejected by filters: 2\nInjector: total number of urls injected after normalization and filtering: 0\nInjector: Merging injected urls into crawl db.\nInjector: overwrite: false\nInjector: update: false\nInjector: java.io.IOException: Job failed!\n    at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:1357)\n    at org.apache.nutch.crawl.Injector.inject(Injector.java:294)\n    at org.apache.nutch.crawl.Injector.run(Injector.java:316)\n    at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n    at org.apache.nutch.crawl.Injector.main(Injector.java:306)\n</code></pre>\n\n<p>I am running nutch for the first time.\ni have checked solr, nutch are installed properly.</p>\n\n<p>below details are from log file</p>\n\n<pre><code>java.io.IOException: The temporary job-output directory file:/usr/share/apache-nutch-1.8/bin/crawl/crawldb/1639805438/_temporary doesn't exist!\n    at org.apache.hadoop.mapred.FileOutputCommitter.getWorkPath(FileOutputCommitter.java:250)\n    at org.apache.hadoop.mapred.FileOutputFormat.getTaskOutputPath(FileOutputFormat.java:244)\n    at org.apache.hadoop.mapred.MapFileOutputFormat.getRecordWriter(MapFileOutputFormat.java:46)\n    at org.apache.hadoop.mapred.ReduceTask$OldTrackingRecordWriter.&lt;init&gt;(ReduceTask.java:449)\n    at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:491)\n    at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:421)\n    at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:398)\n2014-04-02 12:54:46,251 ERROR crawl.Injector - Injector: java.io.IOException: Job failed!\n    at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:1357)\n    at org.apache.nutch.crawl.Injector.inject(Injector.java:294)\n    at org.apache.nutch.crawl.Injector.run(Injector.java:316)\n    at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n    at org.apache.nutch.crawl.Injector.main(Injector.java:306)\n</code></pre>\n", "creation_date": 1396424299, "score": 0},
{"title": "Nutch crawl stopped after parsing one page", "view_count": 703, "is_answered": true, "answers": [{"question_id": 18761812, "owner": {"user_id": 2014559, "accept_rate": 100, "link": "http://stackoverflow.com/users/2014559/andrew-butkus", "user_type": "registered", "reputation": 425}, "body": "<p>check the robots.txt file for wikipedia at</p>\n\n<p><a href=\"http://en.wikipedia.org/robots.txt\" rel=\"nofollow\">http://en.wikipedia.org/robots.txt</a></p>\n\n<p>the robots.txt may deny further depth search. the robot file defines what web crawlers can access, and Nutch complies with this 'netiquitte'</p>\n\n<p>hope that helps</p>\n", "creation_date": 1381677654, "is_accepted": false, "score": 1, "last_activity_date": 1381677654, "answer_id": 19346807}], "question_id": 18761812, "tags": ["web-crawler", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/18761812/nutch-crawl-stopped-after-parsing-one-page", "last_activity_date": 1397504428, "owner": {"user_id": 2772176, "answer_count": 8, "creation_date": 1378980769, "accept_rate": 29, "view_count": 60, "location": "Hyderabad, India", "reputation": 55}, "body": "<p>While crawling using nutch, it is parsed only one page and not moving forward. Can anyone please help. Below is the nutch output.</p>\n\n<p>After parsing first page, it is stopping and not moving any further. Not parsed successfully.</p>\n\n<pre><code>[Naveen@01hw5189 apache-nutch-1.7]$ bin/nutch crawl urls -dir crawlwiki -depth 10 -topN 10\nsolrUrl is not set, indexing will be skipped...\ncrawl started in: crawlwiki\nrootUrlDir = urls\nthreads = 10\ndepth = 10\nsolrUrl=null\ntopN = 10\nInjector: starting at 2013-09-12 15:51:45\nInjector: crawlDb: crawlwiki/crawldb\nInjector: urlDir: urls\nInjector: Converting injected urls to crawl db entries.\nInjector: total number of urls rejected by filters: 1\nInjector: total number of urls injected after normalization and filtering: 1\nInjector: Merging injected urls into crawl db.\nInjector: finished at 2013-09-12 15:51:47, elapsed: 00:00:02\nGenerator: starting at 2013-09-12 15:51:47\nGenerator: Selecting best-scoring urls due for fetch.\nGenerator: filtering: true\nGenerator: normalizing: true\nGenerator: topN: 10\nGenerator: jobtracker is 'local', generating exactly one partition.\nGenerator: Partitioning selected urls for politeness.\nGenerator: segment: crawlwiki/segments/20130912155149\nGenerator: finished at 2013-09-12 15:51:50, elapsed: 00:00:03\nFetcher: Your 'http.agent.name' value should be listed first in 'http.robots.agents' property.\nFetcher: starting at 2013-09-12 15:51:50\nFetcher: segment: crawlwiki/segments/20130912155149\nUsing queue mode : byHost\nFetcher: threads: 10\nFetcher: time-out divisor: 2\nQueueFeeder finished: total 1 records + hit by time limit :0\nUsing queue mode : byHost\nUsing queue mode : byHost\n-finishing thread FetcherThread, activeThreads=1\nUsing queue mode : byHost\nfetching http://en.wikipedia.org/ (queue crawl delay=5000ms)\n-finishing thread FetcherThread, activeThreads=1\nUsing queue mode : byHost\n-finishing thread FetcherThread, activeThreads=1\nUsing queue mode : byHost\n-finishing thread FetcherThread, activeThreads=1\nUsing queue mode : byHost\n-finishing thread FetcherThread, activeThreads=1\nUsing queue mode : byHost\n-finishing thread FetcherThread, activeThreads=1\nUsing queue mode : byHost\n-finishing thread FetcherThread, activeThreads=1\nUsing queue mode : byHost\n-finishing thread FetcherThread, activeThreads=1\nUsing queue mode : byHost\n-finishing thread FetcherThread, activeThreads=1\nFetcher: throughput threshold: -1\nFetcher: throughput threshold retries: 5\n-activeThreads=1, spinWaiting=0, fetchQueues.totalSize=0\n-finishing thread FetcherThread, activeThreads=0\n-activeThreads=0, spinWaiting=0, fetchQueues.totalSize=0\n-activeThreads=0\nFetcher: finished at 2013-09-12 15:51:53, elapsed: 00:00:03\nParseSegment: starting at 2013-09-12 15:51:53\nParseSegment: segment: crawlwiki/segments/20130912155149\nParseSegment: finished at 2013-09-12 15:51:54, elapsed: 00:00:01\nCrawlDb update: starting at 2013-09-12 15:51:54\nCrawlDb update: db: crawlwiki/crawldb\nCrawlDb update: segments: [crawlwiki/segments/20130912155149]\nCrawlDb update: additions allowed: true\nCrawlDb update: URL normalizing: true\nCrawlDb update: URL filtering: true\nCrawlDb update: 404 purging: false\nCrawlDb update: Merging segment data into db.\nCrawlDb update: finished at 2013-09-12 15:51:56, elapsed: 00:00:02\nGenerator: starting at 2013-09-12 15:51:56\nGenerator: Selecting best-scoring urls due for fetch.\nGenerator: filtering: true\nGenerator: normalizing: true\nGenerator: topN: 10\nGenerator: jobtracker is 'local', generating exactly one partition.\nGenerator: Partitioning selected urls for politeness.\nGenerator: segment: crawlwiki/segments/20130912155159\nGenerator: finished at 2013-09-12 15:52:00, elapsed: 00:00:04\nFetcher: Your 'http.agent.name' value should be listed first in 'http.robots.agents' property.\nFetcher: starting at 2013-09-12 15:52:00\nFetcher: segment: crawlwiki/segments/20130912155159\nUsing queue mode : byHost\nFetcher: threads: 10\nFetcher: time-out divisor: 2\nQueueFeeder finished: total 1 records + hit by time limit :0\nUsing queue mode : byHost\nfetching http://en.wikipedia.org/wiki/Main_Page (queue crawl delay=5000ms)\nUsing queue mode : byHost\n-finishing thread FetcherThread, activeThreads=1\nUsing queue mode : byHost\n-finishing thread FetcherThread, activeThreads=1\nUsing queue mode : byHost\n-finishing thread FetcherThread, activeThreads=1\nUsing queue mode : byHost\n-finishing thread FetcherThread, activeThreads=1\nUsing queue mode : byHost\n-finishing thread FetcherThread, activeThreads=1\nUsing queue mode : byHost\n-finishing thread FetcherThread, activeThreads=1\nUsing queue mode : byHost\n-finishing thread FetcherThread, activeThreads=1\nUsing queue mode : byHost\n-finishing thread FetcherThread, activeThreads=1\nUsing queue mode : byHost\nFetcher: throughput threshold: -1\n-finishing thread FetcherThread, activeThreads=1\nFetcher: throughput threshold retries: 5\n-finishing thread FetcherThread, activeThreads=0\n-activeThreads=0, spinWaiting=0, fetchQueues.totalSize=0\n-activeThreads=0\nFetcher: finished at 2013-09-12 15:52:02, elapsed: 00:00:02\nParseSegment: starting at 2013-09-12 15:52:02\nParseSegment: segment: crawlwiki/segments/20130912155159\nParsed (8ms):http://en.wikipedia.org/wiki/Main_Page\n</code></pre>\n", "creation_date": 1378981849, "score": 2},
{"title": "What exactly is term &#39;segment&#39; in Nutch terminology?", "view_count": 231, "is_answered": true, "answers": [{"question_id": 15371347, "owner": {"user_id": 2082437, "accept_rate": 60, "link": "http://stackoverflow.com/users/2082437/abhinav", "user_type": "registered", "reputation": 767}, "body": "<p>Segment is a partition [hadoop input partition] created, by the map reduce jobs run by nutch, to start crawling from the input set of seed URL's given to crawler to crawl.</p>\n", "creation_date": 1363348996, "is_accepted": false, "score": 1, "last_activity_date": 1363348996, "answer_id": 15431968}], "question_id": 15371347, "tags": ["apache", "web-crawler", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/15371347/what-exactly-is-term-segment-in-nutch-terminology", "last_activity_date": 1397504075, "owner": {"user_id": 1830069, "answer_count": 10, "creation_date": 1353080229, "accept_rate": 63, "view_count": 103, "reputation": 543}, "body": "<p>I just got started on working with Nutch 1.6. I performed my initial crawling which went successful until the point where I came across the following issue:</p>\n\n<blockquote>\n  <p>LinkDb: adding segment:\n  file:/var/apache-nutch/crawl/segments/2013031234747 LinkDb: adding\n  segment: file:/var/apache-nutch/crawl/segments/2013031250939 Exception\n  in thread \"main\" org.apache.hadoop.mapred.InvalidInputException: Input\n  path does not exist:\n  file:/var/apache-nutch/crawl/segments/20130308114306/parse_data Input\n  path does not exist:\n  file:/var/apache-nutch/crawl/segments/20130312135244/parse_data\n      at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:197)\n      at org.apache.hadoop.mapred.SequenceFileInputFormat.listStatus(SequenceFileInputFormat.java:40)\n      at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:208)\n      at org.apache.hadoop.mapred.JobClient.writeOldSplits(JobClient.java:989)\n      at org.apache.hadoop.mapred.JobClient.writeSplits(JobClient.java:981)\n      at org.apache.hadoop.mapred.JobClient.access$600(JobClient.java:174)\n      at org.apache.hadoop.mapred.JobClient$2.run(JobClient.java:897)\n      at org.apache.hadoop.mapred.JobClient$2.run(JobClient.java:850)\n      at java.security.AccessController.doPrivileged(Native Method)\n      at javax.security.auth.Subject.doAs(Subject.java:415)\n      at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1121)\n      at org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:850)\n      at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:824)\n      at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:1261)\n      at org.apache.nutch.crawl.LinkDb.invert(LinkDb.java:180)\n      at org.apache.nutch.crawl.LinkDb.invert(LinkDb.java:151)\n      at org.apache.nutch.crawl.Crawl.run(Crawl.java:143)\n      at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n      at org.apache.nutch.crawl.Crawl.main(Crawl.java:55)</p>\n</blockquote>\n\n<p>I would like to understand what is exactly being \"segmented\" in nutch? In the start of the above error, it says \"LinkdB: adding segment..\" what is it trying to do? what are we segmenting?</p>\n", "creation_date": 1363119682, "score": 0},
{"title": "Creating a dataset by web crawling", "view_count": 419, "owner": {"user_id": 601357, "answer_count": 27, "creation_date": 1296729805, "accept_rate": 58, "view_count": 245, "reputation": 1303}, "is_answered": true, "answers": [{"question_id": 8960996, "owner": {"user_id": 1150329, "accept_rate": 89, "link": "http://stackoverflow.com/users/1150329/tejas-patil", "user_type": "registered", "reputation": 4793}, "body": "<p>You can NOT directly convert the nutch crawled segments to html files directly.</p>\n\n<p>I suggest you these options:</p>\n\n<ol>\n<li>\nYou can try modifying the source code to do that. (study the <code>org.apache.nutch.segment.SegmentReader</code> class. You can then dig into it to modify the working as per your use case).</li>\n<li>EASY SOLUTION if you dont want to invest time to study code: Use nutch to crawl all required pages. Then get the actual urls crawled by using the \"<code>bin/nutch readdb</code>\" command (use dump option). Then write a script to wget the urls and save it in html form. Done !!</li>\n</ol>\n", "creation_date": 1333465840, "is_accepted": true, "score": 1, "last_activity_date": 1333465840, "answer_id": 9996198}], "question_id": 8960996, "tags": ["dataset", "web-crawler", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/8960996/creating-a-dataset-by-web-crawling", "last_activity_date": 1397502427, "accepted_answer_id": 9996198, "body": "<p>I want to build a dataset consisting about 2000-3000 web pages, starting with several seed URLs. I tried it using the Nutch crawler but I was unable to get it done (unable to convert the 'segments' data fetched into html pages) . </p>\n\n<p>Any suggestions of a different crawler that you have used or any other tool? What if web pages contain absolute URLs which will make offline use of the dataset impossible? </p>\n", "creation_date": 1327234897, "score": 0},
{"title": "Apache Nutch does not index the entire website, only subfolders", "view_count": 1637, "is_answered": true, "answers": [{"question_id": 4992026, "owner": {"user_id": 712175, "link": "http://stackoverflow.com/users/712175/luiscappa", "user_type": "unregistered", "reputation": 11}, "body": "<p>Check out if you\u00b4ve got intra domain links limitation (property as false in nutch-site.xml). Also check out other properties as maximun intra-extra links per page and http size. Sometimes they produce wrong results during crawling.</p>\n\n<p>Ciao!</p>\n", "creation_date": 1303048636, "is_accepted": false, "score": 1, "last_activity_date": 1303048636, "answer_id": 5693930}, {"question_id": 4992026, "owner": {"user_id": 1357196, "link": "http://stackoverflow.com/users/1357196/user1357196", "user_type": "registered", "reputation": 81}, "body": "<p>While attempting to crawl all links from an index page, I discovered that nutch was limited to exactly 100 links of around 1000. The setting that was holding me back was:</p>\n\n<pre><code>db.max.outlinks.per.page\n</code></pre>\n\n<p>Setting this to 2000 allowed nutch to index all of them in one shot.</p>\n", "creation_date": 1372790071, "is_accepted": false, "score": 1, "last_activity_date": 1372790071, "answer_id": 17433221}], "question_id": 4992026, "tags": ["solr", "web-crawler", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/4992026/apache-nutch-does-not-index-the-entire-website-only-subfolders", "last_activity_date": 1397502383, "owner": {"user_id": 616146, "view_count": 0, "answer_count": 0, "creation_date": 1297684660, "reputation": 6}, "body": "<p>Apache Nutch 1.2 does not index the entire website, only subfolders. My index-page provides links in most areas/subfolders of my website. For example stuff, students, research... But nutch only crawl in one specific folder - \"students\" in this case. Seems as if links in other directories are not followed.</p>\n\n<p>crawl-urlfilter.txt:\n+^http://www5.my-domain.de/</p>\n\n<p>seed.txt in the URLs-folder:\n<a href=\"http://www5.my-domain.de/\" rel=\"nofollow\">http://www5.my-domain.de/</a></p>\n\n<p>Starting nutch with(windows/linux both used):\nnutch crawl \"D:\\Programme\\nutch-1.2\\URLs\" -dir \"D:\\Programme\\nutch-1.2\\crawl\" -depth 10 -topN 1000000</p>\n\n<p>Different variants for depth(5-23) and topN(100-1000000) are tested. Providing more links in seed.txt doesnt help at all, still not following links found in injected pages.</p>\n\n<p>Interestingly, crawling gnu.org works perfect. No robots.txt or preventing meta-tags used in my site.</p>\n\n<p>Any ideas?</p>\n", "creation_date": 1297685708, "score": 1},
{"title": "Nutch crawler not indexing HTML content", "view_count": 523, "is_answered": true, "answers": [{"last_edit_date": 1321692899, "owner": {"user_id": 950178, "link": "http://stackoverflow.com/users/950178/jayendra", "user_type": "registered", "reputation": 34896}, "body": "<p>Nutch basically crawls through links on the pages.<br>\nHowever, there are no links on the <a href=\"http://www.imd.gov.in/section/nhac/distforecast/INDIAct.htm%20page\" rel=\"nofollow\">India page</a> for it to reach the <a href=\"http://www.imd.gov.in/section/nhac/distforecast/delhi.htm\" rel=\"nofollow\">Delhi page</a> mentioned by you.<br>\nSo it won't be able to navigate it down to that page.</p>\n\n<p>You can create your own dummy html page, acting as the start url for indexing, and have all the links you want Nutch to index.</p>\n\n<p>Whats the default search field in you schema ?<br>\nUsually its the text field, and querying for delhi would look into that field for matches.<br>\nAs <code>*:*</code> returns the delhi result, and delhi does not. Its not matching the indexed tokens on the field it is searching on.</p>\n\n<p>Whats the field type defined for url in the schema ?<br>\nYou can copy the field to an other field with text analysis, which would produce the delhi token and querying for <code>url_copy:delhi</code> should return you the results.</p>\n", "question_id": 8182899, "creation_date": 1321641682, "is_accepted": false, "score": 1, "last_activity_date": 1321692899, "answer_id": 8187424}], "question_id": 8182899, "tags": ["solr", "web-crawler", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/8182899/nutch-crawler-not-indexing-html-content", "last_activity_date": 1397501584, "owner": {"age": 28, "answer_count": 0, "creation_date": 1315574433, "user_id": 941266, "accept_rate": 43, "view_count": 17, "location": "Mysore, India", "reputation": 47}, "body": "<p>I am trying to develop a search functionality where I enter a city name and it gives me the weather conditions for that city.<br>\nI have set up Nutch-1.3 and Solr-3.4.0 on my system. The website I am crawling is <a href=\"http://www.imd.gov.in/section/nhac/distforecast/INDIAct.htm\" rel=\"nofollow\">here</a> and passing the index to Solr for searching.Now, I want to retrieve the information displayed on <a href=\"http://www.imd.gov.in/section/nhac/distforecast/delhi.htm\" rel=\"nofollow\">this link</a>, on querying for delhi.</p>\n\n<p>How can I achieve this? Does it require any plugin to be written?</p>\n\n<pre><code> &lt;doc&gt;&lt;float name=\"score\"&gt;1.0&lt;/float&gt;&lt;float name=\"boost\"&gt;0.1879294&lt;/float&gt;&lt;str name=\"content\"/&gt;&lt;str name=\"digest\"&gt;d41d8cd98f00b204e9800998ecf8427e&lt;/str&gt;&lt;str name=\"id\"&gt;http://www.imd.gov.in/section/nhac/distforecast/delhi.htm&lt;/str&gt;&lt;str name=\"segment\"&gt;20111118153543&lt;/str&gt;&lt;str name=\"title\"/&gt;&lt;date name=\"tstamp\"&gt;2011-11-18T10:06:45.604Z&lt;/date&gt;&lt;str name=\"url\"&gt;http://www.imd.gov.in/section/nhac/distforecast/delhi.htm&lt;/str&gt;&lt;/doc&gt;\n</code></pre>\n", "creation_date": 1321621731, "score": 0},
{"title": "Web Cralwer Algorithm: depth?", "view_count": 9261, "owner": {"user_id": 253976, "answer_count": 23, "creation_date": 1263902980, "accept_rate": 88, "view_count": 4439, "reputation": 10123}, "is_answered": true, "answers": [{"question_id": 4356524, "owner": {"user_id": 89806, "accept_rate": 58, "link": "http://stackoverflow.com/users/89806/marcog", "user_type": "registered", "reputation": 60528}, "body": "<p>Link depth means how many links you have to follow before you reach a given link.</p>\n\n<p>Example: example.com links to example.com/foo.html which links to google.com. Therefore google.com has a link depth of 2 relative to example.com as you can reach it following 2 links.</p>\n\n<p>To crawl example.com to a depth of 3, you would follow links to a maximum depth of 3 and then stop following links. Without this restriction, you could easily go on forever.</p>\n\n<p>Example: example.com links to example.com/foo.html and example.com/bar.html. You follow those two links, the links they link to and stop there.</p>\n\n<p>Note: The root page has a depth of 0.</p>\n", "creation_date": 1291507017, "is_accepted": false, "score": 0, "last_activity_date": 1291507017, "answer_id": 4356537}, {"question_id": 4356524, "owner": {"user_id": 300420, "link": "http://stackoverflow.com/users/300420/roadmaster", "user_type": "registered", "reputation": 4405}, "body": "<p>A web site's root is at depth 0. Documents you can reach by using links in the root are at depth 1. Documents you can in turn reach from links in documents at depth 1 would be at depth 2. And so on.</p>\n\n<p>Depending on your crawler this might apply to only documents in the same site/domain (usual) or documents hosted elsewhere.</p>\n\n<p>Most web sites are not representable by a binary tree, as the \"root\" might have more than two \"nodes\". </p>\n", "creation_date": 1291507090, "is_accepted": false, "score": 0, "last_activity_date": 1291507090, "answer_id": 4356544}, {"last_edit_date": 1291746619, "owner": {"user_id": 529237, "link": "http://stackoverflow.com/users/529237/chrisj", "user_type": "registered", "reputation": 3951}, "body": "<p>I guess the \"depth\" is the number of times the crawler \"follows a link\".</p>\n\n<p>Say you start from the root page. You follow each of the links on this page: this is depth 1. For each of the target pages, you follow the links: this is depth 2, etc.</p>\n\n<p>Note that there may be \"cycles\" while following links. The structure is not a tree, but a graph.</p>\n", "question_id": 4356524, "creation_date": 1291507121, "is_accepted": false, "score": 2, "last_activity_date": 1291746619, "answer_id": 4356545}, {"question_id": 4356524, "owner": {"user_id": 492364, "accept_rate": 82, "link": "http://stackoverflow.com/users/492364/thejh", "user_type": "registered", "reputation": 27647}, "body": "<ol>\n<li>Make a list that you use as a queue.</li>\n<li>Append <code>www.domain.com, depth 0</code> to it</li>\n<li>Pull the first element off it</li>\n<li>current depth is the elements depth+1</li>\n<li>Crawl that site</li>\n<li>Append each link on the site to the queue if the current depth isn't greater than the maximum depth</li>\n<li>If the list isn't empty, go back to 3..</li>\n</ol>\n", "creation_date": 1291507150, "is_accepted": false, "score": 2, "last_activity_date": 1291507150, "answer_id": 4356547}, {"last_edit_date": 1291508944, "owner": {"user_id": 166749, "accept_rate": 94, "link": "http://stackoverflow.com/users/166749/fred-foo", "user_type": "registered", "reputation": 229875}, "body": "<p>Link depth means the number of \"hops\" a page is be away from the root, where a \"hop\" means following a link on a page. The reason Nutch has this restriction is that links very \"far away\" from the main page are unlikely to hold much information (the main page will link to the most important information, so the farther you get, the more detailed info you find), while there can be very many of them, so they take up lots of storage space, computing time for ranking, and bandwidth.</p>\n\n<p>Nutch thus uses an algorithm scheme known as <a href=\"http://en.wikipedia.org/wiki/Depth-limited_search\">depth-limited search</a> to bound its running time and space usage. If it didn't use this heuristic, it would have to crawl an entire site to rank all the pages in it and find the top <em>N</em>.</p>\n\n<p>To crawl to depth 3, implement this algorithm and give it a depth bound of three. The nice thing about depth-limited search is that it's a variant of depth-first search (DFS), so it's quite space-efficient:</p>\n\n<pre><code>function depth-limited-crawl(page p, int d)\n    if d == 0\n        return\n    /* do something with p, store it or so */\n    foreach (page l in links(p))\n        depth-limited-crawl(linked, d-1)\n</code></pre>\n\n<p>And no, a site cannot in general be represented as a binary tree; it's a directed graph. If you somehow remove the backlinks, then it becomes a multiway tree. Either way, many sites are too large to store for your crawler.</p>\n", "question_id": 4356524, "creation_date": 1291507344, "is_accepted": true, "score": 9, "last_activity_date": 1291508944, "answer_id": 4356560}, {"question_id": 4356524, "owner": {"user_id": 484046, "accept_rate": 81, "link": "http://stackoverflow.com/users/484046/millebii", "user_type": "registered", "reputation": 976}, "body": "<p>Well in the case of Nutch, the depth argument is quite a misnommer, it just means the number of loops the crawler is going through. So  you will reach pages which are 3 links away from your seed urls... on a given site it might in depth 3... that is if they hand up being within the top N limits.</p>\n", "creation_date": 1291578610, "is_accepted": false, "score": 0, "last_activity_date": 1291578610, "answer_id": 4360789}, {"question_id": 4356524, "owner": {"user_id": 2433063, "accept_rate": 18, "link": "http://stackoverflow.com/users/2433063/ricky-wilson", "user_type": "registered", "reputation": 899}, "body": "<p>Depth is the number of slashes it the url path</p>\n\n<p>example <a href=\"http://www.google.com/foo/bar/baz\" rel=\"nofollow\">http://www.google.com/foo/bar/baz</a> has a depth of 3</p>\n\n<pre><code>def depth(self,link):\n    return len(urlparse.urlparse(url).path.split(\"/\")) -1\n</code></pre>\n", "creation_date": 1396936471, "is_accepted": false, "score": 0, "last_activity_date": 1396936471, "answer_id": 22928619}], "question_id": 4356524, "tags": ["algorithm", "web-crawler", "nutch"], "answer_count": 7, "link": "http://stackoverflow.com/questions/4356524/web-cralwer-algorithm-depth", "last_activity_date": 1397501555, "accepted_answer_id": 4356560, "body": "<p>I'm working on a crawler and need to understand exactly what is meant by \"link depth\". Take nutch for example: <a href=\"http://wiki.apache.org/nutch/NutchTutorial\" rel=\"nofollow\">http://wiki.apache.org/nutch/NutchTutorial</a></p>\n\n<blockquote>\n  <p>depth indicates the link depth from the root page that should be\n  crawled.</p>\n</blockquote>\n\n<p>So, say I have the domain <code>www.domain.com</code> and wanted to crawl a depth of, say, <code>3</code> -- what do I need to do? If a site could be represented as a binary tree, then it wouldn't be a problem I think.</p>\n", "creation_date": 1291506844, "score": 2},
{"title": "Best web graph crawler for speed?", "view_count": 2376, "owner": {"user_id": 384954, "answer_count": 91, "creation_date": 1278449018, "accept_rate": 76, "view_count": 293, "reputation": 2899}, "is_answered": true, "answers": [{"last_edit_date": 1283437046, "owner": {"user_id": 438033, "link": "http://stackoverflow.com/users/438033/whalebot-helmsman", "user_type": "registered", "reputation": 56}, "body": "<p>This my be fault of server not Scrapy. Server may be not so fast as you want or may be it (or webmaster) detects crawling and limit speed for this connection/cookie.\nDo you use proxy? This may slow down crawling too.\nThis may be Scrapy wisdom, if you will crawl too intensive you may get ban on this server. For my C++ <a href=\"http://code.google.com/p/whalebot\" rel=\"nofollow\">handwritten crawler</a> I artificially set 1 request per second limit. But this speed is enough for 1 thread ( 1 req * 60 secs * 60 minutes * 24 hours = 86400 req / day ). If you interested you may write email to whalebot.helmsman {AT} gmail.com .</p>\n", "question_id": 3424027, "creation_date": 1283435910, "is_accepted": true, "score": 4, "last_activity_date": 1283437046, "answer_id": 3627808}, {"question_id": 3424027, "owner": {"user_id": 395287, "accept_rate": 84, "link": "http://stackoverflow.com/users/395287/tim-mcnamara", "user_type": "registered", "reputation": 10493}, "body": "<p>Scrapy allows you to determine the number of concurrent requests and the delay between the requests in <a href=\"http://doc.scrapy.org/topics/settings.html\" rel=\"nofollow\">its settings</a>.</p>\n", "creation_date": 1283819044, "is_accepted": false, "score": 1, "last_activity_date": 1283819044, "answer_id": 3655019}, {"question_id": 3424027, "owner": {"user_id": 87451, "link": "http://stackoverflow.com/users/87451/pablo-hoffman", "user_type": "registered", "reputation": 1285}, "body": "<p>Do you know where the bottleneck is?. As whalebot.helmsman pointed out, the limit may not be on Scrapy itself, but on the server you're crawling.</p>\n\n<p>You should start by finding out whether the bottleneck is the network or CPU.</p>\n", "creation_date": 1284926245, "is_accepted": false, "score": 0, "last_activity_date": 1284926245, "answer_id": 3747226}], "question_id": 3424027, "tags": ["scrapy", "web-crawler", "nutch"], "answer_count": 3, "link": "http://stackoverflow.com/questions/3424027/best-web-graph-crawler-for-speed", "last_activity_date": 1397501534, "accepted_answer_id": 3627808, "body": "<p>For the past month I've been using Scrapy for a web crawling project I've begun.</p>\n\n<p>This project involves pulling down the full document content of all web pages in a single domain name that are reachable from the home page. Writing this using Scrapy was quite easy, but it simply runs too slowly. In 2-3 days I can only pull down 100,000 pages.</p>\n\n<p>I've realized that my initial notion that Scrapy isn't meant for this type of crawl is revealing itself.</p>\n\n<p>I've begun to focus my sights on Nutch and <a href=\"http://bithack.se/projects/methabot/\" rel=\"nofollow\">Methabot</a> in hopes of better performance. The only data that I need to store during the crawl is the full content of the web page and preferably all the links on the page (but even that can be done in post-processing).</p>\n\n<p>I'm looking for a crawler that is fast and employs many parallel requests.</p>\n", "creation_date": 1281100127, "score": 4},
{"title": "To enable crawling urls with special characters in nutch", "view_count": 469, "is_answered": false, "answers": [{"last_edit_date": 1364854162, "owner": {"user_id": 2225199, "link": "http://stackoverflow.com/users/2225199/nimeshjm", "user_type": "registered", "reputation": 1137}, "body": "<p>Have a look at crawl-urlfilter.txt.\nThere's a similar entry and it should also be commented if you <em>really</em> want to crawl these urls.</p>\n\n<pre><code># skip URLs containing certain characters as probable queries, etc.\n-[?*!@=]\n</code></pre>\n\n<p>Be careful or you might end in an infinite loop situation, e.g paging that never ends.</p>\n\n<p>You should add an exception instead of changing the rule.</p>\n\n<pre><code>   +www.my-website.com/page.*\n   # skip URLs containing certain characters as probable queries, etc. \n   -[?*!@=] \n</code></pre>\n", "question_id": 15742939, "creation_date": 1364853772, "is_accepted": false, "score": 0, "last_activity_date": 1364854162, "answer_id": 15752704}], "question_id": 15742939, "tags": ["solr", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/15742939/to-enable-crawling-urls-with-special-characters-in-nutch", "last_activity_date": 1396871804, "owner": {"age": 26, "answer_count": 1, "creation_date": 1351144807, "user_id": 1773245, "view_count": 16, "location": "Hyderabad", "reputation": 56}, "body": "<p>I am using apache nutch-1.5.1 version and solr 3.6.2 integrated with hadoop 1.0.3.\nI  I want to crawl domain for example \"www.my-website.com\" there are different pages like\n 1. www.my-website.com/page.1\n 2. www.my-website.com/page.2..\nwww.my-website.com/page.1 there is lot of information like(www.my-website.com/page.1/search?page=2&amp;tab=relevance&amp;q=222) i am unable to crawl this type of links (which contains the special chracters(?,=))..\nI commented in regex.urlfilter.txt</p>\n\n<blockquote>\n  <h1>skip URLs containing certain characters as probable queries, etc.</h1>\n  \n  <h1>-[?*!@=]..</h1>\n</blockquote>\n\n<p>.\nbut still there is no difference in the output ....Please share your thoughts\nThanks in advance\nJaipal R</p>\n", "creation_date": 1364817012, "score": 0},
{"title": "Error while indexing in solr data crawled by nutch", "view_count": 3775, "is_answered": true, "answers": [{"question_id": 13429481, "owner": {"user_id": 453596, "accept_rate": 77, "link": "http://stackoverflow.com/users/453596/kamaci", "user_type": "registered", "reputation": 21412}, "body": "<p>Changing configuration at Nutch side does not effect the schema of Solr. You have to define that field at schema.xml of Solr.</p>\n", "creation_date": 1396817623, "is_accepted": false, "score": 2, "last_activity_date": 1396817623, "answer_id": 22899888}], "question_id": 13429481, "tags": ["solr", "indexing", "runtime-error", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/13429481/error-while-indexing-in-solr-data-crawled-by-nutch", "last_activity_date": 1396817623, "owner": {"user_id": 1831647, "view_count": 19, "answer_count": 0, "creation_date": 1353145680, "reputation": 41}, "body": "<p>I have starting working with nutch and solr and I have a problem with integrating Solr with Nutch. I followed this tutorial: <a href=\"http://wiki.apache.org/nutch/NutchTutorial\" rel=\"nofollow\">http://wiki.apache.org/nutch/NutchTutorial</a> and after using:\n<code>bin/nutch crawl urls -solr http://localhost:8983/solr/ -depth 3 -topN 5</code>\nnutch shows message: </p>\n\n<blockquote>\n  <p>java.io.IOException: Job failed!</p>\n</blockquote>\n\n<p>and solr is showing: </p>\n\n<blockquote>\n  <p>SEVERE: org.apache.solr.common.SolrException: ERROR:\n  [doc=http://nutch.apache.org/] unknown field 'host'</p>\n</blockquote>\n\n<p>I thought that the reason might be a missing 'host' field in the $SOLR_HOME/example/solr/conf/schema.xml but it is there.\nI would be very grateful for your help.</p>\n", "creation_date": 1353146164, "score": 8},
{"title": "Nutch how to crawl all links from one website?", "view_count": 140, "is_answered": false, "answers": [{"question_id": 22688913, "owner": {"user_id": 2623052, "accept_rate": 45, "link": "http://stackoverflow.com/users/2623052/allan-macmillan", "user_type": "registered", "reputation": 866}, "body": "<p><code>bin/nutch crawl</code> is the command you are looking for</p>\n", "creation_date": 1396609675, "is_accepted": false, "score": 0, "last_activity_date": 1396609675, "answer_id": 22861125}], "question_id": 22688913, "tags": ["solr", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/22688913/nutch-how-to-crawl-all-links-from-one-website", "last_activity_date": 1396609675, "owner": {"user_id": 3460377, "answer_count": 3, "creation_date": 1395760847, "accept_rate": 17, "view_count": 11, "reputation": 1}, "body": "<p>for now i use the follow commands to crawl a website:</p>\n\n<pre><code> bin/nutch generate -topN 20\n bin/nutch fetch -all\n bin/nutch parse -all\n bin/nutch updatedb\n</code></pre>\n\n<p>but with this method it takes ages before i have all links from that website. I want to crawl one website and get all the links.</p>\n\n<p>how can i achieve this?</p>\n", "creation_date": 1395926357, "score": 0},
{"title": "Apache nutch 1.5 and solr 4.7 indexing", "view_count": 572, "is_answered": false, "question_id": 22826295, "tags": ["java", "solr", "lucene", "nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/22826295/apache-nutch-1-5-and-solr-4-7-indexing", "last_activity_date": 1396609570, "owner": {"user_id": 1895298, "answer_count": 5, "creation_date": 1355243827, "accept_rate": 62, "view_count": 81, "reputation": 624}, "body": "<p>I have crawled websites using apache nutch and want to index the data in solr. I have been following the tutorial mentioned <a href=\"http://www.gastongonzalez.com/tech-blog/2013/10/13/a-step-by-step-guide-to-indexing-cq-with-nutch\" rel=\"nofollow\">here</a>\nHowever the tutorial mentions about indexing as it crawls except in my case I need to index the data that already has been crawled. </p>\n\n<p>I am running the below command</p>\n\n<pre><code>bin/nutch solrindex http://127.0.0.1:8983/solr/ crawl/crawldb -linkdb crawl/linkdb crawl/segments/*\n\n[abc@xyz nutch-crawler]$ bin/nutch index http://abc.xyz:8983/solr/ pryder/crawldb/ -linkdb pryder/linkdb/ pryder/segments/20140330021243/\nIndexer: starting at 2014-04-02 20:34:09\nIndexer: deleting gone documents: false\nIndexer: URL filtering: false\nIndexer: URL normalizing: false\nException in thread \"main\" java.lang.NoClassDefFoundError: org/apache/solr/client/solrj/impl/CommonsHttpSolrServer\n    at java.lang.Class.getDeclaredConstructors0(Native Method)\n    at java.lang.Class.privateGetDeclaredConstructors(Class.java:2398)\n    at java.lang.Class.getConstructor0(Class.java:2708)\n    at java.lang.Class.newInstance0(Class.java:328)\n    at java.lang.Class.newInstance(Class.java:310)\n    at org.apache.nutch.plugin.Extension.getExtensionInstance(Extension.java:157)\n    at org.apache.nutch.indexer.IndexWriters.&lt;init&gt;(IndexWriters.java:57)\n    at org.apache.nutch.indexer.IndexingJob.index(IndexingJob.java:91)\n    at org.apache.nutch.indexer.IndexingJob.run(IndexingJob.java:176)\n    at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n    at org.apache.nutch.indexer.IndexingJob.main(IndexingJob.java:186)\nCaused by: java.lang.ClassNotFoundException: org.apache.solr.client.solrj.impl.CommonsHttpSolrServer\n    at java.net.URLClassLoader$1.run(URLClassLoader.java:202)\n    at java.security.AccessController.doPrivileged(Native Method)\n    at java.net.URLClassLoader.findClass(URLClassLoader.java:190)\n    at java.lang.ClassLoader.loadClass(ClassLoader.java:306)\n    at java.lang.ClassLoader.loadClass(ClassLoader.java:247)\n    ... 11 more\n</code></pre>\n\n<p>What would be going wrong here?</p>\n", "creation_date": 1396489264, "score": 3},
{"title": "Nutch - Crawler not following next pages in paginated content", "view_count": 954, "is_answered": false, "answers": [{"question_id": 16720395, "owner": {"user_id": 1150329, "accept_rate": 89, "link": "http://stackoverflow.com/users/1150329/tejas-patil", "user_type": "registered", "reputation": 4793}, "body": "<p>Unfortunately Nutch 1.6 lacks the support for crawling Ajax based sites. See <a href=\"http://lucene.472066.n3.nabble.com/How-to-crowl-AJAX-populated-pages-td3783398.html\" rel=\"nofollow\">this</a> and <a href=\"https://issues.apache.org/jira/browse/NUTCH-1323\" rel=\"nofollow\">this</a>. There are no immediate plans to add the same.</p>\n", "creation_date": 1369565539, "is_accepted": false, "score": 0, "last_activity_date": 1369565539, "answer_id": 16758520}, {"question_id": 16720395, "owner": {"user_id": 2225199, "link": "http://stackoverflow.com/users/2225199/nimeshjm", "user_type": "registered", "reputation": 1137}, "body": "<p>The regex-urlfilter blocks urls that have querystring parameters:</p>\n\n<p># skip URLs containing certain characters as probable queries, etc.</p>\n\n<p>-[?*!@=]</p>\n\n<p>Modify that file so that urls with querystring parameters are crawled:</p>\n\n<p># skip URLs containing certain characters as probable queries, etc.</p>\n\n<p>-[*!@]</p>\n", "creation_date": 1371402023, "is_accepted": false, "score": 0, "last_activity_date": 1371402023, "answer_id": 17135649}], "question_id": 16720395, "tags": ["solr", "web-crawler", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/16720395/nutch-crawler-not-following-next-pages-in-paginated-content", "last_activity_date": 1396535433, "owner": {"age": 26, "answer_count": 34, "creation_date": 1364721047, "user_id": 2228912, "accept_rate": 88, "view_count": 141, "reputation": 446}, "body": "<p>I'm using nutch 1.6 to crawl a paginated web page containing 20 products/page, with this command:  </p>\n\n<p><code>./nutch crawl urls -dir &lt;dir&gt; -depth 4 -topN 100 -threads 100</code></p>\n\n<p>I'm getting the 20 first products &amp; the links to the following pages. But the crawler is not following my next pages link? Am I missing a parameter?</p>\n", "creation_date": 1369330583, "score": 0},
{"title": "Installing nutch and elasticsearch", "view_count": 267, "is_answered": false, "question_id": 22826832, "tags": ["lucene", "installation", "elasticsearch", "nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/22826832/installing-nutch-and-elasticsearch", "last_activity_date": 1396492787, "owner": {"user_id": 3491908, "view_count": 0, "answer_count": 0, "creation_date": 1396492158, "reputation": 1}, "body": "<p>I am a beginner, studying networking, and am investigating nutch 2.x and Elasticsearch 1.x combination. I want to install them on win 7 professional to crawl and create index for our school library. (The library already has its' own search platform, this is just my semestral task...). My issue is that I am lost between conflicting information on internet and do not understand whether I need Apache Lucene for Elasticsearch or not, since it is a \"standalone\" indexing server, yet it is \"built upon Lucene\", then which application should I install first and how to integrate everything. Including Nutch, Elasticsearch and (or not) Apache Lucene.</p>\n\n<p>A step-by-step advice would be very much appreciated. </p>\n\n<p>Matt</p>\n", "creation_date": 1396492787, "score": 0},
{"title": "How to parse content located in specific HTML tags using nutch plugin?", "view_count": 4206, "is_answered": true, "answers": [{"question_id": 17972582, "owner": {"user_id": 1881318, "link": "http://stackoverflow.com/users/1881318/tahagh", "user_type": "registered", "reputation": 619}, "body": "<p>You can use this plugin to extract data from your pages based on css rules:</p>\n\n<p><a href=\"https://github.com/BayanGroup/nutch-custom-search\" rel=\"nofollow\">https://github.com/BayanGroup/nutch-custom-search</a></p>\n\n<p>In your example, you can configure it in this way:</p>\n\n<pre><code>&lt;config&gt;\n    &lt;fields&gt;\n        &lt;field name=\"custom_content\" /&gt;\n    &lt;/fields&gt;\n    &lt;documents&gt;\n        &lt;document url=\".+\" engine=\"css\"&gt;\n            &lt;extract-to field=\"custom_content\"&gt;\n                &lt;text&gt;\n                    &lt;expr value=\"#abc\" /&gt;\n                &lt;/text&gt;\n                &lt;text&gt;\n                    &lt;expr value=\".efg\" /&gt;\n                &lt;/text&gt;\n            &lt;/extract-to&gt;\n        &lt;/document&gt;\n    &lt;/documents&gt;\n&lt;/config&gt;\n</code></pre>\n", "creation_date": 1387368522, "is_accepted": false, "score": 4, "last_activity_date": 1387368522, "answer_id": 20657837}], "question_id": 17972582, "tags": ["nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/17972582/how-to-parse-content-located-in-specific-html-tags-using-nutch-plugin", "last_activity_date": 1396368870, "owner": {"user_id": 1474659, "answer_count": 6, "creation_date": 1340364399, "accept_rate": 48, "view_count": 118, "reputation": 321}, "body": "<p>I am using Nutch to crawl websites and I want to parse specific sections of html pages crawled by Nutch. For example,</p>\n\n<pre><code>  &lt;h&gt;&lt;title&gt; title to search &lt;/title&gt;&lt;/h&gt;\n   &lt;div id=\"abc\"&gt;\n        content to search\n   &lt;/div&gt;\n   &lt;div class=\"efg\"&gt;\n        other content to search\n   &lt;/div&gt;\n</code></pre>\n\n<p>I want to parse div element with id =\"abc\" and class=\"efg\" and so on. </p>\n\n<p>I know that I have to create a plugin for customized parsing as htmlparser plugin provided by Nutch removes all html tags, css and javascript content and leaves only text content. I refered to this blog <a href=\"http://sujitpal.blogspot.in/2009/07/nutch-custom-plugin-to-parse-and-add.html\">http://sujitpal.blogspot.in/2009/07/nutch-custom-plugin-to-parse-and-add.html</a> but I found that this is for parsing with html tag whereas I want to parse html tags with attribute having specific value. I found that Jericho has been mentioned as useful for parsing specific html tags but I could find any example for nutch plugin associated with Jericho.</p>\n\n<p>I need some guidance about how to devise a strategy for parsing html pages on the basis of tags with attribute having specific value.</p>\n", "creation_date": 1375279359, "score": 6},
{"title": "OutOfMemoryError for bin/nutch elasticindex &lt;$cluser&gt; -all (Nutch 2.1)", "view_count": 1381, "owner": {"user_id": 859955, "answer_count": 66, "creation_date": 1311484167, "accept_rate": 91, "view_count": 167, "reputation": 1052}, "is_answered": true, "answers": [{"question_id": 16729940, "owner": {"user_id": 2553841, "link": "http://stackoverflow.com/users/2553841/artis", "user_type": "registered", "reputation": 26}, "body": "<p>I have exactly the same problem. I work with elasticsearch 0.90.2.\nI found a solution : with elasticsearch 0.19.4 it works !</p>\n", "creation_date": 1373029416, "is_accepted": true, "score": 1, "last_activity_date": 1373029416, "answer_id": 17489595}, {"question_id": 16729940, "owner": {"user_id": 3307087, "link": "http://stackoverflow.com/users/3307087/stephan", "user_type": "registered", "reputation": 11}, "body": "<p>I had a similar problem caused by incompatible versions of HBase and elastic search. Using Hbase Version 0.90.4 and Elastic Search Version 0.90.9 worked for me.</p>\n\n<p>I have done some changes in Configuration. In ~/apache-nutch-2.2.1/ivy/ivy.xml the revision  of the dependency for elasticsearch must be set to 0.90.9</p>\n\n<p>In the file ElasticWriter.java in line 104 the statement:   </p>\n\n<pre><code>if (item.failed())\n</code></pre>\n\n<p>had to be changed to:</p>\n\n<pre><code>if (item.isFailed())\n</code></pre>\n\n<p>Then it worked for me.</p>\n", "creation_date": 1396179484, "is_accepted": false, "score": 1, "last_activity_date": 1396179484, "answer_id": 22743518}], "question_id": 16729940, "tags": ["elasticsearch", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/16729940/outofmemoryerror-for-bin-nutch-elasticindex-cluser-all-nutch-2-1", "last_activity_date": 1396179484, "accepted_answer_id": 17489595, "body": "<p>I have been following the instructions at <a href=\"http://wiki.apache.org/nutch/Nutch2Tutorial\" rel=\"nofollow\">http://wiki.apache.org/nutch/Nutch2Tutorial</a> to see if I can get a nutch installation running with ElasticSearch. I have successfully done a crawl with no real issues, but then when I try and load the results into elasticsearch I run into trouble.</p>\n\n<p>I issue the command:</p>\n\n<pre><code>bin/nutch elasticindex &lt;$cluser&gt; -all\n</code></pre>\n\n<p>And it waits around for a long time and then comes back with an error:\nException in thread \"main\" java.lang.RuntimeException: job failed: name=elastic-index [ocpnutch], jobid=job_local_0001</p>\n\n<p>If I look in the logs at:</p>\n\n<pre><code>~/apache-nutch-2.1/runtime/local/logs/hadoop.log\n</code></pre>\n\n<p>I see several errors like this:</p>\n\n<blockquote>\n  <p>Exception caught on netty layer [[id: 0x569764bd, /192.168.17.39:52554\n  => /192.168.17.60:9300]] java.lang.OutOfMemoryError: Java heap space</p>\n</blockquote>\n\n<p>There is nothing in the logs on the elastic search.</p>\n\n<p>I have tried changing:\nelastic.max.bulk.docs and elastic.max.bulk.size to small sizes and allocating large amounts of GB to nutch, but to no avail.</p>\n\n<p>The jvm is:\nJava(TM) SE Runtime Environment (build 1.7.0_21-b11)</p>\n\n<p>Does anyone have any idea what I am doing wrong - what other diagnostic information would be helpful to solve this problem?</p>\n", "creation_date": 1369381113, "score": 0},
{"title": "solr index multiple urls to the same page", "view_count": 57, "is_answered": false, "answers": [{"question_id": 22740798, "owner": {"user_id": 101762, "accept_rate": 85, "link": "http://stackoverflow.com/users/101762/persimmonium", "user_type": "registered", "reputation": 7882}, "body": "<p>you could set up <a href=\"https://cwiki.apache.org/confluence/display/solr/De-Duplication\" rel=\"nofollow\">deduplication</a> so that duplicates are discarded.</p>\n", "creation_date": 1396173251, "is_accepted": false, "score": 0, "last_activity_date": 1396173251, "answer_id": 22742576}], "question_id": 22740798, "tags": ["solr", "search-engine", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/22740798/solr-index-multiple-urls-to-the-same-page", "last_activity_date": 1396173251, "owner": {"user_id": 764618, "answer_count": 6, "creation_date": 1306052225, "accept_rate": 42, "view_count": 41, "reputation": 93}, "body": "<p>I'm using Apache Nutch and Solr to build my search engine.\nI found in the result that there is multiple urls that point to the same page, these urls indexed in solr as different result</p>\n\n<p>EX:</p>\n\n<p><a href=\"http://www.adab.com/modules.php?name=Sh3er&amp;doWhat=shqas&amp;qid=83067&amp;r=&amp;rc=13\" rel=\"nofollow\">http://www.adab.com/modules.php?name=Sh3er&amp;doWhat=shqas&amp;qid=83067&amp;r=&amp;rc=13</a>\n<a href=\"http://www.adab.com/modules.php?name=Sh3er&amp;doWhat=shqas&amp;qid=83067&amp;r=&amp;rc=15\" rel=\"nofollow\">http://www.adab.com/modules.php?name=Sh3er&amp;doWhat=shqas&amp;qid=83067&amp;r=&amp;rc=15</a></p>\n\n<p>How to avoid this duplication in my search engine?</p>\n", "creation_date": 1396159084, "score": 0},
{"title": "how to search in specific field in solr 4.3.0", "view_count": 173, "owner": {"user_id": 764618, "answer_count": 6, "creation_date": 1306052225, "accept_rate": 42, "view_count": 41, "reputation": 93}, "is_answered": true, "answers": [{"last_edit_date": 1391437236, "owner": {"user_id": 995449, "link": "http://stackoverflow.com/users/995449/arun", "user_type": "registered", "reputation": 1332}, "body": "<p>There is no change to this. You should be able to do fieldname:value. \nCheck your schema.xml and make sure you have the field defined that you are looking for. \nDo a *:* search and look at the documents to see what you documents have to get an idea of your data, fields and their values.</p>\n", "question_id": 20542479, "creation_date": 1386852350, "is_accepted": false, "score": 1, "last_activity_date": 1391437236, "answer_id": 20543989}, {"question_id": 20542479, "owner": {"user_id": 764618, "accept_rate": 42, "link": "http://stackoverflow.com/users/764618/haya-aziz", "user_type": "registered", "reputation": 93}, "body": "<p>found that there is no problem in searching in a specific field in the new version, my problem was in filtering.\nthanks</p>\n", "creation_date": 1396159218, "is_accepted": true, "score": 0, "last_activity_date": 1396159218, "answer_id": 22740810}], "question_id": 20542479, "tags": ["solr", "nutch", "solr4"], "answer_count": 2, "link": "http://stackoverflow.com/questions/20542479/how-to-search-in-specific-field-in-solr-4-3-0", "last_activity_date": 1396159218, "accepted_answer_id": 22740810, "body": "<p>In the previous version of Solr, we were able to search in specific field using this \n\"fieldName:value\"\nNow, it is not working any more in the updated version of Solr (4.3.0)</p>\n\n<p>Would you please help.</p>\n\n<p>Thanks.</p>\n\n<p>Schema:</p>\n\n<pre><code>&lt;fieldType name=\"text_general\" class=\"solr.TextField\" positionIncrementGap=\"100\"&gt;\n  &lt;analyzer type=\"index\"&gt;\n    &lt;tokenizer class=\"solr.StandardTokenizerFactory\"/&gt;\n    &lt;filter class=\"solr.WordDelimiterFilterFactory\" protected=\"protwords.txt\" generateWordParts=\"1\" preserveOriginal=\"1\" generateNumberParts=\"0\" catenateWords=\"1\" catenateNumbers=\"0\" catenateAll=\"0\" splitOnNumerics='0' splitOnCaseChange=\"1\"/&gt;\n    &lt;filter class=\"solr.HyphenatedWordsFilterFactory\"/&gt;\n    &lt;filter class=\"solr.TrimFilterFactory\"/&gt;\n    &lt;filter class=\"solr.StopFilterFactory\" ignoreCase=\"true\" words=\"stopwords.txt\" enablePositionIncrements=\"true\" /&gt;\n    &lt;filter class=\"solr.LowerCaseFilterFactory\"/&gt;\n    &lt;filter class=\"solr.RemoveDuplicatesTokenFilterFactory\"/&gt;\n  &lt;/analyzer&gt;\n\n  &lt;analyzer type=\"query\"&gt;\n    &lt;tokenizer class=\"solr.StandardTokenizerFactory\"/&gt;\n    &lt;filter class=\"solr.HyphenatedWordsFilterFactory\"/&gt;\n    &lt;filter class=\"solr.StopFilterFactory\" ignoreCase=\"true\" words=\"stopwords.txt\" enablePositionIncrements=\"true\" /&gt;\n    &lt;filter class=\"solr.SynonymFilterFactory\" synonyms=\"synonyms.txt\" ignoreCase=\"true\" expand=\"true\"/&gt;\n    &lt;filter class=\"solr.LowerCaseFilterFactory\"/&gt;\n    &lt;filter class=\"solr.ArabicNormalizationFilterFactory\"/&gt;\n    &lt;filter class=\"solr.ArabicStemFilterFactory\"/&gt; \n  &lt;/analyzer&gt;\n&lt;/fieldType&gt;\n&lt;fields&gt;\n&lt;field name=\"Poet\" type=\"text_general\" stored=\"true\" indexed=\"true\" /&gt;\n&lt;/fields&gt;\n</code></pre>\n\n<p>I found that if I remove \"solr.ArabicStemFilterFactory\" from query analyzer, it works fine! so what is the link between them ?!</p>\n", "creation_date": 1386848324, "score": 0},
{"title": "how can i connect apache nutch 2.x to a remote hbase cluster", "view_count": 514, "owner": {"user_id": 3226517, "view_count": 12, "answer_count": 1, "creation_date": 1390458055, "reputation": 58}, "is_answered": true, "answers": [{"question_id": 22678932, "owner": {"user_id": 3226517, "link": "http://stackoverflow.com/users/3226517/zahid-adeel", "user_type": "registered", "reputation": 58}, "body": "<p>I finally did it.I was easy to do.\ni am sharing my experience here. May be it can help someone.</p>\n\n<p>1- change the configuration file of hbase-site.xml for pseudo distributed mode.</p>\n\n<p>2- MOST IMPORTANT THING: on hbase machine, replace localhost ip in /etc/hosts with your real network ip like this</p>\n\n<p>10.11.22.189    master localhost</p>\n\n<p>hbase machine's ip = 10.11.22.189\n(note: if you won't change your hbase machine's localhost ip, remote nutch crawler won't be able to connect to it)</p>\n\n<p>4- copy/symlink hbase-site.xml into $NUTCH_HOME/conf</p>\n\n<p>5- start your crawler and see it working </p>\n", "creation_date": 1395983534, "is_accepted": true, "score": 2, "last_activity_date": 1395983534, "answer_id": 22704691}], "question_id": 22678932, "tags": ["hadoop", "hbase", "nutch", "zookeeper"], "answer_count": 1, "link": "http://stackoverflow.com/questions/22678932/how-can-i-connect-apache-nutch-2-x-to-a-remote-hbase-cluster", "last_activity_date": 1395983534, "accepted_answer_id": 22704691, "body": "<p>i have two machines. One machine is running hbase-0.92.2 in pseudo distributed mode while the other one is using nutch2.x crawler.\nHow can i configure these two machines in such a way that, one machine with hbase-0.92.2 acts as a back end storage and the other one with nutch-2.x acts as a crawler. </p>\n\n<p>Kindly, help please.Thanks in advance.</p>\n", "creation_date": 1395897747, "score": 1},
{"title": "Need help understanding Solr", "view_count": 628, "owner": {"user_id": 874970, "answer_count": 57, "creation_date": 1312299644, "accept_rate": 82, "view_count": 163, "reputation": 1805}, "is_answered": true, "answers": [{"question_id": 11146419, "owner": {"user_id": 974045, "link": "http://stackoverflow.com/users/974045/gibolt", "user_type": "registered", "reputation": 51}, "body": "<p>I'm assuming you are using Nutch 1.4 or up. If that is the case, you need to change the type of the fields you added in the solr/conf/schema.xml file from \"text\" to \"text_general\", without the quotes.</p>\n\n<p>I am working towards a similar goal right now and have used that fix to at least get solr working properly, although I still cannot get solr to search the indexed sites. Hope this helps, let me know if you get it working.</p>\n", "creation_date": 1340317032, "is_accepted": true, "score": 2, "last_activity_date": 1340317032, "answer_id": 11147598}], "question_id": 11146419, "tags": ["solr", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/11146419/need-help-understanding-solr", "last_activity_date": 1395828803, "accepted_answer_id": 11147598, "body": "<p>I'm just getting started with Nutch and Solr.  I ran the crawl once with just one seed URL.\nI ran this command: </p>\n\n<p><code>bin/nutch crawl urls -dir crawl -solr http://localhost:8983/solr/ -depth 3 -topN 5</code></p>\n\n<p>Everything goes fine and I'm assuming Solr indexes the pages?  So how do I go about searching now?  I went here <strong>localhost:8983/solr/admin/</strong> but when I put a search query and click search I get this:</p>\n\n<pre><code>HTTP ERROR 400\nProblem accessing /solr/select/.\nReason: undefined field text\n</code></pre>\n\n<p>I also tried an example from the <a href=\"http://lucene.apache.org/solr/api/doc-files/tutorial.html\" rel=\"nofollow\">tutorial</a> but when I run this command:</p>\n\n<p><code>java -jar post.jar solr.xml monitor.xml</code></p>\n\n<p>I get this:</p>\n\n<pre><code>SimplePostTool: version 1.4\nSimplePostTool: POSTing files to http://localhost:8983/solr/update..\nSimplePostTool: POSTing file solr.xml\nSimplePostTool: FATAL: Solr returned an error #400 ERROR: [doc=SOLR1000] unknown field 'name'\n</code></pre>\n\n<p>My ultimate goal is to somehow add this data into Accumulo and use it for a search engine.</p>\n", "creation_date": 1340311402, "score": 1},
{"title": "How to index apache nutch fetched content without parsing into solr", "view_count": 1041, "is_answered": true, "answers": [{"question_id": 16075580, "owner": {"user_id": 2332454, "link": "http://stackoverflow.com/users/2332454/vetus", "user_type": "registered", "reputation": 1}, "body": "<p>You could use nutch 2.1 with Cassandra backend, or Mysql ( it has some bugs ), or HBase. Then you will be able to make Queries in the database, and obtain all html code from pages.</p>\n", "creation_date": 1367247716, "is_accepted": false, "score": -1, "last_activity_date": 1367247716, "answer_id": 16281959}, {"last_edit_date": 1395762861, "owner": {"user_id": 2225199, "link": "http://stackoverflow.com/users/2225199/nimeshjm", "user_type": "registered", "reputation": 1137}, "body": "<p>Nutch has a series of parsers and filters that will extract content from the fetched HTML.</p>\n\n<p>You need to implement an HtmlParserFilter, write the raw content into a metatag and insert it into a SOLR field.</p>\n\n<p>The tutorial below is about an indexing filter but it follows the same flow.</p>\n\n<p><a href=\"http://wiki.apache.org/nutch/WritingPluginExample\" rel=\"nofollow\">Nutch plugin</a></p>\n\n<p>Your class should implement \"HtmlParseFilter\" instead of \"IndexingFilter\".\noverride the filter() method:</p>\n\n<pre><code>@Override\npublic ParseResult filter(Content content, ParseResult parseResult, HTMLMetaTags metaTags, DocumentFragment doc) {\n    Metadata metadata = parseResult.get(content.getUrl()).getData().getParseMeta();\n    byte[] rawContent = content.getContent();\n    String str = new String(rawContent, \"UTF-8\");\n    metadata.add(\"rawcontent\", str);\n        return parseResult;\n}\n</code></pre>\n\n<p>After that, change your schema.xml and add the new field:</p>\n\n<pre><code>&lt;field name=\"metatag.rawcontent\" type=\"text\" stored=\"true\" indexed=\"true\" multiValued=\"false\"/&gt;\n</code></pre>\n\n<p>Compile, deploy, re-crawl, re-index.</p>\n\n<p>You should now see raw HTML content in your SOLR index.</p>\n\n<p>Note: -- </p>\n\n<p>Make sure you have enabled <a href=\"http://wiki.apache.org/nutch/IndexMetatags\" rel=\"nofollow\">metatags plugins</a>. This is important because you are essentially storing rawcontent as the metadata.</p>\n", "question_id": 16075580, "creation_date": 1368562982, "is_accepted": false, "score": 2, "last_activity_date": 1395762861, "answer_id": 16552182}], "question_id": 16075580, "tags": ["solr", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/16075580/how-to-index-apache-nutch-fetched-content-without-parsing-into-solr", "last_activity_date": 1395762861, "owner": {"user_id": 2293660, "view_count": 3, "answer_count": 0, "creation_date": 1366264534, "reputation": 6}, "body": "<p>I need to index fetched content crawled by nutch into solr. Solrjob in nutch indexes only parse content. And i need the content with all HTML tags. Can anyone guide me on this?</p>\n\n<p>Thanks\nSudh</p>\n", "creation_date": 1366264963, "score": 1},
{"title": "Getting started with Nutch and Solr: where&#39;s Solr&#39;s conf/ directory?", "view_count": 2688, "owner": {"user_id": 765287, "answer_count": 50, "creation_date": 1306112708, "accept_rate": 87, "view_count": 271, "location": "Montreal, Canada", "reputation": 2470}, "is_answered": true, "answers": [{"question_id": 17582772, "owner": {"user_id": 1795646, "link": "http://stackoverflow.com/users/1795646/aujasvi-chitkara", "user_type": "registered", "reputation": 700}, "body": "<p>Since you are on Solr 4.x, you will have conf inside collection1, you should be able to do following:</p>\n\n<pre><code>cp ${NUTCH_RUNTIME_HOME}/conf/schema.xml ${APACHE_SOLR_HOME}/example/solr/collection1/conf/\n</code></pre>\n", "creation_date": 1373500912, "is_accepted": true, "score": 5, "last_activity_date": 1373500912, "answer_id": 17582940}, {"last_edit_date": 1375149257, "owner": {"user_id": 765287, "accept_rate": 87, "link": "http://stackoverflow.com/users/765287/db", "user_type": "registered", "reputation": 2470}, "body": "<p>I think my mistake was to use version 4.3.1 of Solr with this particular Nutch tutorial. Previous versions of Solr, like 3.6.2, have a <code>conf</code> directory where the tutorial says they should. I think the tutorial was written with v 3.6.2 in mind.</p>\n\n<p>I've switched to Solr v. 3.6.2, and now things are running more smoothly.</p>\n", "question_id": 17582772, "creation_date": 1373504270, "is_accepted": false, "score": 1, "last_activity_date": 1375149257, "answer_id": 17583384}, {"question_id": 17582772, "owner": {"user_id": 3458480, "link": "http://stackoverflow.com/users/3458480/frankenstien", "user_type": "registered", "reputation": 11}, "body": "<p>I am trying to integrate Apache Nutch 1.7 with Apache Solr 3.6.2 and there is this line, right after the line you mentioned:</p>\n\n<pre><code>Copy exactly in 351 line: \n\n&lt;field name=\"_version_\" type=\"long\" indexed=\"true\" stored=\"true\"/&gt;\n</code></pre>\n\n<p>Now I am confused as the schema.xml copied from the Nutch directory has only 124 lines. Where to paste that line??</p>\n\n<p>Thank you!</p>\n", "creation_date": 1395730326, "is_accepted": false, "score": 1, "last_activity_date": 1395730326, "answer_id": 22627322}], "question_id": 17582772, "tags": ["solr", "nutch"], "answer_count": 3, "link": "http://stackoverflow.com/questions/17582772/getting-started-with-nutch-and-solr-wheres-solrs-conf-directory", "last_activity_date": 1395730326, "accepted_answer_id": 17582940, "body": "<p>I'm trying to get started with Nutch (v 1.7) and Solr (v 4.3.1). I'm following this tutorial: <a href=\"http://wiki.apache.org/nutch/NutchTutorial\" rel=\"nofollow\">http://wiki.apache.org/nutch/NutchTutorial</a></p>\n\n<p>I'm confused by <a href=\"http://wiki.apache.org/nutch/NutchTutorial#A6._Integrate_Solr_with_Nutch\" rel=\"nofollow\">step 6</a> where I'm told to copy a Nutch schema file into one of Solr's directories.</p>\n\n<blockquote>\n  <p><strong>6. Integrate Solr with Nutch</strong></p>\n  \n  <p>We have both Nutch and Solr installed and setup correctly. And Nutch\n  already created crawl data from the seed URL(s). Below are the steps\n  to delegate searching to Solr for links to be searchable:</p>\n  \n  <p>cp ${NUTCH_RUNTIME_HOME}/conf/schema.xml\n  ${APACHE_SOLR_HOME}/example/solr/conf/</p>\n</blockquote>\n\n<p>The problem is that <code>${APACHE_SOLR_HOME}/example/solr/conf/</code> doesn't exist.  <code>/example/solr/</code> only contains</p>\n\n<pre><code>README.txt  \nbin     \ncollection1 \nsolr.xml\nzoo.cfg\n</code></pre>\n\n<p>Does anyone know how to proceed? Should I <em>create</em> an <code>./example/solr/conf</code> directory and copy the Nutch files to that?</p>\n\n<p>Many thanks for you help.</p>\n", "creation_date": 1373499704, "score": 3},
{"title": "Nutch crawling with seeds urls are in range", "view_count": 396, "is_answered": true, "answers": [{"question_id": 3029669, "owner": {"user_id": 183100, "link": "http://stackoverflow.com/users/183100/pascal-dimassimo", "user_type": "registered", "reputation": 5766}, "body": "<p>I think the easiest way would be to have a script to generate your initial list of urls.</p>\n", "creation_date": 1276522777, "is_accepted": false, "score": 1, "last_activity_date": 1276522777, "answer_id": 3037646}, {"question_id": 3029669, "owner": {"user_id": 1841456, "accept_rate": 64, "link": "http://stackoverflow.com/users/1841456/anu", "user_type": "registered", "reputation": 570}, "body": "<p>no. you have inject them manually or using a script</p>\n", "creation_date": 1395165272, "is_accepted": false, "score": 0, "last_activity_date": 1395165272, "answer_id": 22487255}], "question_id": 3029669, "tags": ["nutch", "web-crawler"], "answer_count": 2, "link": "http://stackoverflow.com/questions/3029669/nutch-crawling-with-seeds-urls-are-in-range", "last_activity_date": 1395165272, "owner": {"user_id": 365345, "view_count": 0, "answer_count": 0, "creation_date": 1276366926, "reputation": 1}, "body": "<p>Some site have url pattern as <code>www.___.com/id=1</code> to <code>www.___.com/id=1000</code>. How can I crawl the site using nutch. Is there any wway to provide seed for fetching in range??</p>\n", "creation_date": 1276366928, "score": 0},
{"title": "How to merge two crawldb in nutch", "view_count": 210, "is_answered": false, "answers": [{"question_id": 15770337, "owner": {"user_id": 81194, "link": "http://stackoverflow.com/users/81194/claude", "user_type": "registered", "reputation": 827}, "body": "<p>There is only one way to find out if it works, give it a try. And read the manual first, you're missing your merged DB parameter. See <a href=\"http://wiki.apache.org/nutch/bin/nutch%20mergedb\" rel=\"nofollow\">http://wiki.apache.org/nutch/bin/nutch%20mergedb</a></p>\n", "creation_date": 1365008242, "is_accepted": false, "score": 0, "last_activity_date": 1365008242, "answer_id": 15793509}, {"question_id": 15770337, "owner": {"user_id": 1841456, "accept_rate": 64, "link": "http://stackoverflow.com/users/1841456/anu", "user_type": "registered", "reputation": 570}, "body": "<p>You can merge the two db's all URLS in crawldb are always stored in full. Nutch never stores URL relative to anything.</p>\n\n<p>The command you wrote would merge crawldb2 into crawldb1</p>\n\n<p>if you want to merge crawldb localcrawldb and stackoverflowcrawldb</p>\n\n<p>you write </p>\n\n<p>bin/nutch crawldb mergedcrawldb localcrawldb stackoverflowcrawldb</p>\n\n<p>and both crawldbs will be merged into the mergedcrawldb.</p>\n", "creation_date": 1395151507, "is_accepted": false, "score": 0, "last_activity_date": 1395151507, "answer_id": 22481786}], "question_id": 15770337, "tags": ["search-engine", "web-crawler", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/15770337/how-to-merge-two-crawldb-in-nutch", "last_activity_date": 1395151507, "owner": {"user_id": 1874105, "answer_count": 8, "creation_date": 1354577337, "accept_rate": 74, "view_count": 49, "location": "San Fransisco", "reputation": 663}, "body": "<p>Suppose I have two different machines that were crawling two different domains using nutch.</p>\n\n<p>Now I want to merge their crawldb into one. How can I do that ?</p>\n\n<p>I read about it somewhere - Would the command:</p>\n\n<pre><code>bin/nutch mergedb &lt;crawldb1&gt; &lt;crawldb2&gt;\n</code></pre>\n\n<p>do what I intend to do ?</p>\n\n<p>Also, in case lets say that one of those crawldb is generated by a site for locally hosted website i.e. say wikipedia crawled itself and stored it as the crawldb1</p>\n\n<p>and some other site lets say stackoverflow also did the same.</p>\n\n<p>In this case can I merge these two crawldb into one but which modifies these in a way to reflect their actual urls not the relative one (I mean the url wrt the new location of the database).</p>\n\n<p>Sorry if I am not very clear in the description. Thanks in advance</p>\n", "creation_date": 1364923423, "score": 0},
{"title": "Nutch 2.2.1 and Elasticsearch 0.90.11 NoSuchFieldError: STOP_WORDS_SET", "view_count": 615, "owner": {"age": 32, "answer_count": 6, "creation_date": 1395001341, "user_id": 3426717, "view_count": 52, "location": "Amsterdam", "reputation": 31}, "is_answered": true, "answers": [{"last_edit_date": 1395008501, "owner": {"user_id": 706724, "accept_rate": 100, "link": "http://stackoverflow.com/users/706724/pickypg", "user_type": "registered", "reputation": 14087}, "body": "<p>Having never used <a href=\"https://nutch.apache.org/\" rel=\"nofollow\">Apache Nutch</a>, but briefly reading about it, I would suspect that your inclusion of Elasticsearch is causing a classpath collision with a different version of Lucene that is <em>also</em> on the classpath. Based on its <a href=\"http://search.maven.org/#artifactdetails%7Corg.apache.nutch%7Cnutch%7C2.2.1%7Cjar\" rel=\"nofollow\">Maven POM</a>, which does not specify Lucene directly, then I would suggest only including the Lucene bundled with Elasticsearch, <a href=\"http://www.elasticsearch.org/downloads/0-90-11/\" rel=\"nofollow\">which should be Apache Lucene 4.6.1 for your version</a>.</p>\n\n<p>Duplicated code (differing versions of the same jar) <em>tend</em> to be the cause of <code>NoClassDefFoundError</code> when you are certain that you have the necessary code. Based on the fact that you switched from Solr to Elasticsearch, then it would make sense that you left whatever jars from Solr on your classpath, which would cause the collision at hand. The current release of Solr is 4.7.0, which is the same as Lucene and that would collide with 4.6.1.</p>\n", "question_id": 22443319, "creation_date": 1395008195, "is_accepted": true, "score": 1, "last_activity_date": 1395008501, "answer_id": 22443811}], "question_id": 22443319, "tags": ["elasticsearch", "web-crawler", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/22443319/nutch-2-2-1-and-elasticsearch-0-90-11-nosuchfielderror-stop-words-set", "last_activity_date": 1395008501, "accepted_answer_id": 22443811, "body": "<p>I am trying to integrate <code>Apache Nutch 2.2.1</code> with <code>Elasticsearch 0.90.11</code>.</p>\n\n<p>I have followed all tutorials available (although there are not so many) and even changed <code>bin/crawl.sh</code> to use elasticsearch to index instead of solr.\nIt seems that all works when I run the script until elasticsearch is trying to index the crawled data.</p>\n\n<p>I checked hadoop.log inside logs folder under nutch and found the following errors:</p>\n\n<ol>\n<li>Error injecting constructor, <code>java.lang.NoSuchFieldError: STOP_WORDS_SET</code></li>\n<li>Error injecting constructor, <code>java.lang.NoClassDefFoundError: Could not initialize class org.apache.lucene.analysis.en.EnglishAnalyzer$DefaultSetHolder</code></li>\n</ol>\n\n<p>If you managed to get it working I would very much appreciate the help.</p>\n\n<p>Thanks,\nAndrei.</p>\n", "creation_date": 1395005588, "score": 0},
{"title": "org.apache.nutch.searcher in nutch 1.7", "view_count": 167, "is_answered": false, "answers": [{"question_id": 19137311, "owner": {"user_id": 1766130, "link": "http://stackoverflow.com/users/1766130/vidya-vasudevan", "user_type": "registered", "reputation": 16}, "body": "<p>This error seems to be because of incompatibility of Nutch version1.1 and 1.2 with versions of Lucene higher than 3.1.0. I faced the same problem and resolved it by using the Nutch and SOLR integration instead of Nutch and - Lucene integration directly. You can look at this tutorial:\n<a href=\"http://cmusphinx.sourceforge.net/2012/06/building-a-java-application-with-apache-nutch-and-solr/\" rel=\"nofollow\">http://cmusphinx.sourceforge.net/2012/06/building-a-java-application-with-apache-nutch-and-solr/</a></p>\n", "creation_date": 1394755866, "is_accepted": false, "score": 0, "last_activity_date": 1394755866, "answer_id": 22393665}], "question_id": 19137311, "tags": ["eclipse", "apache", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/19137311/org-apache-nutch-searcher-in-nutch-1-7", "last_activity_date": 1394755866, "owner": {"user_id": 2834966, "view_count": 3, "answer_count": 0, "creation_date": 1380632565, "reputation": 6}, "body": "<p>I'm new in nutch. I'm using nutch 1.7 and the crawl is done. \nMy problem is how can I retrieve crawl data?\nIn nutch 1.1 there was a package called org.apache.nutch.searcher that provide this.\nBut if I import the nutch1.1.jar in my Java Application and try to run the old code </p>\n\n<p><a href=\"http://wiki.apache.org/nutch/JavaDemoApplication\" rel=\"nofollow\">http://wiki.apache.org/nutch/JavaDemoApplication</a> (visible at this link)</p>\n\n<p>it gives me this error:</p>\n\n<pre><code>java.lang.VerifyError: class org.apache.nutch.indexer.NutchSimilarity overrides final method lengthNorm.(Ljava/lang/String;I)F\n    at java.lang.ClassLoader.defineClass1(Native Method)\n    at java.lang.ClassLoader.defineClass(ClassLoader.java:792)\n    at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)\n    at java.net.URLClassLoader.defineClass(URLClassLoader.java:449)\n    at java.net.URLClassLoader.access$100(URLClassLoader.java:71)\n    at java.net.URLClassLoader$1.run(URLClassLoader.java:361)\n    at java.net.URLClassLoader$1.run(URLClassLoader.java:355)\n    at java.security.AccessController.doPrivileged(Native Method)\n    at java.net.URLClassLoader.findClass(URLClassLoader.java:354)\n    at java.lang.ClassLoader.loadClass(ClassLoader.java:424)\n    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)\n    at java.lang.ClassLoader.loadClass(ClassLoader.java:357)\n    at org.apache.nutch.searcher.LuceneSearchBean.init(LuceneSearchBean.java:84)\n    at org.apache.nutch.searcher.LuceneSearchBean.&lt;init&gt;(LuceneSearchBean.java:51)\n    at org.apache.nutch.searcher.NutchBean.&lt;init&gt;(NutchBean.java:103)\n    at org.apache.nutch.searcher.NutchBean.&lt;init&gt;(NutchBean.java:78)\n    at MyCrawler.main(MyCrawler.java:57)\n</code></pre>\n\n<p>Any ideas?\nThanks\nDanilo</p>\n", "creation_date": 1380718208, "score": 1},
{"title": "Nutch on EMR problem reading from S3", "view_count": 540, "is_answered": false, "answers": [{"question_id": 7237927, "owner": {"user_id": 3406722, "link": "http://stackoverflow.com/users/3406722/dmitry", "user_type": "registered", "reputation": 1}, "body": "<p>try to specify in </p>\n\n<blockquote>\n  <p>hadoop-site.xml</p>\n</blockquote>\n\n<pre><code>&lt;property&gt;\n  &lt;name&gt;fs.default.name&lt;/name&gt;\n  &lt;value&gt;org.apache.hadoop.fs.s3.S3FileSystem&lt;/value&gt;\n&lt;/property&gt;\n</code></pre>\n\n<p>This will mention to Nutch that by default S3 should be used</p>\n\n<p>Properties </p>\n\n<blockquote>\n  <p>fs.s3.awsAccessKeyId \n  and \n  fs.s3.awsSecretAccessKey</p>\n</blockquote>\n\n<p>specification you need only in case when your S3 objects are placed under authentication (In S3 object can be accessed to all users, or only by authentication)</p>\n", "creation_date": 1394614168, "is_accepted": false, "score": 0, "last_activity_date": 1394614168, "answer_id": 22346248}], "question_id": 7237927, "tags": ["java", "hadoop", "amazon-web-services", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/7237927/nutch-on-emr-problem-reading-from-s3", "last_activity_date": 1394614168, "owner": {"user_id": 643854, "answer_count": 5, "creation_date": 1299194624, "view_count": 38, "location": "Santa Barbara, CA", "reputation": 473}, "body": "<p>Hi I am trying to run <a href=\"http://nutch.apache.org/\">Apache Nutch</a> 1.2 on Amazon's EMR.<br>\nTo do this I specifiy an input directory from S3.  I get the following error: </p>\n\n<pre>\nFetcher: java.lang.IllegalArgumentException:\n    This file system object (hdfs://ip-11-202-55-144.ec2.internal:9000)\n    does not support access to the request path \n    's3n://crawlResults2/segments/20110823155002/crawl_fetch'\n    You possibly called FileSystem.get(conf) when you should have called\n    FileSystem.get(uri, conf) to obtain a file system supporting your path.\n</pre>\n\n<p>I understand the difference between <code>FileSystem.get(uri, conf)</code>, and <code>FileSystem.get(conf)</code>.  If I were writing this myself I would <code>FileSystem.get(uri, conf)</code> however I am trying to use existing Nutch code.  </p>\n\n<p>I asked this question, and someone told me that I needed to modify <code>hadoop-site.xml</code> to include the following properties: <code>fs.default.name</code>, <code>fs.s3.awsAccessKeyId</code>, <code>fs.s3.awsSecretAccessKey</code>.  I updated these properties in <code>core-site.xml</code> (<code>hadoop-site.xml</code> does not exist), but that didn't make a difference.  Does anyone have any other ideas? \nThanks for the help.</p>\n", "creation_date": 1314669144, "score": 6},
{"title": "Extracting HTML meta tags in Nutch 2.x and having Solr 4 index it", "view_count": 3073, "owner": {"user_id": 89558, "answer_count": 6, "creation_date": 1239388786, "accept_rate": 64, "view_count": 71, "location": "Toronto, Canada", "reputation": 413}, "is_answered": true, "answers": [{"question_id": 15504828, "owner": {"user_id": 326543, "accept_rate": 82, "link": "http://stackoverflow.com/users/326543/srikanth-venugopalan", "user_type": "registered", "reputation": 6268}, "body": "<p>Take a look at <a href=\"http://wiki.apache.org/nutch/IndexMetatags\" rel=\"nofollow\">IndexMetaTags</a> plugin for Nutch, available from version 1.5 onwards. </p>\n\n<p>This will allow you to specify which meta tags to parse and index.</p>\n\n<p>Note:  The names of the fields must be prefixed with 'metatags.' </p>\n\n<p>You can check the index using <a href=\"http://wiki.apache.org/nutch/bin/nutch%20indexchecker\" rel=\"nofollow\">Nutch indexchecker</a></p>\n", "creation_date": 1363711108, "is_accepted": false, "score": 2, "last_activity_date": 1363711108, "answer_id": 15505334}, {"last_edit_date": 1363802483, "owner": {"user_id": 81194, "link": "http://stackoverflow.com/users/81194/claude", "user_type": "registered", "reputation": 827}, "body": "<p>Make sure to include the <code>parse-metatags</code> and <code>index-metadata</code> plugins in your <code>plugin.includes</code> definition in <code>nutch-site.xml</code></p>\n\n<p>Then add <code>metatags.names</code> <code>index.parse.md</code> and <code>index.content.md</code> properties and point them to the appropriate tags. Take a look at mine:</p>\n\n<pre><code>&lt;property&gt;\n        &lt;name&gt;plugin.includes&lt;/name&gt;\n        &lt;value&gt;protocol-http|protocol-httpclient|urlfilter-regex|parse-(html|tika|metatags)|index-(basic|anchor|metadata)|scoring-opic|urlnormalizer-(pass|regex|basic)&lt;/value&gt;\n&lt;/property&gt;\n&lt;property&gt;\n        &lt;name&gt;metatags.names&lt;/name&gt;\n        &lt;value&gt;*&lt;/value&gt;\n&lt;/property&gt;\n&lt;property&gt;\n        &lt;name&gt;index.parse.md&lt;/name&gt;\n        &lt;value&gt;metatag.description,metatag.author,metatag.twitter:image&lt;/value&gt;\n&lt;/property&gt;\n&lt;property&gt;\n        &lt;name&gt;index.content.md&lt;/name&gt;\n        &lt;value&gt;author,description,twitter:image&lt;/value&gt;\n&lt;/property&gt;\n</code></pre>\n\n<p>Test your configuration. I ran this test against an article on readwrite.com:</p>\n\n<pre><code>bin/nutch indexchecker http://readwrite.com/2013/03/20/whats-behind-china-attacks-on-apple-and-android\n</code></pre>\n\n<p>The output will tell you if you're parsing the correct values. In my case I wanted <code>author</code>, <code>description</code> and <code>twitter:image</code>:</p>\n\n<pre><code>fetching: http://readwrite.com/2013/03/20/whats-behind-china-attacks-on-apple-and-android\nparsing: http://readwrite.com/2013/03/20/whats-behind-china-attacks-on-apple-and-android\ncontentType: text/html\ncontent :   What's Really Behind China's Attacks On Apple And Android? \u2013 ReadWrite Sections Sections Social Mobi\ntitle : What's Really Behind China's Attacks On Apple And Android? \u2013 ReadWrite\nhost :  readwrite.com\nmetatag.author :    Brian S Hall\ntstamp :    Wed Mar 20 13:33:38 EDT 2013\nmetatag.twitter:image : http://readwrite.com/files/styles/150_150sc/public/fields/China%20graphic%20brian%20final.jpg\nmetatag.description :   Repeated outbursts suggest China could be growing concerned over America's dominance in the smartpho\nurl :   http://readwrite.com/2013/03/20/whats-behind-china-attacks-on-apple-and-android\n</code></pre>\n\n<p>A downside is that <code>parse-metatags</code> will only match tags by name and not property. For example <code>&lt;meta name=\"foo\" content=\"bar\"&gt;</code> is fine while an Open Graph tags like <code>&lt;meta property=\"og:image\" content=\"http://readwrite.com/sample.jpg\" /&gt;</code> will be missed.</p>\n", "question_id": 15504828, "creation_date": 1363801704, "is_accepted": true, "score": 4, "last_activity_date": 1363802483, "answer_id": 15531058}, {"last_edit_date": 1364009824, "owner": {"user_id": 1143887, "accept_rate": 67, "link": "http://stackoverflow.com/users/1143887/kich", "user_type": "registered", "reputation": 169}, "body": "<p>Index-Metatags plugin is not included in the 2.x series. Please check <a href=\"http://wiki.apache.org/nutch/Nutch2Plugins\" rel=\"nofollow\">http://wiki.apache.org/nutch/Nutch2Plugins</a> for more information. There is a patch in there that makes the plugin work for 2.x series.</p>\n\n<p>1.6 is the stable version for Nutch right now as the above author has pointed out in the comment.</p>\n", "question_id": 15504828, "creation_date": 1363968697, "is_accepted": false, "score": 0, "last_activity_date": 1364009824, "answer_id": 15574969}, {"question_id": 15504828, "owner": {"user_id": 256090, "link": "http://stackoverflow.com/users/256090/chris", "user_type": "registered", "reputation": 1062}, "body": "<p>Take a look at the <a href=\"https://issues.apache.org/jira/browse/NUTCH-1478\" rel=\"nofollow\">latest patches</a> regarding Nutch 2.x\n<br/>\nAlthough i can store metadata in the database, i can't figure out how to transfer it to Solr.</p>\n", "creation_date": 1394579073, "is_accepted": false, "score": 0, "last_activity_date": 1394579073, "answer_id": 22338554}], "question_id": 15504828, "tags": ["apache", "solr", "nutch"], "answer_count": 4, "link": "http://stackoverflow.com/questions/15504828/extracting-html-meta-tags-in-nutch-2-x-and-having-solr-4-index-it", "last_activity_date": 1394579073, "accepted_answer_id": 15531058, "body": "<p>I am using Nutch 2.0 to crawl some websites but I do not see HTML meta tags like title, description are extracted and stored in MySQL database. Any idea how can I get it work?</p>\n\n<p>Thanks\nArash</p>\n", "creation_date": 1363709865, "score": 1},
{"title": "Updating documents indexed in solr", "view_count": 161, "is_answered": false, "question_id": 22326772, "tags": ["solr", "nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/22326772/updating-documents-indexed-in-solr", "last_activity_date": 1394543531, "owner": {"user_id": 1784775, "answer_count": 3, "creation_date": 1351579848, "accept_rate": 40, "view_count": 11, "reputation": 60}, "body": "<p>I am able to crawl a website (using nutch with tika plugin) which contains some pdf and docx files and index the documents created to solr. But after crawling, the data on website changed (some new files added and some old files deleted). So by recrawling the changes are indexed to solr. While querying solr, I also get the links to files that were deleted from the website(obviously). So how do I make configuration/settings so that while indexing to solr (from nutch) all the documents (containing info about files that were deleted from website) also get deleted?</p>\n", "creation_date": 1394543531, "score": 1},
{"title": "Apache Nutch: Get outlink URL&#39;s text context", "view_count": 895, "owner": {"user_id": 3367701, "answer_count": 2, "creation_date": 1393661808, "accept_rate": 60, "view_count": 29, "reputation": 161}, "is_answered": true, "answers": [{"last_edit_date": 1394489082, "owner": {"user_id": 3016219, "link": "http://stackoverflow.com/users/3016219/avanz", "user_type": "registered", "reputation": 7211}, "body": "<p>What you want to do is Web Scraping. Python and Hadoop offers tools for that. To achieve it, you can use selectors.</p>\n\n<p>Here you find some examples how to do that using Python Scrapy:</p>\n\n<ul>\n<li><a href=\"http://doc.scrapy.org/en/latest/topics/selectors.html\" rel=\"nofollow\">Selectors</a></li>\n<li><a href=\"http://doc.scrapy.org/en/latest/intro/tutorial.html\" rel=\"nofollow\">Scrapy Tutorial</a></li>\n</ul>\n\n<p>On Hadoop the best way to go is to implement a crawling using selectors: </p>\n\n<ul>\n<li><a href=\"http://www.slideshare.net/hadoopusergroup/building-a-scalable-web-crawler-with-hadoop\" rel=\"nofollow\">Web crawl with Hadoop</a></li>\n<li><a href=\"https://cwiki.apache.org/confluence/display/Hive/GettingStarted#GettingStarted-SELECTSandFILTERS\" rel=\"nofollow\">enter link description here</a></li>\n<li><a href=\"http://stackoverflow.com/questions/15023661/hive-how-to-do-a-select-query-to-output-a-unique-primary-key-using-hiveql\">HiveQL</a></li>\n</ul>\n\n<p>The <a href=\"http://stackoverflow.com/questions/17679363/hadoop-cascading-cascadeexception-no-loops-allowed-in-cascade-when-cogroup-p\">cascading</a> can be used to address the URL you specify:</p>\n\n<ul>\n<li><a href=\"http://www.slideshare.net/cwensel/building-scale-free-applications-with-hadoop-and-cascading-1616859\" rel=\"nofollow\">Hadoop and Cascading</a></li>\n</ul>\n\n<p>After having the data, you can also use R to optimize analysis:</p>\n\n<ul>\n<li><a href=\"http://www.slideshare.net/RevolutionAnalytics/high-performance-predictive-analytics-in-r-and-hadoop-25653406\" rel=\"nofollow\">R and Hadoop</a></li>\n<li><a href=\"http://www.slideshare.net/Hadoop_Summit/enabling-r-on-hadoop\" rel=\"nofollow\">Enabling R on Hadoop</a></li>\n</ul>\n\n<p>If you haven't done anything with Hadoop yet, here is a good starting <a href=\"http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-single-node-cluster/\" rel=\"nofollow\">point</a>. You may also want to have a look in <a href=\"http://www.youtube.com/watch?v=Zq9PiKQ_hC8\" rel=\"nofollow\">HUE Beeswax</a> as an interactive tool that is very useful for data analysis.</p>\n", "question_id": 22283624, "creation_date": 1394446958, "is_accepted": true, "score": 3, "last_activity_date": 1394489082, "answer_id": 22297541}], "question_id": 22283624, "tags": ["apache", "hadoop", "web-scraping", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/22283624/apache-nutch-get-outlink-urls-text-context", "last_activity_date": 1394489082, "accepted_answer_id": 22297541, "body": "<p>Anyone knows an efficient way to extract the text context that wraps an outlink URL. For example, given this sample text containing an outlink:</p>\n\n<blockquote>\n  <p>Nutch can run on a single machine, but gains a lot of its strength from running in a Hadoop cluster. You can download Nutch <a href=\"https://nutch.apache.org/downloads.html\">here</a>.\n  For more information about Apache Nutch, please see the Nutch wiki. </p>\n</blockquote>\n\n<p>In this example, I would like to get the sentence containing the link, and a sentence before and after that sentence. Any way to do this efficiently? Any methods I can invoke to get something like the position of the link within a fetched content? Or even a part of the nutch code I can modify to do this? Thanks!</p>\n", "creation_date": 1394376475, "score": 7},
{"title": "Nutch Fetch Scheduler to recrawl web", "view_count": 418, "owner": {"user_id": 1784775, "answer_count": 3, "creation_date": 1351579848, "accept_rate": 40, "view_count": 11, "reputation": 60}, "is_answered": true, "answers": [{"last_edit_date": 1394212573, "owner": {"user_id": 3393626, "link": "http://stackoverflow.com/users/3393626/bayu-w", "user_type": "unregistered", "reputation": 26}, "body": "<p>You should create cronjob to automatically crawling.\ndb.fetch.interval.default only tells nutch whether this page should be crawled after last crawled.</p>\n", "question_id": 22196933, "creation_date": 1394211435, "is_accepted": true, "score": 1, "last_activity_date": 1394212573, "answer_id": 22256070}], "question_id": 22196933, "tags": ["nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/22196933/nutch-fetch-scheduler-to-recrawl-web", "last_activity_date": 1394212573, "accepted_answer_id": 22256070, "body": "<p>I've followed this <a href=\"http://pascaldimassimo.com/2010/06/11/how-to-re-crawl-with-nutch/\" rel=\"nofollow\">article</a> to configure Fetch Schedule for recrawling a website but it doesn't seem to work.\nAdded the following property in nutch-site.xml to schedule recrawling every 10 minutes as I want to recrawl a particular website to get the changed pages.- </p>\n\n<pre><code>&lt;property&gt;\n  &lt;name&gt;db.fetch.schedule.class&lt;/name&gt;\n  &lt;value&gt;org.apache.nutch.crawl.DefaultFetchSchedule&lt;/value&gt;\n  &lt;description&gt;The implementation of fetch schedule. DefaultFetchSchedule simply \n  adds the original fetchInterval to the last fetch time, regardless of \n  page changes.&lt;/description&gt;\n&lt;/property&gt;\n&lt;property&gt;\n  &lt;name&gt;db.fetch.interval.default&lt;/name&gt;\n  &lt;value&gt;600&lt;/value&gt;\n  &lt;description&gt;The default number of seconds between re-fetches of a page (30 days). \n  &lt;/description&gt;\n&lt;/property&gt;\n</code></pre>\n\n<p>My question is will recrawling start automatically after every 10 minutes or do I need to do any other configuration or do I have to trigger it somehow?</p>\n", "creation_date": 1394020284, "score": 0},
{"title": "Nutch - How to crawl only urls newly added in the last 24 hours using nutch?", "view_count": 87, "is_answered": false, "answers": [{"question_id": 21849404, "owner": {"user_id": 2586595, "link": "http://stackoverflow.com/users/2586595/mohsen89z", "user_type": "registered", "reputation": 128}, "body": "<p>you gain your new urls by parsing HTML!</p>\n\n<p>there is no way that you could specify an anchor's lifetime by parsing an \n <code>&lt;a&gt;</code>\n tag!</p>\n\n<p>you have to have a list for old urls in your DB so you can skip them!</p>\n", "creation_date": 1394014350, "is_accepted": false, "score": 0, "last_activity_date": 1394014350, "answer_id": 22194647}], "question_id": 21849404, "tags": ["plugins", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/21849404/nutch-how-to-crawl-only-urls-newly-added-in-the-last-24-hours-using-nutch", "last_activity_date": 1394014350, "owner": {"user_id": 3260152, "view_count": 0, "answer_count": 0, "creation_date": 1391249588, "reputation": 1}, "body": "<p>I'm using Nutch 1.7 and everything seems to be working just fine. However, there is one big issue I don't know how to overcome. </p>\n\n<p>How can I crawl ONLY urls newly added in the last 24 hours. Of course we could use adaptive fetching but we hope there would be another better way that we are not aware until now. </p>\n\n<p>We only need the urls that are added in the last 24 hours as we visit our source-websites every each day. </p>\n\n<p>Please let me know if nutch can be configured and setup to do that or if there is a written plugin for crawling only urls added in the last 24 hours. </p>\n\n<p>Kind regards,\nChristian</p>\n", "creation_date": 1392715923, "score": 0},
{"title": "Should i use cygwin for nutch and solr integration?", "view_count": 178, "owner": {"user_id": 3323502, "view_count": 4, "answer_count": 0, "creation_date": 1392727800, "reputation": 3}, "is_answered": true, "answers": [{"question_id": 22014089, "owner": {"user_id": 2623052, "accept_rate": 45, "link": "http://stackoverflow.com/users/2623052/allan-macmillan", "user_type": "registered", "reputation": 866}, "body": "<p>Use cygwin, heres an excellent guide to set them up together:</p>\n\n<p><a href=\"http://amac4.blogspot.com/2013/07/setting-up-solr-with-apache-tomcat-be.html\" rel=\"nofollow\">http://amac4.blogspot.com/2013/07/setting-up-solr-with-apache-tomcat-be.html</a></p>\n", "creation_date": 1393688697, "is_accepted": true, "score": 0, "last_activity_date": 1393688697, "answer_id": 22116474}], "question_id": 22014089, "tags": ["solr", "cygwin", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/22014089/should-i-use-cygwin-for-nutch-and-solr-integration", "last_activity_date": 1393688697, "accepted_answer_id": 22116474, "body": "<p>Hi I am creating a search engine for a website using nutch and solr but i am unable to execute bin/nutch(command) in my command prompt. can i execute it in command prompt or should i use cygwin? </p>\n\n<p>I am using solr 3.6.2 and nutch 1.7, pls provide me solution as soon as possibe </p>\n", "creation_date": 1393330915, "score": 0},
{"title": "Fetch particular tags from HTML docs obtained after crawling and parsing using Apache Nutch 1.4", "view_count": 301, "owner": {"user_id": 1211452, "answer_count": 13, "creation_date": 1329313121, "accept_rate": 100, "view_count": 60, "reputation": 180}, "is_answered": true, "answers": [{"question_id": 9765777, "owner": {"user_id": 1211452, "accept_rate": 100, "link": "http://stackoverflow.com/users/1211452/lina-clark", "user_type": "registered", "reputation": 180}, "body": "<p>Finally, I am able to do it. Sharing in case someone else needs it.\nYou can use index-metatags plugin provided here:\n<a href=\"http://wiki.apache.org/nutch/IndexMetatags\" rel=\"nofollow\">http://wiki.apache.org/nutch/IndexMetatags</a></p>\n\n<p>It will solve this problem\nCheers :)</p>\n", "creation_date": 1332336919, "is_accepted": true, "score": 0, "last_activity_date": 1332336919, "answer_id": 9805726}, {"question_id": 9765777, "owner": {"user_id": 1275577, "accept_rate": 100, "link": "http://stackoverflow.com/users/1275577/%c4%b0smet-alkan", "user_type": "registered", "reputation": 3787}, "body": "<p>Do a crawling. After that enter this into terminal.</p>\n\n<pre><code>bin/nutch readseg -dump crawl/segments/* output -nocontent -nofetch -nogenerate -noparse -noparsedata\n</code></pre>\n\n<p>If it runs, you will have a file with header informations plus contents in it. After that you can easily modify the file and get whatever info you want by string operations.</p>\n", "creation_date": 1334920786, "is_accepted": false, "score": 0, "last_activity_date": 1334920786, "answer_id": 10245208}], "question_id": 9765777, "tags": ["java", "apache", "meta-tags", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/9765777/fetch-particular-tags-from-html-docs-obtained-after-crawling-and-parsing-using-a", "last_activity_date": 1393297035, "accepted_answer_id": 9805726, "body": "<p>I used nutch 1.4 and crawled a website.\nI got the website crawled successfully and all the pages were dumped into segments.\nI merged all the segments to one segment and then i used readseg command to obtain a text version of all the crawled pages.\nNow I need to find out, URL of page and the meta data stored in that page.\nI don't know which command to use or shall i need to do something different.</p>\n\n<p>Have made a lot of efforts on google Some people said that you have to write a separate plugin for it. Can someone tell me please.</p>\n\n<p>Thanks a lot :) :)</p>\n", "creation_date": 1332137732, "score": 0},
{"title": "why nutch dosen&#39;t crawl all links in no English language sites?", "view_count": 267, "is_answered": false, "answers": [{"question_id": 9073932, "owner": {"user_id": 1150329, "accept_rate": 89, "link": "http://stackoverflow.com/users/1150329/tejas-patil", "user_type": "registered", "reputation": 4793}, "body": "<p>Nutch runs URL normalization and other url processing stuff on each url before adding it the crawldb. Your url might have got filtered there itself. You can remove those plugins from the list of plugins used (<code>plugin.includes</code> property in <code>conf/nutch-site.xml</code>) and try again.</p>\n", "creation_date": 1333467434, "is_accepted": false, "score": 0, "last_activity_date": 1333467434, "answer_id": 9996673}, {"question_id": 9073932, "owner": {"user_id": 3340022, "link": "http://stackoverflow.com/users/3340022/user3340022", "user_type": "unregistered", "reputation": 1}, "body": "<p>One reason it might fail to fetch the non-English URL is because different URL-encoding used by the web-server at www.irna.ir and the used nutch client.</p>\n", "creation_date": 1393054261, "is_accepted": false, "score": 0, "last_activity_date": 1393054261, "answer_id": 21951279}], "question_id": 9073932, "tags": ["nutch", "web-crawler"], "answer_count": 2, "link": "http://stackoverflow.com/questions/9073932/why-nutch-dosent-crawl-all-links-in-no-english-language-sites", "last_activity_date": 1393054261, "owner": {"user_id": 1045194, "answer_count": 0, "creation_date": 1321258730, "accept_rate": 0, "view_count": 19, "reputation": 14}, "body": "<p>i crawl a site with nutch 1.4, i understand that nutch dosen't crawl all links in this site. i have no filter and no limit rule to crawling. for example nutch never crawl this link:</p>\n\n<p><a href=\"http://www.irna.ir/News/30786427\" rel=\"nofollow\">http://www.irna.ir/News/30786427</a>/\u0633\u0648\u0621-\u0627\u0633\u062a\u0641\u0627\u062f\u0647-\u0627\u0632-\u0646\u0627\u0645-\u0643\u0645\u06cc\u062a\u0647-\u0627\u0645\u062f\u0627\u062f-\u0628\u0631\u0627\u06cc-\u062c\u0645\u0639-\u0622\u0648\u0631\u06cc-\u0631\u0627\u06cc-\u062f\u0631-\u0645\u0646\u0627\u0637\u0642-\u0645\u062d\u0631\u0648\u0645/\u0633\u064a\u0627\u0633\u064a/</p>\n\n<p>if i give this link to nutch to crawl, nutch never crawl this link. this site is farsi and not English.\nhow i can crawl this link? </p>\n", "creation_date": 1327979877, "score": 0},
{"title": "How to use luke check nutch index", "view_count": 159, "is_answered": false, "question_id": 21870831, "tags": ["indexing", "nutch", "lucene", "luke"], "answer_count": 0, "link": "http://stackoverflow.com/questions/21870831/how-to-use-luke-check-nutch-index", "last_activity_date": 1392782865, "owner": {"user_id": 1468790, "answer_count": 0, "creation_date": 1340185492, "accept_rate": 80, "view_count": 21, "location": "China", "reputation": 27}, "body": "<p>I'm using the nutch 1.7 version.</p>\n\n<p>Crawl the end, I have the results file.</p>\n\n<p>Whether I open any files in this directory will appear under this error.</p>\n\n<p>Please,How to use luke check index in this results file?</p>\n\n<p><img src=\"http://i.stack.imgur.com/OdAEI.png\" alt=\"enter image description here\">\n<img src=\"http://i.stack.imgur.com/2VqK4.png\" alt=\"enter image description here\"></p>\n", "creation_date": 1392782865, "score": 0},
{"title": "nutch and sitemap.xml", "view_count": 1000, "is_answered": false, "answers": [{"question_id": 3997932, "owner": {"user_id": 484046, "accept_rate": 81, "link": "http://stackoverflow.com/users/484046/millebii", "user_type": "registered", "reputation": 976}, "body": "<p>Not that I'm aware of.\nDepending on the behaviour you expect their are multiple implementations, can u be more specific?\nFor instance:\n+ you can make it that new sitemaps submitted are 'injected' whith a high score so they will get crawled earlier. For this just add an inject command before starting a new crawl/fetch/index cycle\n+ you can create a scoring plug-in which will boost URL found in a sitemaps... \nBut you can not define recrawl periods at a URL level, as the sitemap would indicate. Nutch has build-in fonction which will recrawl more often URL that changes more an vice-versa. However you could decide to boost score of URL with frequent refresh rate, so that they get crawled earlier...</p>\n", "creation_date": 1294346295, "is_accepted": false, "score": 0, "last_activity_date": 1294346295, "answer_id": 4619589}, {"question_id": 3997932, "owner": {"user_id": 2674407, "link": "http://stackoverflow.com/users/2674407/user2674407", "user_type": "registered", "reputation": 38}, "body": "<p>I guess they support it now. I found it on this link </p>\n\n<p><a href=\"https://wiki.apache.org/nutch/SitemapFeature\" rel=\"nofollow\">https://wiki.apache.org/nutch/SitemapFeature</a></p>\n", "creation_date": 1392746072, "is_accepted": false, "score": 0, "last_activity_date": 1392746072, "answer_id": 21861659}], "question_id": 3997932, "tags": ["search", "solr", "search-engine", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/3997932/nutch-and-sitemap-xml", "last_activity_date": 1392746072, "owner": {"user_id": 183038, "answer_count": 3, "creation_date": 1254481801, "accept_rate": 17, "view_count": 37, "reputation": 268}, "body": "<p>does apache-nutch support sitemaps?\nor how can i implement it myself? how can i use priority field, should it be multiplied to boost field?</p>\n", "creation_date": 1287758678, "score": 1},
{"title": "nutch and solr for multiple domains", "view_count": 469, "is_answered": false, "answers": [{"question_id": 20339335, "owner": {"user_id": 973840, "accept_rate": 85, "link": "http://stackoverflow.com/users/973840/diaa", "user_type": "registered", "reputation": 385}, "body": "<p>There are a couple of things you need to do to make this work</p>\n\n<ol>\n<li>Ensure that the domain in your filter is exactly like the domain that is stored in the field site. This is because you used for that field the type string which doesn't do fuzzy matching.</li>\n<li>Format your characters to their html equivalent</li>\n<li>Your domain string needs to be in quotes \"\"</li>\n</ol>\n\n<p>Example query:</p>\n\n<pre><code>http://localhost:8983/solr/collection1/select?q=keyword&amp;fq=site%3A%22http%3A%2F%2Fdomain1.com%22&amp;wt=json&amp;indent=true\n</code></pre>\n\n<p>Where the site field in my index is <a href=\"http://domain1.com\" rel=\"nofollow\">http://domain1.com</a> </p>\n", "creation_date": 1392379810, "is_accepted": false, "score": 0, "last_activity_date": 1392379810, "answer_id": 21779059}], "question_id": 20339335, "tags": ["solr", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/20339335/nutch-and-solr-for-multiple-domains", "last_activity_date": 1392379810, "owner": {"user_id": 1490200, "answer_count": 3, "creation_date": 1340940390, "accept_rate": 17, "view_count": 24, "reputation": 70}, "body": "<p>I am using nutch 1.7 and solr 4.6 to create a search engine for two website. In the seed.txt file, I have:</p>\n\n<pre><code>http://domain1.com/\nhttp://domain2.com/\n</code></pre>\n\n<p>I run this </p>\n\n<pre><code>bin/nutch crawl urls -dir crawl -solr http://localhost:8983/solr/ -depth 1\n</code></pre>\n\n<p>Here is my Solr schema.xml</p>\n\n<pre><code>&lt;?xml version=\"1.0\" encoding=\"UTF-8\" ?&gt;\n&lt;!--\n Licensed to the Apache Software Foundation (ASF) under one or more\n contributor license agreements.  See the NOTICE file distributed with\n this work for additional information regarding copyright ownership.\n The ASF licenses this file to You under the Apache License, Version 2.0\n (the \"License\"); you may not use this file except in compliance with\n the License.  You may obtain a copy of the License at\n\n     http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an \"AS IS\" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n--&gt;\n&lt;!--\n    Description: This document contains Solr 4.x schema definition to\n    be used with Solr integration currently build into Nutch.\n    This schema is not minimal, there are some useful field type definitions left,\n    and the set of fields and their flags (indexed/stored/term vectors) can be\n    further optimized depending on needs.  See\n    http://svn.apache.org/viewvc/lucene/dev/trunk/solr/example/solr/conf/schema.xml?view=markup\n    for more info.\n--&gt;\n\n&lt;schema name=\"nutch\" version=\"1.5\"&gt;\n\n  &lt;types&gt;\n\n    &lt;!-- The StrField type is not analyzed, but indexed/stored verbatim. --&gt;\n    &lt;fieldType name=\"string\" class=\"solr.StrField\" sortMissingLast=\"true\" omitNorms=\"true\"/&gt;\n    &lt;fieldType name=\"int\" class=\"solr.TrieIntField\" precisionStep=\"0\" omitNorms=\"true\" positionIncrementGap=\"0\"/&gt;\n    &lt;fieldType name=\"float\" class=\"solr.TrieFloatField\" precisionStep=\"0\" omitNorms=\"true\" positionIncrementGap=\"0\"/&gt;\n    &lt;fieldType name=\"long\" class=\"solr.TrieLongField\" precisionStep=\"0\" omitNorms=\"true\" positionIncrementGap=\"0\"/&gt;\n    &lt;fieldType name=\"double\" class=\"solr.TrieDoubleField\" precisionStep=\"0\" omitNorms=\"true\" positionIncrementGap=\"0\"/&gt;\n\n    &lt;fieldType name=\"tint\" class=\"solr.TrieIntField\" precisionStep=\"8\" omitNorms=\"true\" positionIncrementGap=\"0\"/&gt;\n    &lt;fieldType name=\"tfloat\" class=\"solr.TrieFloatField\" precisionStep=\"8\" omitNorms=\"true\" positionIncrementGap=\"0\"/&gt;\n    &lt;fieldType name=\"tlong\" class=\"solr.TrieLongField\" precisionStep=\"8\" omitNorms=\"true\" positionIncrementGap=\"0\"/&gt;\n    &lt;fieldType name=\"tdouble\" class=\"solr.TrieDoubleField\" precisionStep=\"8\" omitNorms=\"true\" positionIncrementGap=\"0\"/&gt;\n\n    &lt;fieldType name=\"date\" class=\"solr.TrieDateField\" omitNorms=\"true\" precisionStep=\"0\" positionIncrementGap=\"0\"/&gt;\n\n    &lt;fieldType name=\"tdate\" class=\"solr.TrieDateField\" omitNorms=\"true\" precisionStep=\"6\" positionIncrementGap=\"0\"/&gt;\n\n    &lt;fieldType name=\"text_general\" class=\"solr.TextField\" positionIncrementGap=\"100\"&gt;\n      &lt;analyzer type=\"index\"&gt;\n        &lt;tokenizer class=\"solr.StandardTokenizerFactory\"/&gt;\n        &lt;filter class=\"solr.StopFilterFactory\" ignoreCase=\"true\" words=\"stopwords.txt\" enablePositionIncrements=\"true\" /&gt;\n        &lt;!-- in this example, we will only use synonyms at query time\n        &lt;filter class=\"solr.SynonymFilterFactory\" synonyms=\"index_synonyms.txt\" ignoreCase=\"true\" expand=\"false\"/&gt;\n        --&gt;\n        &lt;filter class=\"solr.LowerCaseFilterFactory\"/&gt;\n      &lt;/analyzer&gt;\n      &lt;analyzer type=\"query\"&gt;\n        &lt;tokenizer class=\"solr.StandardTokenizerFactory\"/&gt;\n        &lt;filter class=\"solr.StopFilterFactory\" ignoreCase=\"true\" words=\"stopwords.txt\" enablePositionIncrements=\"true\" /&gt;\n        &lt;filter class=\"solr.SynonymFilterFactory\" synonyms=\"synonyms.txt\" ignoreCase=\"true\" expand=\"true\"/&gt;\n        &lt;filter class=\"solr.LowerCaseFilterFactory\"/&gt;\n      &lt;/analyzer&gt;\n    &lt;/fieldType&gt;\n\n    &lt;fieldType name=\"text_en\" class=\"solr.TextField\" positionIncrementGap=\"100\"&gt;\n      &lt;analyzer type=\"index\"&gt;\n        &lt;tokenizer class=\"solr.StandardTokenizerFactory\"/&gt;\n        &lt;filter class=\"solr.StopFilterFactory\"\n                ignoreCase=\"true\"\n                words=\"stopwords.txt\"\n                enablePositionIncrements=\"true\"\n                /&gt;\n        &lt;filter class=\"solr.LowerCaseFilterFactory\"/&gt;\n    &lt;filter class=\"solr.EnglishPossessiveFilterFactory\"/&gt;\n        &lt;filter class=\"solr.KeywordMarkerFilterFactory\" protected=\"protwords.txt\"/&gt;\n\n        &lt;filter class=\"solr.PorterStemFilterFactory\"/&gt;\n      &lt;/analyzer&gt;\n      &lt;analyzer type=\"query\"&gt;\n        &lt;tokenizer class=\"solr.StandardTokenizerFactory\"/&gt;\n        &lt;filter class=\"solr.SynonymFilterFactory\" synonyms=\"synonyms.txt\" ignoreCase=\"true\" expand=\"true\"/&gt;\n        &lt;filter class=\"solr.StopFilterFactory\"\n                ignoreCase=\"true\"\n                words=\"stopwords.txt\"\n                enablePositionIncrements=\"true\"\n                /&gt;\n        &lt;filter class=\"solr.LowerCaseFilterFactory\"/&gt;\n    &lt;filter class=\"solr.EnglishPossessiveFilterFactory\"/&gt;\n        &lt;filter class=\"solr.KeywordMarkerFilterFactory\" protected=\"protwords.txt\"/&gt;\n        &lt;filter class=\"solr.PorterStemFilterFactory\"/&gt;\n      &lt;/analyzer&gt;\n    &lt;/fieldType&gt;\n    &lt;fieldType name=\"text_en_splitting\" class=\"solr.TextField\" positionIncrementGap=\"100\" autoGeneratePhraseQueries=\"true\"&gt;\n      &lt;analyzer type=\"index\"&gt;\n        &lt;tokenizer class=\"solr.WhitespaceTokenizerFactory\"/&gt;\n        &lt;filter class=\"solr.StopFilterFactory\"\n                ignoreCase=\"true\"\n                words=\"stopwords.txt\"\n                enablePositionIncrements=\"true\"\n                /&gt;\n        &lt;filter class=\"solr.WordDelimiterFilterFactory\" generateWordParts=\"1\" generateNumberParts=\"1\" catenateWords=\"1\" catenateNumbers=\"1\" catenateAll=\"0\" splitOnCaseChange=\"1\"/&gt;\n        &lt;filter class=\"solr.LowerCaseFilterFactory\"/&gt;\n        &lt;filter class=\"solr.KeywordMarkerFilterFactory\" protected=\"protwords.txt\"/&gt;\n        &lt;filter class=\"solr.PorterStemFilterFactory\"/&gt;\n      &lt;/analyzer&gt;\n      &lt;analyzer type=\"query\"&gt;\n        &lt;tokenizer class=\"solr.WhitespaceTokenizerFactory\"/&gt;\n        &lt;filter class=\"solr.SynonymFilterFactory\" synonyms=\"synonyms.txt\" ignoreCase=\"true\" expand=\"true\"/&gt;\n        &lt;filter class=\"solr.StopFilterFactory\"\n                ignoreCase=\"true\"\n                words=\"stopwords.txt\"\n                enablePositionIncrements=\"true\"\n                /&gt;\n        &lt;filter class=\"solr.WordDelimiterFilterFactory\" generateWordParts=\"1\" generateNumberParts=\"1\" catenateWords=\"0\" catenateNumbers=\"0\" catenateAll=\"0\" splitOnCaseChange=\"1\"/&gt;\n        &lt;filter class=\"solr.LowerCaseFilterFactory\"/&gt;\n        &lt;filter class=\"solr.KeywordMarkerFilterFactory\" protected=\"protwords.txt\"/&gt;\n        &lt;filter class=\"solr.PorterStemFilterFactory\"/&gt;\n      &lt;/analyzer&gt;\n    &lt;/fieldType&gt;\n\n    &lt;!-- Less flexible matching, but less false matches.  Probably not ideal for product names,\n         but may be good for SKUs.  Can insert dashes in the wrong place and still match. --&gt;\n    &lt;fieldType name=\"text_en_splitting_tight\" class=\"solr.TextField\" positionIncrementGap=\"100\" autoGeneratePhraseQueries=\"true\"&gt;\n      &lt;analyzer&gt;\n        &lt;tokenizer class=\"solr.WhitespaceTokenizerFactory\"/&gt;\n        &lt;filter class=\"solr.SynonymFilterFactory\" synonyms=\"synonyms.txt\" ignoreCase=\"true\" expand=\"false\"/&gt;\n        &lt;filter class=\"solr.StopFilterFactory\" ignoreCase=\"true\" words=\"stopwords.txt\"/&gt;\n        &lt;filter class=\"solr.WordDelimiterFilterFactory\" generateWordParts=\"0\" generateNumberParts=\"0\" catenateWords=\"1\" catenateNumbers=\"1\" catenateAll=\"0\"/&gt;\n        &lt;filter class=\"solr.LowerCaseFilterFactory\"/&gt;\n        &lt;filter class=\"solr.KeywordMarkerFilterFactory\" protected=\"protwords.txt\"/&gt;\n        &lt;filter class=\"solr.EnglishMinimalStemFilterFactory\"/&gt;\n        &lt;!-- this filter can remove any duplicate tokens that appear at the same position - sometimes\n             possible with WordDelimiterFilter in conjuncton with stemming. --&gt;\n        &lt;filter class=\"solr.RemoveDuplicatesTokenFilterFactory\"/&gt;\n      &lt;/analyzer&gt;\n    &lt;/fieldType&gt;\n\n    &lt;!-- Just like text_general except it reverses the characters of\n     each token, to enable more efficient leading wildcard queries. --&gt;\n    &lt;fieldType name=\"text_general_rev\" class=\"solr.TextField\" positionIncrementGap=\"100\"&gt;\n      &lt;analyzer type=\"index\"&gt;\n        &lt;tokenizer class=\"solr.StandardTokenizerFactory\"/&gt;\n        &lt;filter class=\"solr.StopFilterFactory\" ignoreCase=\"true\" words=\"stopwords.txt\" enablePositionIncrements=\"true\" /&gt;\n        &lt;filter class=\"solr.LowerCaseFilterFactory\"/&gt;\n        &lt;filter class=\"solr.ReversedWildcardFilterFactory\" withOriginal=\"true\"\n           maxPosAsterisk=\"3\" maxPosQuestion=\"2\" maxFractionAsterisk=\"0.33\"/&gt;\n      &lt;/analyzer&gt;\n      &lt;analyzer type=\"query\"&gt;\n        &lt;tokenizer class=\"solr.StandardTokenizerFactory\"/&gt;\n        &lt;filter class=\"solr.SynonymFilterFactory\" synonyms=\"synonyms.txt\" ignoreCase=\"true\" expand=\"true\"/&gt;\n        &lt;filter class=\"solr.StopFilterFactory\" ignoreCase=\"true\" words=\"stopwords.txt\" enablePositionIncrements=\"true\" /&gt;\n        &lt;filter class=\"solr.LowerCaseFilterFactory\"/&gt;\n      &lt;/analyzer&gt;\n    &lt;/fieldType&gt;\n\n    &lt;fieldtype name=\"phonetic\" stored=\"false\" indexed=\"true\" class=\"solr.TextField\" &gt;\n      &lt;analyzer&gt;\n        &lt;tokenizer class=\"solr.StandardTokenizerFactory\"/&gt;\n        &lt;filter class=\"solr.DoubleMetaphoneFilterFactory\" inject=\"false\"/&gt;\n      &lt;/analyzer&gt;\n    &lt;/fieldtype&gt;\n\n    &lt;fieldtype name=\"payloads\" stored=\"false\" indexed=\"true\" class=\"solr.TextField\" &gt;\n      &lt;analyzer&gt;\n        &lt;tokenizer class=\"solr.WhitespaceTokenizerFactory\"/&gt;\n        &lt;filter class=\"solr.DelimitedPayloadTokenFilterFactory\" encoder=\"float\"/&gt;\n      &lt;/analyzer&gt;\n    &lt;/fieldtype&gt;\n\n    &lt;!-- lowercases the entire field value, keeping it as a single token.  --&gt;\n    &lt;fieldType name=\"lowercase\" class=\"solr.TextField\" positionIncrementGap=\"100\"&gt;\n      &lt;analyzer&gt;\n        &lt;tokenizer class=\"solr.KeywordTokenizerFactory\"/&gt;\n        &lt;filter class=\"solr.LowerCaseFilterFactory\" /&gt;\n      &lt;/analyzer&gt;\n    &lt;/fieldType&gt;\n\n    &lt;fieldType name=\"url\" class=\"solr.TextField\" positionIncrementGap=\"100\"&gt;\n      &lt;analyzer&gt;\n        &lt;tokenizer class=\"solr.StandardTokenizerFactory\"/&gt;\n           &lt;filter class=\"solr.LowerCaseFilterFactory\"/&gt;\n           &lt;filter class=\"solr.WordDelimiterFilterFactory\" generateWordParts=\"1\" generateNumberParts=\"1\"/&gt;\n      &lt;/analyzer&gt;\n    &lt;/fieldType&gt;\n\n\n    &lt;fieldType name=\"text_path\" class=\"solr.TextField\" positionIncrementGap=\"100\"&gt;\n      &lt;analyzer&gt;\n        &lt;tokenizer class=\"solr.PathHierarchyTokenizerFactory\"/&gt;\n      &lt;/analyzer&gt;\n    &lt;/fieldType&gt;\n\n    &lt;!-- since fields of this type are by default not stored or indexed,\n         any data added to them will be ignored outright.  --&gt; \n    &lt;fieldtype name=\"ignored\" stored=\"false\" indexed=\"false\" multiValued=\"true\" class=\"solr.StrField\" /&gt;\n\n &lt;/types&gt;\n\n &lt;fields&gt;\n    &lt;field name=\"id\" type=\"string\" stored=\"true\" indexed=\"true\"/&gt;\n\n    &lt;!-- core fields --&gt;\n    &lt;field name=\"segment\" type=\"string\" stored=\"true\" indexed=\"false\"/&gt;\n    &lt;field name=\"digest\" type=\"string\" stored=\"true\" indexed=\"false\"/&gt;\n    &lt;field name=\"boost\" type=\"float\" stored=\"true\" indexed=\"false\"/&gt;\n    &lt;field name=\"_version_\" type=\"long\" indexed=\"true\" stored=\"true\"/&gt;\n\n    &lt;!-- fields for index-basic plugin --&gt;\n    &lt;field name=\"host\" type=\"url\" stored=\"false\" indexed=\"true\"/&gt;\n    &lt;field name=\"site\" type=\"string\" stored=\"true\" indexed=\"true\"/&gt;\n    &lt;field name=\"url\" type=\"url\" stored=\"true\" indexed=\"true\" required=\"true\"/&gt;\n    &lt;!-- stored=true for highlighting, use term vectors  and positions for fast highlighting --&gt;\n    &lt;field name=\"content\" type=\"text_general\" stored=\"true\" indexed=\"true\"/&gt;\n    &lt;field name=\"title\" type=\"text_general\" stored=\"true\" indexed=\"true\"/&gt;\n    &lt;field name=\"cache\" type=\"string\" stored=\"true\" indexed=\"false\"/&gt;\n    &lt;field name=\"tstamp\" type=\"date\" stored=\"true\" indexed=\"false\"/&gt;\n\n    &lt;!-- catch-all field --&gt;\n    &lt;field name=\"text\" type=\"text_general\" stored=\"false\" indexed=\"true\" multiValued=\"true\"/&gt;\n\n    &lt;!-- fields for index-anchor plugin --&gt;\n    &lt;field name=\"anchor\" type=\"text_general\" stored=\"true\" indexed=\"true\"\n        multiValued=\"true\"/&gt;\n\n    &lt;!-- fields for index-more plugin --&gt;\n    &lt;field name=\"type\" type=\"string\" stored=\"true\" indexed=\"true\" multiValued=\"true\"/&gt;\n    &lt;field name=\"contentLength\" type=\"string\" stored=\"true\" indexed=\"false\"/&gt;\n    &lt;field name=\"lastModified\" type=\"date\" stored=\"true\" indexed=\"false\"/&gt;\n    &lt;field name=\"date\" type=\"tdate\" stored=\"true\" indexed=\"true\"/&gt;\n\n    &lt;!-- fields for languageidentifier plugin --&gt;\n    &lt;field name=\"lang\" type=\"string\" stored=\"true\" indexed=\"true\"/&gt;\n\n    &lt;!-- fields for subcollection plugin --&gt;\n    &lt;field name=\"subcollection\" type=\"string\" stored=\"true\" indexed=\"true\" multiValued=\"true\"/&gt;\n\n    &lt;!-- fields for feed plugin (tag is also used by microformats-reltag)--&gt;\n    &lt;field name=\"author\" type=\"string\" stored=\"true\" indexed=\"true\"/&gt;\n    &lt;field name=\"tag\" type=\"string\" stored=\"true\" indexed=\"true\" multiValued=\"true\"/&gt;\n    &lt;field name=\"feed\" type=\"string\" stored=\"true\" indexed=\"true\"/&gt;\n    &lt;field name=\"publishedDate\" type=\"date\" stored=\"true\" indexed=\"true\"/&gt;\n    &lt;field name=\"updatedDate\" type=\"date\" stored=\"true\" indexed=\"true\"/&gt;\n\n    &lt;!-- fields for creativecommons plugin --&gt;\n    &lt;field name=\"cc\" type=\"string\" stored=\"true\" indexed=\"true\" multiValued=\"true\"/&gt;\n &lt;/fields&gt;\n &lt;uniqueKey&gt;id&lt;/uniqueKey&gt;\n &lt;defaultSearchField&gt;text&lt;/defaultSearchField&gt;\n &lt;solrQueryParser defaultOperator=\"OR\"/&gt;\n\n  &lt;!-- copyField commands copy one field to another at the time a document\n        is added to the index.  It's used either to index the same field differently,\n        or to add multiple fields to the same field for easier/faster searching.  --&gt;\n\n &lt;copyField source=\"content\" dest=\"text\"/&gt;\n &lt;copyField source=\"url\" dest=\"text\"/&gt;\n &lt;copyField source=\"title\" dest=\"text\"/&gt;\n &lt;copyField source=\"anchor\" dest=\"text\"/&gt;\n &lt;copyField source=\"author\" dest=\"text\"/&gt;\n\n&lt;/schema&gt;\n</code></pre>\n\n<p>when I do a search like that:</p>\n\n<pre><code>http://127.0.0.1:8983/solr/collection1/select?q=keyword&amp;wt=json&amp;indent=true&amp;fq=site:domain1.com\n</code></pre>\n\n<p>The response is that</p>\n\n<pre><code>{\n  \"responseHeader\":{\n    \"status\":0,\n    \"QTime\":0,\n    \"params\":{\n      \"indent\":\"true\",\n      \"q\":\"keyword\",\n      \"wt\":\"json\",\n      \"fq\":\"site:domain1.com\"}},\n  \"response\":{\"numFound\":0,\"start\":0,\"docs\":[]\n  }}\n</code></pre>\n\n<p>Do any one know how to filter the result by domain/site?</p>\n", "creation_date": 1386023981, "score": 0},
{"title": "How to search records SOLR with any filter option?", "view_count": 120, "is_answered": false, "answers": [{"question_id": 9273294, "owner": {"user_id": 973840, "accept_rate": 85, "link": "http://stackoverflow.com/users/973840/diaa", "user_type": "registered", "reputation": 385}, "body": "<p>What you wanna do is facet search. </p>\n\n<p>You can find a StackOverflow answer of how to do that <a href=\"http://stackoverflow.com/questions/2357582/solr-and-facet-search\">here</a></p>\n", "creation_date": 1392378273, "is_accepted": false, "score": 0, "last_activity_date": 1392378273, "answer_id": 21778507}], "question_id": 9273294, "tags": ["solr", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/9273294/how-to-search-records-solr-with-any-filter-option", "last_activity_date": 1392378273, "owner": {"age": 29, "answer_count": 0, "creation_date": 1322902565, "user_id": 1078790, "accept_rate": 48, "view_count": 102, "location": "Pakistan", "reputation": 153}, "body": "<p>I am using <strong>apache-nutch-1.4</strong> with <strong>apache-solr-3.2.0</strong></p>\n\n<p>I am able to <strong><em>install both and integrate</em></strong> successfully</p>\n\n<p>when i search word as <strong>'Disease'</strong> it gives result with fields like <strong><em>title,content and URL</em></strong> etc</p>\n\n<p>Now i want to search records with some filters for example</p>\n\n<p>I have one <strong>centers page</strong> which display list of centers like <strong>cancer , Men's , Women's</strong> etc</p>\n\n<p>When user click on particular center like cancer a web-page open which display areas and articles.</p>\n\n<p>There is one search box with select box filter in which all areas of cancer appearing.</p>\n\n<p>I want to search records with respect to any area filter that user selected.</p>\n\n<p>Please help me ...</p>\n", "creation_date": 1329205138, "score": 0},
{"title": "Nutch : Crawl Broken Links &amp; Index it in Solr", "view_count": 561, "is_answered": false, "answers": [{"last_edit_date": 1392377324, "owner": {"user_id": 973840, "accept_rate": 85, "link": "http://stackoverflow.com/users/973840/diaa", "user_type": "registered", "reputation": 385}, "body": "<p>You don't need to index to solr to find out broken links.\nDo the following:</p>\n\n<p><code>bin/nutch readdb &lt;crawlFolder&gt;/crawldb/ -dump myDump</code></p>\n\n<p>It will give you the links that are 404 as:</p>\n\n<pre>\nStatus: 3 (db_gone)\nMetadata: _pst_: notfound(14)\n</pre>\n\n<p>go through the output file and you'll find all broken links.</p>\n\n<p><strong>Example:</strong></p>\n\n<ol>\n<li>Put in the url file \"<a href=\"http://www.wikipedia.com/somethingUnreal\" rel=\"nofollow\">http://www.wikipedia.com/somethingUnreal</a> <a href=\"http://en.wikipedia.org/wiki/NocontentPage\" rel=\"nofollow\">http://en.wikipedia.org/wiki/NocontentPage</a>\"</li>\n<li>Run the crawl command:<code>bin/nutch crawl urls.txt -depth 1</code></li>\n<li>Run the readdb command:<code>bin/nutch readdb crawl-20140214115539/crawldb/ -dump mydump</code></li>\n<li>Open the output file \"part-xxxxx\" with a text editor</li>\n</ol>\n\n<p><strong>Results:</strong></p>\n\n<pre><code>http://en.wikipedia.org/wiki/NocontentPage  Version: 7\nStatus: 1 (db_unfetched)\n...\nMetadata: _pst_: exception(16), lastModified=0: Http code=503, url=http://en.wikipedia.org/wiki/NocontentPage\n\nhttp://www.wikipedia.com/somethingUnreal    Version: 7\nStatus: 5 (db_redir_perm)\n...\nMetadata: Content-Type: text/html_pst_: moved(12), lastModified=0: http://www.wikipedia.org/somethingUnreal\n</code></pre>\n", "question_id": 20513035, "creation_date": 1392373198, "is_accepted": false, "score": 0, "last_activity_date": 1392377324, "answer_id": 21776628}], "question_id": 20513035, "tags": ["nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/20513035/nutch-crawl-broken-links-index-it-in-solr", "last_activity_date": 1392377324, "owner": {"age": 27, "answer_count": 0, "creation_date": 1367929486, "user_id": 2358303, "view_count": 2, "location": "Mumbai, India", "reputation": 3}, "body": "<p>My purpose is to find how many URLs in an HTML page are invalid (404, 500, HostNotFound). So in Nutch is there a config change that we can do through which the web crawler crawls through broken links and indexes it in solr.</p>\n\n<p>Once all the broken links &amp; valid links are indexed in Solr I can just check the URLs which are invalid and can remove it from my HTML page.</p>\n\n<p>Any help will be highly appreciated.</p>\n\n<p>Thanks in advance.</p>\n", "creation_date": 1386746886, "score": 0},
{"title": "Nutch: Authentication via putting a cookie in the header", "view_count": 977, "owner": {"user_id": 340648, "answer_count": 11, "creation_date": 1273781720, "accept_rate": 94, "view_count": 119, "location": "New York, NY, United States", "reputation": 695}, "is_answered": true, "answers": [{"question_id": 17581298, "owner": {"user_id": 2701795, "link": "http://stackoverflow.com/users/2701795/mouli", "user_type": "registered", "reputation": 169}, "body": "<p>I have added custom code to nutch protocol-httpclient plugin to solve the issue.</p>\n\n<p>Shared the changes in the link below</p>\n\n<p><a href=\"http://www.gingercart.com/Home/search-and-crawl/nutch-custom-authentication-cookies-session-management-to-crawl-secure-enterprise-websites\" rel=\"nofollow\">http://www.gingercart.com/Home/search-and-crawl/nutch-custom-authentication-cookies-session-management-to-crawl-secure-enterprise-websites</a></p>\n", "creation_date": 1392162168, "is_accepted": true, "score": 3, "last_activity_date": 1392162168, "answer_id": 21715631}], "question_id": 17581298, "tags": ["http", "authentication", "cookies", "solr", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/17581298/nutch-authentication-via-putting-a-cookie-in-the-header", "last_activity_date": 1392162168, "accepted_answer_id": 21715631, "body": "<p>I am surprised that there is so little support or information out there for getting Nutch to be able to crawl parts of a website that require authentication.</p>\n\n<p>I am aware that maybe Apache Nutch is not currently able to (<a href=\"http://wiki.apache.org/nutch/HttpPostAuthentication\" rel=\"nofollow\">but apparently hopes to</a>) support Http POST authentication.</p>\n\n<p>However, all we really want to do is be able to add a cookie to our Nutch bot header that will allow it to access those parts of the site that way (rather than post a username and password to a form and then receive the cookie).</p>\n\n<p>So I have spent a good amount of time searching and am surprised that most discussions about this are all the way back in 2005 or 2008: <a href=\"http://web.archiveorange.com/archive/v/bhVEvThyYNqvmgycVlKM#0N7zAPMhbM3xOnX\" rel=\"nofollow\">here</a>, <a href=\"http://lucene.472066.n3.nabble.com/httpclient-and-cookies-td616513.html\" rel=\"nofollow\">there</a>, <a href=\"http://lucene.472066.n3.nabble.com/How-to-authenticate-with-cookies-td614810.html\" rel=\"nofollow\">everywhere</a>.</p>\n\n<p>After all these years, is there anyway to work around this limitation or is there just still no way to authenticate by giving Nutch a 'prebaked' cookie so it can access member only parts of our site?.</p>\n", "creation_date": 1373492142, "score": 2},
{"title": "Error in execution of Apache nutch-2.2.1", "view_count": 1416, "is_answered": true, "answers": [{"question_id": 21090125, "owner": {"user_id": 1965370, "link": "http://stackoverflow.com/users/1965370/do-do", "user_type": "registered", "reputation": 389}, "body": "<p>Command /bin/nutch crawl is deprecated. You should use /bin/crawl instead.</p>\n", "creation_date": 1392034763, "is_accepted": false, "score": 3, "last_activity_date": 1392034763, "answer_id": 21677066}], "question_id": 21090125, "tags": ["java", "apache", "hadoop", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/21090125/error-in-execution-of-apache-nutch-2-2-1", "last_activity_date": 1392034763, "owner": {"user_id": 1851265, "answer_count": 0, "creation_date": 1353854278, "accept_rate": 50, "view_count": 5, "reputation": 15}, "body": "<p>I have installed Nutch 2.2.1 in Ubuntu 12.04 but on executing the command \n./nutch crawl urls/ -dir download/ -depth 3 -topN 5 </p>\n\n<p>I get the following error:-</p>\n\n<p>InjectorJob: Using class org.apache.gora.memory.store.MemStore as the Gora storage class.\nInjectorJob: total number of urls rejected by filters: 0\nInjectorJob: total number of urls injected after normalization and filtering: 0\nException in thread \"main\" java.lang.RuntimeException: job failed: name=generate: null, jobid=job_local1165582916_0002\n    at org.apache.nutch.util.NutchJob.waitForCompletion(NutchJob.java:54)\n    at org.apache.nutch.crawl.GeneratorJob.run(GeneratorJob.java:199)\n    at org.apache.nutch.crawl.Crawler.runTool(Crawler.java:68)\n    at org.apache.nutch.crawl.Crawler.run(Crawler.java:152)\n    at org.apache.nutch.crawl.Crawler.run(Crawler.java:250)\n    at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n    at org.apache.nutch.crawl.Crawler.main(Crawler.java:257)</p>\n", "creation_date": 1389612403, "score": 0},
{"title": "Apache Nutch-2.2.1 installation", "view_count": 863, "is_answered": false, "answers": [{"question_id": 21587472, "owner": {"user_id": 1965370, "link": "http://stackoverflow.com/users/1965370/do-do", "user_type": "registered", "reputation": 389}, "body": "<p>You should go to $NUTCH_HOME/runtime/local/ directory to run the the commands.</p>\n", "creation_date": 1392030686, "is_accepted": false, "score": 0, "last_activity_date": 1392030686, "answer_id": 21675571}], "question_id": 21587472, "tags": ["nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/21587472/apache-nutch-2-2-1-installation", "last_activity_date": 1392030686, "owner": {"user_id": 3250183, "answer_count": 12, "creation_date": 1391019858, "accept_rate": 67, "view_count": 40, "reputation": 141}, "body": "<p>I am installing nutch2.2.1 on my centOS virtual machine and getting an error injecting the seed urls(directory name). I used this command:</p>\n\n<pre><code>/usr/share/apache-nutch-2.1/src/bin/nutch inject root/apache-nutch-2.1/src/testresources/testcrawl urls\n</code></pre>\n\n<p>And i got an error :</p>\n\n<pre><code>Error: Could not find or load main class org.apache.nutch.crawl.InjectorJob\n</code></pre>\n\n<p>Similarly, for the command</p>\n\n<pre><code>/usr/share/apache-nutch-2.1/src/bin/nutch readdb\n</code></pre>\n\n<p>gives me an error:</p>\n\n<pre><code>Error: Could not find or load main class org.apache.nutch.crawl.WebTableReader\n</code></pre>\n\n<p>What should i do to fix these errors?\nI am following the tutorial from: <a href=\"http://wiki.apache.org/nutch/Nutch2Tutorial\" rel=\"nofollow\">http://wiki.apache.org/nutch/Nutch2Tutorial</a> and followed the same steps as suggested.</p>\n\n<p>Also my query also revolves around setting the path for ant. Every time i open a new session i have to set the ANT_HOME and PATH environment variable manually. And then they work all fine. Same is the case with setting JAVA_HOME.</p>\n", "creation_date": 1391631147, "score": 0},
{"title": "Nucth 2.2.1 build stuck issue", "view_count": 493, "owner": {"user_id": 1830069, "answer_count": 10, "creation_date": 1353080229, "accept_rate": 63, "view_count": 103, "reputation": 543}, "is_answered": true, "answers": [{"question_id": 21639210, "owner": {"user_id": 256618, "link": "http://stackoverflow.com/users/256618/mark-oconnor", "user_type": "registered", "reputation": 54719}, "body": "<p>Run the build on a machine connected to the internet. Ivy will cache your build's dependencies here (by default):</p>\n\n<pre><code>$HOME/.ivy2/cache\n</code></pre>\n\n<p>A populated cache will enable you to repeat the build inside your firewall. </p>\n\n<p>I would also suggest enhancing the build to run the ivy <a href=\"https://ant.apache.org/ivy/history/latest-milestone/use/report.html\" rel=\"nofollow\">report</a> task. This will produce webpage listing the dependency tree. If you want to build a local repository this information is very useful.</p>\n\n<p>Finally consider running a repository manager inside your firewall. Tools like Nexus and Artifactory make repository hosting a lot simpler to manage. Consider obtaining a firewall exception for this server. Repository managers are used to cache useful internet repositories like Maven Central.</p>\n", "creation_date": 1391822575, "is_accepted": true, "score": 1, "last_activity_date": 1391822575, "answer_id": 21640672}], "question_id": 21639210, "tags": ["java", "ant", "ivy", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/21639210/nucth-2-2-1-build-stuck-issue", "last_activity_date": 1391822575, "accepted_answer_id": 21640672, "body": "<p>I am having issues building Nutch 2.2.1 behind my company firewall. My build gets stuck here:</p>\n\n<pre><code>[ivy:resolve] :: loading settings :: file = ~/nutchtest/nutch/ivy/ivysettings.xml\n</code></pre>\n\n<p>When I contacted the hosting admin, they said - \"Ant is trying to download files from internet and it will have problems with our firewalls. You will either have to download the files yourself and then scp/sftp them to the machine. Unfortunately we don't have an http proxy.\"</p>\n\n<p>From further digging, I could see Ant is trying to access this link <a href=\"http://ant.apache.org/ivy/\" rel=\"nofollow\">http://ant.apache.org/ivy/</a>. Could anyone please advise what I should do to make Ant compile Nutch without accessing the internet? I can download required files from <a href=\"http://ant.apache.org/ivy/\" rel=\"nofollow\">http://ant.apache.org/ivy/</a> and scp/sftp to the server but I am not sure what files to download and where to put them?</p>\n", "creation_date": 1391813416, "score": 1},
{"title": "I can&#39;t inject seeds in Nutch crawler", "view_count": 758, "owner": {"age": 27, "answer_count": 13, "creation_date": 1351961026, "user_id": 1796802, "accept_rate": 89, "view_count": 227, "location": "Bucharest, Romania", "reputation": 275}, "is_answered": true, "answers": [{"question_id": 21558721, "owner": {"user_id": 1796802, "accept_rate": 89, "link": "http://stackoverflow.com/users/1796802/greatdane", "user_type": "registered", "reputation": 275}, "body": "<p>I've restarted the tutorial from scratch on a clean snapshot of the virtual machine and I had no problem.</p>\n", "creation_date": 1391680936, "is_accepted": true, "score": 0, "last_activity_date": 1391680936, "answer_id": 21599541}], "question_id": 21558721, "tags": ["web-crawler", "hbase", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/21558721/i-cant-inject-seeds-in-nutch-crawler", "last_activity_date": 1391680936, "accepted_answer_id": 21599541, "body": "<p>I'm using <code>Nutch</code> to crawl through a certain site (namely <a href=\"http://allrecipes.com\" rel=\"nofollow\">this one</a>). I followed this <a href=\"https://github.com/renepickhardt/metalcon/wiki/simpleNutchSolrSetup\" rel=\"nofollow\">tutorial</a> and it worked just fine, but when I tried to inject other url for <code>Nutch</code> to crawl I received</p>\n\n<pre><code>$ bin/nutch inject urls\nInjectorJob: starting at 2014-02-04 18:26:18\nInjectorJob: Injecting urlDir: urls\nInjectorJob: org.apache.gora.util.GoraException: java.lang.RuntimeException: org.apache.hadoop.hbase.ZooKeeperConnectionException: HBase is able to connect to ZooKeeper but the connection closes immediately. This could be a sign that the server has too many connections (30 is the default). Consider inspecting your ZK server logs for that error and then make sure you are reusing HBaseConfiguration as often as you can. See HTable's javadoc for more information.\n    at org.apache.gora.store.DataStoreFactory.createDataStore(DataStoreFactory.java:167)\n    at org.apache.gora.store.DataStoreFactory.createDataStore(DataStoreFactory.java:135)\n    at org.apache.nutch.storage.StorageUtils.createWebStore(StorageUtils.java:75)\n    at org.apache.nutch.crawl.InjectorJob.run(InjectorJob.java:221)\n    at org.apache.nutch.crawl.InjectorJob.inject(InjectorJob.java:251)\n    at org.apache.nutch.crawl.InjectorJob.run(InjectorJob.java:273)\n    at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n    at org.apache.nutch.crawl.InjectorJob.main(InjectorJob.java:282)\nCaused by: java.lang.RuntimeException: org.apache.hadoop.hbase.ZooKeeperConnectionException: HBase is able to connect to ZooKeeper but the connection closes immediately. This could be a sign that the server has too many connections (30 is the default). Consider inspecting your ZK server logs for that error and then make sure you are reusing HBaseConfiguration as often as you can. See HTable's javadoc for more information.\n    at org.apache.gora.hbase.store.HBaseStore.initialize(HBaseStore.java:127)\n    at org.apache.gora.store.DataStoreFactory.initializeDataStore(DataStoreFactory.java:102)\n    at org.apache.gora.store.DataStoreFactory.createDataStore(DataStoreFactory.java:161)\n    ... 7 more\nCaused by: org.apache.hadoop.hbase.ZooKeeperConnectionException: HBase is able to connect to ZooKeeper but the connection closes immediately. This could be a sign that the server has too many connections (30 is the default). Consider inspecting your ZK server logs for that error and then make sure you are reusing HBaseConfiguration as often as you can. See HTable's javadoc for more information.\n    at org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.&lt;init&gt;(ZooKeeperWatcher.java:155)\n    at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.getZooKeeperWatcher(HConnectionManager.java:1002)\n    at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.setupZookeeperTrackers(HConnectionManager.java:304)\n    at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.&lt;init&gt;(HConnectionManager.java:295)\n    at org.apache.hadoop.hbase.client.HConnectionManager.getConnection(HConnectionManager.java:157)\n    at org.apache.hadoop.hbase.client.HBaseAdmin.&lt;init&gt;(HBaseAdmin.java:90)\n    at org.apache.gora.hbase.store.HBaseStore.initialize(HBaseStore.java:109)\n    ... 9 more\nCaused by: org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss for /hbase\n    at org.apache.zookeeper.KeeperException.create(KeeperException.java:90)\n    at org.apache.zookeeper.KeeperException.create(KeeperException.java:42)\n    at org.apache.zookeeper.ZooKeeper.exists(ZooKeeper.java:809)\n    at org.apache.zookeeper.ZooKeeper.exists(ZooKeeper.java:837)\n    at org.apache.hadoop.hbase.zookeeper.ZKUtil.createAndFailSilent(ZKUtil.java:903)\n    at org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.&lt;init&gt;(ZooKeeperWatcher.java:133)\n    ... 15 more\n</code></pre>\n\n<p>Now I've tried restarting the machine, I've tried changing <code>/etc/hosts</code> as <a href=\"http://stackoverflow.com/questions/7791788/hbase-client-do-not-able-to-connect-with-remote-hbase-server\">here</a>, but it didn't work.</p>\n\n<p>I'm using <code>apache-nutch-2.2.1</code> and <code>hbase-0.90.4</code>.</p>\n", "creation_date": 1391533274, "score": 0},
{"title": "Referenced classpath entry for JAR in local maven repository missing when importing nutch", "view_count": 104, "is_answered": false, "answers": [{"question_id": 17784833, "owner": {"user_id": 1523648, "accept_rate": 90, "link": "http://stackoverflow.com/users/1523648/oberlies", "user_type": "registered", "reputation": 6970}, "body": "<p>I've seen this behaviour when I had a project already imported in Eclipse, but then deleted the local Maven repository.</p>\n\n<p>To solve this, make m2eclipse re-download the project's dependencies:</p>\n\n<ol>\n<li>Right-click on the project and select <em>Maven > Update Project...</em></li>\n<li>Select <em>Force Update of Snapshots/Releases</em> and hit <em>OK</em></li>\n</ol>\n", "creation_date": 1390988654, "is_accepted": false, "score": 0, "last_activity_date": 1390988654, "answer_id": 21427235}], "question_id": 17784833, "tags": ["eclipse", "maven", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/17784833/referenced-classpath-entry-for-jar-in-local-maven-repository-missing-when-import", "last_activity_date": 1390988919, "owner": {"user_id": 2060673, "view_count": 5, "answer_count": 0, "creation_date": 1360571916, "reputation": 14}, "body": "<p>I am trying to import nutch 1.4 into Eclipse. I cloned git repository and then build it using Maven. Then, I imported it in Eclipse as Maven project, but I'm getting the following error:</p>\n\n<blockquote>\n  <p>The archive: /home/devang/.m2/repository/javax/jms/jms/1.1/jms-1.1.jar which is referenced by the classpath, does not exist.</p>\n</blockquote>\n", "creation_date": 1374487672, "score": 0},
{"title": "How to make nutch crawl files and subfolders - it only crawls the index of the folder", "view_count": 1508, "owner": {"age": 38, "answer_count": 46, "creation_date": 1240697384, "user_id": 96061, "accept_rate": 83, "view_count": 294, "location": "Denmark", "reputation": 3089}, "is_answered": true, "answers": [{"last_edit_date": 1390396843, "owner": {"user_id": 1881318, "link": "http://stackoverflow.com/users/1881318/tahagh", "user_type": "registered", "reputation": 619}, "body": "<p>Investigating File and FileResponse sources, I found the followings:</p>\n\n<ol>\n<li>There is a configuration parameter named \"file.crawl.parent\" which controls whether nutch should also crawl the parent of a directory or not. By default it is true. </li>\n<li>In this implementation, when nutch encounters a directory, it generates the list of files in it as a set of hyperlinks in the content otherwise it reads the file content. Nutch uses File.isDirectory() to determine the given path is a directory or not. So check that your path is really interpreted as a directory.</li>\n</ol>\n", "question_id": 21185364, "creation_date": 1390160281, "is_accepted": false, "score": 2, "last_activity_date": 1390396843, "answer_id": 21221633}, {"question_id": 21185364, "owner": {"user_id": 96061, "accept_rate": 83, "link": "http://stackoverflow.com/users/96061/boris", "user_type": "registered", "reputation": 3089}, "body": "<p>I found out that in order to crawl a local file system, you have to add slashes at the end of the seed url, otherwise nutch would not identify the last part of the path as a directory.</p>\n\n<p>So I changed my seed url from</p>\n\n<pre><code>file:////mnt/ntserver\n</code></pre>\n\n<p>to </p>\n\n<pre><code>file:////mnt/ntserver/\n</code></pre>\n\n<p>and then things worked.</p>\n\n<hr>\n\n<p>More details:</p>\n\n<p>If for instance I had the file <code>test.txt</code> under my <code>/mnt/ntserver</code> and had <code>file:////mnt/ntserver</code> as my seed url, then nutch would correctly parse the index of <code>/mnt/ntserver</code>, and find out that there was a file called <code>test.txt</code>, but then it would try to fetch the file <code>/mnt/test.txt</code>. After adding the trailing slash to the seed url, making it <code>file:////mnt/ntserver/</code>, nutch now tried to fetch the file <code>/mnt/ntserver/test.txt</code>, solving my problem.</p>\n\n<p>Incidentally, in order to stop nutch from going up the folder tree towards the root, I set <code>file.crawl.parent</code> to false in nutch-default.xml, but it could also be done via regex-urlfilter.xml.</p>\n", "creation_date": 1390388792, "is_accepted": true, "score": 0, "last_activity_date": 1390388792, "answer_id": 21281194}], "question_id": 21185364, "tags": ["regex", "solr", "nutch", "web-crawler"], "answer_count": 2, "link": "http://stackoverflow.com/questions/21185364/how-to-make-nutch-crawl-files-and-subfolders-it-only-crawls-the-index-of-the-f", "last_activity_date": 1390833526, "accepted_answer_id": 21281194, "body": "<p>EDIT: I found my answer and wrote it up below, but gave the bounty to tahagh, since he provided some good suggestions.</p>\n\n<hr>\n\n<p>I am setting up nutch to crawl a local folder (a samba mount). I have followed <a href=\"http://amac4.blogspot.dk/2013/07/setting-up-nutch-to-crawl-filesystem.html\" rel=\"nofollow\">this</a> tutorial. </p>\n\n<p>My folder looks like this:</p>\n\n<pre><code>nutch@ubuntu:~$ ls /mnt/ntserver/\nexpansion.docx  test-folder  test-shared.txt\n</code></pre>\n\n<p>with some files and folders below <code>test-folder</code> also.</p>\n\n<p>When I run nutch, it doesn't index the files or the subfolder. It only puts a single document into solr, which is the index of the folder. This is what I get in solr after running nutch on an empty solr index:</p>\n\n<pre><code>\"response\": {\n    \"numFound\": 1,\n    \"start\": 0,\n    \"docs\": [\n      {\n        \"content\": [\n          \"Index of /mnt/ntserver Index of /mnt/ntserver ../ - - - expansion.docx Mon, 30 Dec 2013 14:00:42 GMT 70524 test-folder/ Fri, 17 Jan 2014 09:38:50 GMT - test-shared.txt Thu, 16 Jan 2014 11:33:42 GMT 16\"\n        ],\n      .....\n</code></pre>\n\n<p>How can I get nutch to index the files and the subfolders?</p>\n\n<hr>\n\n<p>edit: if I set regex-urlfilter to allow everything (after filtering for gifs, http etc) like this <code>+.</code>, then nutch seems to go up the folder hierarchy, but not down, and still only crawling the indexes, not the files. This is what I get in solr:</p>\n\n<pre><code>\"response\": {\n    \"numFound\": 26,\n    \"start\": 0,\n    \"docs\": [\n      {\n        \"title\": [\n          \"Index of /\"\n        ]\n      },\n      {\n        \"title\": [\n          \"Index of /bin\"\n        ]\n      },\n      ...\n      {\n        \"title\": [\n          \"Index of /mnt\"\n        ]\n      },\n      {\n        \"title\": [\n          \"Index of /mnt/ntserver\"\n        ]\n      },\n      ...\n    ]\n</code></pre>\n\n<hr>\n\n<p>Additional info:</p>\n\n<p>This is the crawl command I use:</p>\n\n<pre><code>apache-nutch-1.7/bin/nutch crawl -dir fileCrawl -urls apache-nutch-1.7/urls/ -solr http://localhost:8983/solr -depth 3 -topN 10000\n</code></pre>\n\n<p>This is the content of my seed urls file:</p>\n\n<pre><code>nutch@ubuntu:~$ cat apache-nutch-1.7/urls/urls_to_be_crawled.txt \nfile:////mnt/ntserver\n</code></pre>\n\n<p>this is my regex-urlfilter.xml:</p>\n\n<pre><code>nutch@ubuntu:~$ cat apache-nutch-1.7/conf/regex-urlfilter.txt\n# skip http: ftp: and mailto: urls\n-^(http|ftp|mailto):\n\n# skip image and other suffixes we can't yet parse\n# for a more extensive coverage use the urlfilter-suffix plugin\n-\\.(gif|GIF|jpg|JPG|png|PNG|ico|ICO|css|CSS|sit|SIT|eps|EPS|wmf|WMF|zip|ZIP|ppt|PPT|mpg|MPG|gz|GZ|rpm|RPM|tgz|TGZ|mov|MOV|exe|EXE|jpeg|JPEG|bmp|BMP|js|JS|asp|ASP|xxx|XXX|yyy|YYY|cs|CS|dll|DLL|refresh|REFRESH)$\n\n# accept any files\n+.*mnt/ntserver.*\n</code></pre>\n\n<p>I have included <code>protocol-file</code> and set no limit on file size in nutch-site.xml:</p>\n\n<pre><code>nutch@ubuntu:~$ cat apache-nutch-1.7/conf/nutch-site.xml\n...\n&lt;property&gt;\n    &lt;name&gt;plugin.includes&lt;/name&gt;\n    &lt;value&gt;protocol-file|urlfilter-regex|parse-(html|tika|text)|index-(basic|anchor)|indexer-solr|scoring-opic|urlnormalizer-(pass|regex|basic)|index-more&lt;!--|remove-empty-document|title-adder--&gt;&lt;/value&gt;\n    &lt;description&gt;&lt;/description&gt;\n&lt;/property&gt;\n\n&lt;property&gt;\n    &lt;name&gt;file.content.limit&lt;/name&gt;\n    &lt;value&gt;-1&lt;/value&gt;\n    &lt;description&gt; Needed to stop buffer overflow errors - Unable to read.....&lt;/description&gt;\n&lt;/property&gt;\n\n...\n</code></pre>\n\n<p>and I have commented out the duplicate slash removal in regex-normalize.xml:</p>\n\n<pre><code>nutch@ubuntu:~$ cat apache-nutch-1.7/conf/regex-normalize.xml\n...\n&lt;!-- removes duplicate slashes - commented out, so we won't get invalid filenames \n&lt;regex&gt;\n    &lt;pattern&gt;(?&amp;lt;!:)/{2,}&lt;/pattern&gt;\n    &lt;substitution&gt;/&lt;/substitution&gt;\n&lt;/regex&gt;\n--&gt;\n...\n</code></pre>\n", "creation_date": 1389959693, "score": 1},
{"title": "Add metadata from database to Solr Index created by Nutch", "view_count": 534, "is_answered": false, "answers": [{"question_id": 21295911, "owner": {"user_id": 1881318, "link": "http://stackoverflow.com/users/1881318/tahagh", "user_type": "registered", "reputation": 619}, "body": "<p>Solr has the ability to partially update a document, provided that all your document fields are stored. See <a href=\"http://solr.pl/en/2012/07/09/solr-4-0-partial-documents-update/\" rel=\"nofollow\">this</a>. This way, you can define several fields for your document, that are not originally filled by nutch, but after the document is added to solr by nutch, you can update those fields with your database values.</p>\n\n<p>In spite of this, I think there is one major problem to be solved. Whenever nutch recrawls a page, it updates the entire document in solr, so your updated fields are missed. Even in the first time, you must be sure that nutch first added the document, and then the fields are updated. To solve this, I think you need to write a plugin for nutch or a special request handler for solr to know when updates are happening.</p>\n", "creation_date": 1390563286, "is_accepted": false, "score": 0, "last_activity_date": 1390563286, "answer_id": 21331637}], "question_id": 21295911, "tags": ["solr", "nutch", "solr4"], "answer_count": 1, "link": "http://stackoverflow.com/questions/21295911/add-metadata-from-database-to-solr-index-created-by-nutch", "last_activity_date": 1390563286, "owner": {"age": 32, "answer_count": 0, "creation_date": 1364992010, "user_id": 2240394, "view_count": 7, "location": "Johannesburg, South Africa", "reputation": 13}, "body": "<p>I have a bespoke CMS that needs to be searchable in Solr. Currently, I am using Nutch to crawl the pages based on a seed list generated from the CMS itself.</p>\n\n<p>I need to be able to add metadata stored in the CMS database to the document indexed in Solr. So, the thought here is that the page text (html generated by the CMS) is crawled via Nutch and the metadata is added to the Solr document where the unique ID (in this instance, the URL) is the same.</p>\n\n<p>As such, the metadata from the DB can be used for facets / filtering etc while full-text search and ranking is handled via the document added by Nutch.</p>\n\n<p>Is this pattern possible? Is there any way to update the fields expected from the CMS DB after  Nutch has added it to Solr?</p>\n", "creation_date": 1390431762, "score": 0},
{"title": "Anyone has worked with a PHP API to read &#39;Nutch search engine&#39; crawl results?", "view_count": 1484, "owner": {"age": 35, "answer_count": 2, "creation_date": 1242048279, "user_id": 104785, "accept_rate": 89, "view_count": 112, "reputation": 1757}, "is_answered": true, "answers": [{"question_id": 1643116, "owner": {"user_id": 484046, "accept_rate": 81, "link": "http://stackoverflow.com/users/484046/millebii", "user_type": "registered", "reputation": 976}, "body": "<p>for your question #1 you need to inject these URLs to the crawler. Relatively simple :\n+ create a file with the URLs you want added\n+ issue inject command with these URL's (may need to wait for the end of the previous crawl/fetch/index cycle)\n+ start a new crawl</p>\n\n<p>note : you need to make sure the urls are not filtered out as well</p>\n", "creation_date": 1294383986, "is_accepted": false, "score": 0, "last_activity_date": 1294383986, "answer_id": 4623266}, {"question_id": 1643116, "owner": {"user_id": 484046, "accept_rate": 81, "link": "http://stackoverflow.com/users/484046/millebii", "user_type": "registered", "reputation": 976}, "body": "<p>regarding #2, Nutch is written in JSP &amp; Java, I don't know any PHP implementation (if you find I'm interested).\nSo basically you weed to create an AJAX or SOAP kind of communication scheme between your PHP script and the Nutch Server.\nHave you tried the nutch mailing list for help ?</p>\n", "creation_date": 1294384197, "is_accepted": false, "score": -1, "last_activity_date": 1294384197, "answer_id": 4623284}, {"question_id": 1643116, "owner": {"user_id": 770035, "link": "http://stackoverflow.com/users/770035/ugs", "user_type": "registered", "reputation": 1296}, "body": "<p>I'm looking for a real good way to do this too. But as of now, Im using a JSP API to display search results.\n<a href=\"http://today.java.net/pub/a/today/2006/02/16/introduction-to-nutch-2.html\" rel=\"nofollow\">This</a> should start you off.</p>\n\n<p>You could alternatively use php to recieve your results as JSON objects.</p>\n\n<p>To kick you off in this direction, there's a interesting <a href=\"http://malsup.com/jquery/form/#json\" rel=\"nofollow\">page</a> to get you started on JSON using jquery.\nGoogle for other tutorials on JSON. There are plenty of'em.</p>\n", "creation_date": 1306437718, "is_accepted": true, "score": 0, "last_activity_date": 1306437718, "answer_id": 6143919}, {"question_id": 1643116, "owner": {"user_id": 2946107, "link": "http://stackoverflow.com/users/2946107/lsroudi", "user_type": "registered", "reputation": 26}, "body": "<p>you need to use solr for search or another search platform, nutch is just a crawler, the idea is simple :</p>\n\n<ul>\n<li>==> nutch for crawling</li>\n<li>==> solr to create an index</li>\n<li>==> build an interface to search inside the index( step 2). i used\nSolariumBundle\u200e  for this step</li>\n</ul>\n", "creation_date": 1390489431, "is_accepted": false, "score": 0, "last_activity_date": 1390489431, "answer_id": 21311861}], "question_id": 1643116, "tags": ["php", "nutch", "phpcrawl"], "answer_count": 4, "link": "http://stackoverflow.com/questions/1643116/anyone-has-worked-with-a-php-api-to-read-nutch-search-engine-crawl-results", "last_activity_date": 1390489431, "accepted_answer_id": 6143919, "body": "<p>I have set up 'Nutch search engine' to crawl websites.\nNow,I need to write an php API to talk to the Nutch search engine.\nI need to do 2 things:</p>\n\n<ol>\n<li><p>using a PHP script I need to specify to Nutch as to which URLs to crawl \n(for this I have some pointers from   <a href=\"http://www.cs.sjsu.edu/faculty/pollett/masters/Semesters/Fall07/sheetal/?Deliverable2.html\" rel=\"nofollow\">http://www.cs.sjsu.edu/faculty/pollett/masters/Semesters/Fall07/sheetal/?Deliverable2.html</a></p></li>\n<li><p>using a PHP script I need to retrieve the crawl result from the Nutch crawl DB.\nI cant seem to find any help on this (or I might be too dumb to see the answer if it's already there :()</p></li>\n</ol>\n\n<p>If anyone has used a PHP API to read Nutch crawl results, please share some pointers with me.</p>\n\n<p>Desperately waiting for some help.</p>\n", "creation_date": 1256816156, "score": 2},
{"title": "Nutch updatedb WrongRegionException", "view_count": 57, "is_answered": false, "question_id": 21309295, "tags": ["hbase", "nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/21309295/nutch-updatedb-wrongregionexception", "last_activity_date": 1390482964, "owner": {"user_id": 1337352, "answer_count": 21, "creation_date": 1334613000, "accept_rate": 32, "view_count": 155, "location": "Prague, Czech Republic", "reputation": 623}, "body": "<p>I run generate-fetch-parse and after update I got this exception.</p>\n\n<pre><code>2014-01-23 13:40:56,905 ERROR store.HBaseStore - Failed 747 actions: WrongRegionException: 747 times, servers with issues: server.eu:43556, \n    2014-01-23 13:40:56,905 ERROR store.HBaseStore - [Ljava.lang.StackTraceElement;@12101d00\n</code></pre>\n\n<p>Can you please help me to understand where can be problem?</p>\n\n<p>using versions: Nutch 2.2.1, Hbase 0.90.6</p>\n", "creation_date": 1390482964, "score": 1},
{"title": "Can&#39;t access hadoop web ui for job tracker", "view_count": 2951, "is_answered": false, "question_id": 4128153, "tags": ["jetty", "hadoop", "nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/4128153/cant-access-hadoop-web-ui-for-job-tracker", "last_activity_date": 1390395438, "owner": {"user_id": 810896, "view_count": 6, "answer_count": 0, "creation_date": 1289251054, "reputation": 26}, "body": "<p>I'm trying to set up hadoop and nutch to run on EC2. To get started, I have followed the excellent <a href=\"http://wiki.apache.org/nutch/NutchHadoopTutorial\" rel=\"nofollow\">NutchHadoopTutorial</a>. Most everything works as it should, except that I am unable to access any of the web interfaces (e.g. JobTracker). The JobTracker starts without errors, and I can hit <code>nutch-master:50030</code>, however I'm getting what looks like jetty's default servlet, which returns a link to the webapps directory, and then from there a job directory, and then a link to <code>nutch-master:50030/webapps/job/jobtracker.jsp</code> -- which in turn returns a 404 for <code>RequestURI=/webapps/job/jobtracker.jsp</code>. I've checked the classpath, and everything that is supposed to be there is in fact available:</p>\n\n<pre><code>/usr/lib/jvm/java-6-openjdk/bin/java -Xmx1000m -Dhadoop.log.dir=/nutch/search/logs -Dhadoop.log.file=hadoop-nutch-jobtracker-nutch-master.log -Dhadoop.home.dir=/nutch/search -Dhadoop.id.str=nutch -Dhadoop.root.logger=INFO,DRFA -Djava.library.path=/nutch/search/lib/native/Linux-i386-32 -Dhadoop.policy.file=hadoop-policy.xml -classpath /nutch/search/bin/../conf:/usr/lib/jvm/java-6-openjdk/lib/tools.jar:/nutch/search/hadoop-0.20.2-core.jar:/nutch/search/lib/apache-solr-core-1.4.0.jar:/nutch/search/lib/apache-solr-solrj-1.4.0.jar:/nutch/search/lib/commons-beanutils-1.8.0.jar:/nutch/search/lib/commons-cli-1.2.jar:/nutch/search/lib/commons-codec-1.3.jar:/nutch/search/lib/commons-collections-3.2.1.jar:/nutch/search/lib/commons-el-1.0.jar:/nutch/search/lib/commons-httpclient-3.1.jar:/nutch/search/lib/commons-io-1.4.jar:/nutch/search/lib/commons-lang-2.1.jar:/nutch/search/lib/commons-logging-1.0.4.jar:/nutch/search/lib/commons-logging-api-1.0.4.jar:/nutch/search/lib/commons-net-1.4.1.jar:/nutch/search/lib/core-3.1.1.jar:/nutch/search/lib/geronimo-stax-api_1.0_spec-1.0.1.jar:/nutch/search/lib/hadoop-0.20.2-core.jar:/nutch/search/lib/hadoop-0.20.2-tools.jar:/nutch/search/lib/hsqldb-1.8.0.10.jar:/nutch/search/lib/icu4j-4_0_1.jar:/nutch/search/lib/jakarta-oro-2.0.8.jar:/nutch/search/lib/jasper-compiler-5.5.12.jar:/nutch/search/lib/jasper-runtime-5.5.12.jar:/nutch/search/lib/jcl-over-slf4j-1.5.5.jar:/nutch/search/lib/jets3t-0.6.1.jar:/nutch/search/lib/jetty-6.1.14.jar:/nutch/search/lib/jetty-util-6.1.14.jar:/nutch/search/lib/junit-3.8.1.jar:/nutch/search/lib/kfs-0.2.2.jar:/nutch/search/lib/log4j-1.2.15.jar:/nutch/search/lib/lucene-core-3.0.1.jar:/nutch/search/lib/lucene-misc-3.0.1.jar:/nutch/search/lib/oro-2.0.8.jar:/nutch/search/lib/resolver.jar:/nutch/search/lib/serializer.jar:/nutch/search/lib/servlet-api-2.5-6.1.14.jar:/nutch/search/lib/slf4j-api-1.5.5.jar:/nutch/search/lib/slf4j-log4j12-1.4.3.jar:/nutch/search/lib/taglibs-i18n.jar:/nutch/search/lib/tika-core-0.7.jar:/nutch/search/lib/wstx-asl-3.2.7.jar:/nutch/search/lib/xercesImpl.jar:/nutch/search/lib/xml-apis.jar:/nutch/search/lib/xmlenc-0.52.jar:/nutch/search/lib/jsp-2.1/jsp-2.1.jar:/nutch/search/lib/jsp-2.1/jsp-api-2.1.jar org.apache.hadoop.mapred.JobTracker\n</code></pre>\n\n<p>I've been googling and trying different things for about 8 hours now, and I'm just absolutely stuck as to what might be wrong. I'm sure it's something painfully obvious that I'm overlooking.  Does anyone have any idea?</p>\n\n<p>A few more details: this is a three node cluster on EC2, I can ssh w/out a password between each, and the nodes seem to be communicating w/out issue (ie no exceptions in logs).  They are all ubuntu 10.04 server.  Hadoop 0.20.2.</p>\n\n<p>Thanks in advance.</p>\n", "creation_date": 1289251054, "score": 5},
{"title": "Nutch tips how to test it", "view_count": 87, "is_answered": false, "answers": [{"question_id": 21270031, "owner": {"user_id": 2623052, "accept_rate": 45, "link": "http://stackoverflow.com/users/2623052/allan-macmillan", "user_type": "registered", "reputation": 866}, "body": "<p>It shouldnt take ages for a thousand records in the database, have you enabled multithreading?</p>\n", "creation_date": 1390344638, "is_accepted": false, "score": 0, "last_activity_date": 1390344638, "answer_id": 21270732}], "question_id": 21270031, "tags": ["testing", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/21270031/nutch-tips-how-to-test-it", "last_activity_date": 1390344638, "owner": {"user_id": 1337352, "answer_count": 21, "creation_date": 1334613000, "accept_rate": 32, "view_count": 155, "location": "Prague, Czech Republic", "reputation": 623}, "body": "<p>For my testing purpose I want to make small fetch-generate-parse-index cycle. Now I have about thousands records in my database and when I run index it starts indexing many many records which takes hours.\nDo you have any tip how to test this use case? Is there any way to generate limited number of pages generated in one batch?</p>\n\n<p>Well I generally appreciate for any advice on testing Nutch 2.2.1 with Hbase and Solr 4.</p>\n", "creation_date": 1390341907, "score": 1},
{"title": "Nutch Unknown Filter and Normalization", "view_count": 914, "owner": {"age": 27, "answer_count": 159, "creation_date": 1357505010, "user_id": 1953475, "accept_rate": 72, "view_count": 540, "location": "Denver, CO", "reputation": 5595}, "is_answered": true, "answers": [{"question_id": 20670454, "owner": {"user_id": 582789, "link": "http://stackoverflow.com/users/582789/alfonso-nishikawa", "user_type": "registered", "reputation": 1263}, "body": "<p><a href=\"http://svn.apache.org/repos/asf/nutch/tags/release-2.2.1/conf/\" rel=\"nofollow\">http://svn.apache.org/repos/asf/nutch/tags/release-2.2.1/conf/</a></p>\n\n<p>The file is <code>regex-urlfilter.txt</code> (make a copy of the template).\nTake a look at the regular expressions there. Specifically the line:</p>\n\n<pre><code># skip URLs containing certain characters as probable queries, etc.\n-[?*!@=]\n</code></pre>\n\n<p>that surely is filtering out your urls considering them \"queries, etc\".</p>\n", "creation_date": 1388677909, "is_accepted": true, "score": 2, "last_activity_date": 1388677909, "answer_id": 20886569}], "question_id": 20670454, "tags": ["regex", "web-scraping", "web-crawler", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/20670454/nutch-unknown-filter-and-normalization", "last_activity_date": 1390176865, "accepted_answer_id": 20886569, "body": "<p>I have a list of URLs that I want to test scraping using Nutch... specifically that list of URLs and no crawling..</p>\n\n<p>I was referring to this <a href=\"http://stackoverflow.com/questions/9154429/using-nutch-to-crawl-a-specified-url-list\">post</a> for disabling crawling..</p>\n\n<p>And I noticed that my 5 test URLs turned out to be 0 after normalization and filtering. </p>\n\n<pre><code>$:~/apache-nutch-1.7$ bin/nutch crawl urls -dir crawl -depth 3 -topN 1000\nsolrUrl is not set, indexing will be skipped...\ncrawl started in: crawl\nrootUrlDir = urls\nthreads = 10\ndepth = 3\nsolrUrl=null\ntopN = 1000\nInjector: starting at 2013-12-18 23:07:32\nInjector: crawlDb: crawl/crawldb\nInjector: urlDir: urls\nInjector: Converting injected urls to crawl db entries.\nInjector: total number of urls rejected by filters: 5\nInjector: total number of urls injected after normalization and filtering: 0\nInjector: Merging injected urls into crawl db.\nInjector: finished at 2013-12-18 23:07:39, elapsed: 00:00:06\nGenerator: starting at 2013-12-18 23:07:39\nGenerator: Selecting best-scoring urls due for fetch.\nGenerator: filtering: true\nGenerator: normalizing: true\nGenerator: topN: 1000\nGenerator: jobtracker is 'local', generating exactly one partition.\nGenerator: 0 records selected for fetching, exiting ...\nStopping at depth=0 - no more URLs to fetch.\nNo URLs to fetch - check your seed list and URL filters.\ncrawl finished: crawl\n</code></pre>\n\n<p>And actually I leave the filter and normalization as default which I guess doesn't filter anything.. </p>\n\n<p>Can anyone help me understand what is going on?</p>\n\n<pre><code>Injector: total number of urls rejected by filters: 5\n</code></pre>\n\n<p>Can anyone tell me which configuration file should I change to remove the 'filters' mentioned in the line above</p>\n\n<p>Also my test URLs looks like this:</p>\n\n<pre><code>http://example.com/store/em?action=products&amp;cat=1&amp;catalogId=500201&amp;No=0\nhttp://example.com/store/em?action=products&amp;cat=1&amp;catalogId=500201&amp;No=25\nhttp://example.com/store/em?action=products&amp;cat=1&amp;catalogId=500201&amp;No=50\nhttp://example.com/store/em?action=products&amp;cat=1&amp;catalogId=500201&amp;No=75\nhttp://example.com/store/em?action=products&amp;cat=1&amp;catalogId=500201&amp;No=100\n</code></pre>\n", "creation_date": 1387408415, "score": 1},
{"title": "java.lang.NullPointerException ( nutch 2.2.1 and MySql as datastore)", "view_count": 637, "is_answered": false, "answers": [{"question_id": 21198202, "owner": {"user_id": 1646674, "accept_rate": 75, "link": "http://stackoverflow.com/users/1646674/jawad-abbassi", "user_type": "registered", "reputation": 123}, "body": "<p>I have never used nutch, but it seems a common error, a NPE launched at init means the UTF8 instance failed at creation.</p>\n\n<p>the reason is that the 'crawl' function is deprecated in Nutch2 in favor of a java file located in 'bin/crawl'</p>\n\n<p>just copy the file $NUTCH_HOME/src/bin/crawl to the deploy directory: $NUTCH_HOME/runtime/deploy/bin and then run the crawl commands, take a look here : </p>\n\n<p><a href=\"http://wiki.apache.org/nutch/NutchTutorial#A3.1_Using_the_Crawl_Command\" rel=\"nofollow\">http://wiki.apache.org/nutch/NutchTutorial#A3.1_Using_the_Crawl_Command</a></p>\n\n<p>Hope this helps.</p>\n", "creation_date": 1390006105, "is_accepted": false, "score": 0, "last_activity_date": 1390006105, "answer_id": 21198640}], "question_id": 21198202, "tags": ["java", "mysql", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/21198202/java-lang-nullpointerexception-nutch-2-2-1-and-mysql-as-datastore", "last_activity_date": 1390006105, "owner": {"user_id": 2464288, "view_count": 2, "answer_count": 0, "creation_date": 1370621875, "reputation": 1}, "body": "<p>I  am new in this area. \nI started with this tutorial:<a href=\"http://nlp.solutions.asia/?p=362#more-362\" rel=\"nofollow\">http://nlp.solutions.asia/?p=362#more-362</a>. WHen i first crawled this url: nutch.apache.org, i had sucess, but when i tried a different url, i had this exception in my hadoop.log.</p>\n\n<pre><code>**java.lang.NullPointerException\n    at org.apache.avro.util.Utf8.&lt;init&gt;(Utf8.java:37)\n    at org.apache.nutch.crawl.GeneratorReducer.setup(GeneratorReducer.java:100)\n    at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:174)\n    at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:649)\n    at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:418)\n    at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:398)**\n</code></pre>\n\n<hr>\n\n<p>this is my nutch-site.xml:</p>\n\n<pre><code>&lt;?xml version=\"1.0\"?&gt;\n&lt;?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?&gt;\n\n&lt;!-- Put site-specific property overrides in this file. --&gt;\n\n&lt;configuration&gt;\n&lt;property&gt;\n&lt;name&gt;http.agent.name&lt;/name&gt;\n&lt;value&gt;Maria&lt;/value&gt;\n&lt;/property&gt;\n\n&lt;property&gt; \n&lt;name&gt;http.robots.agents&lt;/name&gt; \n&lt;value&gt;Maria&lt;/value&gt; ....\n&lt;/description&gt; \n&lt;/property&gt;\n\n&lt;property&gt;\n&lt;name&gt;http.accept.language&lt;/name&gt;\n&lt;value&gt;ja-jp, en-us,en-gb,en;q=0.7,*;q=0.3&lt;/value&gt;\n&lt;description&gt;Value of the \u201cAccept-Language\u201d request header field.\nThis allows selecting non-English language as default one to retrieve.\nIt is a useful setting for search engines build for certain national group.\n&lt;/description&gt;\n&lt;/property&gt;\n\n&lt;property&gt;\n&lt;name&gt;parser.character.encoding.default&lt;/name&gt;\n&lt;value&gt;utf-8&lt;/value&gt;\n&lt;description&gt;The character encoding to fall back to when no other information\nis available&lt;/description&gt;\n&lt;/property&gt;\n\n&lt;property&gt;\n&lt;name&gt;storage.data.store.class&lt;/name&gt;\n&lt;value&gt;org.apache.gora.sql.store.SqlStore&lt;/value&gt;\n&lt;description&gt;The Gora DataStore class for storing and retrieving data.\nCurrently the following stores are available: \u2026.\n&lt;/description&gt;\n&lt;/property&gt;\n\n&lt;/configuration&gt;\n</code></pre>\n\n<hr>\n\n<p>And this is the regex-ulrfilter.txt:</p>\n\n<pre><code># Licensed to the Apache Software Foundation (ASF) under one or more\n# contributor license agreements.  See the NOTICE file distributed with\n# this work for additional information regarding copyright ownership.\n# The ASF licenses this file to You under the Apache License, Version 2.0\n# (the \"License\"); you may not use this file except in compliance with\n# the License.  You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\n# The default url filter.\n# Better for whole-internet crawling.\n\n# Each non-comment, non-blank line contains a regular expression\n# prefixed by '+' or '-'.  The first matching pattern in the file\n# determines whether a URL is included or ignored.  If no pattern\n# matches, the URL is ignored.\n\n# skip file: ftp: and mailto: urls\n-^(file|ftp|mailto):\n\n# skip image and other suffixes we can't yet parse\n# for a more extensive coverage use the urlfilter-suffix plugin\n-\\.          \n(gif|GIF|jpg|JPG|png|PNG|ico|ICO|css|CSS|sit|SIT|eps|EPS|wmf|WMF|zip\n|ZIP|ppt|PPT|mpg|MPG|xls|XLS|gz|GZ|rpm|RPM|tgz|TGZ|mov\n|MOV|exe|EXE|jpeg|JPEG|bmp|BMP|js|JS)$\n\n# skip URLs containing certain characters as probable queries, etc.\n-[?*!@=]\n\n# skip URLs with slash-delimited segment that repeats 3+ times, to break loops\n-.*(/[^/]+)/[^/]+\\1/[^/]+\\1/\n\n# accept anything else\n#+.\n\n+^http://([a-z0-9]*\\.)* nutch.apache.org/\n\n#\n-.\n</code></pre>\n\n<hr>\n\n<p>I would appreciate any suggestion to solve this problem </p>\n", "creation_date": 1390003220, "score": 0},
{"title": "How does Nutch work with Hadoop cluster?", "view_count": 1117, "owner": {"age": 27, "answer_count": 19, "creation_date": 1331644819, "user_id": 1266556, "accept_rate": 75, "view_count": 41, "location": "Shanghai, China", "reputation": 362}, "is_answered": true, "answers": [{"question_id": 10360759, "owner": {"user_id": 1150329, "accept_rate": 89, "link": "http://stackoverflow.com/users/1150329/tejas-patil", "user_type": "registered", "reputation": 4793}, "body": "<p>The phases of nutch are :\nInject -> generate -> Fetch -> Parse -> Update -> Index</p>\n\n<p>Of these Fetch phase is the place where nutch sends request for the urls (and hence i will be talking only about this phase and generate phase in answer.)</p>\n\n<p>Generate phase creates fetch list of the urls in the crawldb. While creation of fetchlist, the urls belonging to the same host typically fall in the same partition as the partitioning function is based on the hostname. So, the final fetch list will look like this:</p>\n\n<pre><code>fetch list 1 : all urls of host a1, b1, c1\nfetch list 2 : all urls of host a2, b2, c2\n.............\n.............\n</code></pre>\n\n<p>Now, when Fetch phase reads these fetchlists, <strong>each fetchlist is processed by /assigned to a single mapper of fetch phase</strong>. So, </p>\n\n<pre><code>number of reducers in generate partition phase \n                        = the number of fetchlists created\n                        = number of maps in fetch phase\n</code></pre>\n\n<p><strong>If a mapper in fetch phase gets the bunch of urls of host A, no other map will have urls of the same host.</strong> Offcourse, each map can have urls of multiple hosts but no other mapper will have urls from those hosts.</p>\n\n<p>Now digging deep into mapper of fetch:</p>\n\n<p>It will have urls of say n hosts h1, h2,... hn. Then fetchqueues are formed per host basis. All urls (fetch items) are populated in the fetchqueue of their respective hosts. Fetcher threads polls on the fetchqueues, pick up urls from there and send the request and write back the results to hdfs. After this is done, they look out for other fetchitems(urls) which can be processed.</p>\n\n<p>I think that i could manage to put in the mess in understandable way. For more details see the <a href=\"http://www.docjar.com/html/api/org/apache/nutch/fetcher/Fetcher.java.html\">Fetcher.java</a> code for the working. </p>\n\n<p>Note: The urls can be grouped on basis of IP too. Even u can tweak to make nutch not to group urls based on hostname/IP. Both these things depend on yr configurations. By default it will use hostname for grouping urls.</p>\n", "creation_date": 1335716654, "is_accepted": true, "score": 5, "last_activity_date": 1335716654, "answer_id": 10373759}], "question_id": 10360759, "tags": ["hadoop", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/10360759/how-does-nutch-work-with-hadoop-cluster", "last_activity_date": 1389507759, "accepted_answer_id": 10373759, "body": "<p>all\n<br> I'm wondering how nutch works with hadoop cluster. How does it split a job to the other nodes? How does it assure that different nodes in the cluster won't request the same url?\n<br>Thanks in adv.</p>\n", "creation_date": 1335589487, "score": 3},
{"title": "Error while indexing nutch data to solr", "view_count": 435, "is_answered": false, "answers": [{"last_edit_date": 1389232089, "owner": {"user_id": 2146864, "accept_rate": 60, "link": "http://stackoverflow.com/users/2146864/billni", "user_type": "registered", "reputation": 82}, "body": "<p>Did you see solr log?  Those logs record the error reason.\nI ever had same problem in nutch  and found a message \"unknown field host\" in solr's log.\nAfter I edited the scheme.xml,  the problem vanished.</p>\n", "question_id": 19723259, "creation_date": 1389174529, "is_accepted": false, "score": 0, "last_activity_date": 1389232089, "answer_id": 20992071}], "question_id": 19723259, "tags": ["linux", "solr", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/19723259/error-while-indexing-nutch-data-to-solr", "last_activity_date": 1389232089, "owner": {"user_id": 2919794, "view_count": 1, "answer_count": 0, "creation_date": 1382701288, "reputation": 1}, "body": "<p>I'm trying to index my crawl data from nutch into solr and I recieve the following error. Any help would be greatly appreciated.</p>\n\n<pre><code>SOLRIndexWriter\nsolr.server.url : URL of the SOLR instance (mandatory)\nsolr.commit.size : buffer size when sending to SOLR (default 1000)\nsolr.mapping.file : name of the mapping file for fields (default solrindex-mapping.xml)\nsolr.auth : use authentication (default false)\nsolr.auth.username : use authentication (default false)\nsolr.auth : username for authentication\nsolr.auth.password : password for authentication\n\n\nException in thread \"main\" java.io.IOException: Job failed!\nat org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:1357)\nat org.apache.nutch.indexer.IndexingJob.index(IndexingJob.java:123)\nat org.apache.nutch.indexer.IndexingJob.index(IndexingJob.java:81)\nat org.apache.nutch.indexer.IndexingJob.index(IndexingJob.java:65)\nat org.apache.nutch.crawl.Crawl.run(Crawl.java:155)\nat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\nat org.apache.nutch.crawl.Crawl.main(Crawl.java:55)\n</code></pre>\n", "creation_date": 1383294713, "score": 0},
{"title": "Persuading Apache Nutch to commit more frequently to Solr", "view_count": 189, "owner": {"user_id": 180416, "answer_count": 74, "creation_date": 1254147585, "accept_rate": 69, "view_count": 347, "reputation": 8216}, "is_answered": true, "answers": [{"question_id": 20924845, "owner": {"user_id": 1881318, "link": "http://stackoverflow.com/users/1881318/tahagh", "user_type": "registered", "reputation": 619}, "body": "<p>There is a configuration parameter in nutch named \"solr.commit.size\" which according to the description in nutch-default.xml is:</p>\n\n<blockquote>\n  <p>Defines the number of documents to send to Solr in a single update batch.\n    Decrease when handling very large documents to prevent Nutch from running\n    out of memory. NOTE: It does not explicitly trigger a server side commit.</p>\n</blockquote>\n\n<p>As it said, it does not explicitly commit, because it is more optimized to left the decision of commit times to solr. So you should also tune your solr configuration parameters: autoCommit and autoSoftCommit. You can find their descriptions in solrconfig.xml file.</p>\n", "creation_date": 1388905537, "is_accepted": true, "score": 2, "last_activity_date": 1388905537, "answer_id": 20931050}], "question_id": 20924845, "tags": ["solr", "lucene", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/20924845/persuading-apache-nutch-to-commit-more-frequently-to-solr", "last_activity_date": 1388905537, "accepted_answer_id": 20931050, "body": "<p>I'm running Apache Nutch, which seems to work and in small runs will index documents and commit to Solr at the end of the run.</p>\n\n<p>Unfortunately, I want to index deep within some large sites and Nutch won't commit to the end of a run.</p>\n\n<p>This has obvious issues when you're looking at 100k+ documents being stacked up waiting to commit with pressure on memory, having to wait so long for the data, etc.</p>\n\n<p>Is there a way to persuade Nutch to commit more frequently?</p>\n", "creation_date": 1388859622, "score": 0},
{"title": "How to produce massive amount of data?", "view_count": 734, "owner": {"user_id": 726706, "answer_count": 7, "creation_date": 1303887958, "accept_rate": 68, "view_count": 92, "reputation": 1044}, "is_answered": true, "answers": [{"question_id": 8668175, "owner": {"user_id": 57695, "accept_rate": 83, "link": "http://stackoverflow.com/users/57695/peter-lawrey", "user_type": "registered", "reputation": 353449}, "body": "<p>I would write a simple program to do it.  The program doesn't need to be too clear as the speed of writing to disk is likely to be your bottle neck.</p>\n", "creation_date": 1325163747, "is_accepted": false, "score": 0, "last_activity_date": 1325163747, "answer_id": 8668219}, {"question_id": 8668175, "owner": {"user_id": 773113, "accept_rate": 79, "link": "http://stackoverflow.com/users/773113/mike-nakis", "user_type": "registered", "reputation": 20510}, "body": "<p>Download the <a href=\"http://hjsplit.en.softonic.com/\" rel=\"nofollow\">HJSPLIT</a> utility or any other which splits and merges files and use it to concatenate multiple copies of your favorite movie to whatever length you like.</p>\n\n<p><em>\"No knowledge of programming needed to operate\".</em></p>\n", "creation_date": 1325163789, "is_accepted": false, "score": 0, "last_activity_date": 1325163789, "answer_id": 8668225}, {"question_id": 8668175, "owner": {"user_id": 830666, "link": "http://stackoverflow.com/users/830666/jrennie", "user_type": "registered", "reputation": 1404}, "body": "<p>If you only need to avoid exact duplicates, you could try a combination of your two ideas---create corrupted copies of a relatively small data set.  \"Corruption\" operations might include: replacement, insertion, deletion, and character swapping.</p>\n", "creation_date": 1325164244, "is_accepted": false, "score": 1, "last_activity_date": 1325164244, "answer_id": 8668304}, {"question_id": 8668175, "owner": {"user_id": 813951, "accept_rate": 76, "link": "http://stackoverflow.com/users/813951/mister-smith", "user_type": "registered", "reputation": 12985}, "body": "<p>Just about the long time comment: I've recently extended a disk partition and I know well how long can it take to move or create a great number of files. It would be much faster to request the OS a range of free space on disk, and then create a new entry in the FAT for that range, without writing a single bit of content (reusing the previously existing information). This would serve your purpose (since you don't care about file content) and would be as fast as deleting a file. </p>\n\n<p>The problem is that this might be difficult to achieve in Java. I've found an open source library, named <a href=\"http://code.google.com/p/fat32-lib/\" rel=\"nofollow\">fat32-lib</a>, but since it doesn't resort to native code I don't think it is useful here. For a given filesystem, and using a lower level language (like C), if you have the time and motivation I think it would be achievable.</p>\n", "creation_date": 1325166765, "is_accepted": false, "score": 0, "last_activity_date": 1325166765, "answer_id": 8668689}, {"question_id": 8668175, "owner": {"user_id": 805808, "accept_rate": 83, "link": "http://stackoverflow.com/users/805808/iterator", "user_type": "registered", "reputation": 12377}, "body": "<p>This may be a better question for the statistics StackExchange site (see, for instance, <a href=\"http://stats.stackexchange.com/questions/17039/what-are-some-standard-practices-for-creating-synthetic-data-sets\">my question on best practices for generating synthetic data</a>).</p>\n\n<p>However, if you're not so interested in the data properties as the infrastructure to manipulate and work with the data, then you can ignore the statistics site.  In particular, if you are not focused on statistical aspects of the data, and merely want \"big data\", then we can focus on how one can generate a large pile of data.</p>\n\n<p>I can offer several answers:</p>\n\n<ol>\n<li><p>If you are just interested in random numeric data, generate a large stream from your favorite implementation of the Mersenne Twister.  There is also /dev/random (see <a href=\"http://en.wikipedia.org/wiki//dev/random\">this Wikipedia entry for more info</a>).  I prefer a known random number generator, as the results can be reproduced ad nauseam by anyone else.</p></li>\n<li><p>For structured data, you can look at mapping random numbers to indices and create a table that maps indices to, say, strings, numbers, etc., such as one might encounter in producing a database of names, addresses, etc.  If you have a large enough table or a sufficiently rich mapping target, you can reduce the risk of collisions (e.g. same names), though perhaps you'd like to have a few collisions, as these occur in reality, too.</p></li>\n<li><p>Keep in mind that with any generative method you need not store the entire data set before beginning your work.  As long as you record the state (e.g. of the RNG), you can pick up where you left off.</p></li>\n<li><p>For text data, you can look at simple random string generators.  You might create your own estimates for the probability of strings of different lengths or different characteristics.  The same can go for sentences, paragraphs, documents, etc. - just decide what properties you'd like to emulate, create a \"blank\" object, and fill it with text.</p></li>\n</ol>\n", "creation_date": 1325348538, "is_accepted": true, "score": 7, "last_activity_date": 1325348538, "answer_id": 8689171}, {"question_id": 8668175, "owner": {"user_id": 568695, "accept_rate": 100, "link": "http://stackoverflow.com/users/568695/rmetzger", "user_type": "registered", "reputation": 2148}, "body": "<p>Have a look at <a href=\"http://www.tpc.org\" rel=\"nofollow\">TPC.org</a>, they have different Database Benchmarks with data generators and predefined queries.</p>\n\n<p>The generators have a scale-factor which allows to define the target data size.</p>\n\n<p>There is also the <a href=\"https://github.com/TU-Berlin-DIMA/myriad-toolkit\" rel=\"nofollow\">myriad research project</a> (<a href=\"http://vldb.org/pvldb/vol5/p1890_alexanderalexandrov_vldb2012.pdf\" rel=\"nofollow\">paper</a>) that focuses on distributed \"big data\" data generation. Myriad has a steep learning curve, so you might have to ask the authors of the software for help.</p>\n", "creation_date": 1388746665, "is_accepted": false, "score": 0, "last_activity_date": 1388746665, "answer_id": 20902024}], "question_id": 8668175, "tags": ["java", "hadoop", "nutch", "bigdata"], "answer_count": 6, "link": "http://stackoverflow.com/questions/8668175/how-to-produce-massive-amount-of-data", "last_activity_date": 1388746665, "accepted_answer_id": 8689171, "body": "<p>I'm doing some testing with nutch and hadoop and I need a massive amount of data.\nI want to start with 20GB, go to 100 GB, 500 GB and eventually reach 1-2 TB.</p>\n\n<p>The problem is that I don't have this amount of data, so I'm thinking of ways to produce it.</p>\n\n<p>The data itself can be of any kind.\nOne idea is to take an initial set of data and duplicate it. But its not good enough because need files that are different from one another (Identical files are ignored).</p>\n\n<p>Another idea is to write a program that will create files with dummy data.</p>\n\n<p>Any other idea?</p>\n", "creation_date": 1325163542, "score": 8},
{"title": "Null pointer exception when running Nutch on Hadoop with Cassandra", "view_count": 304, "is_answered": false, "question_id": 20892592, "tags": ["hadoop", "cassandra", "nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/20892592/null-pointer-exception-when-running-nutch-on-hadoop-with-cassandra", "last_activity_date": 1388699423, "owner": {"age": 22, "answer_count": 0, "creation_date": 1387817585, "user_id": 3130223, "view_count": 12, "location": "Chennai, India", "reputation": 12}, "body": "<p>I'm running Nutch on a Hadoop cluster and the crawled data is stored in a Cassandra cluster. When running the Nutch job, I get the following error:</p>\n\n<pre><code>java.lang.NullPointerException\n    at org.apache.avro.util.Utf8.&lt;init&gt;(Utf8.java:38)\n    at org.apache.nutch.crawl.GeneratorReducer.setup(GeneratorReducer.java:100)\n    at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:174)\n    at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:649)\n    at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:418)\n    at org.apache.hadoop.mapred.Child$4.run(Child.java:255)\n    at java.security.AccessController.doPrivileged(Native Method)\n    at javax.security.auth.Subject.doAs(Subject.java:396)\n    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)\n    at org.apache.hadoop.mapred.Child.main(Child.java:249)\n</code></pre>\n\n<p>I start the Nutch job like this:</p>\n\n<pre><code>$HADOOP_HOME/bin/hadoop jar /nutch/apache-nutch-2.2.1.job org.apache.nutch.crawl.Crawl urls -dir crawl -depth 3 -topN 5\n</code></pre>\n", "creation_date": 1388699423, "score": 0},
{"title": "get out links from nutch", "view_count": 5374, "owner": {"user_id": 191300, "answer_count": 70, "creation_date": 1255709181, "accept_rate": 55, "view_count": 102, "reputation": 2168}, "is_answered": true, "answers": [{"last_edit_date": 1316537374, "owner": {"user_id": 191300, "accept_rate": 55, "link": "http://stackoverflow.com/users/191300/surajz", "user_type": "registered", "reputation": 2168}, "body": "<p>From command line, you can see the outlinks by using <a href=\"http://wiki.apache.org/nutch/bin/nutch_readseg\">readseg</a> with -dump or -get option. For example,</p>\n\n<pre><code>bin/nutch readseg -dump crawl/segments/20110919084424/ outputdir2 -nocontent -nofetch - nogenerate -noparse -noparsetext\n\nless outputdir2/dump\n</code></pre>\n", "question_id": 7425136, "creation_date": 1316536816, "is_accepted": true, "score": 6, "last_activity_date": 1316537374, "answer_id": 7488648}, {"question_id": 7425136, "owner": {"user_id": 215336, "accept_rate": 22, "link": "http://stackoverflow.com/users/215336/sriwantha-attanayake", "user_type": "registered", "reputation": 4043}, "body": "<p>You can easily do this with <strong>readlinkdb</strong> command. It gives you all the inlinks and outlinks to and from a url. </p>\n\n<pre><code>bin/nutch readlinkdb &lt;linkdb&gt; (-dump &lt;out_dir&gt; | -url &lt;url&gt;)\n</code></pre>\n\n<p>linkdb: This is the linkdb directory we wish to read and obtain information from.</p>\n\n<p>out_dir: This parameter dumps the whole linkdb to a text file in any out_dir we wish to specify.</p>\n\n<p>url: The -url arguement provides us with information about a specific url. This is written to System.out.</p>\n\n<pre><code>e.g. \n\nbin/nutch readlinkdb crawl/linkdb -dump myoutput/out1\n</code></pre>\n\n<p>For more information refer to \n<a href=\"http://wiki.apache.org/nutch/bin/nutch%20readlinkdb\" rel=\"nofollow\">http://wiki.apache.org/nutch/bin/nutch%20readlinkdb</a></p>\n", "creation_date": 1388060709, "is_accepted": false, "score": 1, "last_activity_date": 1388060709, "answer_id": 20784822}], "question_id": 7425136, "tags": ["web-crawler", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/7425136/get-out-links-from-nutch", "last_activity_date": 1388654480, "accepted_answer_id": 7488648, "body": "<p>I am using nutch 1.3 to crawl a website. I want to get a list of urls crawled, and urls originating  from a page.</p>\n\n<p>I get list of urls crawled using readdb command.</p>\n\n<pre><code>bin/nutch readdb crawl/crawldb -dump file\n</code></pre>\n\n<p>Is there a way to find out urls that are on a page by reading crawldb or linkdb ?</p>\n\n<p>in the <code>org.apache.nutch.parse.html.HtmlParser</code> I see outlinks array, I am wondering if there is a quick way to access it from command line.</p>\n", "creation_date": 1316052817, "score": 8},
{"title": "How to configure hadoop mapper num per node(machine)", "view_count": 374, "is_answered": false, "answers": [{"question_id": 20774410, "owner": {"user_id": 248224, "accept_rate": 50, "link": "http://stackoverflow.com/users/248224/rohit-menon", "user_type": "registered", "reputation": 190}, "body": "<p>Ideally you should be setting the mapred.tasktracker.map.tasks.maximum value to the number of cores present on the node of the TaskTracker. So assuming you have 4 cores on each of the nodes and the job spawns 10 map tasks, the max number of map tasks that can run on the TaskTracker will be 4.</p>\n", "creation_date": 1388001507, "is_accepted": false, "score": 0, "last_activity_date": 1388001507, "answer_id": 20776291}], "question_id": 20774410, "tags": ["hadoop", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/20774410/how-to-configure-hadoop-mapper-num-per-nodemachine", "last_activity_date": 1388022944, "owner": {"user_id": 1947579, "view_count": 4, "answer_count": 0, "creation_date": 1357277672, "reputation": 11}, "body": "<p>I have a problem</p>\n\n<p>My problem is I have a job(fetch crawler) that configurate <strong>mapred.map.tasks</strong> to 10,that means my job will create 10 mapper for once .but my cluster configurate <strong>mapred.tasktracker.map.tasks.maximum</strong> to 12</p>\n\n<p>then all mapper will created in one single machine every time</p>\n\n<p>How can I distribute all mapper to all machine homogeneous!</p>\n\n<p>thanks very much</p>\n\n<hr>\n\n<p>It is kind of like something mapper.per.node do ,but I hear some guy said that configuration was deleted.\n1 when mapper.per.node was deleted?\n2 How can i do now?? </p>\n", "creation_date": 1387986215, "score": 0},
{"title": "no segments* file found", "view_count": 16171, "owner": {"age": 28, "answer_count": 3, "creation_date": 1270071082, "user_id": 443380, "accept_rate": 63, "view_count": 138, "location": "Vancouver, Canada", "reputation": 878}, "is_answered": true, "answers": [{"question_id": 3802021, "owner": {"user_id": 1702, "accept_rate": 100, "link": "http://stackoverflow.com/users/1702/yuval-f", "user_type": "registered", "reputation": 17594}, "body": "<p>Basically, the error message says that Lucene did not find the proper files in the index directory. I suggest checking the following:</p>\n\n<ol>\n<li>Verify the path of the index directory fits what you think it should be.</li>\n<li>Do the Nutch and Lucene versions used match? This may stem from a version difference.</li>\n<li>Is there a permissions issue? Can you read the files in the directory?</li>\n<li>Try looking at the index using <a href=\"http://code.google.com/p/luke/\">Luke</a>. If you cannot, there is probably some corruption in the index.</li>\n</ol>\n\n<p>If all these do not help, Please post the indexing part of the code.</p>\n", "creation_date": 1285579587, "is_accepted": true, "score": 5, "last_activity_date": 1285579587, "answer_id": 3802497}, {"question_id": 3802021, "owner": {"user_id": 1292104, "accept_rate": 53, "link": "http://stackoverflow.com/users/1292104/nir", "user_type": "registered", "reputation": 672}, "body": "<p>Another hint, as I was having the same error and found that after creating indexes I did not close IndexWriter and it proved very unforgiven. In my indexdirectory I have some .lock files and no segments or segments.gen files which is what Reader is looking for.\nSee <a href=\"http://wilsonericn.wordpress.com/2011/12/14/my-first-5-lucene-mistakes/\">here</a> #3 for details</p>\n", "creation_date": 1387916047, "is_accepted": false, "score": 8, "last_activity_date": 1387916047, "answer_id": 20766322}], "question_id": 3802021, "tags": ["java", "lucene", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/3802021/no-segments-file-found", "last_activity_date": 1387916047, "accepted_answer_id": 3802497, "body": "<p>I need to access a lucene index ( created by crawling several webpages using Nutch) but it is giving the error shown above : </p>\n\n<pre><code>java.io.FileNotFoundException: no segments* file found in org.apache.lucene.store.FSDirectory@/home/&lt;path&gt;: files:\n    at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:516)\n    at org.apache.lucene.index.IndexReader.open(IndexReader.java:185)\n    at org.apache.lucene.index.IndexReader.open(IndexReader.java:148)\n    at DictionaryGenerator.generateDict(DictionaryGenerator.java:24)\n    at DictionaryGenerator.main(DictionaryGenerator.java:56)\n</code></pre>\n\n<p>I googled but the reasons given were not matching the requirements. \nThe fact that files are being shown ( the path) probably means that the directory is not empty.<br>\nThanks </p>\n", "creation_date": 1285574793, "score": 7},
{"title": "nutch parse custom xml with tika using xpath", "view_count": 997, "owner": {"user_id": 381009, "answer_count": 40, "creation_date": 1277982089, "accept_rate": 67, "view_count": 78, "location": "Milton Keynes, United Kingdom", "reputation": 593}, "is_answered": true, "answers": [{"last_edit_date": 1387696290, "owner": {"user_id": 1881318, "link": "http://stackoverflow.com/users/1881318/tahagh", "user_type": "registered", "reputation": 619}, "body": "<p>I don't think you can easily do this with tika but you may use these custom plugins to parse xml files based on xpath:</p>\n\n<ul>\n<li><a href=\"https://github.com/BayanGroup/nutch-custom-search\" rel=\"nofollow\">https://github.com/BayanGroup/nutch-custom-search</a></li>\n<li><a href=\"http://www.atlantbh.com/precise-data-extraction-with-apache-nutch/\" rel=\"nofollow\">http://www.atlantbh.com/precise-data-extraction-with-apache-nutch/</a></li>\n</ul>\n", "question_id": 20704068, "creation_date": 1387625680, "is_accepted": true, "score": 0, "last_activity_date": 1387696290, "answer_id": 20718541}], "question_id": 20704068, "tags": ["nutch", "apache-tika"], "answer_count": 1, "link": "http://stackoverflow.com/questions/20704068/nutch-parse-custom-xml-with-tika-using-xpath", "last_activity_date": 1387696290, "accepted_answer_id": 20718541, "body": "<p>I am new to nutch.\nnutch 1.7\nI am looking at ways to parse custom xml files based on xpath and store the data.  I did see the xml_parser plugin but that's suspended since tika has taken over.\nHow do I configure tika embedded within nutch 1.7 to parse the url content based on xpath.  I have searched all nutch documentation/wiki but there's not much information there. \ntika tries to parse and extract the content which fails because of the custom format, but I want to store the xml with tags based on the xpath. Where should I put the xpath info in the nutch conf? Or do I have to override the tike parser?</p>\n\n<p>Any hints on the right direction much appreciated.</p>\n\n<p>thanks.</p>\n", "creation_date": 1387544032, "score": 1},
{"title": "Nutch 2.2.1 PDF parse", "view_count": 366, "is_answered": false, "question_id": 20702909, "tags": ["java", "parsing", "pdf", "hadoop", "nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/20702909/nutch-2-2-1-pdf-parse", "last_activity_date": 1387540147, "owner": {"user_id": 3122625, "view_count": 10, "answer_count": 5, "creation_date": 1387539114, "reputation": 48}, "body": "<p>Been trying to parse PDF documents with Nutch and Tika (PDFBox 1.8.3).</p>\n\n<p>Out of the 5 PDF's I have attempted to parse using:</p>\n\n<pre><code>danny@Ubuntu-64:~/Nutch$ ./bin/nutch parsechecker file:///home/danny/Documents/DOC-443.pdf\n</code></pre>\n\n<p>The only output I get is:</p>\n\n<pre><code>fetching: file:///home/danny/Documents/DOC-443.pdf\nparsing: file:///home/danny/Documents/DOC-443.pdf\ncontentType: application/pdf\nsignature: 662453bc32a42af13cb4d5844d978cfc\n---------\nUrl\n---------------\nfile:///home/danny/Documents/DOC-443.pdf\n---------\nMetadata\n---------\nxmpTPg:NPages :     0\nContent-Type :  application/pdf\n</code></pre>\n\n<p>My hadoop.log is:</p>\n\n<pre><code>2013-12-20 11:29:41,646 INFO  parse.ParserChecker - fetching: file:///home/danny/Documents/DOC-443.pdf\n2013-12-20 11:29:42,174 INFO  crawl.SignatureFactory - Using Signature impl: org.apache.nutch.crawl.MD5Signature\n2013-12-20 11:29:42,209 INFO  parse.ParserFactory - The parsing plugins: [org.apache.nutch.parse.tika.TikaParser] are enabled via the plugin.includes system property, and all claim to support the content type application/pdf, but they are not mapped to it  in the parse-plugins.xml file\n2013-12-20 11:29:42,518 WARN  pdfparser.PDFParser - Parsing Error, Skipping Object\njava.io.IOException: expected='endstream' actual='' org.apache.pdfbox.io.PushBackInputStream@2d4b2312\n    at org.apache.pdfbox.pdfparser.BaseParser.parseCOSStream(BaseParser.java:604)\n    at org.apache.pdfbox.pdfparser.PDFParser.parseObject(PDFParser.java:605)\n    at org.apache.pdfbox.pdfparser.PDFParser.parse(PDFParser.java:194)\n    at org.apache.pdfbox.pdmodel.PDDocument.load(PDDocument.java:1224)\n    at org.apache.pdfbox.pdmodel.PDDocument.load(PDDocument.java:1189)\n    at org.apache.tika.parser.pdf.PDFParser.parse(PDFParser.java:123)\n    at org.apache.nutch.parse.tika.TikaParser.getParse(TikaParser.java:116)\n    at org.apache.nutch.parse.ParseCallable.call(ParseCallable.java:36)\n    at org.apache.nutch.parse.ParseCallable.call(ParseCallable.java:23)\n    at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)\n    at java.util.concurrent.FutureTask.run(FutureTask.java:166)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n    at java.lang.Thread.run(Thread.java:724)\n2013-12-20 11:29:42,521 WARN  pdfparser.XrefTrailerResolver - Did not found XRef object at specified startxref position 0\n2013-12-20 11:29:42,611 INFO  parse.ParserChecker - parsing: file:///home/danny/Documents/DOC-443.pdf\n2013-12-20 11:29:42,611 INFO  parse.ParserChecker - contentType: application/pdf\n2013-12-20 11:29:42,611 INFO  parse.ParserChecker - signature: 662453bc32a42af13cb4d5844d978cfc\n2013-12-20 11:29:42,611 INFO  parse.ParserChecker - ---------\nUrl\n---------------\n2013-12-20 11:29:42,612 INFO  parse.ParserChecker - ---------\nMetadata\n---------\n</code></pre>\n\n<p>Can anyone figure out what's wrong? Been attempting to figure this out for the last two days. Upgraded/downgraded PDFBox, rebuilt Nutch etc. Nothing seems to fix this issue?</p>\n", "creation_date": 1387539811, "score": 1},
{"title": "Nutch Domain Regular Expression", "view_count": 1107, "owner": {"age": 27, "answer_count": 159, "creation_date": 1357505010, "user_id": 1953475, "accept_rate": 72, "view_count": 540, "location": "Denver, CO", "reputation": 5595}, "is_answered": true, "answers": [{"question_id": 20640649, "owner": {"user_id": 1881318, "link": "http://stackoverflow.com/users/1881318/tahagh", "user_type": "registered", "reputation": 619}, "body": "<blockquote>\n  <p>(1) Can I just modify conf/regex-urlfilter.txt and replace </p>\n</blockquote>\n\n<p>Sure. You should replace +. with these lines:</p>\n\n<pre><code>#accept all products page\n+www\\.example\\.com/allproducts\n\n#accept categories pages\n+www\\.example\\.com/level1/level2/_/N-\n</code></pre>\n\n<p>One important note about regex in this file: the regular expressions are partially match. So if you write a rule like \"+ab\" it means: accept all urls that contain \"ab\" so it matches with these urls </p>\n\n<ul>\n<li>ab</li>\n<li>abc</li>\n<li><a href=\"http://ab.com/c.html\" rel=\"nofollow\">http://ab.com/c.html</a></li>\n</ul>\n\n<p>By default, nutch filter urls with ? (since mostly they are dynamic pages). To prevent this, comment this line in you regex-urlfilter.txt file:</p>\n\n<pre><code>-[?*!@=]\n</code></pre>\n\n<blockquote>\n  <p>(2) I can see the HTML ...</p>\n</blockquote>\n\n<p>Nutch saves the files in binary format. See <a href=\"http://stackoverflow.com/a/10150402/1881318\">http://stackoverflow.com/a/10150402/1881318</a></p>\n", "creation_date": 1387367538, "is_accepted": true, "score": 2, "last_activity_date": 1387367538, "answer_id": 20657451}], "question_id": 20640649, "tags": ["regex", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/20640649/nutch-domain-regular-expression", "last_activity_date": 1387367538, "accepted_answer_id": 20657451, "body": "<p>I am following the tutorial <a href=\"http://wiki.apache.org/nutch/NutchTutorial\" rel=\"nofollow\">here</a>, trying to build a robot against a website. </p>\n\n<p>I am in a page that contains all the product categories. Say it is <code>www.example.com/allproducts</code>. </p>\n\n<p>After diving into each category. You can see the product list in a table format and you can click the next page to loop through all the pages inside that category. Actually you can only see the 1,2,3,4,5, last page.</p>\n\n<p>The first page in the category has a URL looks like <code>www.example.com/level1/level2/_/N-1</code>, then the second page will looks like <code>www.example.com/level1/level2/_/N-1/?No=100</code> .. so on an so forth..</p>\n\n<p>I personally don't have that much JAVA programming experience and I am wondering </p>\n\n<p><em><strong>can I crawl the all the products list page using Nutch and store the HTML for now..</em></strong> </p>\n\n<p>and maybe later figure out a way to parse the html/index correctly. </p>\n\n<p>(1) Can I just modify <code>conf/regex-urlfilter.txt</code> and replace </p>\n\n<pre><code># accept anything else\n+. \n</code></pre>\n\n<p>with something correct? (I just don't understand how could </p>\n\n<pre><code>+^http://([a-z0-9]*\\.)*nutch.apache.org/\n</code></pre>\n\n<p>only restrict the URLs inside the Nutch domain..., I will interpret that regular expression to be between the double slash and nutch, there could be any characters that are alpha numeric or asterisk, backslash or dot..)</p>\n\n<p>How can I build the regular expression so it only scrape <code>http://www.example.com/.../.../_/N-../...</code></p>\n\n<p>(2) I can see the HTML is stored in the <code>content</code> folder inside <code>segment</code>... However, when I open that file in VI, it just totally looks like nonsense to me... and I am wondering if that is the so-called JAVA serialization which I need to deserialize in JAVA to read it. </p>\n\n<p>Forgive me if those questions are too basic and thanks a lot for reading.</p>\n", "creation_date": 1387300767, "score": 2},
{"title": "How to setup a case-insensitive regex in regex-urlfilter for Nutch 1.7", "view_count": 174, "owner": {"age": 36, "answer_count": 12, "creation_date": 1237810069, "user_id": 81386, "view_count": 56, "location": "Brazil", "reputation": 438}, "is_answered": true, "answers": [{"last_edit_date": 1387279101, "owner": {"user_id": 256196, "accept_rate": 78, "link": "http://stackoverflow.com/users/256196/bohemian", "user_type": "moderator", "reputation": 218026}, "body": "<p>You could try adding the case insensitive flag <code>(?i)</code>:</p>\n\n<pre><code>.*(?i)SEWER\\.PDF \n</code></pre>\n", "question_id": 20632460, "creation_date": 1387278590, "is_accepted": true, "score": 3, "last_activity_date": 1387279101, "answer_id": 20632556}], "question_id": 20632460, "tags": ["regex", "web-crawler", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/20632460/how-to-setup-a-case-insensitive-regex-in-regex-urlfilter-for-nutch-1-7", "last_activity_date": 1387279101, "accepted_answer_id": 20632556, "body": "<p>I need to filter urls based on user input, but the filter must not be case sensitive. \nFor example my user wants to filter for files ending with Sewer.pdf. \nIf I create a regex like this:</p>\n\n<pre><code>+Sewer\\.pdf$ \n</code></pre>\n\n<p>works fine, however user may have entered SEWER.PDF and it will skip the file.\nMy solution was to create a rule like this:</p>\n\n<pre><code>+[Ss][Ee][Ww][Ee][Rr]\\.[Pp][Dd][Ff]\n</code></pre>\n\n<p>However it looks like there should be something like the /i in the regex that would make it much easier.</p>\n\n<p>Does anyone know how to do it in a better way?</p>\n", "creation_date": 1387278281, "score": 2},
{"title": "Crawl and Index specific links on specific page", "view_count": 159, "owner": {"age": 28, "answer_count": 0, "creation_date": 1386915404, "user_id": 3098249, "view_count": 2, "reputation": 3}, "is_answered": true, "answers": [{"question_id": 20560439, "owner": {"user_id": 2623052, "accept_rate": 45, "link": "http://stackoverflow.com/users/2623052/allan-macmillan", "user_type": "registered", "reputation": 866}, "body": "<p>Check out my posts, if you have any questions just comment at the bottom and i'll try help you out as much as I can.</p>\n\n<p><a href=\"http://amac4.blogspot.com/2013/07/setting-up-solr-with-apache-tomcat-be.html\" rel=\"nofollow\">Solr Tutorial</a></p>\n\n<p><a href=\"http://amac4.blogspot.com/2013/07/configuring-nutch-to-crawl-urls.html\" rel=\"nofollow\">Nutch Tutorial</a></p>\n", "creation_date": 1387129790, "is_accepted": true, "score": 0, "last_activity_date": 1387129790, "answer_id": 20597721}], "question_id": 20560439, "tags": ["regex", "url", "solr", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/20560439/crawl-and-index-specific-links-on-specific-page", "last_activity_date": 1387129790, "accepted_answer_id": 20597721, "body": "<p>I am new to nutch so I am just starting my way in. I want to crawl a specific page and under that page, I want to crawl specific links.</p>\n\n<p>for e.g </p>\n\n<p>I want to crawl only <a href=\"http://nutch.apache.org/downloads.html\" rel=\"nofollow\">http://nutch.apache.org/downloads.html</a></p>\n\n<p>Under this page I just want to crawl say only *.txt links.Now they can be active links like &lt;a&gt;&lt;/a&gt;  or they could be embedded in some div like we mostly saw in variety of forums where a link for file upload/download sites are pasted/embedded in some div etc. like <code>http://example.com/movie_abcd/firstpart.avi</code> </p>\n\n<p>Here I just want to crawl links ended with .avi.I am just confused with regex-urlfilter because till now I am only using it and I ma not familiar with other url filters such prefix and suffix urls filters.Does they also play important role in the solution for my problem and if they do what will be their purpose.I was searching all over the internet for the past few days for a proper tutorial for nutch but can not find any such.How can achieve this.Also can anyone recommend me a good book for nutch ans solr which have comprehensive practical working examples and description about them and their methodology. </p>\n\n<p>I will be curiously waiting for the answers. </p>\n\n<p>Thanks</p>\n", "creation_date": 1386916204, "score": 0},
{"title": "Nutch 2.0 and Hadoop. How to prevent caching of conf/regex-urlfilter.txt", "view_count": 259, "is_answered": false, "answers": [{"question_id": 20551829, "owner": {"user_id": 575350, "accept_rate": 94, "link": "http://stackoverflow.com/users/575350/popalka", "user_type": "registered", "reputation": 1877}, "body": "<p>This settings stored in arhive file</p>\n\n<pre><code>/home/hadoop/webcrawer/apache-nutch-2.2.1/build/apache-nutch-2.2.1.job\n</code></pre>\n\n<p>Run </p>\n\n<pre><code>ant clean\nant runtime\n</code></pre>\n\n<p>to replace it with new settings or edit arhive file /home/hadoop/webcrawer/apache-nutch-2.2.1/build/apache-nutch-2.2.1.job</p>\n", "creation_date": 1386918394, "is_accepted": false, "score": 0, "last_activity_date": 1386918394, "answer_id": 20560924}], "question_id": 20551829, "tags": ["hadoop", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/20551829/nutch-2-0-and-hadoop-how-to-prevent-caching-of-conf-regex-urlfilter-txt", "last_activity_date": 1386918394, "owner": {"user_id": 575350, "answer_count": 60, "creation_date": 1294989819, "accept_rate": 94, "view_count": 300, "location": "Russia", "reputation": 1877}, "body": "<p>I have nutch 2.x and hadoop 1.2.1 on single machine.</p>\n\n<p>I configure seed.txt, conf/regex-urlfilter.txt and run command</p>\n\n<pre><code> crawl urls/seed.txt TestCrawl http://localhost:8088/solr/ 2\n</code></pre>\n\n<p>Then I want to change rules in conf/regex-urlfilter.txt</p>\n\n<p>I changed it in 2 files:</p>\n\n<pre><code>~$ find . -name 'regex-urlfilter.txt' \n./webcrawer/apache-nutch-2.2.1/conf/regex-urlfilter.txt\n./webcrawer/apache-nutch-2.2.1/runtime/local/conf/regex-urlfilter.txt\n</code></pre>\n\n<p>Then I run </p>\n\n<pre><code>  crawl urls/seed.txt TestCrawl2 http://localhost:8088/solr/ 2\n</code></pre>\n\n<p>But changes in regex-urlfilter.txt doesn't affect.</p>\n\n<p>Hadoop report that it use file. </p>\n\n<pre><code>cat /home/hadoop/data/hadoop-unjar6761544045585295068/regex-urlfilter.txt\n</code></pre>\n\n<p>When I see content of file I see old file</p>\n\n<p>How to force hadoop to use new config?</p>\n", "creation_date": 1386874356, "score": 1},
{"title": "Integration Nutch- Hbase and Solr using Gora", "view_count": 579, "is_answered": false, "answers": [{"question_id": 19443561, "owner": {"user_id": 1956204, "link": "http://stackoverflow.com/users/1956204/jazz", "user_type": "registered", "reputation": 31}, "body": "<p>As most people might suggest, hadoop.log is a good place to look for a better description of the error. In the absence of that information I will hazard the following guesses:</p>\n\n<ol>\n<li>you have setup nutch on a windows box</li>\n<li>you are running hbase in cygwin (attempting to run hbase directly in\na windows command prompt will most likely fail anyway)</li>\n<li>you are probably running into an hdfs file system bug (checking\nhadoop.log will tell if you this is the case).</li>\n</ol>\n\n<p>Here's a workaround posted in apache issues jira:\n<a href=\"https://issues.apache.org/jira/browse/HADOOP-7682\" rel=\"nofollow\">https://issues.apache.org/jira/browse/HADOOP-7682</a>\nAnother kind soul put out a patch for it: \n<a href=\"https://github.com/congainc/patch-hadoop_7682-1.0.x-win\" rel=\"nofollow\">https://github.com/congainc/patch-hadoop_7682-1.0.x-win</a>\nIf this is indeed the issue you are running into, use the WinLocalFileSystem class mentioned in the patch above and configure nutch to use it by adding the following in your nutch-site.xml:</p>\n\n<pre><code>&lt;property&gt;\n    &lt;name&gt;fs.file.impl&lt;/name&gt;\n    &lt;value&gt;org.apache.nutch.util.WinLocalFileSystem&lt;/value&gt;\n    &lt;description&gt;Enables patch for issue HADOOP-7682 on Windows\n    &lt;/description&gt;\n&lt;/property&gt;\n</code></pre>\n", "creation_date": 1386814764, "is_accepted": false, "score": 0, "last_activity_date": 1386814764, "answer_id": 20533825}], "question_id": 19443561, "tags": ["solr", "hbase", "nutch", "gora"], "answer_count": 1, "link": "http://stackoverflow.com/questions/19443561/integration-nutch-hbase-and-solr-using-gora", "last_activity_date": 1386814764, "owner": {"age": 26, "answer_count": 18, "creation_date": 1375425517, "user_id": 2644816, "accept_rate": 50, "view_count": 168, "location": "Ahamdabad, India", "reputation": 470}, "body": "<p>I have followed nutch2 tutorial and integrate nutch with HBase successfully \nMy Problem is when i crawl the url using following command\n <code>./nutch crawl urls/seed.txt abc -depth 50 -topN 50</code> in <code>runtime/local/bin</code> directory ,</p>\n\n<p>Error occured :</p>\n\n<pre><code>Exception in thread \"main\" java.lang.RuntimeException: job failed: name=generate: null, jobid=job_local1552667151_0002\n        at org.apache.nutch.util.NutchJob.waitForCompletion(NutchJob.java:54)\n        at org.apache.nutch.crawl.GeneratorJob.run(GeneratorJob.java:199)\n        at org.apache.nutch.crawl.Crawler.runTool(Crawler.java:68)\n        at org.apache.nutch.crawl.Crawler.run(Crawler.java:152)\n        at org.apache.nutch.crawl.Crawler.run(Crawler.java:250)\n        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n        at org.apache.nutch.crawl.Crawler.main(Crawler.java:257)\n</code></pre>\n\n<p>please give me solution. Any solution will be appreciated. </p>\n", "creation_date": 1382079869, "score": 0},
{"title": "How to make NTLM authentication in Apache Nutch work?", "view_count": 765, "is_answered": false, "question_id": 19529619, "tags": ["authentication", "sharepoint", "web-crawler", "nutch", "ntlm"], "answer_count": 0, "link": "http://stackoverflow.com/questions/19529619/how-to-make-ntlm-authentication-in-apache-nutch-work", "last_activity_date": 1386288299, "owner": {"age": 49, "answer_count": 145, "creation_date": 1274872362, "user_id": 350825, "accept_rate": 100, "view_count": 509, "location": "Bonn, Germany", "reputation": 3011}, "body": "<p>The web crawler Apache Nutch comes with a built-in support for NTLM. I'm trying to use version 1.7 to crawl a web site (Windows Sharepoint) using NTLM authentication. I have setup Nutch according to <a href=\"https://wiki.apache.org/nutch/HttpAuthenticationSchemes\" rel=\"nofollow\">https://wiki.apache.org/nutch/HttpAuthenticationSchemes</a> which means in particular that I have credentials </p>\n\n<pre><code>&lt;credentials username=\"rickert\" password=\"mypassword\"&gt;\n  &lt;authscope host=\"server-to-be-crawled.com\" port=\"80\" realm=\"CORP\" scheme=\"NTLM\"/&gt;\n&lt;/credentials&gt;\n</code></pre>\n\n<p>configured. When I look at the log files I can see that Nutch tries to access the seed URL and goes through \"normal\" NTLM cycle: obtain an 401 error during the first GET, extract the NTLM challenge and send the NTLM authentication in the next GET (using a keep-alive connection). However, the second GET is not successful either.</p>\n\n<p>That's the point when I was suspecting some fundamental problems with my credentials or the specific setup: I'm running Nutch in a Debian guest Virtual Box on a Windows host. But to my surprise both <code>wget</code> and <code>curl</code> were able to retrieve the document from within the Debian guest using my credentials. The interesting thing is that both command line tools ONLY require a username and a password to work. The full fledge NTLM specification, on the other hand, also requires a <em>host</em> and a <em>domain</em>. According to the specs the <em>host</em> is the one that the request originates from which I would interpret as the one that the http-agent is running on, the <em>domain</em> in the Windows domain that the username is associated with. My assumption is that both tools simply leave this details empty.</p>\n\n<p>This is where the configuration of Nutch comes in: the <em>host</em> is allegedly supplied as <code>http.agent.host</code>in the configuration file. The <em>domain</em> is supposed to be configured as the <em>realm</em> of the credential but the documentation rather says that this a convention and not really necessary. However, it does not matter whether I set a realm or not the result is the same. Again looking at the log file I can see some messages that the authentication is resolved using <code>&lt;any_realm&gt;@server-to-be-crawled.com</code> no matter which realm I use.</p>\n\n<p>My gut feeling is that there is some wrong mapping of the Nutch configuration values onto the NTLM parameters required by the Java class <code>httpclient</code>that executing the GET. I'm helpless. Can anybody give me some hints as to how to further debug this? Does anybody have a concrete config that works for a SharePoint Server? Thanks!</p>\n", "creation_date": 1382481415, "score": 2},
{"title": "Is it possible to use Nutch 2.x and Apache Gora with plain filesystem as backend storage", "view_count": 236, "is_answered": true, "answers": [{"question_id": 20394009, "owner": {"user_id": 582789, "link": "http://stackoverflow.com/users/582789/alfonso-nishikawa", "user_type": "registered", "reputation": 1263}, "body": "<p>You could use the <a href=\"http://gora.apache.org/current/gora-core.html\" rel=\"nofollow\">AvroStore</a> that saves into a file (serialized with Avro).\nI say this only theoretically since I never used it...</p>\n", "creation_date": 1386232547, "is_accepted": false, "score": 1, "last_activity_date": 1386232547, "answer_id": 20395104}], "question_id": 20394009, "tags": ["storage", "nutch", "gora"], "answer_count": 1, "link": "http://stackoverflow.com/questions/20394009/is-it-possible-to-use-nutch-2-x-and-apache-gora-with-plain-filesystem-as-backend", "last_activity_date": 1386232547, "owner": {"user_id": 575350, "answer_count": 60, "creation_date": 1294989819, "accept_rate": 94, "view_count": 300, "location": "Russia", "reputation": 1877}, "body": "<p>Is it possible to use Nutch 2.x and Apache Gora\u2122 with plain filesysem as backend storage?</p>\n\n<p>Official site says:</p>\n\n<blockquote>\n  <p>Nutch 2.x: An emerging alternative taking direct inspiration from 1.x,\n  but which differs in one key area; storage is abstracted away from any\n  specific underlying data store by using Apache Gora\u2122 for handling\n  object to persistent mappings.</p>\n</blockquote>\n\n<p>I want to use latest version of nutch (2.1 currently), but I don't want to setup complex backend with nosql or rdbms backend for storage now. I want to choose backend storage later.</p>\n\n<p>I didn't find any docs for usage of filesystem as storage for Gora. Is it possible?</p>\n", "creation_date": 1386228727, "score": 0},
{"title": "Nutch + Solr on top level page only", "view_count": 220, "is_answered": false, "answers": [{"question_id": 20157173, "owner": {"user_id": 302380, "link": "http://stackoverflow.com/users/302380/paul-ianas", "user_type": "registered", "reputation": 416}, "body": "<p>Try a nutch inject command to insert the \"no-incomming-link\" URL into the nutch DB.</p>\n\n<p>I guess that if you don't see anything in your solr indexes, it is because no data for those URLs is stored in the nutch DB (since nutch will take care to sync its DB with the indexes). Not having data in the DB may be explained by the fact that the URLs are isolated, hence you can try the inject command to include those sites.</p>\n\n<p>I would try to actually see the internal DB to verify the nutch behavior, since before inserting values in the indexes, nutch stores data inside its DBs.</p>\n\n<p>Assigning a higher score has no effect, since lucene will give you a result as long as the data is in the index.</p>\n", "creation_date": 1386057885, "is_accepted": false, "score": 0, "last_activity_date": 1386057885, "answer_id": 20345800}, {"question_id": 20157173, "owner": {"user_id": 2217420, "link": "http://stackoverflow.com/users/2217420/avirr", "user_type": "registered", "reputation": 21}, "body": "<p>Solr now reads HTML files using Tika by default, so that's not a problem.</p>\n\n<p><a href=\"http://wiki.apache.org/solr/TikaEntityProcessor\" rel=\"nofollow\">http://wiki.apache.org/solr/TikaEntityProcessor</a></p>\n\n<p>If all you want is listed pages, is there a specific reason to use the Nutch crawler?  Or could you just feed URLs to Solr and go from there?</p>\n", "creation_date": 1386090732, "is_accepted": false, "score": 0, "last_activity_date": 1386090732, "answer_id": 20357464}], "question_id": 20157173, "tags": ["java", "hadoop", "solr", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/20157173/nutch-solr-on-top-level-page-only", "last_activity_date": 1386090732, "owner": {"age": 30, "answer_count": 9, "creation_date": 1296960803, "user_id": 604955, "accept_rate": 90, "view_count": 83, "location": "Chicago, IL", "reputation": 927}, "body": "<p>I've been trying to use Nutch to crawl over over the first page of the domains in my urls file and then use Solr to make keywords in the crawled data searchable. So far I haven't been able to get anything working this way, unless the two pages are linked together.</p>\n\n<p>I realize this is probably an issue of the pages having no incoming links, and therefore the PageRank algorithm discards the page content. I tried adjusting the parameters so that the default score is higher for urls not in the graph, but I'm still getting the same results.</p>\n\n<p>Is there anything people know of that can build an index over pages with no incoming links?</p>\n\n<p>Thanks!</p>\n", "creation_date": 1385168326, "score": 5},
{"title": "Writing a plugin(Indexing) for Nutch", "view_count": 1106, "is_answered": true, "answers": [{"question_id": 10853564, "owner": {"user_id": 2833323, "link": "http://stackoverflow.com/users/2833323/young-wei", "user_type": "registered", "reputation": 11}, "body": "<p>Check that if the value of plugin.includes in nutch-default.xml contains myPlugin,like this:</p>\n\n<p>myPlugin|protocol-http|urlfilter-regex|parse-(text|html|js)|index-basic|query-(basic|site|url)|summary-basic|scoring-opic|urlnormalizer-(pass|regex|basic)</p>\n\n<p>PS:\nYou can and should answer your own question if you have the solution,just add some comment after this.</p>\n", "creation_date": 1386084011, "is_accepted": false, "score": 1, "last_activity_date": 1386084011, "answer_id": 20354921}], "question_id": 10853564, "tags": ["nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/10853564/writing-a-pluginindexing-for-nutch", "last_activity_date": 1386084011, "owner": {"user_id": 1204751, "answer_count": 10, "creation_date": 1329026289, "accept_rate": 76, "view_count": 49, "reputation": 283}, "body": "<p>I am writing a plugin for nutch which parses text and creates a new field depending on text. For this the plugin which I am writing implements IndexingFilter. I am following the tutorial as in <a href=\"http://florianhartl.com/nutch-plugin-tutorial.html\" rel=\"nofollow\">here</a> (which is similar to that in nutch wiki). I have followed exactly as it is mentioned and I am successfully able to build the plugin. However I do not see the new index being added. Well I am beginner so I am not sure if I am looking at right place. </p>\n\n<p>I did normal crawling using the command</p>\n\n<pre><code>bin/nutch crawl urls -dir crawl -depth 3 -topN 5\n</code></pre>\n\n<p>And then I checked in crawldb and segment using command </p>\n\n<pre><code>bin/nutch readdb crawl/crawldb/ -dump crawlContent\nbin/nutch readseg -dump crawl/segments/* segmentAllContent\n</code></pre>\n\n<p>I do not see the new field added here. Am I doing it right? Or is there any command I need to run. Thanks in advance.</p>\n\n<p>Code:</p>\n\n<p>I copied the directory structure of urlmeta plugin and made couple of changes.</p>\n\n<p>plugin.xml:</p>\n\n<pre><code>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n   &lt;plugin id=\"myPlugin\" name=\"Add Field to Index\"\n      version=\"1.0.0\" provider-name=\"your name\"&gt;\n\n    &lt;runtime&gt;\n       &lt;library name=\"myPlugin.jar\"&gt;\n     &lt;export name=\"*\"/&gt;\n       &lt;/library&gt;\n    &lt;/runtime&gt;\n\n    &lt;extension id=\"org.apache.nutch.indexer.myPlugin\"\n       name=\"Add Field to Index\"\n       point=\"org.apache.nutch.indexer.IndexingFilter\"&gt;\n      &lt;implementation id=\"myPlugin\"\n        class=\"org.apache.nutch.indexer.AddField\"/&gt;\n    &lt;/extension&gt;\n  &lt;/plugin&gt;\n</code></pre>\n\n<p>build.xml:</p>\n\n<pre><code>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n  &lt;project name=\"myPlugin\" default=\"jar\"&gt;\n     &lt;import file=\"../build-plugin.xml\"/&gt;\n  &lt;/project&gt;\n</code></pre>\n\n<p>And other code is same as in the link mentioned. </p>\n", "creation_date": 1338566004, "score": 0},
{"title": "nutch crawling gets stuck in spinwaiting or active. how to reduce fetch cycle?", "view_count": 881, "is_answered": true, "answers": [{"question_id": 14117244, "owner": {"user_id": 1830069, "accept_rate": 63, "link": "http://stackoverflow.com/users/1830069/sunskin", "user_type": "registered", "reputation": 543}, "body": "<p><strong><em>I am using nutch 2.1 and crawling a site. The problem is that the\ncrawler keeps showing fetching url spinwaiting/active and since the\n fetching takes so much time the connection to mysql gets timedout. How\ncan i reduce the number of fetches at a time so that the mysql does\n not get timedout?</em></strong></p>\n\n<p>For reducing number of fetches, you can add the property below to your nutch-site.xml and edit the value based on your need. Please do not edit the nutch-default.xml rather copy the property to nutch-site.xml and manage the value from there:</p>\n\n<pre><code>  &lt;property&gt;\n    &lt;name&gt;fetcher.threads.fetch&lt;/name&gt;\n    &lt;value&gt;20&lt;/value&gt;\n  &lt;/property&gt;\n</code></pre>\n\n<p>Regarding the timeout issue, you can possible add this property to your nutch-site.xml with a value of loading time you think is needed. </p>\n\n<pre><code>&lt;property&gt;\n  &lt;name&gt;http.timeout&lt;/name&gt;\n  &lt;value&gt;240000&lt;/value&gt;\n  &lt;description&gt;The default network timeout, in milliseconds.&lt;/description&gt;\n&lt;/property&gt;\n</code></pre>\n\n<p><em><strong>Is there a setting in nutch where i can say only fetch 100 or 500 urls then parse and store to mysql and then again fetch the next 100 or 500 urls??</em></strong></p>\n\n<p>Nutch crawls in a cycle with steps - generate/fetch/parse/update in a number of iterations called 'depth' which you specify in your crawl command. If you would like to have a control on your crawling, you can perform each step as described in section 3.2(Using Individual Commands for Whole-Web Crawling) of the tutorial link <a href=\"http://wiki.apache.org/nutch/NutchTutorial\" rel=\"nofollow\">http://wiki.apache.org/nutch/NutchTutorial</a>. This will give you good direction and understand exactly what is happening. Do check status while fetching each segment so you will know how many urls are being fetched in each segment</p>\n", "creation_date": 1365448036, "is_accepted": false, "score": 1, "last_activity_date": 1365448036, "answer_id": 15886954}], "question_id": 14117244, "tags": ["nutch", "web-crawler", "spinwait"], "answer_count": 1, "link": "http://stackoverflow.com/questions/14117244/nutch-crawling-gets-stuck-in-spinwaiting-or-active-how-to-reduce-fetch-cycle", "last_activity_date": 1386022829, "owner": {"user_id": 328836, "answer_count": 4, "creation_date": 1272541802, "accept_rate": 62, "view_count": 86, "location": "Goa India", "reputation": 500}, "body": "<p>I am using nutch 2.1 and crawling a site. The problem is that the crawler keeps showing fetching url spinwaiting/active and since the fetching takes so much time the connection to mysql gets timedout. How can i reduce the number of fetches at a time so that the mysql does not get timedout?? Is there a setting in nutch where i can say only fetch 100 or 500 urls then parse and store to mysql and then again fetch the next 100 or 500 urls??</p>\n\n<p>Error message:\nUnexpected error for <a href=\"http://www.xyz.com\" rel=\"nofollow\">http://www.xyz.com</a>\njava.io.IOException: java.sql.BatchUpdateException: The last packet successfully received from the server was 36,928,172 milliseconds ago.  The last packet sent successfully to the server was 36,928,172 milliseconds ago. is longer than the server configured value of 'wait_timeout'. You should consider either expiring and/or testing connection validity before use in your application, increasing the server configured values for client timeouts, or using the Connector/J connection property 'autoReconnect=true' to avoid this problem.\n    at org.apache.gora.sql.store.SqlStore.flush(SqlStore.java:340)\n    at org.apache.gora.mapreduce.GoraRecordWriter.write(GoraRecordWriter.java:65)\n    at org.apache.hadoop.mapred.ReduceTask$NewTrackingRecordWriter.write(ReduceTask.java:587)\n    at org.apache.hadoop.mapreduce.TaskInputOutputContext.write(TaskInputOutputContext.java:80)\n    at org.apache.nutch.fetcher.FetcherReducer$FetcherThread.output(FetcherReducer.java:663)\n    at org.apache.nutch.fetcher.FetcherReducer$FetcherThread.run(FetcherReducer.java:534)\nCaused by: java.sql.BatchUpdateException: The last packet successfully received from the server was 36,928,172 milliseconds ago.  The last packet sent successfully to the server was 36,928,172 milliseconds ago. is longer than the server configured value of 'wait_timeout'. You should consider either expiring and/or testing connection validity before use in your application, increasing the server configured values for client timeouts, or using the Connector/J connection property 'autoReconnect=true' to avoid this problem.\n    at com.mysql.jdbc.PreparedStatement.executeBatchSerially(PreparedStatement.java:2028)\n    at com.mysql.jdbc.PreparedStatement.executeBatch(PreparedStatement.java:1451)\n    at org.apache.gora.sql.store.SqlStore.flush(SqlStore.java:328)\n    ... 5 more\nCaused by: com.mysql.jdbc.exceptions.jdbc4.CommunicationsException: The last packet successfully received from the server was 36,928,172 milliseconds ago.  The last packet sent successfully to the server was 36,928,172 milliseconds ago. is longer than the server configured value of 'wait_timeout'. You should consider either expiring and/or testing connection validity before use in your application, increasing the server configured values for client timeouts, or using the Connector/J connection property 'autoReconnect=true' to avoid this problem.\n    at sun.reflect.GeneratedConstructorAccessor49.newInstance(Unknown Source)\n    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n    at java.lang.reflect.Constructor.newInstance(Constructor.java:525)\n    at com.mysql.jdbc.Util.handleNewInstance(Util.java:411)\n    at com.mysql.jdbc.SQLError.createCommunicationsException(SQLError.java:1116)\n    at com.mysql.jdbc.MysqlIO.send(MysqlIO.java:3364)\n    at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:1983)\n    at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2163)\n    at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2624)\n    at com.mysql.jdbc.PreparedStatement.executeInternal(PreparedStatement.java:2127)\n    at com.mysql.jdbc.PreparedStatement.executeUpdate(PreparedStatement.java:2427)\n    at com.mysql.jdbc.PreparedStatement.executeBatchSerially(PreparedStatement.java:1980)\n    ... 7 more\nCaused by: java.net.SocketException: Broken pipe\n    at java.net.SocketOutputStream.socketWrite0(Native Method)\n    at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:109)\n    at java.net.SocketOutputStream.write(SocketOutputStream.java:153)\n    at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)\n    at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:140)\n    at com.mysql.jdbc.MysqlIO.send(MysqlIO.java:3345)\n    ... 13 more</p>\n", "creation_date": 1357104435, "score": 1},
{"title": "link analysis using nutch", "view_count": 78, "owner": {"user_id": 1000341, "answer_count": 10, "creation_date": 1318909222, "accept_rate": 80, "view_count": 286, "reputation": 722}, "is_answered": true, "answers": [{"question_id": 20306667, "owner": {"user_id": 1881318, "link": "http://stackoverflow.com/users/1881318/tahagh", "user_type": "registered", "reputation": 619}, "body": "<p>The webgraph command is for generating or updating of webgraphs. You can pass anything as the value of webgraphdb argument. If a directory with that name does not exist, nutch will create one for you.</p>\n", "creation_date": 1385974888, "is_accepted": true, "score": 0, "last_activity_date": 1385974888, "answer_id": 20324341}], "question_id": 20306667, "tags": ["hyperlink", "nutch", "web-crawler"], "answer_count": 1, "link": "http://stackoverflow.com/questions/20306667/link-analysis-using-nutch", "last_activity_date": 1385974888, "accepted_answer_id": 20324341, "body": "<p>I am new to nutch. I have crawled some urls using nutch. Now I want to get linkrank of them. I read about it <a href=\"http://wiki.apache.org/nutch/NewScoring\" rel=\"nofollow\">here</a>. The problem is that I can't create webGraphdb. In my crawl directory I have <code>linkdb</code>, <code>segments</code> and <code>crawldb</code> directory. I need it when I run the command </p>\n\n<pre><code>./nutch -webgraph -segment &lt;seg name&gt; -segmentDir &lt;seg dir&gt; webgrapgdb??\n</code></pre>\n\n<p>I need to give the address of webgraphdb. How should I generate it. My nutch version is 1.7.</p>\n", "creation_date": 1385852593, "score": 0},
{"title": "Nutch change seed.txt doesn&#39;t work", "view_count": 118, "is_answered": true, "answers": [{"question_id": 20233125, "owner": {"user_id": 1490200, "accept_rate": 17, "link": "http://stackoverflow.com/users/1490200/tony-huang", "user_type": "registered", "reputation": 70}, "body": "<p>I found the solution. I need to change the regex-urlfilter.txt file also.</p>\n", "creation_date": 1385579202, "is_accepted": false, "score": 1, "last_activity_date": 1385579202, "answer_id": 20250989}], "question_id": 20233125, "tags": ["nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/20233125/nutch-change-seed-txt-doesnt-work", "last_activity_date": 1385579202, "owner": {"user_id": 1490200, "answer_count": 3, "creation_date": 1340940390, "accept_rate": 17, "view_count": 24, "reputation": 70}, "body": "<p>I am using nutch 1.7 and try to crawl domain1.com using </p>\n\n<pre><code>bin/nutch crawl urls -dir crawl -solr http://localhost:8983/solr/ -depth 3 -topN 5\n</code></pre>\n\n<p>But after I change the urls/seed.txt delete the <a href=\"http://domain1.com/\" rel=\"nofollow\">http://domain1.com/</a> and add <a href=\"http://domain2.com/\" rel=\"nofollow\">http://domain2.com/</a> re-run the command above, the crawl still crawl the domain1.com not domain2.com.</p>\n\n<p>Does any one know why's that?</p>\n", "creation_date": 1385521485, "score": 1},
{"title": "Nutch + Solr; SolrDeleteDuplicates deletes all but one index", "view_count": 111, "is_answered": false, "answers": [{"question_id": 20011145, "owner": {"user_id": 417864, "accept_rate": 75, "link": "http://stackoverflow.com/users/417864/alexandre-rafalovitch", "user_type": "registered", "reputation": 6649}, "body": "<p>Most likely you have mis-configured your deduplication setup to look at the field that is identical for all entries. So, Solr thinks they are all the same records. </p>\n\n<p>If this does not, update your question with the configuration you have for the dedupe component.</p>\n", "creation_date": 1384571749, "is_accepted": false, "score": 0, "last_activity_date": 1384571749, "answer_id": 20014197}], "question_id": 20011145, "tags": ["solr", "rss", "indexing", "duplicates", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/20011145/nutch-solr-solrdeleteduplicates-deletes-all-but-one-index", "last_activity_date": 1384571749, "owner": {"user_id": 2997946, "view_count": 0, "answer_count": 0, "creation_date": 1384551494, "reputation": 1}, "body": "<p>I've got a server running Nutch, which feeds to Solr. Nutch's input is an rss feed's xml, and it seems to be using the proper feed plugin to parse it.</p>\n\n<p>When running a basic crawl, it crawls and identifies the various links in the xml properly, and indexes the correct number of documents. However, SolrDeleteDuplicates seems to delete all but one - the one that doesn't get deleted seems to be random.</p>\n\n<pre><code> Indexing 21 documents\n SolrIndexer: finished at 2013-11-15 13:53:53, elapsed: 00:00:22\n SolrDeleteDuplicates: starting at 2013-11-15 13:35:53\n SolrDeleteDuplicates: Solr url: http://localhost:8983/solr\n SolrDeleteDuplicates: deleting 20 duplicates\n</code></pre>\n\n<p>Any ideas?</p>\n", "creation_date": 1384551929, "score": 0},
{"title": "Creating an Akka fat Jar", "view_count": 2658, "owner": {"user_id": 249001, "answer_count": 102, "creation_date": 1263310302, "accept_rate": 83, "view_count": 1053, "location": "Milano, Italy", "reputation": 9541}, "is_answered": true, "answers": [{"question_id": 15203113, "owner": {"user_id": 3827, "accept_rate": 100, "link": "http://stackoverflow.com/users/3827/eugene-yokota", "user_type": "registered", "reputation": 62360}, "body": "<p>The latest version of <a href=\"https://github.com/sbt/sbt-assembly\" rel=\"nofollow\">sbt-assembly</a> as of Nov 2013 is <a href=\"http://notes.implicit.ly/post/65751699253/sbt-assembly-0-10-1\" rel=\"nofollow\">0.10.1</a> for sbt 0.13.\nUsing this, I was able to create a fat jar using the latest stable Akka 2.2.3 using the default merge strategy.</p>\n\n<h2>Files</h2>\n\n<p>project/build.properties:</p>\n\n<pre><code>sbt.verion=0.13.0\n</code></pre>\n\n<p>project/assembly.sbt:</p>\n\n<pre><code>addSbtPlugin(\"com.eed3si9n\" % \"sbt-assembly\" % \"0.10.1\")\n</code></pre>\n\n<p>assembly.sbt:</p>\n\n<pre><code>import AssemblyKeys._\n\nassemblySettings\n</code></pre>\n\n<p>build.sbt:</p>\n\n<pre><code>name := \"sbt-assembly-akka-sample\"\n\nversion := \"0.1.0-SNAPSHOT\"\n\nscalaVersion := \"2.10.3\"\n\nlibraryDependencies ++= Seq(\n  \"com.typesafe.akka\" %% \"akka-actor\" % \"2.2.3\",\n  \"com.typesafe.akka\" %% \"akka-remote\" % \"2.2.3\"\n)\n</code></pre>\n\n<p>src/main/scala/actor.scala:</p>\n\n<pre><code>package hello\n\nimport akka.actor.{Actor, Props}\nimport akka.event.Logging\n\nclass MyActor extends Actor {\n  val log = Logging(context.system, this)\n  def receive = {\n    case \"test\" \u21d2 log.info(\"received test\")\n    case _      \u21d2 log.info(\"received unknown message\")\n  }\n}\n</code></pre>\n\n<p>src/main/scala/app.scala:</p>\n\n<pre><code>package hello\n\nobject Main extends App {\n  import akka.actor.{ActorSystem, Props}\n  val system = ActorSystem(\"mySystem\")\n  val myActor = system.actorOf(Props[MyActor], \"myactor\")\n  myActor ! \"test\"\n}\n</code></pre>\n\n<h2>Output</h2>\n\n<p>Here's what I got by running <code>assembly</code>:</p>\n\n<pre><code>&gt; assembly\n[info] Updating {file:/xxx/sbt-assembly-01/}sbt-assembly-01...\n[info] Resolving org.fusesource.jansi#jansi;1.4 ...\n[info] Done updating.\n[info] Compiling 2 Scala sources to /xxx/sbt-assembly-01/target/scala-2.10/classes...\n[info] Including: uncommons-maths-1.2.2a.jar\n[info] Including: protobuf-java-2.4.1.jar\n[info] Including: config-1.0.2.jar\n[info] Including: netty-3.6.6.Final.jar\n[info] Including: akka-remote_2.10-2.2.3.jar\n[info] Including: akka-actor_2.10-2.2.3.jar\n[info] Passed: Total 0, Failed 0, Errors 0, Passed 0\n[info] No tests to run for test:test\n[info] Including: scala-library-2.10.3.jar\n[info] Checking every *.class/*.jar file's SHA-1.\n[info] Merging files...\n[warn] Merging 'META-INF/NOTICE.txt' with strategy 'rename'\n[warn] Merging 'META-INF/license' with strategy 'rename'\n[warn] Merging 'META-INF/LICENSE.txt' with strategy 'rename'\n[warn] Merging 'META-INF/MANIFEST.MF' with strategy 'discard'\n[warn] Merging 'reference.conf' with strategy 'concat'\n[warn] Strategy 'concat' was applied to a file\n[warn] Strategy 'discard' was applied to a file\n[warn] Strategy 'rename' was applied to 3 files\n[info] SHA-1: 1e9dcebeddc8e2a7d41a0c55a663f9ca0000000\n[info] Packaging /xxx/sbt-assembly-01/target/scala-2.10/sbt-assembly-akka-sample-assembly-0.1.0-SNAPSHOT.jar ...\n[info] Done packaging.\n[success] Total time: 13 s, completed Nov 11, 2013 8:57:18 PM\n</code></pre>\n\n<p>And here's what it looks like running the jar:</p>\n\n<pre><code>$ java -jar target/scala-2.10/sbt-assembly-akka-sample-assembly-0.1.0-SNAPSHOT.jar\n[INFO] [11/11/2013 20:59:48.265] [mySystem-akka.actor.default-dispatcher-2] [akka://mySystem/user/myactor] received test\n</code></pre>\n", "creation_date": 1384221636, "is_accepted": true, "score": 3, "last_activity_date": 1384221636, "answer_id": 19919566}], "question_id": 15203113, "tags": ["scala", "sbt", "akka", "nutch", "sbt-assembly"], "answer_count": 1, "link": "http://stackoverflow.com/questions/15203113/creating-an-akka-fat-jar", "last_activity_date": 1384221636, "accepted_answer_id": 19919566, "body": "<p>I need to create a Nutch plugin that communicate with some external applications using Akka. In order to do this, I need to package the plugin as a fat Jar - I am using <a href=\"https://github.com/sbt/sbt-assembly/\" rel=\"nofollow\">sbt-assembly</a> version 0.8.3.</p>\n\n<p>When I try to run the plugin, I get the exception </p>\n\n<pre><code>com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'akka'\n</code></pre>\n\n<p>as if Akka was not able to find <code>reference.conf</code>. This is weird, because <code>sbt-assembly</code> <a href=\"http://letitcrash.com/post/21706121997/follow-up-sbt-assembly-now-likes-reference-conf\" rel=\"nofollow\">should be able</a> to package that file correctly, and in fact I can see its content in the created jar.</p>\n\n<p>My <code>build.sbt</code> looks like this:</p>\n\n<pre><code>import AssemblyKeys._\n\nname := \"my-project\"\n\nversion := \"0.1-SNAPSHOT\"\n\nscalaVersion := \"2.10.0\"\n\nresolvers ++= Seq(\n  \"Central Repo\" at \"http://repo1.maven.org/maven2\",\n  \"Typesafe Repository\" at \"http://repo.typesafe.com/typesafe/releases/\",\n  \"Akka io\" at \"http://akka.io/repository\"\n)\n\nlibraryDependencies ++= Seq(\n  ...,\n  \"com.typesafe.akka\" %% \"akka-actor\" % \"2.1.1\",\n  \"com.typesafe.akka\" %% \"akka-remote\" % \"2.1.1\"\n)\n\nseq(assemblySettings: _*)\n\nmergeStrategy in assembly &lt;&lt;= (mergeStrategy in assembly) { (old) =&gt;\n  {\n    case \"plugin.xml\" =&gt;\n      MergeStrategy.first\n    case x if x startsWith \"org/apache/jasper\" =&gt;\n      MergeStrategy.last\n    case x if x startsWith \"javax/xml\" =&gt;\n      MergeStrategy.last\n    case x if x startsWith \"javax/servlet\" =&gt;\n      MergeStrategy.last\n    case x if x startsWith \"org/apache/commons\" =&gt;\n      MergeStrategy.last\n    case x if x startsWith \"org/apache/xmlcommons\" =&gt;\n      MergeStrategy.last\n    case x if x startsWith \"org/xml/sax\" =&gt;\n      MergeStrategy.last\n    case x if x startsWith \"org/w3c/dom\" =&gt;\n      MergeStrategy.last\n    case x =&gt; old(x)\n  }\n}\n</code></pre>\n\n<p>The last lines are needed to fix some conflicts between nutch and hadoop.</p>\n\n<blockquote>\n  <p>What is the correct way to package an Akka application?</p>\n</blockquote>\n", "creation_date": 1362405101, "score": 4},
{"title": "Nutch 2.1 cassandra backend generate error", "view_count": 416, "is_answered": false, "question_id": 16219699, "tags": ["cassandra", "nutch", "gora"], "answer_count": 0, "link": "http://stackoverflow.com/questions/16219699/nutch-2-1-cassandra-backend-generate-error", "last_activity_date": 1383683939, "owner": {"user_id": 2319213, "view_count": 1, "answer_count": 0, "creation_date": 1366884604, "reputation": 1}, "body": "<p>I made a choice on cassandra as backend and started to play with nutch.</p>\n\n<p>Small subset of DMOZ urls (~50k), all (inject, generate, fetch) runs fine.</p>\n\n<p>However, after I injected the whole DMOZ url set (~3.5M) and tried to generate a fetchlist, I got the following error, which was reproducible on a another system:</p>\n\n<pre><code>~/software/nutch_dmoz/local$ ./bin/nutch generate -topN 1000\nGeneratorJob: Selecting best-scoring urls due for fetch.\nGeneratorJob: starting\nGeneratorJob: filtering: true\nGeneratorJob: topN: 1000\nGeneratorJob: java.lang.RuntimeException: job failed: name=generate: 1366905487-307733671, jobid=job_local_0001\n    at org.apache.nutch.util.NutchJob.waitForCompletion(NutchJob.java:54)\n    at org.apache.nutch.crawl.GeneratorJob.run(GeneratorJob.java:191)\n    at org.apache.nutch.crawl.GeneratorJob.generate(GeneratorJob.java:213)\n    at org.apache.nutch.crawl.GeneratorJob.run(GeneratorJob.java:241)\n    at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n    at org.apache.nutch.crawl.GeneratorJob.main(GeneratorJob.java:249)\n</code></pre>\n\n<p>logs/hadoop.log:</p>\n\n<pre><code>2013-04-25 17:58:07,986 INFO  crawl.GeneratorJob - GeneratorJob: Selecting best-scoring urls due for fetch.\n2013-04-25 17:58:08,007 INFO  crawl.GeneratorJob - GeneratorJob: starting\n2013-04-25 17:58:08,007 INFO  crawl.GeneratorJob - GeneratorJob: filtering: true\n2013-04-25 17:58:08,007 INFO  crawl.GeneratorJob - GeneratorJob: topN: 1000\n2013-04-25 17:58:08,570 INFO  connection.CassandraHostRetryService - Downed Host Retry service started with queue size -1 and retry delay 10\ns\n2013-04-25 17:58:08,660 INFO  service.JmxMonitor - Registering JMX me.prettyprint.cassandra.service_Test Cluster:ServiceType=hector,MonitorT\nype=hector\n2013-04-25 17:58:09,029 WARN  util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes w\nhere applicable\n2013-04-25 17:58:09,403 INFO  mapreduce.GoraRecordReader - gora.buffer.read.limit = 10000\n2013-04-25 17:58:09,435 INFO  plugin.PluginRepository - Plugins: looking in: /home/sethunder/software/nutch_dmoz/local/plugins\n2013-04-25 17:58:09,560 INFO  plugin.PluginRepository - Plugin Auto-activation mode: [true]\n2013-04-25 17:58:09,560 INFO  plugin.PluginRepository - Registered Plugins:\n2013-04-25 17:58:09,560 INFO  plugin.PluginRepository -         the nutch core extension points (nutch-extensionpoints)\n2013-04-25 17:58:09,560 INFO  plugin.PluginRepository -         Regex URL Normalizer (urlnormalizer-regex)\n2013-04-25 17:58:09,560 INFO  plugin.PluginRepository -         CyberNeko HTML Parser (lib-nekohtml)\n2013-04-25 17:58:09,560 INFO  plugin.PluginRepository -         OPIC Scoring Plug-in (scoring-opic)\n2013-04-25 17:58:09,560 INFO  plugin.PluginRepository -         Basic URL Normalizer (urlnormalizer-basic)\n2013-04-25 17:58:09,560 INFO  plugin.PluginRepository -         Tika Parser Plug-in (parse-tika)\n2013-04-25 17:58:09,560 INFO  plugin.PluginRepository -         Html Parse Plug-in (parse-html)\n2013-04-25 17:58:09,560 INFO  plugin.PluginRepository -         Basic Indexing Filter (index-basic)\n2013-04-25 17:58:09,560 INFO  plugin.PluginRepository -         Anchor Indexing Filter (index-anchor)\n2013-04-25 17:58:09,560 INFO  plugin.PluginRepository -         HTTP Framework (lib-http)\n2013-04-25 17:58:09,561 INFO  plugin.PluginRepository -         Regex URL Filter (urlfilter-regex)\n2013-04-25 17:58:09,561 INFO  plugin.PluginRepository -         Regex URL Filter Framework (lib-regex-filter)\n2013-04-25 17:58:09,561 INFO  plugin.PluginRepository -         Pass-through URL Normalizer (urlnormalizer-pass)\n2013-04-25 17:58:09,561 INFO  plugin.PluginRepository -         Http Protocol Plug-in (protocol-http)\n2013-04-25 17:58:09,561 INFO  plugin.PluginRepository - Registered Extension-Points:\n2013-04-25 17:58:09,561 INFO  plugin.PluginRepository -         Nutch URL Normalizer (org.apache.nutch.net.URLNormalizer)\n2013-04-25 17:58:09,561 INFO  plugin.PluginRepository -         Nutch Protocol (org.apache.nutch.protocol.Protocol)\n2013-04-25 17:58:09,561 INFO  plugin.PluginRepository -         Parse Filter (org.apache.nutch.parse.ParseFilter)\n2013-04-25 17:58:09,561 INFO  plugin.PluginRepository -         Nutch URL Filter (org.apache.nutch.net.URLFilter)\n2013-04-25 17:58:09,561 INFO  plugin.PluginRepository -         Nutch Indexing Filter (org.apache.nutch.indexer.IndexingFilter)\n2013-04-25 17:58:09,561 INFO  plugin.PluginRepository -         Nutch Content Parser (org.apache.nutch.parse.Parser)\n2013-04-25 17:58:09,561 INFO  plugin.PluginRepository -         Nutch Scoring (org.apache.nutch.scoring.ScoringFilter)\n2013-04-25 17:58:09,582 INFO  crawl.FetchScheduleFactory - Using FetchSchedule impl: org.apache.nutch.crawl.DefaultFetchSchedule\n2013-04-25 17:58:09,582 INFO  crawl.AbstractFetchSchedule - defaultInterval=2592000\n2013-04-25 17:58:09,582 INFO  crawl.AbstractFetchSchedule - maxInterval=7776000\n2013-04-25 17:58:11,046 INFO  regex.RegexURLNormalizer - can't find rules for scope 'generate_host_count', using default\n2013-04-25 18:01:02,936 WARN  mapred.FileOutputCommitter - Output path is null in cleanup\n2013-04-25 18:01:02,936 WARN  mapred.LocalJobRunner - job_local_0001\njava.lang.ArrayIndexOutOfBoundsException\n2013-04-25 18:01:03,412 ERROR crawl.GeneratorJob - GeneratorJob: java.lang.RuntimeException: job failed: name=generate: 1366905487-307733671, jobid=job_local_0001\n        at org.apache.nutch.util.NutchJob.waitForCompletion(NutchJob.java:54)\n</code></pre>\n\n<p>I did not run out of disk space, as far as I can see. The /tmp partition has 250G free space, the partition where cassandra is running has 2.5T free space. Is there any possibility to increase verbosity? Also, I wonder that the ArrayOutOfBoundsException does not tell the bound it tried to access, just nothing. The keyspace webpage is existing, I can access it with cassandra-cli. Here is the output of readdb -stats:</p>\n\n<pre><code>~/software/nutch_dmoz/local$ ./bin/nutch readdb -stats\nWebTable statistics start\nStatistics for WebTable: \nmin score:  55.0\nretry 0:    3576393\njobs:   {db_stats-job_local_0001={jobID=job_local_0001, jobName=db_stats, counters={File Input Format Counters ={BYTES_READ=0}, Map-Reduce Framework={MAP_OUTPUT_MATERIALIZED_BYTES=1609, MAP_INPUT_RECORDS=3576393, REDUCE_SHUFFLE_BYTES=0, SPILLED_RECORDS=858, MAP_OUTPUT_BYTES=189548829, COMMITTED_HEAP_BYTES=1521614848, CPU_MILLISECONDS=0, SPLIT_RAW_BYTES=1010, COMBINE_INPUT_RECORDS=14305902, REDUCE_INPUT_RECORDS=114, REDUCE_INPUT_GROUPS=114, COMBINE_OUTPUT_RECORDS=444, PHYSICAL_MEMORY_BYTES=0, REDUCE_OUTPUT_RECORDS=114, VIRTUAL_MEMORY_BYTES=0, MAP_OUTPUT_RECORDS=14305572}, FileSystemCounters={FILE_BYTES_READ=910481, FILE_BYTES_WRITTEN=1028473}, File Output Format Counters ={BYTES_WRITTEN=2421}}}}\nmax score:  1.0\nTOTAL urls: 3576393\nstatus 0 (null):    3576393\navg score:  1.0\nWebTable statistics: done\nmin score:  55.0\nretry 0:    3576393\njobs:   {db_stats-job_local_0001={jobID=job_local_0001, jobName=db_stats, counters={File Input Format Counters ={BYTES_READ=0}, Map-Reduce Framework={MAP_OUTPUT_MATERIALIZED_BYTES=1609, MAP_INPUT_RECORDS=3576393, REDUCE_SHUFFLE_BYTES=0, SPILLED_RECORDS=858, MAP_OUTPUT_BYTES=189548829, COMMITTED_HEAP_BYTES=1521614848, CPU_MILLISECONDS=0, SPLIT_RAW_BYTES=1010, COMBINE_INPUT_RECORDS=14305902, REDUCE_INPUT_RECORDS=114, REDUCE_INPUT_GROUPS=114, COMBINE_OUTPUT_RECORDS=444, PHYSICAL_MEMORY_BYTES=0, REDUCE_OUTPUT_RECORDS=114, VIRTUAL_MEMORY_BYTES=0, MAP_OUTPUT_RECORDS=14305572}, FileSystemCounters={FILE_BYTES_READ=910481, FILE_BYTES_WRITTEN=1028473}, File Output Format Counters ={BYTES_WRITTEN=2421}}}}\nmax score:  1.0\nTOTAL urls: 3576393\nstatus 0 (null):    3576393\navg score:  1.0\n</code></pre>\n", "creation_date": 1366906120, "score": 0},
{"title": "Exclude urls without &#39;www&#39; from Nutch 1.7 crawl", "view_count": 1031, "is_answered": true, "answers": [{"question_id": 19731904, "owner": {"user_id": 1552109, "link": "http://stackoverflow.com/users/1552109/brian", "user_type": "registered", "reputation": 87}, "body": "<p>There are at least a couple solutions.</p>\n\n<p>1.) urlfilter-regex plugin</p>\n\n<p>If you don't want to crawl the non-www pages at all, or else filter them at a later stage such as at index time, that is what the urlfilter-regex plugin is for.  It lets you mark any URLs matching the regex patterns starting with \"+\" to  be crawled.  Anything that does not match a regex prefixed with a \"+\" will not be crawled.  Additionally in case you want to specify a general pattern but exclude certain URLs, you can use a \"-\" prefix to specify URLs to subsequently exclude.  </p>\n\n<p>In your case you would use a rule like:</p>\n\n<pre><code>+^(https?://)?www\\.\n</code></pre>\n\n<p>This will match anything that starts with:</p>\n\n<pre><code>https://www.\nhttp://www.\nwww.\n</code></pre>\n\n<p>and therefore will only allow such URLs to be crawled.</p>\n\n<p>Based on the fact that the URLs listed were already not being excluded given your regex-urlfilter, it means either the plugin wasn't turned on in your nutch-site.xml, or else it is not pointed at that file.</p>\n\n<p>In nutch-site.xml you have to specify regex-urlfilter in the list of plugins, e.g.:</p>\n\n<pre><code>&lt;property&gt;\n  &lt;name&gt;plugin.includes&lt;/name&gt;\n  &lt;value&gt;protocol-httpclient|urlfilter-regex|parse-(html|tika)|index-basic|query-(basic|site|url)|response-(json|xml)|urlnormalizer-(pass|regex|basic)&lt;/value&gt;\n&lt;/property&gt;\n</code></pre>\n\n<p>Additionally check that the property specifying which file to use is not over-written in nutch-site.xml and is correct in nutch-default.xml.  It should be:</p>\n\n<pre><code>&lt;property&gt;\n  &lt;name&gt;urlfilter.regex.file&lt;/name&gt;\n  &lt;value&gt;regex-urlfilter.txt&lt;/value&gt;\n  &lt;description&gt;Name of file on CLASSPATH containing regular expressions\n  used by urlfilter-regex (RegexURLFilter) plugin.&lt;/description&gt;\n&lt;/property&gt;\n</code></pre>\n\n<p>and regex-urlfilter.txt should be in the conf directory for nutch.</p>\n\n<p>There is also the option to only perform the filtering at different steps, e.g., index-time, if you only want to filter than.</p>\n\n<p>2.) solrdedup command</p>\n\n<p>If the URLs point to the exact same page, which I am guessing is the case here, they can be removed by running the nutch command to delete duplicates after crawling:\n<a href=\"http://wiki.apache.org/nutch/bin/nutch%20solrdedup\" rel=\"nofollow\">http://wiki.apache.org/nutch/bin/nutch%20solrdedup</a></p>\n\n<p>This will use the digest values computed from the text of each indexed page to find any pages that were the same and delete all but one.</p>\n\n<p>However you would have to modify the plugin to change which duplicate is kept if you want to specifically keep the \"www\" ones.  </p>\n\n<p>3.) Write a custom indexing filter plugin</p>\n\n<p>You can write a plugin that reads the URL field of a nutch document and converts it in any way you want before indexing.  This would give you more flexible than using an existing plugin like urlnormalize-regex.</p>\n\n<p>It is actually very easy to make plugins and add them to Nutch, which is one of the great things about it.  As a starting point you can copy and look at one of the other plugins including with nutch that implement IndexingFilter, such as the index-basic plugin. </p>\n\n<p>You can also find a lot of examples:\n<a href=\"http://wiki.apache.org/nutch/WritingPluginExample\" rel=\"nofollow\">http://wiki.apache.org/nutch/WritingPluginExample</a>\n<a href=\"http://sujitpal.blogspot.com/2009/07/nutch-custom-plugin-to-parse-and-add.html\" rel=\"nofollow\">http://sujitpal.blogspot.com/2009/07/nutch-custom-plugin-to-parse-and-add.html</a></p>\n", "creation_date": 1383342349, "is_accepted": false, "score": 1, "last_activity_date": 1383342349, "answer_id": 19735900}], "question_id": 19731904, "tags": ["regex", "solr", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/19731904/exclude-urls-without-www-from-nutch-1-7-crawl", "last_activity_date": 1383342349, "owner": {"user_id": 1880601, "answer_count": 2, "creation_date": 1354744967, "accept_rate": 18, "view_count": 78, "reputation": 186}, "body": "<p>I'm currently using Nutch 1.7 to crawl my domain. My issue is specific to URLs being indexed as www vs. non-www.</p>\n\n<p>Specifically, after firing the crawl and index to Solr 4.5 then validating the results on the front-end with AJAX Solr, the search results page lists results/pages that are both 'www' and '' urls such as:</p>\n\n<pre><code>www.mywebsite.com\nmywebsite.com\nwww.mywebsite.com/page1.html\nmywebsite.com/page1.html\n</code></pre>\n\n<p>My understanding is that the url filtering aka regex-urlfilter.txt needs modification. Are there any regex/nutch experts that could suggest a solution?</p>\n\n<p>Here is the code on <a href=\"http://pastebin.com/Cp6vUxPR\" rel=\"nofollow\">pastebin</a>.</p>\n", "creation_date": 1383326509, "score": 1},
{"title": "Running Nutch 1.6 on Hadoop 2.2.0", "view_count": 166, "is_answered": false, "question_id": 19716144, "tags": ["hadoop", "nutch", "yarn"], "answer_count": 0, "link": "http://stackoverflow.com/questions/19716144/running-nutch-1-6-on-hadoop-2-2-0", "last_activity_date": 1383251633, "owner": {"age": 32, "answer_count": 11, "creation_date": 1243248810, "user_id": 112044, "accept_rate": 90, "view_count": 32, "location": "United States", "reputation": 550}, "body": "<p>Has anyone tried running Nutch 1.6 on Hadoop 2.2.0?</p>\n\n<p>I tried running it out of the box and the first problem I got was with the Injector showing the following errors:</p>\n\n<pre><code>ERROR crawl.Injector: Injector: java.lang.IllegalArgumentException: Wrong FS:     \nhdfs://................., expected: file:///\n</code></pre>\n\n<p>Anyone have any ideas how to resolve this? Thanks!</p>\n", "creation_date": 1383251633, "score": 1},
{"title": "Nutch Crawling and ignoring new urls", "view_count": 999, "owner": {"age": 32, "answer_count": 47, "creation_date": 1359241502, "user_id": 2014559, "accept_rate": 100, "view_count": 76, "location": "United Kingdom", "reputation": 425}, "is_answered": true, "answers": [{"question_id": 19482798, "owner": {"user_id": 2623052, "accept_rate": 45, "link": "http://stackoverflow.com/users/2623052/allan-macmillan", "user_type": "registered", "reputation": 866}, "body": "<p>Article <a href=\"http://amac4.blogspot.com/2013/08/nutch-re-crawling.html\" rel=\"nofollow\">Here</a> explains the crawl process in sufficient depth</p>\n", "creation_date": 1383220342, "is_accepted": true, "score": 3, "last_activity_date": 1383220342, "answer_id": 19705793}], "question_id": 19482798, "tags": ["nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/19482798/nutch-crawling-and-ignoring-new-urls", "last_activity_date": 1383220342, "accepted_answer_id": 19705793, "body": "<p>I have an issue where I try to issue a new crawl on something ive already crawled, but with some new URLS.</p>\n\n<p>so first i have</p>\n\n<p>urls/urls.txt -> www.somewebsite.com</p>\n\n<p>i then issue the command</p>\n\n<p><code>bin/nutch crawl urls -dir crawl -depth 60 -threads 50</code></p>\n\n<p>i then update urls/urls.txt -> remove www.somewebsite.com -> add www.anotherwebsite.com</p>\n\n<p>i issue the command </p>\n\n<p><code>bin/nutch inject crawl urls</code></p>\n\n<p><code>bin/nutch crawl urls -dir crawl -depth 60 -threads 50</code></p>\n\n<p>What i would expect here, is that www.anotherwebsite.com is injected into the existing 'crawl' db, and when crawl is issued again it should only crawl the new website ive added www.anotherwebsite.com (as the refetch for the original is set to 30 days)</p>\n\n<p>What I have experienced is that either </p>\n\n<p>1.) no website is crawled</p>\n\n<p>2.) only the original website is crawled</p>\n\n<p>'sometimes' if i leave it for a few hours it starts working and picks up the new website and crawls both the old website and new one (even though the refetch time is set to 30 days)</p>\n\n<p>its very weird and unpredictable behaviour.</p>\n\n<p>Im pretty sure my regex-urlfilter file is set correctly, and my nutch-site / nutch-default is all setup with defaults (near enough).</p>\n\n<p>Questions: </p>\n\n<p>can anyone explain simply (with commands) what is happening during each crawl, and how to update an existing crawl db with some new urls?</p>\n\n<p>can anyone explain (with commands) how i force a recrawl of 'all' urls in the crawl db? - i have issued a readdb and checked the refetch times, and most are set to a month, but what if i want to refetch again sooner?</p>\n", "creation_date": 1382302745, "score": 1},
{"title": "Nutch crawl command", "view_count": 629, "is_answered": false, "answers": [{"question_id": 19592098, "owner": {"user_id": 2225199, "link": "http://stackoverflow.com/users/2225199/nimeshjm", "user_type": "registered", "reputation": 1137}, "body": "<p>You can run bin/nutch solrindex and pass the crawl and segments folders in the parameters.</p>\n\n<p>Nutch will index all documents but will not create duplicates as it will use the ID field to determine if they have already been inserted.</p>\n", "creation_date": 1382717604, "is_accepted": false, "score": 0, "last_activity_date": 1382717604, "answer_id": 19594750}], "question_id": 19592098, "tags": ["solr", "web-crawler", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/19592098/nutch-crawl-command", "last_activity_date": 1382972908, "owner": {"user_id": 1830069, "answer_count": 10, "creation_date": 1353080229, "accept_rate": 63, "view_count": 103, "reputation": 543}, "body": "<p>For Nutch 2.2.1, I am aware of two crawl commands - bin/nutch (step by step), bin/crawl (all in one)</p>\n\n<p>I know how to specify a crawl ID for <code>bin/crawl</code> command. Similarly, how to specify a crawl ID for <code>bin/nutch</code> command?</p>\n\n<p>The reason I am asking is, I ran a large crawl job using <code>all-in-one crawl command \"bin/crawl\"</code> specifying a crawl ID, it broke while indexing in Solr for 9th crawl iteration. Now, I just want to run one step <code>\"bin/nutch solrindex\"</code> command for just that interrupted 9th iteration to complete the solr indexing. How should I specify crawlID in \"<code>bin/nutch solrindex</code>\" command? What is the syntax?</p>\n\n<p>I have all the crawl data stored in a HBase table \"webpage_test\"</p>\n", "creation_date": 1382710064, "score": 1},
{"title": "nutch: crawling through goagent proxy", "view_count": 985, "is_answered": true, "answers": [{"question_id": 11393784, "owner": {"user_id": 1204751, "accept_rate": 76, "link": "http://stackoverflow.com/users/1204751/crs", "user_type": "registered", "reputation": 283}, "body": "<p>I faced somewhat similar problem. Not sure if it is the same. I set path for these from terminal and ran Nutch. It worked fine. I did not do any other additional settings in Nutch. It worked on Ubuntu but somehow it is not working on mac. Let me know if it works.</p>\n\n<pre><code>    export http_proxy=http://server-ip:port/\n    export http_proxy=http://127.0.0.1:8087/\n    export http_proxy=http://proxy-server.mycorp.com:8087/\n</code></pre>\n", "creation_date": 1341869412, "is_accepted": false, "score": 1, "last_activity_date": 1341869412, "answer_id": 11403473}, {"question_id": 11393784, "owner": {"user_id": 2794923, "link": "http://stackoverflow.com/users/2794923/user2794923", "user_type": "registered", "reputation": 71}, "body": "<p>just add these properties to nutch-site.xml (replace XXXX with the correct values)</p>\n\n<pre><code>&lt;property&gt;\n  &lt;name&gt;http.proxy.host&lt;/name&gt;\n  &lt;value&gt;XXXXXX&lt;/value&gt;\n  &lt;description&gt;The proxy hostname.  If empty, no proxy is used.&lt;/description&gt;\n&lt;/property&gt;\n\n&lt;property&gt;\n  &lt;name&gt;http.proxy.port&lt;/name&gt;\n  &lt;value&gt;XXXXXXXX&lt;/value&gt;\n  &lt;description&gt;The proxy port.&lt;/description&gt;\n&lt;/property&gt;\n</code></pre>\n\n<p>And if you need auth:</p>\n\n<pre><code>&lt;property&gt;\n  &lt;name&gt;http.proxy.username&lt;/name&gt;\n  &lt;value&gt;&lt;/value&gt;\n  &lt;description&gt;Username for proxy. This will be used by\n  'protocol-httpclient', if the proxy server requests basic, digest\n  and/or NTLM authentication. To use this, 'protocol-httpclient' must\n  be present in the value of 'plugin.includes' property.\n  NOTE: For NTLM authentication, do not prefix the username with the\n  domain, i.e. 'susam' is correct whereas 'DOMAIN\\susam' is incorrect.\n  &lt;/description&gt;\n&lt;/property&gt;\n\n&lt;property&gt;\n  &lt;name&gt;http.proxy.password&lt;/name&gt;\n  &lt;value&gt;&lt;/value&gt;\n  &lt;description&gt;Password for proxy. This will be used by\n  'protocol-httpclient', if the proxy server requests basic, digest\n  and/or NTLM authentication. To use this, 'protocol-httpclient' must\n  be present in the value of 'plugin.includes' property.\n  &lt;/description&gt;\n&lt;/property&gt;\n</code></pre>\n", "creation_date": 1382704989, "is_accepted": false, "score": 4, "last_activity_date": 1382704989, "answer_id": 19590207}], "question_id": 11393784, "tags": ["proxy", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/11393784/nutch-crawling-through-goagent-proxy", "last_activity_date": 1382704989, "owner": {"age": 27, "answer_count": 0, "creation_date": 1335064291, "user_id": 1349066, "view_count": 6, "location": "China", "reputation": 8}, "body": "<p><p>I'm learning crawling pages with Nutch now. But there is a problem that I can't fix.\nI want to crawl pages via a local proxy server, which address is 127.0.0.1:8087. And the server works well(my FireFox can use it well). In the Nutch-conf file conf/nutch-site.xml, I added some properties as follows:<p>\n<strong>http.agent.host-->   127.0.0.1</strong>\n<p>\n<strong>http.proxy.port-->8087</strong><br>\n<p>\nMy local server doesn't need any authentication.\n<p>\nNutch can run successfully but do not request via the proxy.\n<p>\nTHANKS.</p>\n", "creation_date": 1341831746, "score": 1},
{"title": "Hadoop HBase Pseudo mode - RegionServer disconnects after some time", "view_count": 175, "is_answered": true, "answers": [{"question_id": 19384899, "owner": {"user_id": 2817697, "accept_rate": 77, "link": "http://stackoverflow.com/users/2817697/rkh", "user_type": "registered", "reputation": 531}, "body": "<p>Try killing the master then start it up again... The dead server state is in memory...\nhope that helps!</p>\n", "creation_date": 1382638872, "is_accepted": false, "score": 1, "last_activity_date": 1382638872, "answer_id": 19573623}], "question_id": 19384899, "tags": ["hadoop", "hbase", "hdfs", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/19384899/hadoop-hbase-pseudo-mode-regionserver-disconnects-after-some-time", "last_activity_date": 1382638872, "owner": {"user_id": 1830069, "answer_count": 10, "creation_date": 1353080229, "accept_rate": 63, "view_count": 103, "reputation": 543}, "body": "<p>Please find the attached screenshot of Hbase-master log. I have tried all sorts of settings yet I couldn't overcome this issue. I made sure I don't have 127.0.1.1 in my /etc/hosts.</p>\n\n<p>I am using <strong>Apache Hadoop 0.20.205.0 and Apache HBase 0.90.6 in Pseudo distributed</strong>. I am using Nutch 2.2.1 and trying to store crawled data in HBase Pseudo mode. I am using bin/crawl all-in-one command. Please help!\n<img src=\"http://i.stack.imgur.com/FPcZM.png\" alt=\"enter image description here\"></p>\n", "creation_date": 1381850153, "score": 0},
{"title": "How to skip documents with empty content field during Nutch to Solr indexing?", "view_count": 438, "is_answered": true, "answers": [{"last_edit_date": 1382304313, "owner": {"user_id": 2225199, "link": "http://stackoverflow.com/users/2225199/nimeshjm", "user_type": "registered", "reputation": 1137}, "body": "<p>You might need to implement a new Nutch filter that discards the document if the content is empty.</p>\n\n<p>You can get more information on how to write a plugin following this link: <a href=\"https://wiki.apache.org/nutch/AboutPlugins\" rel=\"nofollow\">https://wiki.apache.org/nutch/AboutPlugins</a></p>\n\n<p>EDIT:<br>\nI wrote a simple plugin just as an example. \nIt looks at the \"content\" field and if it's empty it will ignore the document and not index it.</p>\n\n<p>You can get it from here: <a href=\"https://github.com/nimeshjm/index-discardemptycontent\" rel=\"nofollow\">https://github.com/nimeshjm/index-discardemptycontent</a></p>\n", "question_id": 19388682, "creation_date": 1381961185, "is_accepted": false, "score": 2, "last_activity_date": 1382304313, "answer_id": 19414788}], "question_id": 19388682, "tags": ["apache", "solr", "indexing", "nutch", "web-crawler"], "answer_count": 1, "link": "http://stackoverflow.com/questions/19388682/how-to-skip-documents-with-empty-content-field-during-nutch-to-solr-indexing", "last_activity_date": 1382304313, "owner": {"user_id": 2868351, "view_count": 8, "answer_count": 0, "creation_date": 1381429189, "reputation": 1}, "body": "<p>During solrindex, how to tell Nutch to skip indexing those documents with an empty content field?</p>\n\n<p>I found <a href=\"http://www.atlantbh.com/precise-data-extraction-with-apache-nutch/\" rel=\"nofollow\">http://www.atlantbh.com/precise-data-extraction-with-apache-nutch/</a>, but the index-omit plugin will only allow Nutch to filter those documents without certain metatag fields, not general fields such as content. </p>\n", "creation_date": 1381862344, "score": 0},
{"title": "&#39;hbase.client.scanner.caching&#39; default value for HBase 0.90.6?", "view_count": 655, "is_answered": false, "answers": [{"question_id": 19481081, "owner": {"user_id": 1347281, "accept_rate": 85, "link": "http://stackoverflow.com/users/1347281/vidya", "user_type": "registered", "reputation": 15387}, "body": "<p>Pretty sure it's 100. The <a href=\"http://hbase.apache.org/book/config.files.html\" rel=\"nofollow\">book</a> says so, and 1 would be an odd default for a value defined like this:</p>\n\n<p><em>Number of rows that will be fetched when calling next on a scanner if it is not served from (local, client) memory. Higher caching values will enable faster scanners but will eat up more memory and some calls of next may take longer and longer times when the cache is empty.</em></p>\n", "creation_date": 1382294489, "is_accepted": false, "score": 0, "last_activity_date": 1382294489, "answer_id": 19481304}], "question_id": 19481081, "tags": ["hadoop", "hbase", "hdfs", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/19481081/hbase-client-scanner-caching-default-value-for-hbase-0-90-6", "last_activity_date": 1382294489, "owner": {"user_id": 1830069, "answer_count": 10, "creation_date": 1353080229, "accept_rate": 63, "view_count": 103, "reputation": 543}, "body": "<p>What is the default value of '<strong>hbase.client.scanner.caching</strong>' for <strong>HBase 0.90.6</strong> version in particular? Is it 1 or 100?</p>\n", "creation_date": 1382293285, "score": 0},
{"title": "java.io.IOException: Job failed", "view_count": 2022, "is_answered": false, "answers": [{"last_edit_date": 1347284582, "owner": {"user_id": 1150329, "accept_rate": 89, "link": "http://stackoverflow.com/users/1150329/tejas-patil", "user_type": "registered", "reputation": 4793}, "body": "<p>Looks like Solr is not configured right. (Please ensure that the input linkdb, crawldb and segments are present in the location that you pass command line). </p>\n\n<p>Read </p>\n\n<ul>\n<li><a href=\"http://www.params.me/2011/07/apache-solr-14-set-up.html\" rel=\"nofollow\">Setting up Solr 1.4 with Apache Tomcat 6.X</a></li>\n<li><a href=\"http://www.params.me/2011/07/nutch-13-and-solr-integration.html\" rel=\"nofollow\">Nutch 1.3 and Solr Integration </a>.</li>\n</ul>\n", "question_id": 9916305, "creation_date": 1333996485, "is_accepted": false, "score": 0, "last_activity_date": 1347284582, "answer_id": 10077946}, {"question_id": 9916305, "owner": {"user_id": 350825, "accept_rate": 100, "link": "http://stackoverflow.com/users/350825/marcus-rickert", "user_type": "registered", "reputation": 3011}, "body": "<p>This error often occurs if the mapping of nutch result fields onto Solr field is incorrect or incomplete. This results in the \"update\" action being rejected by the Solr server. Unfortunately, at some point in the call chain this error is converted into a \"IO error\" which is a little misleading. My recommendation is to access the web console of the Solr server (which is accessible using the same URL as for the submissing of links, e.g. in this case <a href=\"http://some.solr.server:8983/solr/\" rel=\"nofollow\">http://some.solr.server:8983/solr/</a>) and go to to the logging tab. Errors concerning the mapping will show up there!</p>\n", "creation_date": 1382268034, "is_accepted": false, "score": 0, "last_activity_date": 1382268034, "answer_id": 19476757}], "question_id": 9916305, "tags": ["apache", "solr", "lucene", "tomcat6", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/9916305/java-io-ioexception-job-failed", "last_activity_date": 1382268034, "owner": {"age": 29, "answer_count": 8, "creation_date": 1316111769, "user_id": 947500, "view_count": 21, "location": "Sao Paulo, Brazil", "reputation": 43}, "body": "<p>I'm trying to index a site with \"Apache Nutch 1.4\" and when I run the command below, the following error occurs \"java.io.IOException: Job failed\"</p>\n\n<pre><code>bin/nutch solrindex http://localhost:8983/solr/ crawl/crawldb -linkdb crawl/linkdb crawl/segments/*\n</code></pre>\n\n<p>I installed \"Tomca6\" and \"Apache Solr 3.5.0\" to work with Nutch but unfortunately is not working</p>\n\n<p><strong>simulation</strong></p>\n\n<pre class=\"lang-csh prettyprint-override\"><code>root@debian:/usr/share/nutch/runtime/local$ bin/nutch solrindex     http://localhost:8983/solr/ crawl/crawldb -linkdb crawl/linkdb crawl/segments/*\nSolrIndexer: starting at 2012-03-28 18:45:25\nAdding 48 documents\njava.io.IOException: Job failed!\nroot@debian:/usr/share/nutch/runtime/local$\n</code></pre>\n\n<p>Can someone help me please?</p>\n", "creation_date": 1332972131, "score": 1},
{"title": "Nutch failing on deleting duplicates (on one solr core but not another)", "view_count": 903, "owner": {"user_id": 340648, "answer_count": 11, "creation_date": 1273781720, "accept_rate": 94, "view_count": 119, "location": "New York, NY, United States", "reputation": 695}, "is_answered": true, "answers": [{"question_id": 17913085, "owner": {"user_id": 1015067, "link": "http://stackoverflow.com/users/1015067/shad-amez", "user_type": "registered", "reputation": 130}, "body": "<p>As documents don't have the url field therefore the id of the documents empty, so its throwing a null pointer exception when it runs the below method.</p>\n\n<p>Below is the code of SolrDeleteDuplicate class  from nutch 1.7 trunk where the solr record is deleted by id field.</p>\n\n<pre><code>updateRequest.deleteById(solrRecord.id);\n</code></pre>\n\n<ul>\n<li><p>updateRequest => instance of org.apache.solr.client.solrj.request.UpdateRequest</p></li>\n<li><p>solrRecord => the solr Document which needs to be deleted.</p></li>\n<li>id => the id of the solr document which is read from the solrindex-mapping.xml present in the conf folder of the nutch distribution. (if this is null then it will throw an exception ) </li>\n</ul>\n", "creation_date": 1375098803, "is_accepted": true, "score": 0, "last_activity_date": 1375098803, "answer_id": 17923317}], "question_id": 17913085, "tags": ["apache", "hadoop", "solr", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/17913085/nutch-failing-on-deleting-duplicates-on-one-solr-core-but-not-another", "last_activity_date": 1382266796, "accepted_answer_id": 17923317, "body": "<p>I am having a Nutch problem that I cannot seem to debug. </p>\n\n<p>I started using Nutch to crawl and index our page to a solr core 1. And it worked fine. Job completed like it should.</p>\n\n<p>Though I wanted to start indexing or page to our solr core 0, along with other items that we want to index. </p>\n\n<p>The indexing is not the problem, it will crawl and index fine. But on core 0 it continues to fail on the deduplication task at the end of the index. I get the following error (below). From what I can tell, the schema.xml and solrconfig.xml files have all the same things across core0 and core1 except for in core0 the url field is no longer required as the other indexed items don't have a url, so the id field is the standard, required field across all of them. Could it be this that's causing the problem? what is the deduper trying to do and what is getting in its way? and how might I get passed this? thanks!:</p>\n\n<p><code>2013-07-26 16:55:17,797 INFO  solr.SolrIndexWriter - Indexing 157 documents\n2013-07-26 16:55:30,407 INFO  solr.SolrMappingReader - source: content dest: content\n2013-07-26 16:55:30,444 INFO  solr.SolrMappingReader - source: title dest: title\n2013-07-26 16:55:30,444 INFO  solr.SolrMappingReader - source: host dest: host\n2013-07-26 16:55:30,444 INFO  solr.SolrMappingReader - source: segment dest: segment\n2013-07-26 16:55:30,444 INFO  solr.SolrMappingReader - source: boost dest: boost\n2013-07-26 16:55:30,444 INFO  solr.SolrMappingReader - source: digest dest: digest\n2013-07-26 16:55:30,444 INFO  solr.SolrMappingReader - source: tstamp dest: tstamp\n2013-07-26 16:55:30,444 INFO  solr.SolrMappingReader - source: url dest: id\n2013-07-26 16:55:30,444 INFO  solr.SolrMappingReader - source: url dest: url\n2013-07-26 16:55:31,590 INFO  indexer.IndexingJob - Indexer: finished at 2013-07-26 16:55:31, elapsed: 00:00:19\n2013-07-26 16:55:31,593 INFO  solr.SolrDeleteDuplicates - SolrDeleteDuplicates: starting at 2013-07-26 16:55:31\n2013-07-26 16:55:31,593 INFO  solr.SolrDeleteDuplicates - SolrDeleteDuplicates: Solr url: http://&lt;domain&gt;:&lt;port&gt;/solr/core0/\n2013-07-26 16:55:32,043 WARN  mapred.FileOutputCommitter - Output path is null in cleanup\n2013-07-26 16:55:32,043 WARN  mapred.LocalJobRunner - job_local1142877999_0055\njava.lang.Exception: java.lang.NullPointerException\n        at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:354)\nCaused by: java.lang.NullPointerException\n        at org.apache.hadoop.io.Text.encode(Text.java:388)\n        at org.apache.hadoop.io.Text.set(Text.java:178)\n        at org.apache.nutch.indexer.solr.SolrDeleteDuplicates$SolrInputFormat$1.next(SolrDeleteDuplicates.java:270)\n        at org.apache.nutch.indexer.solr.SolrDeleteDuplicates$SolrInputFormat$1.next(SolrDeleteDuplicates.java:241)\n        at org.apache.hadoop.mapred.MapTask$TrackedRecordReader.moveToNext(MapTask.java:230)\n        at org.apache.hadoop.mapred.MapTask$TrackedRecordReader.next(MapTask.java:210)\n        at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:48)\n        at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:430)\n        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:366)\n        at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:223)\n        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)\n        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)\n        at java.util.concurrent.FutureTask.run(FutureTask.java:166)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n        at java.lang.Thread.run(Thread.java:679)</code></p>\n", "creation_date": 1375046922, "score": 0},
{"title": "How to show percentage completion of local file crawling using Nutch?", "view_count": 164, "is_answered": true, "answers": [{"last_edit_date": 1382043952, "owner": {"user_id": 2225199, "link": "http://stackoverflow.com/users/2225199/nimeshjm", "user_type": "registered", "reputation": 1137}, "body": "<p>This is not possible due to the nature of Nutch.</p>\n\n<p>Nutch crawls content by starting at a root node (the seed) and find any outlinks from it, crawling them iteratively until there are no more links or the crawl limit is reached.</p>\n\n<p>As Nutch does not know the full count of the nodes to crawl, it is not possible to calculate a percentage.</p>\n\n<p>You can find an overview of Nutch here: <a href=\"http://www.slideshare.net/digitalpebble/large-scale-crawling-with-apache-nutch\" rel=\"nofollow\">http://www.slideshare.net/digitalpebble/large-scale-crawling-with-apache-nutch</a></p>\n\n<p>An alternative to monitor a Nutch crawl: <a href=\"https://wiki.apache.org/nutch/MonitoringNutchCrawls\" rel=\"nofollow\">https://wiki.apache.org/nutch/MonitoringNutchCrawls</a></p>\n\n<p>EDIT:\nI suppose you could get the indexed documents count out of SOLR, or write a plugin that increments a counter every time a file is crawled...</p>\n\n<p>The bigger question is, what problem are you trying to solve?</p>\n", "question_id": 19387264, "creation_date": 1381961894, "is_accepted": false, "score": 1, "last_activity_date": 1382043952, "answer_id": 19414948}], "question_id": 19387264, "tags": ["nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/19387264/how-to-show-percentage-completion-of-local-file-crawling-using-nutch", "last_activity_date": 1382043952, "owner": {"user_id": 125470, "answer_count": 35, "creation_date": 1245368692, "accept_rate": 44, "view_count": 440, "reputation": 2518}, "body": "<p>I wonder, how to show the percentage of completion of local file crawling? </p>\n\n<p>I am going to use Nutch to crawl a single shared disk.</p>\n\n<p>Update:</p>\n\n<p>What if I use \"ls -R\" or \"find ~\" to get all the filenames in advance, and store them as seed? In that way we know the total number of files.</p>\n", "creation_date": 1381857809, "score": -1},
{"title": "how can i retrieve page informations in solr?", "view_count": 28, "is_answered": false, "answers": [{"question_id": 19428500, "owner": {"user_id": 967975, "link": "http://stackoverflow.com/users/967975/mike-r", "user_type": "registered", "reputation": 518}, "body": "<p>The text of the page is stored in a field named \"content\". Note that this is not the raw page, but a parsed version of the page. The searchable content should appear here, but not things like meta tags and javascript.</p>\n", "creation_date": 1382037085, "is_accepted": false, "score": 0, "last_activity_date": 1382037085, "answer_id": 19435203}], "question_id": 19428500, "tags": ["html", "solr", "hyperlink", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/19428500/how-can-i-retrieve-page-informations-in-solr", "last_activity_date": 1382037085, "owner": {"user_id": 2834966, "view_count": 3, "answer_count": 0, "creation_date": 1380632565, "reputation": 6}, "body": "<p>I'm new in nutch and solr.\nI use nutch to crawl web sites and I use solr to indexing theese pages.\nIs there a method for retrieve the content of a link in the solr database?</p>\n\n<p>for Example</p>\n\n<ul>\n<li>if I have indexing <a href=\"http://www.prova.com/prova.html\" rel=\"nofollow\">http://www.prova.com/prova.html</a></li>\n<li>this page contains the text \"this is a new page\"</li>\n<li>in solr, is there the text page saved somewhere?</li>\n</ul>\n\n<p>Thanks</p>\n\n<p>Danilo</p>\n", "creation_date": 1382017590, "score": 0},
{"title": "How to recrawle nutch", "view_count": 2399, "is_answered": true, "answers": [{"question_id": 13873694, "owner": {"user_id": 994000, "link": "http://stackoverflow.com/users/994000/betolink", "user_type": "registered", "reputation": 83}, "body": "<p>This post is a bit outdated but still valid for the most parts: <a href=\"http://pascaldimassimo.com/2010/06/11/how-to-re-crawl-with-nutch/\" rel=\"nofollow\">http://pascaldimassimo.com/2010/06/11/how-to-re-crawl-with-nutch/</a> perhaps the last crawled pages are the ones that change the most. Nutch uses an adaptative algorithm to schedule re-crawls, so when a page is very static it should not be re-crawled very often. You can override how often you want to recrawl using nutch-site.xml. Also, the seed.txt file is supposed to be a seed list, once that you inject the URLs Nutch does not use it anymore(unless you manually re-inject it again) </p>\n\n<p>Another configuration that may help is your regex-urlfilter.txt, if you want to point to an specific place or exclude certain domains/pages etc.</p>\n\n<p>Cheers.</p>\n", "creation_date": 1356316563, "is_accepted": false, "score": 1, "last_activity_date": 1356316563, "answer_id": 14016455}, {"last_edit_date": 1360062361, "owner": {"user_id": 1999894, "accept_rate": 33, "link": "http://stackoverflow.com/users/1999894/dragan-menoski", "user_type": "registered", "reputation": 456}, "body": "<p>I have the same problem. Nutch re-crawl only the old urls, even they not exist in seed.txt. </p>\n\n<p>First time when I start nutch I do the following: </p>\n\n<ul>\n<li><p>Add domain \"www.domain01.com\" in /root/Desktop/apache-nutch 2.1/runtime/local/urls/seed.txt (without quotes)</p></li>\n<li><p>In /root/Desktop/apache-nutch-2.1/runtime/local/conf/regex-urlfilter.txt, add new line: </p>\n\n<p># accept anything else<br>\n^http://([a-z0-9]*.)*www.domain01.com/sport/</p></li>\n<li><p>In /root/Desktop/apache-nutch-2.1/conf/regex-urlfilter.txt, add new line: </p>\n\n<p># accept anything else<br>\n^http://([a-z0-9]*.)*www.domain01.com/sport/</p></li>\n</ul>\n\n<p>... and everything was fine.</p>\n\n<p>Next I made the following changes: </p>\n\n<ul>\n<li><p>Remove www.domain01.com from /root/Desktop/apache-nutch-2.1/runtime/local/urls/seed.txt and add two new domains: www.domain02.com and www.domain03.com</p></li>\n<li><p>Remove www.domain01.com from /root/Desktop/apache-nutch-2.1/runtime/local/conf/regex-urlfilter.txt and add two new lines: </p>\n\n<p># accept anything else<br>\n&nbsp;&nbsp;&nbsp;^http://([a-z0-9]*.)<em>www.domain02.com/sport/<br>\n&nbsp;&nbsp;&nbsp;^http://([a-z0-9]</em>.)*www.domain03.com/sport/</p></li>\n<li><p>Remove www.domain01.com from /root/Desktop/apache-nutch-2.1/conf/regex-urlfilter.txt and add two new lines: </p>\n\n<p># accept anything else<br>\n&nbsp;&nbsp;&nbsp;^http://([a-z0-9]*.)<em>www.domain02.com/sport/<br>\n&nbsp;&nbsp;&nbsp;^http://([a-z0-9]</em>.)*www.domain03.com/sport/</p></li>\n</ul>\n\n<p>Next I execute the following commands:</p>\n\n<pre><code>updatedb\nbin/nutch inject urls\nbin/nutch generate urls\nbin/nutch updatedb\nbin/nutch crawl urls -depth 3\n</code></pre>\n\n<p>And nutch still crawl the www.domain01.com</p>\n\n<p>I don't know why ?</p>\n\n<p>I use Nutch 2.1 on Linux Debian 6.0.5 (x64). And linux is started on virtual machine on Windows 7 (x64).</p>\n", "question_id": 13873694, "creation_date": 1359989860, "is_accepted": false, "score": 2, "last_activity_date": 1360062361, "answer_id": 14689375}, {"question_id": 13873694, "owner": {"user_id": 1688068, "link": "http://stackoverflow.com/users/1688068/arul", "user_type": "registered", "reputation": 721}, "body": "<p>u just add ur nutch-site.xml below property tag. it works for me ,,,,,,,check it..........</p>\n\n<p><code>&lt;property&gt;\n  &lt;name&gt;file.crawl.parent&lt;/name&gt;\n  &lt;value&gt;false&lt;/value&gt;\n  &lt;/property</code></p>\n\n<p>and u just change regex-urlfilter.txt</p>\n\n<p># skip file: ftp: and mailto: urls \n #-^(file|ftp|mailto):<br>\n # accept anything else\n+.</p>\n\n<p>after remove that indexing dir manual or command also like..\nrm -r $NUTCH_HOME/indexdir</p>\n\n<p>after run ur crawl cammand...........</p>\n", "creation_date": 1381998561, "is_accepted": false, "score": 0, "last_activity_date": 1381998561, "answer_id": 19421705}], "question_id": 13873694, "tags": ["nutch", "web-crawler"], "answer_count": 3, "link": "http://stackoverflow.com/questions/13873694/how-to-recrawle-nutch", "last_activity_date": 1381998561, "owner": {"user_id": 1652512, "answer_count": 5, "creation_date": 1346945794, "accept_rate": 13, "view_count": 53, "reputation": 360}, "body": "<p>I am using Nutch 2.1 integrated with mysql. I had crawled 2 sites and Nutch successfully crawled them and stored the data into the Mysql. I am using Solr 4.0.0 for searching.</p>\n\n<p>Now my problem is, when I try to re-crawl some site like trailer.apple.com or any other site, it is always crawl the last crawled urls. Even I have removed the last crawled urls from seeds.txt file and entered the new Urls. But Nutch is not crawling the new Urls.</p>\n\n<p>Can anybody tell me, what actually I am doing wrong. </p>\n\n<p>Also please suggest me any Nutch Plugin that can help for crawling the videos and movies sites. </p>\n\n<p>Any help will really appreciable. </p>\n", "creation_date": 1355466083, "score": 1},
{"title": "Latest compatible versions of Nutch and Solr", "view_count": 2135, "owner": {"user_id": 1788590, "answer_count": 1, "creation_date": 1351687889, "accept_rate": 89, "view_count": 37, "location": "Turin, Italy", "reputation": 80}, "is_answered": true, "answers": [{"last_edit_date": 1368651369, "owner": {"user_id": 2225199, "link": "http://stackoverflow.com/users/2225199/nimeshjm", "user_type": "registered", "reputation": 1137}, "body": "<p>What OS are you using?</p>\n\n<p>In Windows Nutch 1.2 was the latest version I managed to get working.</p>\n\n<p>In Linux Nutch 1.6 works very well with SOLR.</p>\n\n<p>That was in combination with SOLR 3.5 (because of other dependencies) but there's no reason for it not to work with SOLR 3.6 or 4.3.</p>\n\n<p><a href=\"http://wiki.apache.org/nutch/NutchTutorial\" rel=\"nofollow\">Nutch tutorial</a></p>\n", "question_id": 16571463, "creation_date": 1368649770, "is_accepted": false, "score": 1, "last_activity_date": 1368651369, "answer_id": 16574393}, {"question_id": 16571463, "owner": {"user_id": 81194, "link": "http://stackoverflow.com/users/81194/claude", "user_type": "registered", "reputation": 827}, "body": "<p>I implemented Nutch 1.6 and Solr 3.6.2 into a production system earlier this year for a large financial firm. They work very well together.</p>\n", "creation_date": 1368713435, "is_accepted": false, "score": 1, "last_activity_date": 1368713435, "answer_id": 16589740}, {"last_edit_date": 1368714355, "owner": {"user_id": 1736002, "accept_rate": 60, "link": "http://stackoverflow.com/users/1736002/cguzel", "user_type": "registered", "reputation": 576}, "body": "<p>You can use Nutch 2.1 or Nutch 1.6. If you want to use HBase, have to use Nutch 2.x. Because nutch 1.6 not support Hbase. I use nutch 2.1, HBase 0.90.x or 0.94.5 , and Solr 4.3.0. </p>\n\n<p>There are major changes between the two Solr versions (Solr 3.x and Solr 4.x). You must choose one of them according to your requirement. Ex: Solr4 support <code>cloud</code> better.  </p>\n", "question_id": 16571463, "creation_date": 1368713674, "is_accepted": true, "score": 3, "last_activity_date": 1368714355, "answer_id": 16589829}], "question_id": 16571463, "tags": ["solr", "nutch"], "answer_count": 3, "link": "http://stackoverflow.com/questions/16571463/latest-compatible-versions-of-nutch-and-solr", "last_activity_date": 1381986908, "accepted_answer_id": 16589829, "body": "<p>I see different combinations of Nutch and Solr versions being used by people posting about this subject on the web.\nWhich are the latest stable (non beta) and compatible versions of Nutch and Solr that I can download and setup without building sources and just configuring ?</p>\n", "creation_date": 1368639159, "score": 2},
{"title": "how to avoid crawling shared disk without bring it down?", "view_count": 36, "is_answered": true, "answers": [{"question_id": 19387571, "owner": {"user_id": 2225199, "link": "http://stackoverflow.com/users/2225199/nimeshjm", "user_type": "registered", "reputation": 1137}, "body": "<p>You can set the number of threads and wait time between requests in conf/nutch-site.xml.</p>\n\n<p>Try overrinding these properties and set them to a value that you feel comfortable with:</p>\n\n<pre><code>&lt;property&gt;\n  &lt;name&gt;fetcher.threads.fetch&lt;/name&gt;\n  &lt;value&gt;10&lt;/value&gt;\n  &lt;description&gt;The number of FetcherThreads the fetcher should use.\n  This is also determines the maximum number of requests that are\n  made at once (each FetcherThread handles one connection). The total\n  number of threads running in distributed mode will be the number of\n  fetcher threads * number of nodes as fetcher has one map task per node.\n  &lt;/description&gt;\n&lt;/property&gt;\n\n&lt;property&gt;\n  &lt;name&gt;fetcher.threads.per.queue&lt;/name&gt;\n  &lt;value&gt;1&lt;/value&gt;\n  &lt;description&gt;This number is the maximum number of threads that\n    should be allowed to access a queue at one time.\n   &lt;/description&gt;\n&lt;/property&gt;\n</code></pre>\n", "creation_date": 1381962192, "is_accepted": false, "score": 1, "last_activity_date": 1381962192, "answer_id": 19415014}], "question_id": 19387571, "tags": ["nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/19387571/how-to-avoid-crawling-shared-disk-without-bring-it-down", "last_activity_date": 1381962192, "owner": {"user_id": 125470, "answer_count": 35, "creation_date": 1245368692, "accept_rate": 44, "view_count": 440, "reputation": 2518}, "body": "<p>I am using Nutch. I plan to crawl shared disk instead of internet website.</p>\n\n<p>One thing I am worry is that crawling it will make that disk become really slow.\nhow to avoid crawling shared disk without bring it down?</p>\n", "creation_date": 1381858839, "score": 0},
{"title": "Apache Nutch not adding internal links in a web page to fetchlist", "view_count": 974, "is_answered": false, "answers": [{"question_id": 19373714, "owner": {"user_id": 2225199, "link": "http://stackoverflow.com/users/2225199/nimeshjm", "user_type": "registered", "reputation": 1137}, "body": "<p>Your seed url is being ignored by the default filters, so your page is not being crawled.</p>\n\n<p>Edit the following files:</p>\n\n<p>conf/automaton-urlfilter.txt</p>\n\n<p>conf/regex-urlfilter.txt</p>\n\n<p>Replace</p>\n\n<pre><code># skip URLs containing certain characters as probable queries, etc.\n-.*[?*!@=].*\n</code></pre>\n\n<p>With</p>\n\n<pre><code># skip URLs containing certain characters as probable queries, etc.\n-.*[*!@].*\n</code></pre>\n", "creation_date": 1381960610, "is_accepted": false, "score": 0, "last_activity_date": 1381960610, "answer_id": 19414660}], "question_id": 19373714, "tags": ["web-crawler", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/19373714/apache-nutch-not-adding-internal-links-in-a-web-page-to-fetchlist", "last_activity_date": 1381960610, "owner": {"user_id": 1965449, "answer_count": 6, "creation_date": 1357790106, "accept_rate": 55, "view_count": 143, "reputation": 783}, "body": "<p>I am using Apache Nutch 1.7 and I am facing this problem with crawling using the URL <a href=\"http://www.ebay.com/sch/allcategories/all-categories/?_rdc=1\" rel=\"nofollow\">http://www.ebay.com/sch/allcategories/all-categories/?_rdc=1</a> as the seed URL, this URL has many internal links present in  the page and also has many external links to other domains , I am only interested in the internal links.</p>\n\n<p>However when this page is crawled the internal links in it are not added for fetching in the next round of fetching ( I have given a depth of 100).  I have already  set the db.ignore.internal.links as false ,but for some reason the internal links are not getting added to the next round of fetch list.</p>\n\n<p>On the other hand if I set the db.ignore.external.links as false, it correctly picks up all the external links from the page.</p>\n\n<p>This problem is not present in any other domains , can some tell me what is it with this particular page ?</p>\n\n<p>I have also attached the nucth-site.xml that I am using for your review, please advise.</p>\n", "creation_date": 1381813984, "score": 1},
{"title": "update solr index by nutch", "view_count": 834, "is_answered": false, "answers": [{"question_id": 14394065, "owner": {"user_id": 2014559, "accept_rate": 100, "link": "http://stackoverflow.com/users/2014559/andrew-butkus", "user_type": "registered", "reputation": 425}, "body": "<p>you will almost certainly need to inject new urls into an existing crawldb ( lookup bin/nutch inject), you can also issue a bin/nutch readdb ... -dump dumpfolder - this will show you how long it will be until those old urls are crawled again.</p>\n", "creation_date": 1381677944, "is_accepted": false, "score": 0, "last_activity_date": 1381677944, "answer_id": 19346845}, {"question_id": 14394065, "owner": {"user_id": 2623052, "accept_rate": 45, "link": "http://stackoverflow.com/users/2623052/allan-macmillan", "user_type": "registered", "reputation": 866}, "body": "<p>You can alter settings for urls that have become inactive or deleted so when you attempt to re-crawl them it marks them as DB_GONE.  Altering this setting will then delete these urls based on your own choices.</p>\n\n<pre><code>&lt;property&gt;\n  &lt;name&gt;db.update.purge.404&lt;/name&gt;\n  &lt;value&gt;true&lt;/value&gt;\n  &lt;description&gt;If true, updatedb will add purge records with status DB_GONE\n  from the CrawlDB.\n  &lt;/description&gt;\n&lt;/property&gt;\n</code></pre>\n\n<p>Check out <a href=\"http://amac4.blogspot.com/2013/08/nutch-re-crawling.html\" rel=\"nofollow\">http://amac4.blogspot.com/2013/08/nutch-re-crawling.html</a> for more details</p>\n", "creation_date": 1381740063, "is_accepted": false, "score": 0, "last_activity_date": 1381740063, "answer_id": 19356401}], "question_id": 14394065, "tags": ["solr", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/14394065/update-solr-index-by-nutch", "last_activity_date": 1381740063, "owner": {"user_id": 1773304, "answer_count": 0, "creation_date": 1351146931, "accept_rate": 0, "view_count": 3, "reputation": 16}, "body": "<p>I am using nutch 1.5 and solr 3.5. I would like to know the best way to update solr index through nutch. The seedlist.txt in nutch contains about a million urls. New urls will be added daily. Also, some of the urls will be removed or updated. </p>\n\n<p>The nutch command \"./nutch crawl urls -solr /solr/ -dir crawl -depth 1 -topN 10\" will pickup the newly added urls for indexing. However, the updated and removed urls will not be touched at all. </p>\n\n<p>By removing the crawl folder and re-index again will fix the \"add\" and \"update\" issue. However, it will take a long time to crawl a million urls as well as the \"remove\" urls index are still in Solr.</p>\n\n<p>The only want I know to remove Solr index is use the update command like \"update?commit=true&amp;stream.body=id:xxxx\". </p>\n\n<p>Am I in the correct direction ? or there is a better way to do that ?</p>\n", "creation_date": 1358493148, "score": 1},
{"title": "extracting information about outlink of url in nutch", "view_count": 315, "is_answered": false, "answers": [{"question_id": 17336108, "owner": {"user_id": 2014559, "accept_rate": 100, "link": "http://stackoverflow.com/users/2014559/andrew-butkus", "user_type": "registered", "reputation": 425}, "body": "<p>Generally you will have to write your own application to do this. You can provide additional flags to remove unecessary data.</p>\n\n<p><a href=\"http://wiki.apache.org/nutch/bin/nutch%20readseg\" rel=\"nofollow\">http://wiki.apache.org/nutch/bin/nutch%20readseg</a></p>\n\n<p>check here for what flags can be used to reduce the data.</p>\n\n<p>alternatively writing your own application using the hadoop FS library would be better, and then to extract the information directly programatically.</p>\n\n<p><a href=\"http://wiki.apache.org/hadoop/SequenceFile\" rel=\"nofollow\">http://wiki.apache.org/hadoop/SequenceFile</a></p>\n", "creation_date": 1381677370, "is_accepted": false, "score": 0, "last_activity_date": 1381677370, "answer_id": 19346771}], "question_id": 17336108, "tags": ["nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/17336108/extracting-information-about-outlink-of-url-in-nutch", "last_activity_date": 1381677370, "owner": {"user_id": 2526784, "answer_count": 0, "creation_date": 1372313344, "accept_rate": 64, "view_count": 53, "reputation": 77}, "body": "<p>I am quite new to nutch. I have crawled a site successfully using nutch 1.2 and extracted segment dump by <strong>readseg</strong> command but issue is that dump contains lot of information other than url and outlinks also if i want to analyse it, manual approach needs to be adopted. \nIt would be really great if there is any utiltiy, plugin which export link with out links in machine readable format like csv or sql.\nPlease suggest</p>\n", "creation_date": 1372314916, "score": 2},
{"title": "How to Crawl .pdf links using Apache Nutch", "view_count": 2192, "is_answered": true, "answers": [{"question_id": 17442052, "owner": {"user_id": 2627427, "link": "http://stackoverflow.com/users/2627427/olzhas", "user_type": "registered", "reputation": 26}, "body": "<p>you can either write your own own plugin, for pdf mimetype <br> or there is embedded apache-tika parser, that can retrieve text from pdf..</p>\n", "creation_date": 1381387282, "is_accepted": false, "score": -1, "last_activity_date": 1381387282, "answer_id": 19288793}, {"last_edit_date": 1381672568, "owner": {"user_id": 2225199, "link": "http://stackoverflow.com/users/2225199/nimeshjm", "user_type": "registered", "reputation": 1137}, "body": "<p>If you want Nutch to crawl and index your pdf documents, you have to enable document crawling and the Tika plugin:</p>\n\n<ol>\n<li><p>Document crawling</p>\n\n<p>1.1 Edit regex-urlfilter.txt and remove any occurence of \"pdf\"</p>\n\n<pre><code># skip image and other suffixes we can't yet parse\n# for a more extensive coverage use the urlfilter-suffix plugin\n-\\.(gif|GIF|jpg|JPG|png|PNG|ico|ICO|css|CSS|sit|SIT|eps|EPS|wmf|WMF|zip|ZIP|ppt|PPT|mpg|MPG|xls|XLS|gz|GZ|rpm|RPM|tgz|TGZ|mov|MOV|exe|EXE|jpeg|JPEG|bmp|BMP|js|JS)$\n</code></pre>\n\n<p>1.2 Edit suffix-urlfilter.txt and remove any occurence of \"pdf\"</p>\n\n<p>1.3 Edit nutch-site.xml, add \"parse-tika\" and \"parse-html\" in the plugin.includes section</p>\n\n<pre><code>&lt;property&gt;\n  &lt;name&gt;plugin.includes&lt;/name&gt;\n  &lt;value&gt;protocol-http|urlfilter-regex|parse-(html|tika|text)|index-(basic|anchor)|scoring-opic|urlnormalizer-(pass|regex|basic)&lt;/value&gt;\n  &lt;description&gt;Regular expression naming plugin directory names to\n  include.  Any plugin not matching this expression is excluded.\n  In any case you need at least include the nutch-extensionpoints plugin. By\n  default Nutch includes crawling just HTML and plain text via HTTP,\n  and basic indexing and search plugins. In order to use HTTPS please enable \n  protocol-httpclient, but be aware of possible intermittent problems with the \n  underlying commons-httpclient library.\n  &lt;/description&gt;\n&lt;/property&gt;\n</code></pre></li>\n<li><p>If what you really want is to download all pdf files from a page, you can use something like <a href=\"http://www.tenmax.com/teleport/pro/home.htm\" rel=\"nofollow\">Teleport in Windows</a> or Wget in *nix.</p></li>\n</ol>\n", "question_id": 17442052, "creation_date": 1381590367, "is_accepted": false, "score": 3, "last_activity_date": 1381672568, "answer_id": 19335648}], "question_id": 17442052, "tags": ["apache", "hadoop", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/17442052/how-to-crawl-pdf-links-using-apache-nutch", "last_activity_date": 1381672568, "owner": {"user_id": 2469835, "answer_count": 5, "creation_date": 1370845417, "accept_rate": 43, "view_count": 48, "reputation": 128}, "body": "<p>I got a website to crawl which includes some links to pdf files.\nI want nutch to crawl that link and dump them as .pdf files.\nI am using Apache Nutch1.6 also i am tring this in java as</p>\n\n<pre><code>ToolRunner.run(NutchConfiguration.create(), new Crawl(),\n                                 tokenize(crawlArg));\n SegmentReader.main(tokenize(dumpArg));\n</code></pre>\n\n<p>can some one help me on this</p>\n", "creation_date": 1372836341, "score": 4},
{"title": "how to solve segment: crawl/segments/* error", "view_count": 284, "owner": {"user_id": 2845368, "answer_count": 2, "creation_date": 1380866487, "accept_rate": 80, "view_count": 33, "location": "Bangalore, India", "reputation": 56}, "is_answered": true, "answers": [{"question_id": 19317994, "owner": {"user_id": 1881318, "link": "http://stackoverflow.com/users/1881318/tahagh", "user_type": "registered", "reputation": 619}, "body": "<p>This will happen when you want to parse an already parsed segment. Note that if you use the \"crawl\" command it also parses the segment.</p>\n\n<p>If you really want to parse again, just remove the crawl_parse directory inside your segment (i.e. crawl/segments/20131011173126/crawl_parse) and issue the parse command again.</p>\n", "creation_date": 1381648085, "is_accepted": true, "score": 0, "last_activity_date": 1381648085, "answer_id": 19342749}], "question_id": 19317994, "tags": ["nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/19317994/how-to-solve-segment-crawl-segments-error", "last_activity_date": 1381648085, "accepted_answer_id": 19342749, "body": "<p>while following this link i'm getting this error but can't figure out it \n<a href=\"http://wiki.apache.org/nutch/NutchTutorial\" rel=\"nofollow\">http://wiki.apache.org/nutch/NutchTutorial</a></p>\n\n<p>runtime/local$ bin/nutch parse $s1\nParseSegment: starting at 2013-10-11 17:43:36\nParseSegment: segment: crawl/segments/20131011173126\nException in thread \"main\" java.io.IOException: Segment already parsed!\n    at org.apache.nutch.parse.ParseOutputFormat.checkOutputSpecs(ParseOutputFormat.java:89)\n    at org.apache.hadoop.mapred.JobClient$2.run(JobClient.java:975)\n    at org.apache.hadoop.mapred.JobClient$2.run(JobClient.java:936)\n    at java.security.AccessController.doPrivileged(Native Method)\n    at javax.security.auth.Subject.doAs(Subject.java:415)\n    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)\n    at org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:936)\n    at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:910)\n    at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:1353)\n    at org.apache.nutch.parse.ParseSegment.parse(ParseSegment.java:213)\n    at org.apache.nutch.parse.ParseSegment.run(ParseSegment.java:247)\n    at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n    at org.apache.nutch.parse.ParseSegment.main(ParseSegment.java:220)</p>\n", "creation_date": 1381493961, "score": 0},
{"title": "Why does Nutch think it has already parsed all segments when it hasn&#39;t?", "view_count": 681, "is_answered": false, "answers": [{"question_id": 15539437, "owner": {"user_id": 1143887, "accept_rate": 67, "link": "http://stackoverflow.com/users/1143887/kich", "user_type": "registered", "reputation": 169}, "body": "<p>Nutch cannot reparse segments. To overcome this, you would need to delete few folders. Please check the mailing list discussion <a href=\"http://www.mail-archive.com/user@nutch.apache.org/msg09017.html\" rel=\"nofollow\">http://www.mail-archive.com/user@nutch.apache.org/msg09017.html</a>.</p>\n\n<p>You will get faster response at <a href=\"http://nutch.apache.org/mailing_lists.html\" rel=\"nofollow\">http://nutch.apache.org/mailing_lists.html</a></p>\n", "creation_date": 1363968511, "is_accepted": false, "score": 0, "last_activity_date": 1363968511, "answer_id": 15574891}, {"question_id": 15539437, "owner": {"user_id": 2301119, "link": "http://stackoverflow.com/users/2301119/nitramtheugly", "user_type": "registered", "reputation": 28}, "body": "<p>If you want to re-parse, go into the crawl/segments/ and </p>\n\n<pre><code>rm -rf parse_text parse_data crawl_parse\n</code></pre>\n\n<p>then you can run</p>\n\n<pre><code>bin/nutch parse crawldir/segments/&lt;segmentnumber&gt;\n</code></pre>\n", "creation_date": 1381534368, "is_accepted": false, "score": 0, "last_activity_date": 1381534368, "answer_id": 19328761}], "question_id": 15539437, "tags": ["parsing", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/15539437/why-does-nutch-think-it-has-already-parsed-all-segments-when-it-hasnt", "last_activity_date": 1381534368, "owner": {"user_id": 2193598, "view_count": 16, "answer_count": 0, "creation_date": 1363839096, "reputation": 16}, "body": "<p>I'm using Nutch 1.6 to crawl some forums and index them with Solr 1.6.2. I ran a test query on Solr and was surprised that there were only a few results. I was worried that there was a problem either with Nutch's parsing of the pages or with Solr's indexing. After snooping around I found out that Nutch hasn't parsed a lot of the pages it has retrieved:</p>\n\n<pre><code>bin/nutch readseg -list -dir crawl-mothering2/segments/\n\nNAME        GENERATED   FETCHED   PARSED\n20130228001531  23      27        9\n20130228003940  1430    1434      661\n20130228001829  202     206       105\n20130228061337  1068    1090      475\n20130228091009  1       2         0\n20130228085956  34      34        25\n20130228090348  44      45        34\n20130228090851  7       7         6\n20130228080438  364     374       192\n20130228030933  1774    1795      903\n20130228084205  168     169       63\n</code></pre>\n\n<p>But when I try to parse the segments, I get this:</p>\n\n<pre><code>bin/nutch parse crawl-mothering2/segments/*\nParseSegment: starting at 2013-03-21 00:20:43\nParseSegment: segment: crawl-mothering2/segments/20130228001531\nException in thread \"main\" java.io.IOException: Segment already parsed!\n    at org.apache.nutch.parse.ParseOutputFormat.checkOutputSpecs(ParseOutputFormat.java:89)\n    at org.apache.hadoop.mapred.JobClient$2.run(JobClient.java:889)\n    at org.apache.hadoop.mapred.JobClient$2.run(JobClient.java:850)\n    at java.security.AccessController.doPrivileged(Native Method)\n    at javax.security.auth.Subject.doAs(Subject.java:416)\n    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1121)\n    at org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:850)\n    at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:824)\n    at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:1261)\n    at org.apache.nutch.parse.ParseSegment.parse(ParseSegment.java:209)\n    at org.apache.nutch.parse.ParseSegment.run(ParseSegment.java:243)\n    at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n    at org.apache.nutch.parse.ParseSegment.main(ParseSegment.java:216)\n</code></pre>\n\n<p>What gives?</p>\n", "creation_date": 1363839697, "score": 0},
{"title": "Nutch No job jar file set", "view_count": 195, "is_answered": false, "question_id": 19323631, "tags": ["java", "hadoop", "nutch", "jobs"], "answer_count": 0, "link": "http://stackoverflow.com/questions/19323631/nutch-no-job-jar-file-set", "last_activity_date": 1381511253, "owner": {"user_id": 2301119, "view_count": 9, "answer_count": 1, "creation_date": 1366422922, "reputation": 28}, "body": "<p>I messed up my environment somehow and am getting this in hadoop.log</p>\n\n<pre><code>mapred.JobClient - No job jar file set.  User classes may not be found. See JobConf(Class) or JobConf#setJar(String)\n</code></pre>\n\n<p>I've seen it referenced all over, but haven't found a solution.   I'm running nutch 1.5 on a Mac.</p>\n\n<p>It's maddening because this just started happening after more than 18 months!  I'd very much appreciate someone smarter than me telling me what I've done and how to fix it.</p>\n", "creation_date": 1381511253, "score": 1},
{"title": "Nutch and HBase for production", "view_count": 436, "is_answered": false, "question_id": 19169183, "tags": ["hadoop", "hbase", "nutch", "gora"], "answer_count": 0, "link": "http://stackoverflow.com/questions/19169183/nutch-and-hbase-for-production", "last_activity_date": 1381418491, "owner": {"user_id": 1830069, "answer_count": 10, "creation_date": 1353080229, "accept_rate": 63, "view_count": 103, "reputation": 543}, "body": "<p>I am currently using Nutch 2.2.1 and HBase 0.90.4. I am expecting around 300K urls from about 10 URLS in seed. I have already generated so much while using Nutch 1.6. Since I want to manipulate data, I preferred to go Nutch 2.2.1 + HBase route. But I get all sorts of weird errors and crawl doesn't seem to progress.</p>\n\n<p>Various errors such as:</p>\n\n<ol>\n<li><p><strong>zookeeper.ClientCnxn - Session for server null, unexpected error, closing socket connection and attempting reconnect.</strong> - I get this more frequently</p></li>\n<li><p><strong>bin/crawl: line 164: killed</strong> - I get this error from fetch step and the crawling gets killed all of a sudden. </p></li>\n<li><p>RSS parse error</p></li>\n</ol>\n\n<p>I am using a all-in-one crawl command - <code>bin/crawl urls 1 http://localhost:8983/solr/ 10</code> </p>\n\n<pre><code>&lt;crawl&gt; &lt;seed-dir&gt; &lt;crawl-id&gt; &lt;solr-url&gt; &lt;number of rounds&gt;\n</code></pre>\n\n<p>Please suggest where am I going wrong. I have Nutch 2.2.1 <a href=\"http://wiki.apache.org/nutch/Nutch2Tutorial\" rel=\"nofollow\">installed</a> and HBase (standalone) installed as per the <a href=\"http://hbase.apache.org/book/quickstart.html\" rel=\"nofollow\">Quick start guide</a> recommended from Nutch site. I am not sure following HBase 0.90.4 standalone set up from <strong>Quick</strong> start guide link is sufficient to achieve 300K crawled urls.</p>\n\n<hr>\n\n<p>Edit # 1: RSS Parse Error - log information</p>\n\n<p><strong>Error tika.TikaParser - Error parsing <a href=\"http://www.###.###.##/###/abc.xml\">http://www.###.###.##/###/abc.xml</a>\norg.apache.tika.exception.TikaException: RSS parse error</strong></p>\n", "creation_date": 1380834169, "score": 0},
{"title": "Which Hadoop version recommended for HBase 0.90.6?", "view_count": 151, "owner": {"user_id": 1830069, "answer_count": 10, "creation_date": 1353080229, "accept_rate": 63, "view_count": 103, "reputation": 543}, "is_answered": true, "answers": [{"question_id": 19205022, "owner": {"user_id": 1830069, "accept_rate": 63, "link": "http://stackoverflow.com/users/1830069/sunskin", "user_type": "registered", "reputation": 543}, "body": "<p>I figured out <strong>Hadoop 0.20.205.0</strong> is the compatible version.\nI tried Hadoop 1.2.1 but it doesn't seem to work well with HBase 0.90.6</p>\n", "creation_date": 1381344023, "is_accepted": true, "score": 0, "last_activity_date": 1381344023, "answer_id": 19280048}], "question_id": 19205022, "tags": ["hadoop", "hbase", "hdfs", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/19205022/which-hadoop-version-recommended-for-hbase-0-90-6", "last_activity_date": 1381344023, "accepted_answer_id": 19280048, "body": "<p>I have no other option than to install HBase 0.90.6 as it is only recommended stable version for Nutch (web crawler) other than 0.90.4. </p>\n\n<p>My question, which Hadoop version is recommended for HBase 0.90.6 to work on pseudo distributed mode?</p>\n", "creation_date": 1381029072, "score": 0},
{"title": "How do I save the origin html file with Apache Nutch", "view_count": 4205, "owner": {"age": 27, "answer_count": 19, "creation_date": 1331644819, "user_id": 1266556, "accept_rate": 75, "view_count": 41, "location": "Shanghai, China", "reputation": 362}, "is_answered": true, "answers": [{"last_edit_date": 1334025003, "owner": {"user_id": 1150329, "accept_rate": 89, "link": "http://stackoverflow.com/users/1150329/tejas-patil", "user_type": "registered", "reputation": 4793}, "body": "<p>Well, nutch will write the crawled data in binary form so if if you want that to be saved in html format, you will have to modify the code. (this will be painful if you are new to nutch).</p>\n\n<p>If you want quick and easy solution for getting html pages:</p>\n\n<ol>\n<li>If the list of pages/urls that you intend to have is quite low, then better get it done with a script which invokes <code>wget</code> for each url.</li>\n<li>OR use <a href=\"http://www.httrack.com/\">HTTrack</a> tool. </li>\n</ol>\n\n<p><strong>EDIT:</strong></p>\n\n<p>Writing a your own nutch plugin will be great. Your problem will get solved plus you can contribute to nutch by submitting your work !!! If you are new to nutch (in terms of code &amp; design), then you will have to invest lot of time building a new plugin ... else its easy to do. </p>\n\n<p><strong>Few pointers for helping your initiative:</strong></p>\n\n<p><a href=\"http://wiki.apache.org/nutch/WritingPluginExample\">Here</a> is a page which talks about writing own nutch plugin.</p>\n\n<p>Start with <a href=\"http://javasourcecode.org/html/open-source/nutch/nutch-1.3/org/apache/nutch/fetcher/Fetcher.java.html\">Fetcher.java</a>. See lines 647-648. That is the place where you can get the fetched content on per url basis (for those pages which got fetched successfully).</p>\n\n<pre><code>pstatus = output(fit.url, fit.datum, content, status, CrawlDatum.STATUS_FETCH_SUCCESS);\nupdateStatus(content.getContent().length);\n</code></pre>\n\n<p>You should add code right after this to invoke your plugin. Pass <code>content</code> object to it. By now, you would have guessed that <code>content.getContent()</code> is the content for url you want. Inside the plugin code, write it to some file. Filename should be based on the url name else it will be difficult to work with that. Url can be obtained by <code>fit.url</code>.</p>\n", "question_id": 10007178, "creation_date": 1333855827, "is_accepted": true, "score": 9, "last_activity_date": 1334025003, "answer_id": 10060160}, {"question_id": 10007178, "owner": {"user_id": 1275577, "accept_rate": 100, "link": "http://stackoverflow.com/users/1275577/%c4%b0smet-alkan", "user_type": "registered", "reputation": 3787}, "body": "<p>You must do modifications in <a href=\"http://wiki.apache.org/nutch/RunNutchInEclipse\">run <strong>Nutch</strong> in <strong>Eclipse</strong></a>.</p>\n\n<p>When you are able to run, open Fetcher.java and add the lines between \"content saver\" command lines. </p>\n\n<pre><code>case ProtocolStatus.SUCCESS:        // got a page\n            pstatus = output(fit.url, fit.datum, content, status, CrawlDatum.STATUS_FETCH_SUCCESS, fit.outlinkDepth);\n            updateStatus(content.getContent().length);'\n\n\n            //------------------------------------------- content saver ---------------------------------------------\\\\\n            String filename = \"savedsites//\" + content.getUrl().replace('/', '-');  \n\n            File file = new File(filename);\n            file.getParentFile().mkdirs();\n            boolean exist = file.createNewFile();\n            if (!exist) {\n                System.out.println(\"File exists.\");\n            } else {\n                FileWriter fstream = new FileWriter(file);\n                BufferedWriter out = new BufferedWriter(fstream);\n                out.write(content.toString().substring(content.toString().indexOf(\"&lt;!DOCTYPE html\")));\n                out.close();\n                System.out.println(\"File created successfully.\");\n            }\n            //------------------------------------------- content saver ---------------------------------------------\\\\\n</code></pre>\n", "creation_date": 1334919716, "is_accepted": false, "score": 5, "last_activity_date": 1334919716, "answer_id": 10244963}, {"question_id": 10007178, "owner": {"user_id": 2014559, "accept_rate": 100, "link": "http://stackoverflow.com/users/2014559/andrew-butkus", "user_type": "registered", "reputation": 425}, "body": "<p>To update this answer -</p>\n\n<p>It is possible to post process the data from your crawldb segment folder, and read in the html (including other data nutch has stored) directly.</p>\n\n<pre><code>    Configuration conf = NutchConfiguration.create();\n    FileSystem fs = FileSystem.get(conf);\n\n    Path file = new Path(segment, Content.DIR_NAME + \"/part-00000/data\");\n    SequenceFile.Reader reader = new SequenceFile.Reader(fs, file, conf);\n\n    try\n    {\n            Text key = new Text();\n            Content content = new Content();\n\n            while (reader.next(key, content)) \n            {\n                    System.out.println(new String(content.GetContent()));\n            }\n    }\n    catch (Exception e)\n    {\n\n    }\n</code></pre>\n", "creation_date": 1381328182, "is_accepted": false, "score": 5, "last_activity_date": 1381328182, "answer_id": 19274588}], "question_id": 10007178, "tags": ["search-engine", "web-crawler", "nutch"], "answer_count": 3, "link": "http://stackoverflow.com/questions/10007178/how-do-i-save-the-origin-html-file-with-apache-nutch", "last_activity_date": 1381328182, "accepted_answer_id": 10060160, "body": "<p>I'm new to search engines and web crawlers. Now I want to store all the original pages in a particular web site as html files, but with Apache Nutch I can only get the binary database files. How do I get the original html files with Nutch? </p>\n\n<p>Does Nutch support it? If not, what other tools can I use to achieve my goal.(The tools that support distributed crawling are better.)</p>\n", "creation_date": 1333526795, "score": 3},
{"title": "Nutch: failed with: java.net.SocketException: Connection reset", "view_count": 588, "is_answered": false, "answers": [{"question_id": 19254601, "owner": {"user_id": 2586595, "link": "http://stackoverflow.com/users/2586595/mohsen89z", "user_type": "registered", "reputation": 128}, "body": "<p>You have to indicate url's protocol on your seedlist! for example:</p>\n\n<pre><code>http://stackoverflow.com/\nhttps://google.com\nftp://foo.bar\n</code></pre>\n", "creation_date": 1381319390, "is_accepted": false, "score": 0, "last_activity_date": 1381319390, "answer_id": 19271210}], "question_id": 19254601, "tags": ["hadoop", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/19254601/nutch-failed-with-java-net-socketexception-connection-reset", "last_activity_date": 1381319390, "owner": {"user_id": 1458148, "answer_count": 1, "creation_date": 1339748481, "accept_rate": 0, "view_count": 76, "reputation": 467}, "body": "<p>I run nutch in my hadoop cluster.\nWhen the job gets to the step <strong>fetch data</strong> I get <code>java.net.SocketException: Connection reset</code>. Here's the full stacktrace:</p>\n\n<pre><code>2013-10-09 00:34:05,922 INFO org.apache.nutch.fetcher.Fetcher: fetch of Url error : xxxxxxx  failed with: java.net.SocketException: Connection reset\n2013-10-09 00:34:05,923 ERROR org.apache.nutch.protocol.httpclient.Http: Failed to get protocol output\njava.net.SocketException: Connection reset\n    at java.net.SocketInputStream.read(SocketInputStream.java:189)\n    at java.net.SocketInputStream.read(SocketInputStream.java:121)\n    at java.io.BufferedInputStream.fill(BufferedInputStream.java:235)\n    at java.io.BufferedInputStream.read(BufferedInputStream.java:254)\n    at org.apache.commons.httpclient.HttpParser.readRawLine(HttpParser.java:77)\n    at org.apache.commons.httpclient.HttpParser.readLine(HttpParser.java:105)\n    at org.apache.commons.httpclient.HttpConnection.readLine(HttpConnection.java:1115)\n    at org.apache.commons.httpclient.MultiThreadedHttpConnectionManager$HttpConnectionAdapter.readLine(MultiThreadedHttpConnectionManager.java:1373)\n    at org.apache.commons.httpclient.HttpMethodBase.readStatusLine(HttpMethodBase.java:1832)\n    at org.apache.commons.httpclient.HttpMethodBase.readResponse(HttpMethodBase.java:1590)\n    at org.apache.commons.httpclient.HttpMethodBase.execute(HttpMethodBase.java:995)\n    at org.apache.commons.httpclient.HttpMethodDirector.executeWithRetry(HttpMethodDirector.java:397)\n    at org.apache.commons.httpclient.HttpMethodDirector.executeMethod(HttpMethodDirector.java:170)\n    at org.apache.commons.httpclient.HttpClient.executeMethod(HttpClient.java:396)\n    at org.apache.commons.httpclient.HttpClient.executeMethod(HttpClient.java:324)\n    at org.apache.nutch.protocol.httpclient.HttpResponse.&lt;init&gt;(HttpResponse.java:94)\n    at org.apache.nutch.protocol.httpclient.Http.getResponse(Http.java:154)\n    at org.apache.nutch.protocol.http.api.HttpBase.getProtocolOutput(HttpBase.java:140)\n    at org.apache.nutch.fetcher.Fetcher$FetcherThread.run(Fetcher.java:703)\n</code></pre>\n", "creation_date": 1381254094, "score": 0},
{"title": "How to use nutch-2.2.1 for crawling", "view_count": 1527, "owner": {"user_id": 2845368, "answer_count": 2, "creation_date": 1380866487, "accept_rate": 80, "view_count": 33, "location": "Bangalore, India", "reputation": 56}, "is_answered": true, "answers": [{"question_id": 19240812, "owner": {"user_id": 2623052, "accept_rate": 45, "link": "http://stackoverflow.com/users/2623052/allan-macmillan", "user_type": "registered", "reputation": 866}, "body": "<p>Here is a guide for setting up with Nutch 1.4. Obviosuly Nutch 2.x has newer features but the older version still provides most, if not all the features you are looking for.</p>\n\n<p><a href=\"http://amac4.blogspot.com/2013/07/configuring-nutch-to-crawl-urls.html\" rel=\"nofollow\">http://amac4.blogspot.com/2013/07/configuring-nutch-to-crawl-urls.html</a></p>\n", "creation_date": 1381221554, "is_accepted": true, "score": 1, "last_activity_date": 1381221554, "answer_id": 19242925}], "question_id": 19240812, "tags": ["apache", "solr", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/19240812/how-to-use-nutch-2-2-1-for-crawling", "last_activity_date": 1381221554, "accepted_answer_id": 19242925, "body": "<p>I'm new to Nutch please guys help me to figure out crawling using nutch.Help me to figure out any useful link or document which help me to use nutch</p>\n", "creation_date": 1381214456, "score": 1},
{"title": "Indexing issue in Nutch-Solr (Want to display data in solr)", "view_count": 1040, "is_answered": true, "answers": [{"last_edit_date": 1381014415, "owner": {"user_id": 2529954, "link": "http://stackoverflow.com/users/2529954/ericlavault", "user_type": "registered", "reputation": 1169}, "body": "<p>If you want some data to be returned with the search response, check that the targeted fields are stored by solr, then you can set a list of fields to return in your query using <code>fl</code> param (with stored field name as value). You can also set default fl values in solrconfig.xml.</p>\n\n<p>For example, let's say you want <code>content</code> field to be returned. In your schema.xml, in the <code>&lt;fields&gt;</code> declaration you should have the option <code>stored=\"true\"</code> for this field like so :</p>\n\n<pre><code>&lt;field name=\"content\" type=\"text\" indexed=\"true\" stored=\"true\"/&gt;\n</code></pre>\n\n<p>Then in solrconfig.xml, declare default fl params in the requestHandler definition, you can set specific fields (space separated field names). The xml sample (grabbed from the tutorial) should look like this if we just want data stored in the <code>content</code> field to be returned.</p>\n\n<pre><code>&lt;requestHandler name=\"/nutch\" class=\"solr.SearchHandler\" &gt;\n  &lt;lst name=\"defaults\"&gt;\n    &lt;str name=\"defType\"&gt;dismax&lt;/str&gt;\n    &lt;str name=\"echoParams\"&gt;explicit&lt;/str&gt;\n    &lt;float name=\"tie\"&gt;0.01&lt;/float&gt;\n    &lt;str name=\"qf\"&gt;\n    content^0.5 anchor^1.0 title^1.2\n    &lt;/str&gt;\n    &lt;str name=\"pf\"&gt;\n    content^0.5 anchor^1.5 title^1.2 site^1.5\n    &lt;/str&gt;\n    &lt;str name=\"fl\"&gt;\n    url content\n    &lt;/str&gt;\n    &lt;str name=\"mm\"&gt;\n    2&amp;lt;-1 5&amp;lt;-2 6&amp;lt;90%\n    &lt;/str&gt;\n    &lt;int name=\"ps\"&gt;100&lt;/int&gt;\n    &lt;bool hl=\"true\"/&gt;\n    &lt;str name=\"q.alt\"&gt;*:*&lt;/str&gt;\n    &lt;str name=\"hl.fl\"&gt;title url content&lt;/str&gt;\n    &lt;str name=\"f.title.hl.fragsize\"&gt;0&lt;/str&gt;\n    &lt;str name=\"f.title.hl.alternateField\"&gt;title&lt;/str&gt;\n    &lt;str name=\"f.url.hl.fragsize\"&gt;0&lt;/str&gt;\n    &lt;str name=\"f.url.hl.alternateField\"&gt;url&lt;/str&gt;\n    &lt;str name=\"f.content.hl.fragmenter\"&gt;regex&lt;/str&gt;\n  &lt;/lst&gt;\n&lt;/requestHandler&gt;\n</code></pre>\n\n<p>You can override these defaults right in the query. A common use case is to put \"<code>*,score</code>\" in the fl area in solr query interface so that you can see all stored fields (using wildcard character <code>*</code>) along with the score in the results. You might also want to specify the query type parameter (qt) according to the targeted request handler (should be \"/nutch\").</p>\n\n<p>Helpful links :</p>\n\n<p><a href=\"http://wiki.apache.org/solr/SchemaXml#Common_field_options\" rel=\"nofollow\">http://wiki.apache.org/solr/SchemaXml#Common_field_options</a>\n<a href=\"http://wiki.apache.org/solr/CommonQueryParameters#fl\" rel=\"nofollow\">http://wiki.apache.org/solr/CommonQueryParameters#fl</a></p>\n", "question_id": 19194877, "creation_date": 1380988883, "is_accepted": false, "score": 1, "last_activity_date": 1381014415, "answer_id": 19199728}], "question_id": 19194877, "tags": ["solr", "lucene", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/19194877/indexing-issue-in-nutch-solr-want-to-display-data-in-solr", "last_activity_date": 1381147976, "owner": {"age": 26, "answer_count": 18, "creation_date": 1375425517, "user_id": 2644816, "accept_rate": 50, "view_count": 168, "location": "Ahamdabad, India", "reputation": 470}, "body": "<p>I am trying to <strong>crawl data</strong> using <strong>Nutch</strong> and <strong>Index</strong> that Data in <strong>Solr</strong>.</p>\n\n<p>I have follow the steps from this Url <a href=\"http://searchhub.org/2009/03/09/nutch-solr/\" rel=\"nofollow\">Using Nutch with Solr</a> and <a href=\"http://wiki.apache.org/nutch/NutchTutorial\" rel=\"nofollow\">Nutch Wiki Tutorial</a></p>\n\n<p>I've successfully Index data using <strong>Solrindex command</strong></p>\n\n<p><code>bin/nutch solrindex http://127.0.0.1:8983/solr/ crawl/crawldb -linkdb crawl/linkdb crawl/segments/*</code> but in Result I can't find the Indexed data. </p>\n\n<p>I want result as below Image<img src=\"http://i.stack.imgur.com/4Mwot.png\" alt=\"Solr Query\"></p>\n\n<p>But I can't see any result data at right side.</p>\n", "creation_date": 1380955605, "score": 1},
{"title": "HBase Pseudo distributed or Fully distributed mode?", "view_count": 450, "owner": {"user_id": 1830069, "answer_count": 10, "creation_date": 1353080229, "accept_rate": 63, "view_count": 103, "reputation": 543}, "is_answered": true, "answers": [{"question_id": 19199089, "owner": {"user_id": 1206837, "accept_rate": 38, "link": "http://stackoverflow.com/users/1206837/tariq", "user_type": "registered", "reputation": 21845}, "body": "<p>Pseudo distributed mode would be better as in Standalone mode local FS is used. This means you cannot take advantage of the parallelism provided by the HDFS+MR combo.</p>\n", "creation_date": 1380986311, "is_accepted": true, "score": 2, "last_activity_date": 1380986311, "answer_id": 19199313}], "question_id": 19199089, "tags": ["hadoop", "hbase", "hdfs", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/19199089/hbase-pseudo-distributed-or-fully-distributed-mode", "last_activity_date": 1380986311, "accepted_answer_id": 19199313, "body": "<p>I have a single linux(ubuntu) server in development environment and I plan to use a single server for production environment as well. </p>\n\n<p>I have crawl data generated from Nutch 2.2.1 that I would like to store in HBase 0.90.6. Since, <strong>I don't intend to use multiple machines</strong>, (all I have is a single server) which mode of HBase is ideal for production env in my case - pseudo or fully-distributed?</p>\n", "creation_date": 1380984937, "score": 0},
{"title": "Nutch Crawler read Segment results", "view_count": 994, "is_answered": true, "answers": [{"question_id": 17233197, "owner": {"user_id": 2014559, "accept_rate": 100, "link": "http://stackoverflow.com/users/2014559/andrew-butkus", "user_type": "registered", "reputation": 425}, "body": "<p>I generally try to merge all segment first, </p>\n\n<p>bin/nutch mergesegs crawl/merged crawl/segments/*</p>\n\n<p>and then</p>\n\n<p>bin/nutch readseg -dump crawl/merged/* segmentAllContent</p>\n", "creation_date": 1380873568, "is_accepted": false, "score": 3, "last_activity_date": 1380873568, "answer_id": 19176171}], "question_id": 17233197, "tags": ["apache", "nutch", "web-crawler"], "answer_count": 1, "link": "http://stackoverflow.com/questions/17233197/nutch-crawler-read-segment-results", "last_activity_date": 1380873568, "owner": {"age": 27, "answer_count": 125, "creation_date": 1313859912, "user_id": 903907, "accept_rate": 50, "view_count": 306, "location": "Chennai, India", "reputation": 2981}, "body": "<p>I crawled  using apache-nutch-crawler1.6. After crawling when i try to read the content of the  crawled result using the command \uf0d8  </p>\n\n<pre><code> bin/nutch readseg -dump crawl/segments/* segmentAllContent\n</code></pre>\n\n<p>Error is </p>\n\n<pre><code> Exception in thread \"main\" org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/home/ubuntu/nutch/framework/apache-nutch-1.6/blogs/segments/2013062110/crawl_generate\n    Input path does not exist: file:/home/ubuntu/nutch/framework/apache-nutch-1.6/blogs/segments/2013062110/crawl_fetch\n    Input path does not exist: file:/home/ubuntu/nutch/framework/apache-nutch-1.6/blogs/segments/2013062110/crawl_parse\n    Input path does not exist: file:/home/ubuntu/nutch/framework/apache-nutch-1.6/blogs/segments/2013062110/content\n    Input path does not exist: file:/home/ubuntu/nutch/framework/apache-nutch-1.6/blogs/segments/2013062110/parse_data\n    Input path does not exist: file:/home/ubuntu/nutch/framework/apache-nutch-1.6/blogs/segments/2013062110/parse_text\n            at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:197)\n            at org.apache.hadoop.mapred.SequenceFileInputFormat.listStatus(SequenceFileInputFormat.java:40)\n            at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:208)\n            at org.apache.hadoop.mapred.JobClient.writeOldSplits(JobClient.java:989)\n            at org.apache.hadoop.mapred.JobClient.writeSplits(JobClient.java:981)\n            at org.apache.hadoop.mapred.JobClient.access$600(JobClient.java:174)\n            at org.apache.hadoop.mapred.JobClient$2.run(JobClient.java:897)\n            at org.apache.hadoop.mapred.JobClient$2.run(JobClient.java:850)\n            at java.security.AccessController.doPrivileged(Native Method)\n            at javax.security.auth.Subject.doAs(Subject.java:416)\n            at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1121)\n            at org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:850)\n            at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:824)\n            at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:1261)\n            at org.apache.nutch.segment.SegmentReader.dump(SegmentReader.java:224)\n            at org.apache.nutch.segment.SegmentReader.main(SegmentReader.java:572)\n</code></pre>\n\n<p>How would i read the html contents after crawling? Thanks in Advance</p>\n", "creation_date": 1371811196, "score": 1},
{"title": "how to make a search engine with nutch and cassandra?", "view_count": 277, "owner": {"user_id": 2841094, "answer_count": 0, "creation_date": 1380773234, "accept_rate": 67, "view_count": 48, "reputation": 164}, "is_answered": true, "answers": [{"question_id": 19150735, "owner": {"user_id": 2623052, "accept_rate": 45, "link": "http://stackoverflow.com/users/2623052/allan-macmillan", "user_type": "registered", "reputation": 866}, "body": "<p>Check out Solr's Data Import Handler and see if you feel it would work.  It allows you to query your database and store the results with Solr to which then Solr can manipulate the reuslts.  Nutch also has very good integration with Solr should you choose to use it.</p>\n", "creation_date": 1380786125, "is_accepted": true, "score": 0, "last_activity_date": 1380786125, "answer_id": 19153549}], "question_id": 19150735, "tags": ["solr", "indexing", "cassandra", "search-engine", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/19150735/how-to-make-a-search-engine-with-nutch-and-cassandra", "last_activity_date": 1380787166, "accepted_answer_id": 19153549, "body": "<p>I am tring to implement a website search engine with java as an applet,I have used nutch as web crawler and cassandra as my database,I have to use a nosql database(because my teacher wants me to do),now my question is what should I do next to complete my search engine?</p>\n\n<p>I have googled a lot,but all of the sites are mostly about nutch and solr,and they build search engines with integration of these two,cause solr itself is somehow a database,I don't know what should I do,do I have to use solr too to complete my search engine?is it wise to use two databases(solr and cassandra)?or I should do some thing else?</p>\n\n<p>please remember I have to use cassandra.\nand please first explain me if I have understood things in a wrong way and then give me a minus mark,:D</p>\n\n<p>I will be really really thankfull for your help,I have got somehow confused.</p>\n\n<p>by the way does solr counted as a nosql database?excuse me,I am new to them all.</p>\n", "creation_date": 1380774219, "score": 2},
{"title": "SVN Local Repository", "view_count": 52, "is_answered": false, "answers": [{"question_id": 18990347, "owner": {"user_id": 1337799, "link": "http://stackoverflow.com/users/1337799/yunshan", "user_type": "registered", "reputation": 41}, "body": "<p>DVCS systems may help, such as git, Hg, etc.\nSubversion is a centerized version control system, all workspaces directly connect to center repository.\nBut DVCS have local repository, before you commit to remote repository, you need commit to local rep first.</p>\n", "creation_date": 1380089966, "is_accepted": false, "score": 0, "last_activity_date": 1380089966, "answer_id": 18997594}], "question_id": 18990347, "tags": ["apache", "svn", "version-control", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/18990347/svn-local-repository", "last_activity_date": 1380089966, "owner": {"user_id": 2762832, "answer_count": 12, "creation_date": 1378760991, "accept_rate": 37, "view_count": 177, "reputation": 339}, "body": "<p>I imported Apache Nutch source code into my local ubuntu installation. I have subversion installed on my eclipse kepler. The sourcecode has several branches and one main project in trunk. I was thinking of specifying this source-code location as my repo location to svn and check out &amp; or commit changes based off of this. However SVN always seem to expect a remote repo location. Is there anyway I can make this dumping ground of apache nutch source code as my repo location for svn?</p>\n\n<p>Thanks  </p>\n", "creation_date": 1380050862, "score": 0},
{"title": "apache nutch generate only fetchlist", "view_count": 96, "is_answered": false, "answers": [{"question_id": 18891435, "owner": {"user_id": 1074714, "link": "http://stackoverflow.com/users/1074714/nsawaya", "user_type": "registered", "reputation": 602}, "body": "<p>All from <a href=\"http://wiki.apache.org/nutch/CommandLineOptions\" rel=\"nofollow\">http://wiki.apache.org/nutch/CommandLineOptions</a></p>\n\n<p>First Inject using:</p>\n\n<p><a href=\"http://wiki.apache.org/nutch/bin/nutch%20inject\" rel=\"nofollow\">http://wiki.apache.org/nutch/bin/nutch%20inject</a></p>\n\n<p>Then generate fetch-list using:</p>\n\n<p><a href=\"http://wiki.apache.org/nutch/bin/nutch_generate\" rel=\"nofollow\">http://wiki.apache.org/nutch/bin/nutch_generate</a></p>\n\n<p>If you'd like to integrate it with a Java app, I would recommend downloading the source code and checking the test package (then you'll understand how to use the api).</p>\n\n<p>hope this helps</p>\n", "creation_date": 1379836448, "is_accepted": false, "score": 0, "last_activity_date": 1379836448, "answer_id": 18941595}], "question_id": 18891435, "tags": ["apache", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/18891435/apache-nutch-generate-only-fetchlist", "last_activity_date": 1379836448, "owner": {"user_id": 2116143, "answer_count": 0, "creation_date": 1361980865, "accept_rate": 0, "view_count": 3, "reputation": 16}, "body": "<p>I plan to use Apache Nutch for a specific crawling application where I'll provide an initial seed list and I would need to generate only a fetchlist without downloading anything (except the need of parsing), without generating the indexes. </p>\n\n<p>I will then use the fetchlist and pass this fetchlist to the existing application which will perform the download.</p>\n\n<p>Is it possible?</p>\n", "creation_date": 1379584099, "score": 0},
{"title": "Integrating Hbase that Holds Nutch Crawled Data and Solr", "view_count": 727, "is_answered": false, "answers": [{"question_id": 15323530, "owner": {"user_id": 1056563, "accept_rate": 90, "link": "http://stackoverflow.com/users/1056563/javadba", "user_type": "registered", "reputation": 11897}, "body": "<p>You can add the hbase rowkey to the solr documents. Also look at the Lilly project  <a href=\"http://www.lilyproject.org/lily/index.html\" rel=\"nofollow\">http://www.lilyproject.org/lily/index.html</a></p>\n", "creation_date": 1362930914, "is_accepted": false, "score": -1, "last_activity_date": 1362930914, "answer_id": 15324216}, {"question_id": 15323530, "owner": {"user_id": 1830069, "accept_rate": 63, "link": "http://stackoverflow.com/users/1830069/sunskin", "user_type": "registered", "reputation": 543}, "body": "<p>All you have to do is, run this command - <code>sudo bin/nutch solrindex http://localhost:8983/solr/ -reindex</code></p>\n\n<p>But before you do that please make sure your solr instance is up and running which you can verify by visiting this link <code>http://localhost:8983/solr/</code> and if you can find Solr admin then your Solr instance is running.</p>\n", "creation_date": 1379618515, "is_accepted": false, "score": 0, "last_activity_date": 1379618515, "answer_id": 18903184}], "question_id": 15323530, "tags": ["solr", "amazon-web-services", "hbase", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/15323530/integrating-hbase-that-holds-nutch-crawled-data-and-solr", "last_activity_date": 1379618515, "owner": {"user_id": 453596, "answer_count": 127, "creation_date": 1285053991, "accept_rate": 77, "view_count": 2042, "reputation": 21412}, "body": "<p>I have a Hbase Database that holds crawled information of wikipedia.org. My machine is at Amazon Wweb Services.</p>\n\n<p>I have downloaded the Solr and I want to index the data at Hbase after that I will make search on it.</p>\n\n<p>I am new to Solr and Hbase, how can I do that?</p>\n", "creation_date": 1362927051, "score": 0},
{"title": "Crawling twitter,linkedin using nutch", "view_count": 2237, "is_answered": false, "answers": [{"question_id": 18798957, "owner": {"user_id": 2586595, "link": "http://stackoverflow.com/users/2586595/mohsen89z", "user_type": "registered", "reputation": 128}, "body": "<p>f you want to crawl this specific urls you should include following line too</p>\n\n<blockquote>\n  <p>-.*</p>\n</blockquote>\n\n<p>this command will exclude all other urls!\nAlso if you want to crawl twitter or linkedin, you can use specified crawlers like <a href=\"http://twitter4j.org/en/\" rel=\"nofollow\">twit4j</a> or <a href=\"http://code.google.com/p/linkedin-j/\" rel=\"nofollow\">linkedin-j</a>!</p>\n", "creation_date": 1379317614, "is_accepted": false, "score": 0, "last_activity_date": 1379317614, "answer_id": 18822802}, {"question_id": 18798957, "owner": {"user_id": 1965370, "link": "http://stackoverflow.com/users/1965370/do-do", "user_type": "registered", "reputation": 389}, "body": "<p>As I know so far, Nutch did not support crawling Twitter and Linkedin data. For crawling Titter data you should using Twitter API, check this one <a href=\"http://twitter4j.org/en/\" rel=\"nofollow\">http://twitter4j.org/en/</a>. For crawling Linked data, you could have a look on this <a href=\"https://github.com/pondering/scrapy-linkedin\" rel=\"nofollow\">https://github.com/pondering/scrapy-linkedin</a>.</p>\n\n<p>Hope this helps  </p>\n", "creation_date": 1379594929, "is_accepted": false, "score": 0, "last_activity_date": 1379594929, "answer_id": 18895037}], "question_id": 18798957, "tags": ["linkedin", "nutch", "web-crawler"], "answer_count": 2, "link": "http://stackoverflow.com/questions/18798957/crawling-twitter-linkedin-using-nutch", "last_activity_date": 1379594929, "owner": {"user_id": 2778617, "view_count": 3, "answer_count": 0, "creation_date": 1379139136, "reputation": 6}, "body": "<p>I have been trying to use nutch to crawl twitter and linkedin data\nNutch-0.9.</p>\n\n<ol>\n<li>However when i try to crawl twitter the regex-filter doesnt seem to work, my regex-filter file has \n+^https://([a-z0-9]*.)<em>twitter.com/a</em>\nand what i wish to do is to crawl only those urls that follow the above pattern. I end up with urls such as <a href=\"https://twitter.com/document\" rel=\"nofollow\">https://twitter.com/document</a>.</li>\n<li>As for the linkedin part, it always shows a timeout whenever i try to crawl it, robots.txt on linkedin says that you need to mail to get your crawler whitelisted but they never respond.</li>\n</ol>\n\n<p>Appreciate your help !</p>\n", "creation_date": 1379139551, "score": 1},
{"title": "Apache Nutch Crawl Dynamic Products", "view_count": 816, "is_answered": false, "answers": [{"question_id": 18802020, "owner": {"user_id": 1881318, "link": "http://stackoverflow.com/users/1881318/tahagh", "user_type": "registered", "reputation": 619}, "body": "<p>One possible solution is this: </p>\n\n<p>Step 1) Put the description of each dynamic product in its own page. e.g <code>http://domain/product?id=xxx</code> (or with more friendly url such as <code>http://domain/product-x</code>). </p>\n\n<p>Step 2) You need a page or several pages that list urls of these products. The sitemap.xml you mentioned is one choice but a simple html page is also suffice. So, for instance, you can dynamically generate a page named products_list which contains entries like this: <code>&lt;a href=\"http://domain/product?id=xxx\"&gt;Product x&lt;/a&gt;</code>. </p>\n\n<p>Step 3) You should either add url of products_list page to your nutch seed file or include a link to it in one of already crawling pages.</p>\n", "creation_date": 1379331416, "is_accepted": false, "score": 0, "last_activity_date": 1379331416, "answer_id": 18826994}], "question_id": 18802020, "tags": ["solr", "nutch", "web-crawler", "sitemap.xml"], "answer_count": 1, "link": "http://stackoverflow.com/questions/18802020/apache-nutch-crawl-dynamic-products", "last_activity_date": 1379331416, "owner": {"user_id": 2294224, "view_count": 9, "answer_count": 0, "creation_date": 1366274472, "reputation": 89}, "body": "<p>Currently we are using Apache Solr as Search Engine and Apache Nutch as Crawler. Now we have created a site site which contains products which gets generated dynamically.</p>\n\n<p>As current setup will search the content within content field, so whenever we are searching for dynamic Product, then its not coming in search results. </p>\n\n<p>Can you please guide me how to crawl and index Dynamic Product on a Page to Apache Solr? Can we do this using Sitemap.xml, If yes then please suggest how?</p>\n\n<p>Thanks!</p>\n", "creation_date": 1379163716, "score": -1},
{"title": "Is it Possible to Have Nutch Only Crawl Down a Certain File Path?", "view_count": 114, "owner": {"user_id": 2506986, "answer_count": 0, "creation_date": 1371765002, "accept_rate": 57, "view_count": 10, "reputation": 42}, "is_answered": true, "answers": [{"question_id": 18731223, "owner": {"user_id": 2586595, "link": "http://stackoverflow.com/users/2586595/mohsen89z", "user_type": "registered", "reputation": 128}, "body": "<p>The only thing you have to do is to write a regex that matches to your pattern, something like </p>\n\n<blockquote>\n  <p>+.www.foo.com/shoes/</p>\n</blockquote>\n\n<p>and skip everything else by using </p>\n\n<blockquote>\n  <p>-.*</p>\n</blockquote>\n\n<p>at the end of your <code>crawl-urlfilter.txt</code>!</p>\n", "creation_date": 1379318182, "is_accepted": true, "score": 0, "last_activity_date": 1379318182, "answer_id": 18822968}], "question_id": 18731223, "tags": ["apache", "web-crawler", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/18731223/is-it-possible-to-have-nutch-only-crawl-down-a-certain-file-path", "last_activity_date": 1379318182, "accepted_answer_id": 18822968, "body": "<p>I'm trying to user Apache nutch to only crawl down a certain file path. For example if my url is:</p>\n\n<p>www.foo.com/shoes/</p>\n\n<p>I would want to keep crawling urls like: www.foo.com/shoes/nike and www.foo.com/shoes/addidas and www.foo.com/shoes/addidas/soccer but NOT crawl the other directories like www.foo.com/clothes or www.foo.com/watches. Is there anyway nutch can do this?</p>\n", "creation_date": 1378862151, "score": 2},
{"title": "Unable to add index files to solr from nutch", "view_count": 196, "is_answered": false, "answers": [{"question_id": 18783349, "owner": {"user_id": 2586595, "link": "http://stackoverflow.com/users/2586595/mohsen89z", "user_type": "registered", "reputation": 128}, "body": "<p>with this information, I think you forget to run Solr or you didn't get the access permission to nutch!\ndo you have access to <code>http://xxx.xxx.xxx.xxx:8080/solr/</code> in your browser?</p>\n", "creation_date": 1379317118, "is_accepted": false, "score": 0, "last_activity_date": 1379317118, "answer_id": 18822660}], "question_id": 18783349, "tags": ["solr", "lucene", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/18783349/unable-to-add-index-files-to-solr-from-nutch", "last_activity_date": 1379317118, "owner": {"user_id": 2767897, "view_count": 1, "answer_count": 0, "creation_date": 1378888824, "reputation": 1}, "body": "<p>I have setup nutch(1.4) with solr(4.4.0) on windows and have crawled the nutch default pages mentioned in the tutorial. However after the crawl is successful I am unable to add the pages to the index using the command \"<strong>bin/nutch solrindex <a href=\"http://xxx.xxx.xxx.xxx:8080/solr/\" rel=\"nofollow\">http://xxx.xxx.xxx.xxx:8080/solr/</a> crawl/crawldb -linkdb crawl/linkdb crawl/segments/*</strong>\"</p>\n\n<p>Following is the extract from the hadoop logs. Any help is deeply appreciated.</p>\n\n<pre><code>2013-09-13 14:50:24,137 INFO  indexer.IndexingFilters - Adding org.apache.nutch.indexer.basic.BasicIndexingFilter\n2013-09-13 14:50:24,137 INFO  anchor.AnchorIndexingFilter - Anchor deduplication is: off\n2013-09-13 14:50:24,137 INFO  indexer.IndexingFilters - Adding org.apache.nutch.indexer.anchor.AnchorIndexingFilter\n2013-09-13 14:50:24,215 INFO  solr.SolrMappingReader - source: content dest: content\n2013-09-13 14:50:24,215 INFO  solr.SolrMappingReader - source: site dest: site\n2013-09-13 14:50:24,215 INFO  solr.SolrMappingReader - source: title dest: title\n2013-09-13 14:50:24,215 INFO  solr.SolrMappingReader - source: host dest: host\n2013-09-13 14:50:24,215 INFO  solr.SolrMappingReader - source: segment dest: segment\n2013-09-13 14:50:24,215 INFO  solr.SolrMappingReader - source: boost dest: boost\n2013-09-13 14:50:24,215 INFO  solr.SolrMappingReader - source: digest dest: digest\n2013-09-13 14:50:24,215 INFO  solr.SolrMappingReader - source: tstamp dest: tstamp\n2013-09-13 14:50:24,215 INFO  solr.SolrMappingReader - source: url dest: id\n2013-09-13 14:50:24,215 INFO  solr.SolrMappingReader - source: url dest: url\n2013-09-13 14:50:24,277 INFO  solr.SolrWriter - Adding 11 documents\n2013-09-13 14:50:24,511 WARN  mapred.LocalJobRunner - job_local_0001\norg.apache.solr.common.SolrException: Internal Server Error\n\nInternal Server Error\n\nrequest: http://xxx.xxx.xxx.xxx:8080/solr/update?wt=javabin&amp;version=2\n    at org.apache.solr.client.solrj.impl.CommonsHttpSolrServer.request(CommonsHttpSolrServer.java:430)\n    at org.apache.solr.client.solrj.impl.CommonsHttpSolrServer.request(CommonsHttpSolrServer.java:244)\n    at org.apache.solr.client.solrj.request.AbstractUpdateRequest.process(AbstractUpdateRequest.java:105)\n    at org.apache.solr.client.solrj.SolrServer.add(SolrServer.java:49)\n    at org.apache.nutch.indexer.solr.SolrWriter.close(SolrWriter.java:93)\n    at org.apache.nutch.indexer.IndexerOutputFormat$1.close(IndexerOutputFormat.java:48)\n    at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:474)\n    at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:411)\n    at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:216)\n2013-09-13 14:50:25,229 ERROR solr.SolrIndexer - java.io.IOException: Job failed!\n</code></pre>\n", "creation_date": 1379066097, "score": 0},
{"title": "integrate Nutch 1.6 with Solr 4.3 IOException when running &lt;nutch crawl urls -solr http://localhost:8983/solr/&gt; Job Failed. Any ideas?", "view_count": 970, "is_answered": false, "answers": [{"question_id": 16676222, "owner": {"user_id": 1172131, "accept_rate": 33, "link": "http://stackoverflow.com/users/1172131/anand-khatri", "user_type": "registered", "reputation": 221}, "body": "<p>Yes can you please put additional details from log. The possible cause may be you need to define the uniquekey in schema.xml file. like this</p>\n\n<pre><code>&lt;uniqueKey&gt;id&lt;/uniqueKey&gt;.\n</code></pre>\n", "creation_date": 1369237109, "is_accepted": false, "score": 0, "last_activity_date": 1369237109, "answer_id": 16696010}, {"question_id": 16676222, "owner": {"user_id": 1688068, "link": "http://stackoverflow.com/users/1688068/arul", "user_type": "registered", "reputation": 721}, "body": "<p>vera,\ni just use nutch 1.7 and solr 4.4.0.\ni had problem in schema.xml file. i figure out to few change in schema file that change are below</p>\n\n<p>copy ur usr/nutch 1.7/conf/ schema.xml to paste /usr/local/solr-4.4.0/example/solr/collection1/conf/schema and overridded after u change the field type=\"text\" not a text_field.\nchange  to </p>\n\n<p>content= text class \nchange that\nenglishPorterFilterFactory to SnowballPorterFilterFactory</p>\n\n<p>after add</p>\n\n<p>field name=\"<em>version</em>\" type=\"long\" indexed=\"true\" stored=\"true\"</p>\n\n<p>field name=\"text\" type=\"text\" indexed=\"true\" stored=\"false\" multiValued=\"true\"</p>\n\n<p>its working fine for me vera..</p>\n", "creation_date": 1378971129, "is_accepted": false, "score": 0, "last_activity_date": 1378971129, "answer_id": 18758138}], "question_id": 16676222, "tags": ["solr", "indexing", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/16676222/integrate-nutch-1-6-with-solr-4-3-ioexception-when-running-nutch-crawl-urls-so", "last_activity_date": 1378971129, "owner": {"user_id": 2406613, "view_count": 7, "answer_count": 0, "creation_date": 1369157741, "reputation": 11}, "body": "<p>I am trying to integrate Nutch 1.6 with Solr 4.3 (I copied the /apache-nutch-1.6/conf/schema-solr4.xml into collection1/conf/ and rename the file to schema.xml). I also tried Nutch1.5.1 to integrate with solr 4.3. In both situations I am getting IOException when running: </p>\n\n<pre><code>bash$ nutch crawl urls -solr http://127.0.0.1:8983/solr/\n</code></pre>\n\n<p>Job Failed. Any ideas?</p>\n\n<p>I figuered that one out myself, had to look at solr.log and add these fields below to schema.xml under collection1/conf</p>\n\n<p><code>&lt;field name=\"host\" type=\"string\" stored=\"false\" indexed=\"true\"/&gt;</code>\n<code>&lt;field name=\"segment\" type=\"string\" stored=\"true\" indexed=\"false\"/&gt;</code>\n<code>&lt;field name=\"digest\" type=\"string\" stored=\"true\" indexed=\"false\"/&gt;</code>\n<code>&lt;field name=\"boost\" type=\"float\" stored=\"true\" indexed=\"false\"/&gt;</code>\n<code>&lt;field name=\"tstamp\" type=\"date\" stored=\"true\" indexed=\"false\"/&gt;</code>\nand it worked.</p>\n", "creation_date": 1369158059, "score": 2},
{"title": "Understanding of hBase data storage (webpage) for Nutch", "view_count": 847, "owner": {"age": 27, "answer_count": 17, "creation_date": 1332257106, "user_id": 1281305, "accept_rate": 88, "view_count": 129, "location": "Prague, Czech Republic", "reputation": 494}, "is_answered": true, "answers": [{"question_id": 18685735, "owner": {"user_id": 582789, "link": "http://stackoverflow.com/users/582789/alfonso-nishikawa", "user_type": "registered", "reputation": 1263}, "body": "<p>I don't know how are you trying to implement your solution: if as a Nutch plugin, a Hadoop MapReduce or a single process script, but I guess this information will be helpful:</p>\n\n<ul>\n<li><p>As indicated in <a href=\"http://svn.apache.org/repos/asf/nutch/tags/release-2.2.1/conf/gora-hbase-mapping.xml\" rel=\"nofollow\">nutch-src/conf/gora-hbase-mapping.xml</a>, batchId is mapped to HBase's column <code>f:bid</code>.</p></li>\n<li><p>You have to read it using Gora. The instances of <code>WebPage</code> have the method <code>#getBatchId()</code>. Check the <a href=\"http://svn.apache.org/repos/asf/nutch/tags/release-2.2.1/src/gora/webpage.avsc\" rel=\"nofollow\">avro WebPage definition</a> and the <a href=\"http://svn.apache.org/repos/asf/nutch/tags/release-2.2.1/src/java/org/apache/nutch/storage/WebPage.java\" rel=\"nofollow\">compiled class</a>.</p></li>\n</ul>\n\n<p>When developing a plugin, much probably you will see a <code>WebPage</code> parameter in the interface of the plugin.</p>\n\n<p>If you want to access <code>batchId</code> in a <em>raw way in HBase</em>, just read the column <code>f:bid</code> and consider it the raw text. If I am not wrong, Gora is not writing additional information on strings (unlike when serialized).</p>\n", "creation_date": 1378713955, "is_accepted": true, "score": 1, "last_activity_date": 1378713955, "answer_id": 18693921}], "question_id": 18685735, "tags": ["hadoop", "hbase", "nutch", "distributed-database"], "answer_count": 1, "link": "http://stackoverflow.com/questions/18685735/understanding-of-hbase-data-storage-webpage-for-nutch", "last_activity_date": 1378713955, "accepted_answer_id": 18693921, "body": "<p>I am using HBase as my storage for crawled data by Apache Nutch. A location of my storage is in path /data/hbase/webpage and there I can see a lot of folders like:</p>\n\n<pre><code>64b2feb30073eec24d9dba65d421e7f \n482062bc554bd45bf198d9edea971a30\n7c8a6eec12d9f6926a1d912be9a0ca81\nc1f682541b8d1c0559de6df14ae84e2b\n083b28ee75babc718cc28e66b98c9ff5\n809eb4bb5f2be087e2c84a2f51d26653\n</code></pre>\n\n<p>and more...</p>\n\n<p>These folders contains another folders like:</p>\n\n<pre><code>f  h  il  mk  mtdt  ol  p  recovered.edits  s\n</code></pre>\n\n<p>But it is not so important.</p>\n\n<p>I am writing my own indexer for Nutch to get crawled data from HBase to Solr. I need to put it to Solr in batches because when I run it all, I get OutOfMemory exception.</p>\n\n<p>I would like to ask you if it is possible to get batch ids from my HBase storage (to know which batch ids I have and then I can send it to index).</p>\n", "creation_date": 1378656205, "score": 1},
{"title": "solr XML parser Exception", "view_count": 855, "owner": {"user_id": 1363086, "answer_count": 9, "creation_date": 1335628928, "accept_rate": 52, "view_count": 52, "reputation": 338}, "is_answered": true, "answers": [{"question_id": 18629373, "owner": {"user_id": 950178, "link": "http://stackoverflow.com/users/950178/jayendra", "user_type": "registered", "reputation": 34896}, "body": "<p>As <code>&lt; &gt;</code> are xml characters, parsing would fail.<br>\nYou would need to use <code>&amp;gt;</code> and <code>&amp;lt;</code> for > &amp; &lt; respectively in the Solr Config xml file.</p>\n\n<p>e.g. <code>&lt;str name=\"mm\"&gt;4 &amp;lt; 100%&lt;/str&gt;</code></p>\n", "creation_date": 1378363985, "is_accepted": true, "score": 3, "last_activity_date": 1378363985, "answer_id": 18629634}, {"question_id": 18629373, "owner": {"user_id": 787371, "accept_rate": 20, "link": "http://stackoverflow.com/users/787371/smarttechie", "user_type": "registered", "reputation": 118}, "body": "<p>Try replacing '&lt;' with '<code>&amp;lt;</code>'. After changing that, the configuration will be 2<code>&amp;lt;</code>-1 5<code>&amp;lt;</code>-2 6<code>&amp;lt;</code>90%</p>\n", "creation_date": 1378364104, "is_accepted": false, "score": 2, "last_activity_date": 1378364104, "answer_id": 18629666}], "question_id": 18629373, "tags": ["solr", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/18629373/solr-xml-parser-exception", "last_activity_date": 1378364104, "accepted_answer_id": 18629634, "body": "<p>I have configured the solr search engine. I followed the steps from <a href=\"http://tgels.com/wiki/en/Solr_and_Nutch_Steps\" rel=\"nofollow\">Here</a></p>\n\n<p>I am getting following error </p>\n\n<pre><code>Caused by: org.apache.solr.common.SolrException: org.xml.sax.SAXParseException;systemId: solrres:/solrconfig.xml; lineNumber: 813; columnNumber: 19; The content of elements must consist of well-formed character data or markup.\n  at org.apache.solr.core.Config.&lt;init&gt;(Config.java:148)\n  at org.apache.solr.core.Config.&lt;init&gt;(Config.java:86)\n  at org.apache.solr.core.SolrConfig.&lt;init&gt;(SolrConfig.java:120)\n  at org.apache.solr.core.CoreContainer.createFromLocal(CoreContainer.java:589)\n  ... 11 more\nused by: org.xml.sax.SAXParseException; systemId: solrres:/solrconfig.xml; lineNumber: 813; columnNumber: 19; The content of elements must consist of well-formed character data or markup.\n</code></pre>\n\n<p>I checked in configuration file. the error is at</p>\n\n<pre><code>&lt;str name=\"mm\"&gt; 2&lt;-1 5&lt;-2 6&lt;90% &lt;/str&gt;\n</code></pre>\n\n<p>How can i configure the same, if i avoid to configure above what will happen.</p>\n", "creation_date": 1378363088, "score": 1},
{"title": "How to configure apache nutch to remove all a tags and it&#39;s content?", "view_count": 141, "owner": {"user_id": 1271400, "answer_count": 6, "creation_date": 1331811039, "accept_rate": 68, "view_count": 35, "reputation": 322}, "is_answered": true, "answers": [{"question_id": 18524831, "owner": {"user_id": 1881318, "link": "http://stackoverflow.com/users/1881318/tahagh", "user_type": "registered", "reputation": 619}, "body": "<p>If you only wanted to tell nutch to not follow the \"a\" tags you could simply add \"a\" to the \"parser.html.outlinks.ignore_tags\" setting. </p>\n\n<p>If you want to remove \"a\" tags and their contents from parsed data, since the current HtmlParser shipped with nutch does not have any setting related to this, I think you should write a nutch plugin and develop a HtmlParseFilter that do your logic.</p>\n", "creation_date": 1377886822, "is_accepted": true, "score": 3, "last_activity_date": 1377886822, "answer_id": 18539192}], "question_id": 18524831, "tags": ["apache", "configuration", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/18524831/how-to-configure-apache-nutch-to-remove-all-a-tags-and-its-content", "last_activity_date": 1377886822, "accepted_answer_id": 18539192, "body": "<p>Is this even possible?</p>\n\n<p>I'm having problem with navigation that is also included in to the documents as a content. So that's why I want to remove all link tags so that it isn't in the data.</p>\n\n<p>I'm using the 1.7 version.</p>\n", "creation_date": 1377836336, "score": 0},
{"title": "How do I tell Nutch to crawl *through* a url without storing it?", "view_count": 171, "owner": {"user_id": 1132278, "answer_count": 12, "creation_date": 1325771076, "accept_rate": 46, "view_count": 132, "reputation": 1154}, "is_answered": true, "answers": [{"question_id": 18477167, "owner": {"user_id": 1132278, "accept_rate": 46, "link": "http://stackoverflow.com/users/1132278/jun-dai-bates-kobashigawa", "user_type": "registered", "reputation": 1154}, "body": "<p>Looks like the only way to do this is write your own IndexFilter plugin (or find someone's to copy from).</p>\n\n<p>[Will add my sample plugin code here when it's working properly]</p>\n\n<p>References:</p>\n\n<ul>\n<li><a href=\"http://www.atlantbh.com/precise-data-extraction-with-apache-nutch/\" rel=\"nofollow\">http://www.atlantbh.com/precise-data-extraction-with-apache-nutch/</a></li>\n<li><a href=\"http://florianhartl.com/nutch-plugin-tutorial.html\" rel=\"nofollow\">http://florianhartl.com/nutch-plugin-tutorial.html</a></li>\n<li><a href=\"http://stackoverflow.com/questions/16422182/how-to-filter-urls-in-nutch-2-1-solrindex-command\">How to filter URLs in Nutch 2.1 solrindex command</a></li>\n</ul>\n", "creation_date": 1377876994, "is_accepted": true, "score": 1, "last_activity_date": 1377876994, "answer_id": 18536686}], "question_id": 18477167, "tags": ["solr", "search-engine", "nutch", "intranet"], "answer_count": 1, "link": "http://stackoverflow.com/questions/18477167/how-do-i-tell-nutch-to-crawl-through-a-url-without-storing-it", "last_activity_date": 1377876994, "accepted_answer_id": 18536686, "body": "<p>Let's say I have a Confluence instance, and I want to crawl it and store the results in Solr as part of an intranet search engine.</p>\n\n<p>Now let's say I only want to store a subset of the pages (matching a regex) on the Confluence instance as part of the search engine.</p>\n\n<p><strong>But</strong>, I do want Nutch to crawl all the other pages, looking for links to pages that match\u2014I just don't want Nutch to store them (or at least I don't want Solr to return them in the results).</p>\n\n<p>What's the normal or least painful way to set Nutch->Solr up to work like this?</p>\n", "creation_date": 1377646808, "score": 0},
{"title": "Setting Up Nutch 2.2", "view_count": 199, "is_answered": false, "answers": [{"question_id": 18483303, "owner": {"user_id": 582789, "link": "http://stackoverflow.com/users/582789/alfonso-nishikawa", "user_type": "registered", "reputation": 1263}, "body": "<p>You have to configure the proxy for ant, so in console, before <code>ant runtime</code>, you can configure ant options like:</p>\n\n<pre><code>export ANT_OPTS='-Dhttp.proxyHost=proxy -Dhttp.proxyPort=8080 -Dhttp.proxyUser=user -Dhttp.proxyPassword=password -Dhttps.proxyHost=proxy -Dhttps.proxyPort=8080'\n</code></pre>\n\n<p>If that is not enough, maybe will need to configure <code>~/.m2/settings.xml</code> adding (something like):</p>\n\n<pre><code>&lt;proxies&gt;\n    &lt;proxy&gt;\n        &lt;id&gt;proxyId&lt;/id&gt;\n        &lt;active&gt;true&lt;/active&gt;\n        &lt;protocol&gt;http&lt;/protocol&gt;\n        &lt;username&gt;user&lt;/username&gt;\n        &lt;password&gt;password&lt;/password&gt;\n        &lt;host&gt;proxy&lt;/host&gt;\n        &lt;port&gt;8080&lt;/port&gt;\n        &lt;nonProxyHosts&gt;localhost|127.0.0.1&lt;/nonProxyHosts&gt;\n    &lt;/proxy&gt;   \n&lt;/proxies&gt;\n</code></pre>\n", "creation_date": 1377686157, "is_accepted": false, "score": 0, "last_activity_date": 1377686157, "answer_id": 18485542}], "question_id": 18483303, "tags": ["maven", "ant", "solr", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/18483303/setting-up-nutch-2-2", "last_activity_date": 1377686157, "owner": {"age": 24, "answer_count": 43, "creation_date": 1374847004, "user_id": 2623052, "accept_rate": 45, "view_count": 206, "location": "Scotland", "reputation": 866}, "body": "<p>I am trying to set-up Nutch 2.2.\nWhen I run the <code>ant runtime</code> command to build the project it fails to build with:</p>\n\n<pre><code>Server access Error: Connection refused..........\n</code></pre>\n\n<p>Which is followed by the maven repositories of the files it cannot download.</p>\n\n<p>There is a proxy server set-up that often will not allow me to download certain things and I fear this is the problem here.  </p>\n\n<p>Is there any other way of setting this up?</p>\n", "creation_date": 1377679761, "score": 0},
{"title": "Getting null pointer exception when running Nutch crawler 2.2 with Hbase", "view_count": 427, "is_answered": true, "answers": [{"question_id": 18447363, "owner": {"user_id": 582789, "link": "http://stackoverflow.com/users/582789/alfonso-nishikawa", "user_type": "registered", "reputation": 1263}, "body": "<p>Command <code>/bin/nutch crawl</code> is deprecated in Nutch 2.x. Use <code>/bin/crawl</code> instead.</p>\n", "creation_date": 1377674764, "is_accepted": false, "score": 1, "last_activity_date": 1377674764, "answer_id": 18481717}], "question_id": 18447363, "tags": ["hadoop", "nutch", "web-crawler"], "answer_count": 1, "link": "http://stackoverflow.com/questions/18447363/getting-null-pointer-exception-when-running-nutch-crawler-2-2-with-hbase", "last_activity_date": 1377674764, "owner": {"user_id": 1965370, "answer_count": 47, "creation_date": 1357787137, "view_count": 170, "location": "Berlin, Germany", "reputation": 389}, "body": "<p>when I run the Nutch command: ~/nutch/runtime/deploy$ bin/nutch crawl urls -dir /user/dlequoc/urls -depth 2 -topN 5, I got a following exception:</p>\n\n<p>=======================================================</p>\n\n<blockquote>\n  <p>13/08/26 16:30:15 INFO mapred.JobClient:  map 100% reduce 0% 13/08/26\n  16:30:29 INFO mapred.JobClient: Task Id :\n  attempt_201308261546_0004_r_000000_0, Status : FAILED\n  java.lang.NullPointerException    at\n  org.apache.avro.util.Utf8.(Utf8.java:37)    at\n  org.apache.nutch.crawl.GeneratorReducer.setup(GeneratorReducer.java:100)\n    at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:174)    at\n  org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:649)\n    at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:417)     at\n  org.apache.hadoop.mapred.Child$4.run(Child.java:255)  at\n  java.security.AccessController.doPrivileged(Native Method)    at\n  javax.security.auth.Subject.doAs(Subject.java:396)    at\n  org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1121)\n    at org.apache.hadoop.mapred.Child.main(Child.java:249) 13/08/26\n  16:30:32 INFO mapred.JobClient: Task Id :\n  attempt_201308261546_0004_r_000001_0, Status : FAILED\n  java.lang.NullPointerException    at\n  org.apache.avro.util.Utf8.(Utf8.java:37)    at\n  org.apache.nutch.crawl.GeneratorReducer.setup(GeneratorReducer.java:100)\n    at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:174)    at\n  org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:649)\n    at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:417)     at\n  org.apache.hadoop.mapred.Child$4.run(Child.java:255)  at\n  java.security.AccessController.doPrivileged(Native Method)    at\n  javax.security.auth.Subject.doAs(Subject.java:396)    at\n  org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1121)\n    at org.apache.hadoop.mapred.Child.main(Child.java:249) 13/08/26\n  16:30:32 INFO mapred.JobClient: Task Id :\n  attempt_201308261546_0004_r_000005_0, Status : FAILED\n  java.lang.NullPointerException    at\n  org.apache.avro.util.Utf8.(Utf8.java:37)    at\n  org.apache.nutch.crawl.GeneratorReducer.setup(GeneratorReducer.java:100)\n    at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:174)    at\n  org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:649)\n    at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:417)     at\n  org.apache.hadoop.mapred.Child$4.run(Child.java:255)  at\n  java.security.AccessController.doPrivileged(Native Method)    at\n  javax.security.auth.Subject.doAs(Subject.java:396)    at\n  org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1121)\n    at org.apache.hadoop.mapred.Child.main(Child.java:249) 13/08/26\n  16:30:32 INFO mapred.JobClient: Task Id :\n  attempt_201308261546_0004_r_000004_0, Status : FAILED\n  java.lang.NullPointerException    at\n  org.apache.avro.util.Utf8.(Utf8.java:37)    at\n  org.apache.nutch.crawl.GeneratorReducer.setup(GeneratorReducer.java:100)\n    at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:174)    at\n  org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:649)\n    at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:417)     at\n  org.apache.hadoop.mapred.Child$4.run(Child.java:255)  at\n  java.security.AccessController.doPrivileged(Native Method)    at\n  javax.security.auth.Subject.doAs(Subject.java:396)    at\n  org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1121)\n    at org.apache.hadoop.mapred.Child.main(Child.java:249) 13/08/26\n  16:30:32 INFO mapred.JobClient: Task Id :\n  attempt_201308261546_0004_r_000002_0, Status : FAILED\n  java.lang.NullPointerException    at\n  org.apache.avro.util.Utf8.(Utf8.java:37)    at\n  org.apache.nutch.crawl.GeneratorReducer.setup(GeneratorReducer.java:100)\n    at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:174)    at\n  org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:649)\n    at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:417)     at\n  org.apache.hadoop.mapred.Child$4.run(Child.java:255)  at\n  java.security.AccessController.doPrivileged(Native Method)    at\n  javax.security.auth.Subject.doAs(Subject.java:396)    at\n  org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1121)\n    at org.apache.hadoop.mapred.Child.main(Child.java:249) 13/08/26\n  16:30:32 INFO mapred.JobClient: Task Id :\n  attempt_201308261546_0004_r_000003_0, Status : FAILED\n  java.lang.NullPointerException    at\n  org.apache.avro.util.Utf8.(Utf8.java:37)    at\n  org.apache.nutch.crawl.GeneratorReducer.setup(GeneratorReducer.java:100)\n    at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:174)    at\n  org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:649)\n    at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:417)     at\n  org.apache.hadoop.mapred.Child$4.run(Child.java:255)  at\n  java.security.AccessController.doPrivileged(Native Method)    at\n  javax.security.auth.Subject.doAs(Subject.java:396)    at\n  org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1121)\n    at org.apache.hadoop.mapred.Child.main(Child.java:249) 13/08/26\n  16:30:44 INFO mapred.JobClient: Task Id :\n  attempt_201308261546_0004_r_000001_1, Status : FAILED\n  java.lang.NullPointerException    at\n  org.apache.avro.util.Utf8.(Utf8.java:37)    at\n  org.apache.nutch.crawl.GeneratorReducer.setup(GeneratorReducer.java:100)\n    at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:174)    at\n  org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:649)\n    at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:417)     at\n  org.apache.hadoop.mapred.Child$4.run(Child.java:255)  at\n  java.security.AccessController.doPrivileged(Native Method)    at\n  javax.security.auth.Subject.doAs(Subject.java:396)    at\n  org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1121)\n    at org.apache.hadoop.mapred.Child.main(Child.java:249) 13/08/26\n  16:30:47 INFO mapred.JobClient: Task Id :\n  attempt_201308261546_0004_r_000000_1, Status : FAILED\n  java.lang.NullPointerException    at\n  org.apache.avro.util.Utf8.(Utf8.java:37)    at\n  org.apache.nutch.crawl.GeneratorReducer.setup(GeneratorReducer.java:100)\n    at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:174)    at\n  org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:649)\n    at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:417)     at\n  org.apache.hadoop.mapred.Child$4.run(Child.java:255)  at\n  java.security.AccessController.doPrivileged(Native Method)    at\n  javax.security.auth.Subject.doAs(Subject.java:396)    at\n  org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1121)\n    at org.apache.hadoop.mapred.Child.main(Child.java:249) 13/08/26\n  16:30:47 INFO mapred.JobClient: Task Id :\n  attempt_201308261546_0004_r_000005_1, Status : FAILED\n  java.lang.NullPointerException    at\n  org.apache.avro.util.Utf8.(Utf8.java:37)    at\n  org.apache.nutch.crawl.GeneratorReducer.setup(GeneratorReducer.java:100)\n    at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:174)    at\n  org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:649)\n    at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:417)     at\n  org.apache.hadoop.mapred.Child$4.run(Child.java:255)  at\n  java.security.AccessController.doPrivileged(Native Method)    at\n  javax.security.auth.Subject.doAs(Subject.java:396)    at\n  org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1121)\n    at org.apache.hadoop.mapred.Child.main(Child.java:249) 13/08/26\n  16:30:47 INFO mapred.JobClient: Task Id :\n  attempt_201308261546_0004_r_000002_1, Status : FAILED\n  java.lang.NullPointerException    at\n  org.apache.avro.util.Utf8.(Utf8.java:37)    at\n  org.apache.nutch.crawl.GeneratorReducer.setup(GeneratorReducer.java:100)\n    at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:174)    at\n  org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:649)\n    at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:417)     at\n  org.apache.hadoop.mapred.Child$4.run(Child.java:255)  at\n  java.security.AccessController.doPrivileged(Native Method)    at\n  javax.security.auth.Subject.doAs(Subject.java:396)    at\n  org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1121)\n    at org.apache.hadoop.mapred.Child.main(Child.java:249) 13/08/26\n  16:30:47 INFO mapred.JobClient: Task Id :\n  attempt_201308261546_0004_r_000004_1, Status : FAILED\n  java.lang.NullPointerException    at\n  org.apache.avro.util.Utf8.(Utf8.java:37)    at\n  org.apache.nutch.crawl.GeneratorReducer.setup(GeneratorReducer.java:100)\n    at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:174)    at\n  org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:649)\n    at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:417)     at\n  org.apache.hadoop.mapred.Child$4.run(Child.java:255)  at\n  java.security.AccessController.doPrivileged(Native Method)    at\n  javax.security.auth.Subject.doAs(Subject.java:396)    at\n  org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1121)\n    at org.apache.hadoop.mapred.Child.main(Child.java:249) 13/08/26\n  16:30:47 INFO mapred.JobClient: Task Id :\n  attempt_201308261546_0004_r_000003_1, Status : FAILED\n  java.lang.NullPointerException    at\n  org.apache.avro.util.Utf8.(Utf8.java:37)    at\n  org.apache.nutch.crawl.GeneratorReducer.setup(GeneratorReducer.java:100)\n    at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:174)    at\n  org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:649)\n    at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:417)     at\n  org.apache.hadoop.mapred.Child$4.run(Child.java:255)  at\n  java.security.AccessController.doPrivileged(Native Method)    at\n  javax.security.auth.Subject.doAs(Subject.java:396)    at\n  org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1121)\n    at org.apache.hadoop.mapred.Child.main(Child.java:249) 13/08/26\n  16:30:59 INFO mapred.JobClient: Task Id :\n  attempt_201308261546_0004_r_000000_2, Status : FAILED\n  java.lang.NullPointerException    at\n  org.apache.avro.util.Utf8.(Utf8.java:37)    at\n  org.apache.nutch.crawl.GeneratorReducer.setup(GeneratorReducer.java:100)\n    at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:174)    at\n  org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:649)\n    at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:417)     at\n  org.apache.hadoop.mapred.Child$4.run(Child.java:255)  at\n  java.security.AccessController.doPrivileged(Native Method)    at\n  javax.security.auth.Subject.doAs(Subject.java:396)    at\n  org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1121)\n    at org.apache.hadoop.mapred.Child.main(Child.java:249) 13/08/26\n  16:30:59 INFO mapred.JobClient: Task Id :\n  attempt_201308261546_0004_r_000002_2, Status : FAILED\n  java.lang.NullPointerException    at\n  org.apache.avro.util.Utf8.(Utf8.java:37)    at\n  org.apache.nutch.crawl.GeneratorReducer.setup(GeneratorReducer.java:100)\n    at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:174)    at\n  org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:649)\n    at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:417)     at\n  org.apache.hadoop.mapred.Child$4.run(Child.java:255)  at\n  java.security.AccessController.doPrivileged(Native Method)    at\n  javax.security.auth.Subject.doAs(Subject.java:396)    at\n  org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1121)\n    at org.apache.hadoop.mapred.Child.main(Child.java:249) 13/08/26\n  16:30:59 INFO mapred.JobClient: Task Id :\n  attempt_201308261546_0004_r_000001_2, Status : FAILED\n  java.lang.NullPointerException    at\n  org.apache.avro.util.Utf8.(Utf8.java:37)    at\n  org.apache.nutch.crawl.GeneratorReducer.setup(GeneratorReducer.java:100)\n    at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:174)    at\n  org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:649)\n    at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:417)     at\n  org.apache.hadoop.mapred.Child$4.run(Child.java:255)  at\n  java.security.AccessController.doPrivileged(Native Method)    at\n  javax.security.auth.Subject.doAs(Subject.java:396)    at\n  org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1121)\n    at org.apache.hadoop.mapred.Child.main(Child.java:249) 13/08/26\n  16:31:02 INFO mapred.JobClient: Task Id :\n  attempt_201308261546_0004_r_000005_2, Status : FAILED\n  java.lang.NullPointerException    at\n  org.apache.avro.util.Utf8.(Utf8.java:37)    at\n  org.apache.nutch.crawl.GeneratorReducer.setup(GeneratorReducer.java:100)\n    at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:174)    at\n  org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:649)\n    at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:417)     at\n  org.apache.hadoop.mapred.Child$4.run(Child.java:255)  at\n  java.security.AccessController.doPrivileged(Native Method)    at\n  javax.security.auth.Subject.doAs(Subject.java:396)    at\n  org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1121)\n    at org.apache.hadoop.mapred.Child.main(Child.java:249) 13/08/26\n  16:31:05 INFO mapred.JobClient: Task Id :\n  attempt_201308261546_0004_r_000003_2, Status : FAILED\n  java.lang.NullPointerException    at\n  org.apache.avro.util.Utf8.(Utf8.java:37)    at\n  org.apache.nutch.crawl.GeneratorReducer.setup(GeneratorReducer.java:100)\n    at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:174)    at\n  org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:649)\n    at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:417)     at\n  org.apache.hadoop.mapred.Child$4.run(Child.java:255)  at\n  java.security.AccessController.doPrivileged(Native Method)    at\n  javax.security.auth.Subject.doAs(Subject.java:396)    at\n  org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1121)\n    at org.apache.hadoop.mapred.Child.main(Child.java:249) 13/08/26\n  16:31:11 INFO mapred.JobClient: Task Id :\n  attempt_201308261546_0004_r_000004_2, Status : FAILED\n  java.lang.NullPointerException    at\n  org.apache.avro.util.Utf8.(Utf8.java:37)    at\n  org.apache.nutch.crawl.GeneratorReducer.setup(GeneratorReducer.java:100)\n    at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:174)    at\n  org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:649)\n    at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:417)     at\n  org.apache.hadoop.mapred.Child$4.run(Child.java:255)  at\n  java.security.AccessController.doPrivileged(Native Method)    at\n  javax.security.auth.Subject.doAs(Subject.java:396)    at\n  org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1121)\n    at org.apache.hadoop.mapred.Child.main(Child.java:249) 13/08/26\n  16:31:20 INFO mapred.JobClient: Job complete: job_201308261546_0004\n  13/08/26 16:31:20 INFO mapred.JobClient: Counters: 24 13/08/26\n  16:31:20 INFO mapred.JobClient:   Job Counters  13/08/26 16:31:20 INFO\n  mapred.JobClient:     Launched reduce tasks=23 13/08/26 16:31:20 INFO\n  mapred.JobClient:     SLOTS_MILLIS_MAPS=113452 13/08/26 16:31:20 INFO\n  mapred.JobClient:     Total time spent by all reduces waiting after\n  reserving slots (ms)=0 13/08/26 16:31:20 INFO mapred.JobClient:<br>\n  Total time spent by all maps waiting after reserving slots (ms)=0\n  13/08/26 16:31:20 INFO mapred.JobClient:     Rack-local map tasks=1\n  13/08/26 16:31:20 INFO mapred.JobClient:     Launched map tasks=1\n  13/08/26 16:31:20 INFO mapred.JobClient:     Failed reduce tasks=1\n  13/08/26 16:31:20 INFO mapred.JobClient:<br>\n  SLOTS_MILLIS_REDUCES=268210 13/08/26 16:31:20 INFO mapred.JobClient:<br>\n  FileSystemCounters 13/08/26 16:31:20 INFO mapred.JobClient:<br>\n  FILE_BYTES_READ=25743276 13/08/26 16:31:20 INFO mapred.JobClient:<br>\n  HDFS_BYTES_READ=704 13/08/26 16:31:20 INFO mapred.JobClient:<br>\n  FILE_BYTES_WRITTEN=51473783 13/08/26 16:31:20 INFO mapred.JobClient:<br>\n  File Input Format Counters  13/08/26 16:31:20 INFO mapred.JobClient:<br>\n  Bytes Read=0 13/08/26 16:31:20 INFO mapred.JobClient:   Map-Reduce\n  Framework 13/08/26 16:31:20 INFO mapred.JobClient:     Map output\n  materialized bytes=25720344 13/08/26 16:31:20 INFO mapred.JobClient:<br>\n  Combine output records=0 13/08/26 16:31:20 INFO mapred.JobClient:<br>\n  Map input records=333988 13/08/26 16:31:20 INFO mapred.JobClient:<br>\n  Physical memory (bytes) snapshot=449036288 13/08/26 16:31:20 INFO\n  mapred.JobClient:     Spilled Records=667976 13/08/26 16:31:20 INFO\n  mapred.JobClient:     Map output bytes=25052332 13/08/26 16:31:20 INFO\n  mapred.JobClient:     CPU time spent (ms)=81870 13/08/26 16:31:20 INFO\n  mapred.JobClient:     Total committed heap usage (bytes)=208011264\n  13/08/26 16:31:20 INFO mapred.JobClient:     Virtual memory (bytes)\n  snapshot=740638720 13/08/26 16:31:20 INFO mapred.JobClient:<br>\n  Combine input records=0 13/08/26 16:31:20 INFO mapred.JobClient:<br>\n  Map output records=333988 13/08/26 16:31:20 INFO mapred.JobClient:<br>\n  SPLIT_RAW_BYTES=704 Exception in thread \"main\"\n  java.lang.RuntimeException: job failed: name=generate: null,\n  jobid=job_201308261546_0004   at\n  org.apache.nutch.util.NutchJob.waitForCompletion(NutchJob.java:54)    at\n  org.apache.nutch.crawl.GeneratorJob.run(GeneratorJob.java:199)    at\n  org.apache.nutch.crawl.Crawler.runTool(Crawler.java:68)   at\n  org.apache.nutch.crawl.Crawler.run(Crawler.java:152)  at\n  org.apache.nutch.crawl.Crawler.run(Crawler.java:250)  at\n  org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)     at\n  org.apache.nutch.crawl.Crawler.main(Crawler.java:257)     at\n  sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)   at\n  sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n    at\n  sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n    at java.lang.reflect.Method.invoke(Method.java:597)     at\n  org.apache.hadoop.util.RunJar.main(RunJar.java:156)</p>\n</blockquote>\n\n<p>could you please help?\nThanks!</p>\n", "creation_date": 1377529862, "score": 0},
{"title": "Running Nutch crawls on EMR (newbie)", "view_count": 1136, "is_answered": false, "answers": [{"last_edit_date": 1377629454, "owner": {"user_id": 1341806, "accept_rate": 78, "link": "http://stackoverflow.com/users/1341806/dan-ciborowski-msft", "user_type": "registered", "reputation": 2723}, "body": "<p>So to start off, this can not really to be done using the GUI. Instead I got nutch working using the AWS Java API.</p>\n\n<p>I have my seed files located in s3, and I transfer my output back to s3.</p>\n\n<p>I use the dsdistcp jar to copy the data from s3 to hdfs. </p>\n\n<p>Here is my basic step config. MAINCLASS is going to be the package details of your nutch crawl. Something like org.apach.nutch.mainclass.</p>\n\n<pre><code>    String HDFSDIR = \"/user/hadoop/data/\";\n\n    stepconfigs.add(new StepConfig()\n            .withName(\"Add Data\")\n            .withActionOnFailure(ActionOnFailure.TERMINATE_JOB_FLOW)\n            .withHadoopJarStep(new HadoopJarStepConfig(prop.getProperty(\"S3DISTCP\"))\n            .withArgs(\"--src\", prop.getProperty(\"DATAFOLDER\"), \"--dest\", HDFSDIR)));\n\n    stepconfigs.add(new StepConfig()\n            .withName(\"Run Main Job\")\n            .withActionOnFailure(ActionOnFailure.CONTINUE)\n            .withHadoopJarStep(new HadoopJarStepConfig(nutch-1.7.jar)\n            .withArgs(\"org.apache.nutch.crawl.Crawl\", prop.getProperty(\"CONF\"), prop.getProperty(\"STEPS\"), \"-id=\" + jobId)));\n\n\n    stepconfigs.add(new StepConfig()\n            .withName(\"Pull Output\")\n            .withActionOnFailure(ActionOnFailure.TERMINATE_JOB_FLOW)\n            .withHadoopJarStep(new HadoopJarStepConfig(prop.getProperty(\"S3DISTCP\"))\n            .withArgs(\"--src\", HDFSDIR, \"--dest\", prop.getProperty(\"DATAFOLDER\"))));\n\n    new AmazonElasticMapReduceClient(new PropertiesCredentials(new File(\"AwsCredentials.properties\")), proxy ? new ClientConfiguration().withProxyHost(\"dawebproxy00.americas.nokia.com\").withProxyPort(8080) : null)\n                .runJobFlow(new RunJobFlowRequest()\n                .withName(\"Job: \" + jobId)\n                .withLogUri(prop.getProperty(\"LOGDIR\"))\n                .withAmiVersion(prop.getProperty(\"AMIVERSION\"))\n                .withSteps(getStepConfig(prop, jobId))\n                .withBootstrapActions(getBootStrap(prop))\n                .withInstances(getJobFlowInstancesConfig(prop)));\n</code></pre>\n", "question_id": 16890167, "creation_date": 1377629078, "is_accepted": false, "score": 0, "last_activity_date": 1377629454, "answer_id": 18473159}], "question_id": 16890167, "tags": ["amazon-web-services", "nutch", "emr"], "answer_count": 1, "link": "http://stackoverflow.com/questions/16890167/running-nutch-crawls-on-emr-newbie", "last_activity_date": 1377629454, "owner": {"user_id": 2446562, "view_count": 24, "answer_count": 12, "creation_date": 1370234508, "reputation": 355}, "body": "<p>I'm a first time EMR/Hadoop user and first time Apache Nutch user.  I'm trying to use Apache Nutch 2.1 to do some screen scraping.  I'd like to run it on hadoop, but don't want to setup my own cluster (one learning curve at a time).  So I'm using EMR.  And I'd like S3 to be used for output (and whatever input I need).</p>\n\n<p>I've been reading the setup wikis for Nutch:</p>\n\n<p><a href=\"http://wiki.apache.org/nutch/NutchTutorial\" rel=\"nofollow\">http://wiki.apache.org/nutch/NutchTutorial</a><br/>\n<a href=\"http://wiki.apache.org/nutch/NutchHadoopTutorial\" rel=\"nofollow\">http://wiki.apache.org/nutch/NutchHadoopTutorial</a><br/></p>\n\n<p>And they've been very helpful in getting me up to speed on the very basics of nutch.  I realize I can build nutch from source, preconfigure some regexes, then be left with a hadoop friendly jar:</p>\n\n<pre><code>$NUTCH_HOME/runtime/deploy/apache-nutch-2.1.job\n</code></pre>\n\n<p>Most of the tutorials culminate in a crawl command being run.  In the Hadoop examples, it's:</p>\n\n<pre><code>hadoop jar nutch-${version}.jar org.apache.nutch.crawl.Crawl urls -dir crawl -depth 3 -topN 5\n</code></pre>\n\n<p>And in the local deployment example it's something like:</p>\n\n<pre><code>bin/nutch crawl urls -dir crawl -depth 3 -topN 5\n</code></pre>\n\n<p>My question is as follows.  What do I have to do to get my apache-nutch-2.1.job to run on EMR?  What arguments to I pass it?  For the hadoop crawl example above, the \"urls\" file is already on hdfs with seed URLs.  How do I do this on EMR?  Also, what do I specify on the command line to have my final output to go S3 instead of HDFS?</p>\n", "creation_date": 1370235051, "score": 2},
{"title": "Dumping Nutch Crawldb", "view_count": 630, "owner": {"age": 24, "answer_count": 43, "creation_date": 1374847004, "user_id": 2623052, "accept_rate": 45, "view_count": 206, "location": "Scotland", "reputation": 866}, "is_answered": true, "answers": [{"last_edit_date": 1377606490, "owner": {"user_id": 2695269, "link": "http://stackoverflow.com/users/2695269/mgs", "user_type": "registered", "reputation": 709}, "body": "<p>CrawlDbReader in Nutch 1.4 don't generate dump of crawldb on the basis of Document's status. In 1.5 and later versions of Nutch you can specify status of document during crawldb reading and readdb will generate dump of documents with specified status.</p>\n\n<pre><code>[root@srchengn nutch]# bin/nutch readdb &lt;path_crawldb&gt; -dump &lt;output_directory&gt; -status db_gone\n</code></pre>\n\n<p>If you want to do the same in Nutch 1.4 you have to modify <code>org.apache.nutch.crawl.CrawlDbReader</code> class.</p>\n", "question_id": 18446024, "creation_date": 1377605459, "is_accepted": true, "score": 2, "last_activity_date": 1377606490, "answer_id": 18465039}], "question_id": 18446024, "tags": ["solr", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/18446024/dumping-nutch-crawldb", "last_activity_date": 1377606490, "accepted_answer_id": 18465039, "body": "<p>How can I get a dump of the Nutch crawldb of all the urls with status 3 (db_gone). The version of Nutch I am using 1.4.</p>\n\n<p>I looked at the wiki but it is unclear on how to do this</p>\n", "creation_date": 1377525762, "score": 0},
{"title": "org.apache.nutch.crawl.Crawler NPE at org.apache.avro.util.Utf8.&lt;init&gt;(Utf8.java:37)", "view_count": 804, "owner": {"user_id": 1051305, "answer_count": 20, "creation_date": 1321518068, "accept_rate": 66, "view_count": 243, "location": "Bangalore", "reputation": 999}, "is_answered": true, "answers": [{"question_id": 17951954, "owner": {"user_id": 1965370, "link": "http://stackoverflow.com/users/1965370/do-do", "user_type": "registered", "reputation": 389}, "body": "<p>You should copy file $NutchHome/src/bin/crawl to the deploy directory: $NutchHome/runtime/deploy/bin and then run the crawl command script : </p>\n\n<blockquote>\n  <p>crawl &lt; seedDir>  &lt; crawlId>  &lt; numberOfRounds></p>\n</blockquote>\n\n<p>Hope this helps.</p>\n", "creation_date": 1377540339, "is_accepted": true, "score": 0, "last_activity_date": 1377540339, "answer_id": 18450208}], "question_id": 17951954, "tags": ["java", "cygwin", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/17951954/org-apache-nutch-crawl-crawler-npe-at-org-apache-avro-util-utf8-initutf8-java", "last_activity_date": 1377540339, "accepted_answer_id": 18450208, "body": "<p>When i am trying to run org.apache.nutch.crawl.Crawler class using the eclipse launcher, getting the following exception. I don't have any idea on this. </p>\n\n<pre><code>java.lang.NullPointerException\n    at org.apache.avro.util.Utf8.&lt;init&gt;(Utf8.java:37)\n    at org.apache.nutch.crawl.GeneratorReducer.setup(GeneratorReducer.java:100)\n    at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:174)\n    at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:566)\n    at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:408)\n    at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:216)\n13/07/30 21:14:26 INFO mapred.JobClient:  map 100% reduce 0%\n13/07/30 21:14:26 INFO mapred.JobClient: Job complete: job_local_0002\n13/07/30 21:14:26 INFO mapred.JobClient: Counters: 12\n13/07/30 21:14:26 INFO mapred.JobClient:   FileSystemCounters\n13/07/30 21:14:26 INFO mapred.JobClient:     FILE_BYTES_READ=47606\n13/07/30 21:14:26 INFO mapred.JobClient:     FILE_BYTES_WRITTEN=97164\n13/07/30 21:14:26 INFO mapred.JobClient:   Map-Reduce Framework\n13/07/30 21:14:26 INFO mapred.JobClient:     Reduce input groups=0\n13/07/30 21:14:26 INFO mapred.JobClient:     Combine output records=0\n13/07/30 21:14:26 INFO mapred.JobClient:     Map input records=0\n13/07/30 21:14:26 INFO mapred.JobClient:     Reduce shuffle bytes=0\n13/07/30 21:14:26 INFO mapred.JobClient:     Reduce output records=0\n13/07/30 21:14:26 INFO mapred.JobClient:     Spilled Records=0\n13/07/30 21:14:26 INFO mapred.JobClient:     Map output bytes=0\n13/07/30 21:14:26 INFO mapred.JobClient:     Combine input records=0\n13/07/30 21:14:26 INFO mapred.JobClient:     Map output records=0\n13/07/30 21:14:26 INFO mapred.JobClient:     Reduce input records=0\nException in thread \"main\" java.lang.RuntimeException: job failed: name=generate: null, jobid=null\n    at org.apache.nutch.util.NutchJob.waitForCompletion(NutchJob.java:54)\n    at org.apache.nutch.crawl.GeneratorJob.run(GeneratorJob.java:199)\n    at org.apache.nutch.crawl.Crawler.runTool(Crawler.java:68)\n    at org.apache.nutch.crawl.Crawler.run(Crawler.java:152)\n    at org.apache.nutch.crawl.Crawler.run(Crawler.java:250)\n    at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n    at org.apache.nutch.crawl.Crawler.main(Crawler.java:257)\n</code></pre>\n\n<p>After doing a bit of google came across <a href=\"http://lucene.472066.n3.nabble.com/Null-Pointer-Exception-trying-to-run-Nutch-td4079866.html\" rel=\"nofollow\">this</a> (Mentioned class is deprecated in Nutch2.x, instead of that  <strong>$NutchHome/src/bin/crawl</strong> script should be used). Even i have tried running crawl script from cygwin terminal, but no luck. Screen shot of error from terminal.</p>\n\n<p><img src=\"http://i.stack.imgur.com/ZI0WO.png\" alt=\"**enter image description here**\"></p>\n", "creation_date": 1375200188, "score": 0},
{"title": "Id based crawling with Nutch2.x with HBase -&gt; SolrIndexerJob not working", "view_count": 216, "owner": {"user_id": 2452071, "answer_count": 1, "creation_date": 1370354387, "accept_rate": 62, "view_count": 24, "reputation": 116}, "is_answered": true, "answers": [{"question_id": 17292841, "owner": {"user_id": 404308, "accept_rate": 25, "link": "http://stackoverflow.com/users/404308/mustafa", "user_type": "registered", "reputation": 61}, "body": "<p>You can run solrIndexerJob with script below by crawlId. This will index only C1_webpage table.</p>\n\n<pre><code>bin/nutch solrindex http://localhost:8983/solr -all -crawlId C1\n</code></pre>\n", "creation_date": 1377335110, "is_accepted": true, "score": 0, "last_activity_date": 1377335110, "answer_id": 18416813}], "question_id": 17292841, "tags": ["solr", "hbase", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/17292841/id-based-crawling-with-nutch2-x-with-hbase-solrindexerjob-not-working", "last_activity_date": 1377335110, "accepted_answer_id": 18416813, "body": "<p>I am using Nutch2.x with hbase 0.90.6 and the first strange thing I have noticed is that it is creating its 'webpage' table with prefix of crawlId i.e. if my crawlId is C1 then it creates table as 'C1_webpage' , which I think should not be. But it is doing so and my nutch jobs [ Inject -> Generate -> Fetch -> Parse -> DBUpdate ] are running fine.</p>\n\n<p>Now problem I am facing is that with this setup and 'C1_webpage' table , SolrIndexjob is not inserting any document to Solr , because I think it looks for docs in 'webpage' table which always have 0 rows and actual data is in 'C1_webpage'. </p>\n\n<p>How should I solve my this problem ? \nAny one is using nutch2.x with hbase 0.90.6 and doing Id based crawling ?</p>\n\n<p>Thanks,\nTony</p>\n", "creation_date": 1372149836, "score": 0},
{"title": "SolrClean isn&#39;t deleting from Solr", "view_count": 509, "is_answered": true, "answers": [{"question_id": 18331166, "owner": {"user_id": 2695269, "link": "http://stackoverflow.com/users/2695269/mgs", "user_type": "registered", "reputation": 709}, "body": "<p>Take dump of crawldb and find urls with status db_gone and check for these urls in solr index. If you are doing fresh crawl you wont find any of these urls in your Solr index as db_gone urls are not considered for indexing. <br/> </p>\n\n<p>Now when we run solrclean command nutch finds all the urls with status db_gone in crawldb and deletes all these urls from Solr index. If any url with status db_gone found in solr index then only index will be changed otherwise index will remain unchanged. <br/></p>\n\n<p>This may be happening in your case.<br/></p>\n\n<p>Note: <br/>In case of re-crawl only we may find out some urls which already crawled and indexed successfully during first crawling but marked as db_gone in re-crawl phase as these urls are not available anymore. So these urls will be deleted from solr index when we run solrclean command and solr index will be changed.<br/></p>\n\n<p>Have a look into source code of SolrClean class.<br/>\n<a href=\"http://grepcode.com/file/repo1.maven.org/maven2/org.apache.nutch/nutch/1.3/org/apache/nutch/indexer/solr/SolrClean.java\" rel=\"nofollow\">http://grepcode.com/file/repo1.maven.org/maven2/org.apache.nutch/nutch/1.3/org/apache/nutch/indexer/solr/SolrClean.java</a></p>\n\n<p>Hope helped you....</p>\n", "creation_date": 1377186881, "is_accepted": false, "score": 1, "last_activity_date": 1377186881, "answer_id": 18385381}], "question_id": 18331166, "tags": ["solr", "nutch", "solr4"], "answer_count": 1, "link": "http://stackoverflow.com/questions/18331166/solrclean-isnt-deleting-from-solr", "last_activity_date": 1377254213, "owner": {"age": 24, "answer_count": 43, "creation_date": 1374847004, "user_id": 2623052, "accept_rate": 45, "view_count": 206, "location": "Scotland", "reputation": 866}, "body": "<p>When I run Nutch and a link is no longer present, I can run the <code>readdb</code> command and it shows me that there are urls marked as <code>db_gone</code>.</p>\n\n<p>So I run the SolrClean command and it says:</p>\n\n<pre><code>SolrClean deleting a total of 1 documents\n</code></pre>\n\n<p>Which is correct, but nothing is removed from Solr</p>\n\n<p>Help?</p>\n\n<p>If you want to check my configuration then I have a blog with how my own Solr/Nutch set-up is configured <a href=\"http://amac4.blogspot.co.uk/2013/07/setting-up-solr-with-apache-tomcat-be.html\" rel=\"nofollow\">Here</a></p>\n\n<p><em><strong>Edit</em></strong>:</p>\n\n<p>There is a good chance it is not just the SolrClean command that isnt working, I have a feeling it is something to do with my set-up where deletes arent being committed?</p>\n\n<p>This is the delete request issued for the document - yet the document exists:</p>\n\n<pre><code>INFO  - 2013-08-09 15:54:52.729; \norg.apache.solr.update.processor.LogUpdateProcessor; [collection1] webapp=/solr path=/update params={wt=javabin&amp;version=2} \n{delete=file:/C:/Users/alamil/Documents/TextFiles/Y2012.doc \n(-1442903587791306752)]} 0 2\n</code></pre>\n\n<p>This is the whole log:</p>\n\n<pre><code> INFO  - 2013-08-09 15:54:51.785; org.apache.solr.search.SolrIndexSearcher; Opening Searcher@f5331a main\nINFO  - 2013-08-09 15:54:51.786; org.apache.solr.core.QuerySenderListener; QuerySenderListener sending requests to Searcher@f5331a main{StandardDirectoryReader(segments_7w:549:nrt _6j(4.3.1):C12/11 _6k(4.3.1):C12)}\nINFO  - 2013-08-09 15:54:51.787; org.apache.solr.core.QuerySenderListener; QuerySenderListener done.\nINFO  - 2013-08-09 15:54:51.787; org.apache.solr.update.DirectUpdateHandler2; end_commit_flush\nINFO  - 2013-08-09 15:54:51.788; org.apache.solr.core.SolrCore; [collection1] Registered new searcher Searcher@f5331a main{StandardDirectoryReader(segments_7w:549:nrt _6j(4.3.1):C12/11 _6k(4.3.1):C12)}\nINFO  - 2013-08-09 15:54:51.789; org.apache.solr.update.processor.LogUpdateProcessor; [collection1] webapp=/solr path=/update params={waitSearcher=true&amp;commit=true&amp;wt=javabin&amp;waitFlush=true&amp;version=2} {commit=} 0 903\nINFO  - 2013-08-09 15:54:52.053; org.apache.solr.core.SolrCore; [collection1] webapp=/solr path=/select params={fl=id&amp;q=id:[*+TO+*]&amp;wt=javabin&amp;version=2&amp;rows=1} hits=13 status=0 QTime=1 \nINFO  - 2013-08-09 15:54:52.355; org.apache.solr.core.SolrCore; [collection1] webapp=/solr path=/select params={fl=id&amp;q=id:[*+TO+*]&amp;wt=javabin&amp;version=2&amp;rows=1} hits=13 status=0 QTime=0 \nINFO  - 2013-08-09 15:54:52.413; org.apache.solr.core.SolrCore; [collection1] webapp=/solr path=/select params={fl=id,boost,tstamp,digest&amp;start=0&amp;q=id:[*+TO+*]&amp;wt=javabin&amp;version=2&amp;rows=13} hits=13 status=0 QTime=1 \nINFO  - 2013-08-09 15:54:52.729; org.apache.solr.update.processor.LogUpdateProcessor; [collection1] webapp=/solr path=/update params={wt=javabin&amp;version=2} {delete=[file:/C:/Users/alamil/Documents/TextFiles/Y2012.doc (-1442903587791306752)]} 0 2\nINFO  - 2013-08-09 15:54:52.733; org.apache.solr.update.DirectUpdateHandler2; start commit{,optimize=false,openSearcher=true,waitSearcher=true,expungeDeletes=false,softCommit=false,prepareCommit=false}\nINFO  - 2013-08-09 15:54:52.835; org.apache.solr.core.SolrDeletionPolicy; SolrDeletionPolicy.onCommit: commits:num=2\ncommit{dir=NRTCachingDirectory(org.apache.lucene.store.SimpleFSDirectory@C:\\Users\\alamil\\Documents\\Test\\solr_home\\data\\index lockFactory=org.apache.lucene.store.NativeFSLockFactory@1ae0e7d; maxCacheMB=48.0 maxMergeSizeMB=4.0),segFN=segments_7w,generation=284,filenames=[_6j_1.del, _6k_Lucene41_0.pos, segments_7w, _6j.nvd, _6j_Lucene41_0.tim, _6j_Lucene41_0.tip, _6k.fdt, _6k.fnm, _6j_Lucene41_0.pos, _6j.nvm, _6k_Lucene41_0.doc, _6k_Lucene41_0.tim, _6k.si, _6j.si, _6k.nvd, _6k.fdx, _6j_Lucene41_0.doc, _6j.fdt, _6k.nvm, _6j.fdx, _6k_Lucene41_0.tip, _6j.fnm]\ncommit{dir=NRTCachingDirectory(org.apache.lucene.store.SimpleFSDirectory@C:\\Users\\alamil\\Documents\\Test\\solr_home\\data\\index lockFactory=org.apache.lucene.store.NativeFSLockFactory@1ae0e7d; maxCacheMB=48.0 maxMergeSizeMB=4.0),segFN=segments_7x,generation=285,filenames=[_6j_1.del, _6k_Lucene41_0.pos, _6j.nvd, _6j_Lucene41_0.tim, _6j_Lucene41_0.tip, _6k.fdt, _6k.fnm, _6j_Lucene41_0.pos, _6j.nvm, _6k_Lucene41_0.doc, _6k_Lucene41_0.tim, _6k.si, _6j.si, _6k.nvd, _6k.fdx, segments_7x, _6j_Lucene41_0.doc, _6j.fdt, _6k.nvm, _6j.fdx, _6k_Lucene41_0.tip, _6j.fnm]\nINFO  - 2013-08-09 15:54:52.835; org.apache.solr.core.SolrDeletionPolicy; newest commit = 285[_6j_1.del, _6k_Lucene41_0.pos, _6j.nvd, _6j_Lucene41_0.tim, _6j_Lucene41_0.tip, _6k.fdt, _6k.fnm, _6j_Lucene41_0.pos, _6j.nvm, _6k_Lucene41_0.doc, _6k_Lucene41_0.tim, _6k.si, _6j.si, _6k.nvd, _6k.fdx, segments_7x, _6j_Lucene41_0.doc, _6j.fdt, _6k.nvm, _6j.fdx, _6k_Lucene41_0.tip, _6j.fnm]\n</code></pre>\n", "creation_date": 1376989772, "score": 1},
{"title": "Best Tika integration on Solr or Nutch", "view_count": 2181, "owner": {"user_id": 1663662, "answer_count": 26, "creation_date": 1347385562, "accept_rate": 75, "view_count": 143, "location": "MEX", "reputation": 827}, "is_answered": true, "answers": [{"question_id": 18132608, "owner": {"user_id": 2695269, "link": "http://stackoverflow.com/users/2695269/mgs", "user_type": "registered", "reputation": 709}, "body": "<p>Apply tika parser in Nutch's parsing phase.</p>\n", "creation_date": 1377069921, "is_accepted": false, "score": 0, "last_activity_date": 1377069921, "answer_id": 18351253}, {"question_id": 18132608, "owner": {"user_id": 2623052, "accept_rate": 45, "link": "http://stackoverflow.com/users/2623052/allan-macmillan", "user_type": "registered", "reputation": 866}, "body": "<p>Set up the Tika plugin with Nutch, Nutch will parse the data for you and will do all the hard work for you.  </p>\n\n<p>I would suggest setting it up on Solr as well, you may wish to send documents to Solr via the <code>curl</code> command and it would help to have it set up on Solr too.  It comes with little extra configuration and no performance costs:</p>\n\n<p>There is a guide to setting up Tika &amp; extracting request handler <a href=\"http://amac4.blogspot.com/2013/07/setting-up-tika-extracting-request.html\" rel=\"nofollow\">here</a></p>\n", "creation_date": 1377073785, "is_accepted": true, "score": 1, "last_activity_date": 1377073785, "answer_id": 18352479}], "question_id": 18132608, "tags": ["solr", "nutch", "apache-tika"], "answer_count": 2, "link": "http://stackoverflow.com/questions/18132608/best-tika-integration-on-solr-or-nutch", "last_activity_date": 1377073785, "accepted_answer_id": 18352479, "body": "<p>Which is the best integration for Apache Tika assuming that I already connected and used  Nutch(2.2.1) + Solr (4.3)? </p>\n\n<p>I understand that Tika can be integrated within Nutch and/or Solr, but which one is the best decision?</p>\n", "creation_date": 1375983974, "score": 0},
{"title": "Nutch not moving documents to status DB_GONE", "view_count": 293, "owner": {"age": 24, "answer_count": 43, "creation_date": 1374847004, "user_id": 2623052, "accept_rate": 45, "view_count": 206, "location": "Scotland", "reputation": 866}, "is_answered": true, "answers": [{"question_id": 18310402, "owner": {"user_id": 2695269, "link": "http://stackoverflow.com/users/2695269/mgs", "user_type": "registered", "reputation": 709}, "body": "<p>Check <strong>db.fetch.retry.max</strong> property in nutch configuration file. By default it set to 3. Only after maximum number of retries nutch will mark the document as <strong>db_gone</strong>. \ni.e. by default after 3rd retry nutch will mark a document <strong>db_gone</strong>, before that status will remain <strong>db_unfetched</strong>.</p>\n", "creation_date": 1376981033, "is_accepted": true, "score": 1, "last_activity_date": 1376981033, "answer_id": 18328579}], "question_id": 18310402, "tags": ["solr", "nutch", "web-crawler"], "answer_count": 1, "link": "http://stackoverflow.com/questions/18310402/nutch-not-moving-documents-to-status-db-gone", "last_activity_date": 1376981033, "accepted_answer_id": 18328579, "body": "<p>From my understanding of Nutch, when Nutch is doing a recrawl and attempts to fetch a document that no longer exists, it generates a 404 and sets the status of that document to DB_GONE.  When I recrawl with Nutch it generates the 404 error , but when i do a:</p>\n\n<pre><code>readdb folder/crawldb - stats\n</code></pre>\n\n<p>it shows the file as db_unfetched as opposed to db_gone.  This is causing big problems as I cannot keep my Solr index up-to-date</p>\n\n<p>If you want to check my Solr or Nutch set-up then follow my blog guides and it should be pretty identical: <a href=\"http://amac4.blogspot.co.uk\" rel=\"nofollow\">http://amac4.blogspot.co.uk</a></p>\n", "creation_date": 1376904183, "score": 0},
{"title": "Nutch ERROR tika.TikaParser on Eclipse", "view_count": 293, "owner": {"user_id": 1663662, "answer_count": 26, "creation_date": 1347385562, "accept_rate": 75, "view_count": 143, "location": "MEX", "reputation": 827}, "is_answered": true, "answers": [{"question_id": 18264214, "owner": {"user_id": 2623052, "accept_rate": 45, "link": "http://stackoverflow.com/users/2623052/allan-macmillan", "user_type": "registered", "reputation": 866}, "body": "<p>Check out this thread, seems like he had a similar problem and resolved it:  </p>\n\n<p><a href=\"http://lucene.472066.n3.nabble.com/Nutch-2-x-Eclipse-Can-t-retrieve-Tika-parser-for-mime-type-application-pdf-td4015896.html\" rel=\"nofollow\">http://lucene.472066.n3.nabble.com/Nutch-2-x-Eclipse-Can-t-retrieve-Tika-parser-for-mime-type-application-pdf-td4015896.html</a></p>\n", "creation_date": 1376917513, "is_accepted": true, "score": 0, "last_activity_date": 1376917513, "answer_id": 18314538}], "question_id": 18264214, "tags": ["eclipse", "nutch", "apache-tika"], "answer_count": 1, "link": "http://stackoverflow.com/questions/18264214/nutch-error-tika-tikaparser-on-eclipse", "last_activity_date": 1376917513, "accepted_answer_id": 18314538, "body": "<p>I am running Nutch 2.2.1 on Eclipse Juno SR1 and JRE 1.7.0_25</p>\n\n<p>The PARSE step is failing with this error:</p>\n\n<pre><code>2013-08-15 19:35:26,555 ERROR tika.TikaParser - Can't retrieve Tika parser for mime-type application/pdf\n2013-08-15 19:35:26,557 WARN  parse.ParseUtil - Unable to successfully parse content\n</code></pre>\n\n<p>It seems like this error is coming from TikaConfig.java because I get an empty iterator:</p>\n\n<pre><code>Iterator&lt;Parser&gt; iterator = ServiceRegistry.lookupProviders(\n    Parser.class, this.getClass().getClassLoader());\n</code></pre>\n\n<p>The same PARSE call from CYGWIN is successful, I get all PARSERS from Service registry. So maybe the solution is  configuration for Eclipse related to <a href=\"http://docs.oracle.com/javase/7/docs/api/javax/imageio/spi/ServiceRegistry.html\" rel=\"nofollow\">Service registry.</a></p>\n\n<p>Nutch-Eclipse is well configured about nutch-site.xml, parse-plugins.xml</p>\n\n<p>I appreciate any idea to solve this.</p>\n", "creation_date": 1376614641, "score": 0},
{"title": "Basic doubts about Nutch", "view_count": 59, "is_answered": false, "answers": [{"question_id": 18089800, "owner": {"user_id": 1076249, "accept_rate": 50, "link": "http://stackoverflow.com/users/1076249/mark-leighton-fisher", "user_type": "registered", "reputation": 4435}, "body": "<p>Lucene is used for indexing and search by Nutch. As I understand Nutch, it passes the pages it finds to Lucene for indexing.</p>\n", "creation_date": 1376680390, "is_accepted": false, "score": 0, "last_activity_date": 1376680390, "answer_id": 18280534}, {"question_id": 18089800, "owner": {"user_id": 2623052, "accept_rate": 45, "link": "http://stackoverflow.com/users/2623052/allan-macmillan", "user_type": "registered", "reputation": 866}, "body": "<p>Nutch will not index your data, it does not use Lucene so therefore it cannot create its own indexes.  Nutch must pass the documents to Solr for it to be indexed. </p>\n\n<p>Check out : <a href=\"http://stackoverflow.com/questions/10844792/nutch-vs-solr-indexing\">nutch vs solr indexing</a></p>\n", "creation_date": 1376746222, "is_accepted": false, "score": 0, "last_activity_date": 1376746222, "answer_id": 18289368}], "question_id": 18089800, "tags": ["lucene", "indexing", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/18089800/basic-doubts-about-nutch", "last_activity_date": 1376746222, "owner": {"user_id": 2658352, "view_count": 1, "answer_count": 0, "creation_date": 1375820106, "reputation": 1}, "body": "<p>right now i have a project in which i need to build a search engine, but i cannot use Solr, only nutch and lucece, tho while im searching in forums and such i find out alot of people saying nutch does the indexing, i installed nutch (1.4) and crawled data, but realized i got no index folder or something like that, only the crawled data.. So, the question is, does nutch actually index what it crawls or it needs Lucene for indexing and search?</p>\n\n<p>PS. for this project, i cant use Solr, only pure nutch and lucene and i need to build everything using Java, so im really confused when people says that nutch does in fact index... Sorry for my bad english, its not my native language...</p>\n", "creation_date": 1375820789, "score": 0},
{"title": "When do I use solrindex [-filter] and [-normalize]?", "view_count": 240, "is_answered": false, "answers": [{"question_id": 18275388, "owner": {"user_id": 2623052, "accept_rate": 45, "link": "http://stackoverflow.com/users/2623052/allan-macmillan", "user_type": "registered", "reputation": 866}, "body": "<p>When indexing to Solr these config files are set to false by default, so if you wish the indexes you are passing to Solr to be normalized or filetered then you would enable these options.</p>\n\n<p>To me it seems like a pointless option but only because that is not how I would like my configuration of Solr to work, but it is a more advanced feature that will benefit a small amount of people</p>\n", "creation_date": 1376663224, "is_accepted": false, "score": 0, "last_activity_date": 1376663224, "answer_id": 18275657}], "question_id": 18275388, "tags": ["nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/18275388/when-do-i-use-solrindex-filter-and-normalize", "last_activity_date": 1376663224, "owner": {"user_id": 2687125, "view_count": 2, "answer_count": 0, "creation_date": 1376594202, "reputation": 1}, "body": "<p>In the Nutch wiki it suggests use of the following:</p>\n\n<pre><code>bin/nutch solrindex &lt;solr url&gt; &lt;crawldb&gt; [-linkdb &lt;linkdb&gt;] [-params k1=v1&amp;k2=v2...] (&lt;segment&gt; ... | -dir &lt;segments&gt;) [-noCommit] [-deleteGone] [-filter] [-normalize]\n</code></pre>\n\n<p>What is the purpose of</p>\n\n<pre><code>[-filter] [-normalize]\n</code></pre>\n\n<p>when Nutch has numerous filter and normalization configuration files?</p>\n\n<pre><code>automaton-urlfilter.txt\ndomain-urlfilter.txt\nregex-urlfilter.txt\nsuffix-urlfilter.txt\nregex-normalize.xml\nhost-urlnormalizer.txt\n</code></pre>\n", "creation_date": 1376662401, "score": 0},
{"title": "how to make nutch crawl file system?", "view_count": 4538, "is_answered": true, "answers": [{"question_id": 941519, "owner": {"user_id": 56150, "accept_rate": 98, "link": "http://stackoverflow.com/users/56150/sumit-ghosh", "user_type": "registered", "reputation": 1728}, "body": "<p>nutch has the Intranet crawling available.  you can read the details <a href=\"http://wiki.apache.org/nutch/IntranetRecrawl\" rel=\"nofollow\">here</a></p>\n", "creation_date": 1244831153, "is_accepted": false, "score": 1, "last_activity_date": 1244831153, "answer_id": 988232}, {"question_id": 941519, "owner": {"user_id": 39663, "link": "http://stackoverflow.com/users/39663/robert-nickens", "user_type": "registered", "reputation": 489}, "body": "<p><strong><em>From the Nutch Wiki:</em></strong> </p>\n\n<p><strong>How do I index my local file system?</strong></p>\n\n<p><a href=\"http://wiki.apache.org/nutch/FAQ#head-c721b23b43b15885f5ea7d8da62c1c40a37878e6\" rel=\"nofollow\">http://wiki.apache.org/nutch/FAQ#head-c721b23b43b15885f5ea7d8da62c1c40a37878e6</a></p>\n\n<p>1) crawl-urlfilter.txt needs a change to allow file: URLs while not following http: ones, otherwise it either won't index anything, or it'll jump off your disk onto web sites.\n      Change this line:</p>\n\n<pre><code>  -^(file|ftp|mailto|https):\n\n  to this:\n\n  -^(http|ftp|mailto|https):\n</code></pre>\n\n<p>2) crawl-urlfilter.txt may have rules at the bottom to reject some URLs. If it has this fragment it's probably ok:</p>\n\n<pre><code>  # accept anything else +.*\n</code></pre>\n\n<p>3) I changed my nutch.xml to include the following:</p>\n\n<pre><code>&lt;Parameter override=\"false\" name=\"plugin.includes\" value=\"protocol-file|protocol-http|urlfilter-regex|parse-(msword|pdf|text|html|js)|index-(basic|anchor)|query-(basic|site|url)|response-(json|xml)|summary-basic|scoring-opic|urlnormalizer-(pass|regex|basic)\"/&gt;\n</code></pre>\n", "creation_date": 1247369963, "is_accepted": false, "score": 4, "last_activity_date": 1247369963, "answer_id": 1115259}], "question_id": 941519, "tags": ["filesystems", "nutch", "web-crawler"], "answer_count": 2, "link": "http://stackoverflow.com/questions/941519/how-to-make-nutch-crawl-file-system", "last_activity_date": 1376573809, "owner": {"user_id": 104015, "answer_count": 1, "creation_date": 1241863152, "accept_rate": 44, "view_count": 8383, "reputation": 34802}, "body": "<p>not based on http,</p>\n\n<p>like <a href=\"http://localhost:81\" rel=\"nofollow\">http://localhost:81</a> and so on,</p>\n\n<p>but directly crawl a certain directory on local file system,</p>\n\n<p>is there any way out?</p>\n", "creation_date": 1243971863, "score": 3},
{"title": "How can recrawl different sites with different scheduled crawling in nutch 1.3?", "view_count": 354, "is_answered": true, "answers": [{"question_id": 7818169, "owner": {"user_id": 1211452, "accept_rate": 100, "link": "http://stackoverflow.com/users/1211452/lina-clark", "user_type": "registered", "reputation": 180}, "body": "<p>You can write a shell script in which you can specify the command names which you use to run crawler and use cron command in linux to scedule the execution of this script.</p>\n\n<p><a href=\"http://www.thegeekstuff.com/2011/07/cron-every-5-minutes/\" rel=\"nofollow\">http://www.thegeekstuff.com/2011/07/cron-every-5-minutes/</a></p>\n\n<p>Even google crawls the whole web repeatedly after some interval of time.</p>\n", "creation_date": 1332737723, "is_accepted": false, "score": 2, "last_activity_date": 1332737723, "answer_id": 9866759}, {"question_id": 7818169, "owner": {"user_id": 1881318, "link": "http://stackoverflow.com/users/1881318/tahagh", "user_type": "registered", "reputation": 619}, "body": "<p>You can specify fetch interval (time between two consecutive crawls) for each entry in your seed file like this:</p>\n\n<pre><code>http://daily.com \\t nutch.fetchInterval=86400\nhttp://montly.com \\t nutch.fetchInterval=2592000\n</code></pre>\n\n<p>If you are using <code>AdaptiveFetchSchedule</code> the above entries just set the starting interval and after each recrawl depending on whether the page is changed or not this interval will be increased or decreased. In this case, if you always want a fixed interval you can use <code>nutch.fetchInterval.fixed</code> instead of <code>nutch.fetchInterval</code> in above lines.</p>\n", "creation_date": 1376255010, "is_accepted": false, "score": 1, "last_activity_date": 1376255010, "answer_id": 18176659}], "question_id": 7818169, "tags": ["nutch", "web-crawler"], "answer_count": 2, "link": "http://stackoverflow.com/questions/7818169/how-can-recrawl-different-sites-with-different-scheduled-crawling-in-nutch-1-3", "last_activity_date": 1376255010, "owner": {"age": 29, "answer_count": 0, "creation_date": 1316882275, "user_id": 962809, "view_count": 10, "location": "Iran", "reputation": 1}, "body": "<p>I have many sites; contents of some change every month and content of some change every day. nutch 1.3 crawled them  befor now i want to recrawl them with different scheduled crawling.\nhow i can do that?\nthanks. </p>\n", "creation_date": 1319010851, "score": 0},
{"title": "Nutch 2.x No errors, No results neither", "view_count": 612, "is_answered": false, "answers": [{"question_id": 18109130, "owner": {"user_id": 2661560, "link": "http://stackoverflow.com/users/2661560/ralf-r-kotowski", "user_type": "registered", "reputation": 1}, "body": "<p>I've now started again with a complete new install of Nutch 2.2.1 - Hbase-0.94.10 and Solr 4.4.0 as advised vy someone on the mailinglist, due to that the versions mentioned in the tutorial are years old, and now the error I'm getting is:</p>\n\n<p>[root@localhost local]# bin/nutch inject /urls/seed.txt\nInjectorJob: starting at 2013-08-11 17:59:32\nInjectorJob: Injecting urlDir: /urls/seed.txt\nInjectorJob: org.apache.gora.util.GoraException: java.lang.RuntimeException: java.lang.IllegalArgumentException: Not a host:port pair: \ufffd2249@localhost.localdomainlocalhost,45431,1376235201648</p>\n", "creation_date": 1376236967, "is_accepted": false, "score": 0, "last_activity_date": 1376236967, "answer_id": 18173875}], "question_id": 18109130, "tags": ["apache", "solr", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/18109130/nutch-2-x-no-errors-no-results-neither", "last_activity_date": 1376236967, "owner": {"user_id": 2661560, "view_count": 10, "answer_count": 1, "creation_date": 1375893383, "reputation": 1}, "body": "<p>I've been playing with nutch 2.x for awhile, have it set up according to the Nutch 2.x tutorial as advised in <a href=\"http://stackoverflow.com/questions/15995457/nutch-crawl-no-error-but-result-is-nothing/16028007#16028007\">this post</a> , still I can't figure it out - any help would be greatly appreciated.</p>\n\n<p>When using the INJECT command as per tutorial, it injects the 2 URLS I have in seeds.txt: </p>\n\n<pre><code>nutch inject ../local/urls/seed.txt \n</code></pre>\n\n<p>but when running the script it doesn't visit any of the urls: </p>\n\n<pre><code>bin/crawl ../local/urls/seed.txt TestCrawl *ttp://l*calhost:8983/solr 2\n</code></pre>\n", "creation_date": 1375894038, "score": 0},
{"title": "Nutch not deleting duplicates from Solr", "view_count": 509, "owner": {"age": 24, "answer_count": 43, "creation_date": 1374847004, "user_id": 2623052, "accept_rate": 45, "view_count": 206, "location": "Scotland", "reputation": 866}, "is_answered": true, "answers": [{"question_id": 17901592, "owner": {"user_id": 2623052, "accept_rate": 45, "link": "http://stackoverflow.com/users/2623052/allan-macmillan", "user_type": "registered", "reputation": 866}, "body": "<p>In the signatureField tag I had \"id\" instead of \"signature\"   </p>\n\n<pre><code>&lt;updateRequestProcessorChain name=\"dedupe\"&gt;\n  &lt;processor class=\"org.apache.solr.update.processor.SignatureUpdateProcessorFactory\"&gt;\n    &lt;bool name=\"enabled\"&gt;true&lt;/bool&gt;\n    &lt;bool name=\"overwriteDupes\"&gt;true&lt;/bool&gt;\n    &lt;str name=\"signatureField\"&gt;signature&lt;/str&gt;\n    &lt;str name=\"fields\"&gt;id&lt;/str&gt;\n    &lt;str name=\"signatureClass\"&gt;org.apache.solr.update.processor.Lookup3Signature&lt;/str&gt;\n  &lt;/processor&gt;\n  &lt;processor class=\"solr.LogUpdateProcessorFactory\" /&gt;\n  &lt;processor class=\"solr.RunUpdateProcessorFactory\" /&gt;\n&lt;/updateRequestProcessorChain&gt;\n</code></pre>\n\n<p>Works perfectly now</p>\n", "creation_date": 1376060558, "is_accepted": true, "score": 0, "last_activity_date": 1376060558, "answer_id": 18149812}], "question_id": 17901592, "tags": ["solr", "duplicates", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/17901592/nutch-not-deleting-duplicates-from-solr", "last_activity_date": 1376060558, "accepted_answer_id": 18149812, "body": "<p>When Nutch finishes its crawl it recognises that there are duplicates to delete and goes through saying \"deleting xxx duplicates\" and completes with no problems.  The only problem is that it actually hasnt deleted the duplicates although it said it has.</p>\n\n<p>I've also tried using the dedup command on its own and the result is the same. </p>\n\n<p>I have Solr &amp; Nutch Set-up as shown on my blog if you wish to delve a little deeper, each stage in a different post:</p>\n\n<p><a href=\"http://amac4.blogspot.co.uk/2013/07/setting-up-solr-with-apache-tomcat-be.html\" rel=\"nofollow\">http://amac4.blogspot.co.uk/2013/07/setting-up-solr-with-apache-tomcat-be.html</a>\n<a href=\"http://amac4.blogspot.co.uk/2013/07/setting-up-nutch-to-crawl-filesystem.html\" rel=\"nofollow\">http://amac4.blogspot.co.uk/2013/07/setting-up-nutch-to-crawl-filesystem.html</a></p>\n", "creation_date": 1374952951, "score": 0},
{"title": "MapReduce Nutch tutorials", "view_count": 1263, "owner": {"user_id": 282544, "answer_count": 2, "creation_date": 1267247419, "accept_rate": 69, "view_count": 211, "reputation": 980}, "is_answered": true, "answers": [{"question_id": 3558148, "owner": {"user_id": 183100, "link": "http://stackoverflow.com/users/183100/pascal-dimassimo", "user_type": "registered", "reputation": 5766}, "body": "<p>The book <a href=\"http://oreilly.com/catalog/9780596521981\" rel=\"nofollow\">Hadoop: The Definitive Guide</a> is a good resource on MapReduce in general. Chapter 14 contains several case studies of Hadoop and one of them is about Nutch. It helps me see the way Nutch uses Hadoop and MapReduce.</p>\n", "creation_date": 1282664605, "is_accepted": true, "score": 1, "last_activity_date": 1282664605, "answer_id": 3558250}], "question_id": 3558148, "tags": ["java", "mapreduce", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/3558148/mapreduce-nutch-tutorials", "last_activity_date": 1376055852, "accepted_answer_id": 3558250, "body": "<p>Could some one give me pointers to tutorials that explains how to write a mapreduce program into Nutch?</p>\n\n<p>Thank you.</p>\n", "creation_date": 1282663935, "score": 1},
{"title": "Launch Solr Indexing in Nutch Source Code", "view_count": 446, "owner": {"age": 26, "answer_count": 118, "creation_date": 1334762714, "user_id": 1341806, "accept_rate": 78, "view_count": 609, "location": "Boston, MA", "reputation": 2723}, "is_answered": true, "answers": [{"last_edit_date": 1376045222, "owner": {"user_id": 1881318, "link": "http://stackoverflow.com/users/1881318/tahagh", "user_type": "registered", "reputation": 619}, "body": "<p>There are two problems in your code: </p>\n\n<ol>\n<li>The <code>solr.server.url</code> must directly set in the configuration object not with -D option. The given message by nutch assumes running from command line and it is misleading here.</li>\n<li>As you mentioned, you are passing two different configuration instances. the <code>NutchConfiguration.create()</code> creates a hadoop configuration internally and it adds some nutch specific resources to it so you don't need to create it by yourself. Also, the ToolRunner passes the conf object to IndexingJob so you don't need to pass it by its constructor.</li>\n</ol>\n\n<p>So the correct code is:</p>\n\n<pre><code>Configuration conf = NutchConfiguration.create();\nconf.set(\"solr.server.url\", solrUrl);\nToolRunner.run(conf, new IndexingJob(), args2);\n</code></pre>\n", "question_id": 18116710, "creation_date": 1376044453, "is_accepted": true, "score": 1, "last_activity_date": 1376045222, "answer_id": 18144765}], "question_id": 18116710, "tags": ["java", "solr", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/18116710/launch-solr-indexing-in-nutch-source-code", "last_activity_date": 1376045222, "accepted_answer_id": 18144765, "body": "<p>I am trying to index my Nutch crawl into solr, but inside of the source code, not from the command line.</p>\n\n<p>I have created the following function</p>\n\n<pre><code>public static int runInjectSolr(String[] args, Properties prop) throws Exception{       \n    String solrUrl = \"http://ec2-X-X-X-X.compute-1.amazonaws.com/solr/collection1\";\n\n    String crawldb = JobBase.getParam(args,\"crawldb\", null, true);\n    String segments = JobBase.getParam(args,\"segments\", null, true);\n    String args2[] = {crawldb, segments};\n\n    Configuration conf = new Configuration();\n    conf.set(\"-D solr.server.url\",solrUrl);\n    int code = ToolRunner.run(NutchConfiguration.create(),\n            new IndexingJob(conf), args2);\n    return code;\n}\n</code></pre>\n\n<p>But I am receiving the following error:</p>\n\n<pre><code>2013-08-07 19:37:13,338 ERROR org.apache.nutch.indexwriter.solr.SolrIndexWriter (main): Missing SOLR URL. Should be set via -D solr.server.url \nSOLRIndexWriter\nsolr.server.url : URL of the SOLR instance (mandatory)\nsolr.commit.size : buffer size when sending to SOLR (default 1000)\nsolr.mapping.file : name of the mapping file for fields (default solrindex-mapping.xml)\nsolr.auth : use authentication (default false)\nsolr.auth.username : use authentication (default false)\nsolr.auth : username for authentication\nsolr.auth.password : password for authentication\n</code></pre>\n\n<p>So I am assuming I am not creating my configuration correctly. Any suggestions?</p>\n\n<p>Or should I be passing my config field into run a different way? Maybe not using</p>\n\n<pre><code>NutchConfiguration.create()\n</code></pre>\n", "creation_date": 1375926683, "score": 0},
{"title": "Improper results from searching on a URL in solr", "view_count": 595, "owner": {"user_id": 2301119, "view_count": 9, "answer_count": 1, "creation_date": 1366422922, "reputation": 28}, "is_answered": true, "answers": [{"question_id": 18115311, "owner": {"user_id": 326543, "accept_rate": 82, "link": "http://stackoverflow.com/users/326543/srikanth-venugopalan", "user_type": "registered", "reputation": 6268}, "body": "<p>Assuming that you are on Solr 3.1 or greater.</p>\n\n<p>StandardTokenizerFactory - It creates token based on <a href=\"http://wiki.apache.org/solr/AnalyzersTokenizersTokenFilters#solr.KeywordTokenizerFactory\" rel=\"nofollow\">Word Boundary rules</a>. This means URLs will be broken into multiple tokens and match on any one of them would be considered a hit.</p>\n\n<p>Try using <a href=\"http://wiki.apache.org/solr/AnalyzersTokenizersTokenFilters#solr.KeywordTokenizerFactory\" rel=\"nofollow\">KeywordTokenizerFactory</a>, for your <code>url</code> fieldtype. This should preserve the complete URL and match against it only.</p>\n", "creation_date": 1375924748, "is_accepted": false, "score": 1, "last_activity_date": 1375924748, "answer_id": 18116437}, {"question_id": 18115311, "owner": {"user_id": 2665648, "link": "http://stackoverflow.com/users/2665648/elyograg", "user_type": "registered", "reputation": 514}, "body": "<p>In addition to using KeywordTokenizerFactory, you will have to remove the WordDelimiterFilterFactory.  WDF splits tokens on punctuation and other delimiters ... which are very plentiful in URLs.  You'll have to rebuild your index after making the change and restarting Solr or reloading the core.</p>\n\n<p>An alternate idea, if you don't need to force URLs to lowercase: Switch from TextField to StrField and get rid of the analyzer config entirely.</p>\n", "creation_date": 1375993402, "is_accepted": true, "score": 0, "last_activity_date": 1375993402, "answer_id": 18135423}], "question_id": 18115311, "tags": ["url", "solr", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/18115311/improper-results-from-searching-on-a-url-in-solr", "last_activity_date": 1375993402, "accepted_answer_id": 18135423, "body": "<p>I must be doing something wrong trying to run the following search </p>\n\n<pre><code>http://localhost:8983/solr/collection1/select?q=url:www.abc.com&amp;wt=xml&amp;indent=true\n</code></pre>\n\n<p>It is not giving this sites results back, it's giving everything back.  The schema.xml is pretty vanilla in how url is set up.</p>\n\n<pre><code>&lt;fieldType name=\"text\" class=\"solr.TextField\"\n        positionIncrementGap=\"100\"&gt;\n        &lt;analyzer&gt;\n            &lt;tokenizer class=\"solr.WhitespaceTokenizerFactory\"/&gt;\n            &lt;filter class=\"solr.StopFilterFactory\"\n                ignoreCase=\"true\" words=\"stopwords.txt\"/&gt;\n            &lt;filter class=\"solr.WordDelimiterFilterFactory\"\n                generateWordParts=\"1\" generateNumberParts=\"1\"\n                catenateWords=\"1\" catenateNumbers=\"1\" catenateAll=\"0\"\n                splitOnCaseChange=\"1\"/&gt;\n            &lt;filter class=\"solr.LowerCaseFilterFactory\"/&gt;\n            &lt;filter class=\"solr.PorterStemFilterFactory\"/&gt;\n            &lt;filter class=\"solr.RemoveDuplicatesTokenFilterFactory\"/&gt;\n        &lt;/analyzer&gt;\n    &lt;/fieldType&gt;\n\n&lt;fieldType name=\"url\" class=\"solr.TextField\"\n        positionIncrementGap=\"100\"&gt;\n        &lt;analyzer&gt;\n            &lt;tokenizer class=\"solr.StandardTokenizerFactory\"/&gt;\n            &lt;filter class=\"solr.LowerCaseFilterFactory\"/&gt;\n            &lt;filter class=\"solr.WordDelimiterFilterFactory\"\n                generateWordParts=\"1\" generateNumberParts=\"1\"/&gt;\n        &lt;/analyzer&gt;\n    &lt;/fieldType&gt;\n</code></pre>\n\n<p>If I use host:www.abc.com, it works.  </p>\n\n<p>Why the seemingly incorrect results when using the url field?</p>\n\n<p>Thanks for any and all help.</p>\n", "creation_date": 1375917165, "score": 0},
{"title": "Using new script bin/crawl - skipping urls different batch id (null)", "view_count": 521, "owner": {"age": 27, "answer_count": 17, "creation_date": 1332257106, "user_id": 1281305, "accept_rate": 88, "view_count": 129, "location": "Prague, Czech Republic", "reputation": 494}, "is_answered": true, "answers": [{"question_id": 17393663, "owner": {"user_id": 1281305, "accept_rate": 88, "link": "http://stackoverflow.com/users/1281305/johnny-greenwood", "user_type": "registered", "reputation": 494}, "body": "<p>Ok, solution is that I used older version of the nutch (2.1). After updating to 2.2.1 this problem dissapeared.</p>\n", "creation_date": 1372966780, "is_accepted": true, "score": 0, "last_activity_date": 1372966780, "answer_id": 17477198}], "question_id": 17393663, "tags": ["apache", "solr", "web-crawler", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/17393663/using-new-script-bin-crawl-skipping-urls-different-batch-id-null", "last_activity_date": 1375895103, "accepted_answer_id": 17477198, "body": "<p>I want to crawl a lot of websites from my seed.txt with Nutch 2.1 new crawl script bin/crawl.</p>\n\n<p>The problem is that everytime I run my script, it does not fetch or parse anything (no urls) with message \"Skipoing [here is concrete url] different batch id (null)\"</p>\n\n<p>Here is some output from the log:</p>\n\n<pre><code>Start old crawling linked TV:\nInjectorJob: starting\nInjectorJob: urlDir: /opt/ir/nutch/urls\nInjectorJob: finished\n</code></pre>\n\n<p>It looks like that the injection of urls was ok</p>\n\n<pre><code>Sun Jun 30 19:45:10 CEST 2013 : Iteration 1 of 2\nGenerating batchId\nGenerating a new fetchlist\nGeneratorJob: Selecting best-scoring urls due for fetch.\nGeneratorJob: starting\nGeneratorJob: filtering: false\nGeneratorJob: topN: 50000\nGeneratorJob: done\nGeneratorJob: generated batch id: 1372614310-1071860715\nFetching :\nFetcherJob: starting\nFetcherJob: batchId: 1372614310-24672\nFetcher: Your 'http.agent.name' value should be listed first in 'http.robots.agents' property.\nFetcherJob: threads: 50\nFetcherJob: parsing: false\nFetcherJob: resuming: false\nFetcherJob : timelimit set for : 1372614928303\nUsing queue mode : byHost\nFetcher: threads: 50\nQueueFeeder finished: total 0 records. Hit by time limit :0\n-finishing thread FetcherThread0, activeThreads=0\n-finishing thread FetcherThread1, activeThreads=0\n-finishing thread FetcherThread2, activeThreads=0\n-finishing thread FetcherThread3, activeThreads=0\n</code></pre>\n\n<p>.... here is iteration to FetcherThread48 and it continues</p>\n\n<pre><code>Fetcher: throughput threshold: -1\n-finishing thread FetcherThread49, activeThreads=0\nFetcher: throughput threshold sequence: 5\n0/0 spinwaiting/active, 0 pages, 0 errors, 0.0 0.0 pages/s, 0 0 kb/s, 0 URLs in 0 queues\n-activeThreads=0\nFetcherJob: done\nParsing :\nParserJob: starting\nParserJob: resuming:    false\nParserJob: forced reparse:      false\nParserJob: batchId:     1372614310-24672\nSkipping http://www.brugge.be/internet/en/musea/bruggemuseum/stadhuis/index.htm; different batch id (null)\nSkipping http://www.galloromeinsmuseum.be/; different batch id (null)\nSkipping http://www.museumdrguislain.be/; different batch id (null)\nSkipping http://www.muzee.be/; different batch id (null)\nSkipping http://musea.sint-niklaas.be/; different batch id (null)\n</code></pre>\n\n<p>...\n...\nand skipping more urls from my seed\n...\n...</p>\n\n<pre><code>ParserJob: success\nCrawlDB update\nDbUpdaterJob: starting\nLimit reached, skipping further inlinks for de.ard.www:http/\nLimit reached, skipping further inlinks for de.rbb-online.mediathek:http/\nLimit reached, skipping further inlinks for de.rbb-online.www:http/\nDbUpdaterJob: done\n</code></pre>\n\n<p>Do you know where is the probleam, please? I am absolutely exhausted of configuration of this tool and trying to work well with it...</p>\n", "creation_date": 1372617831, "score": 0},
{"title": "How to crawl PDF documents using Nutch 1.6?", "view_count": 729, "is_answered": true, "answers": [{"question_id": 18094693, "owner": {"user_id": 2552545, "accept_rate": 0, "link": "http://stackoverflow.com/users/2552545/subbu", "user_type": "registered", "reputation": 69}, "body": "<p>As per my knowledge ...Check pdpage.class(path for this class: pdfbox-app-1.8.2/org/apache/pdfbox/pdmodel/PDPage.class) contains your pdfbox or not. it is required for your problem.</p>\n", "creation_date": 1375851413, "is_accepted": false, "score": 2, "last_activity_date": 1375851413, "answer_id": 18094964}], "question_id": 18094693, "tags": ["parsing", "pdf", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/18094693/how-to-crawl-pdf-documents-using-nutch-1-6", "last_activity_date": 1375851413, "owner": {"user_type": "does_not_exist"}, "body": "<p>I'm using Apache-nutch 1.6,my requirement is to crawl PDF documents as .pdf file itself but I couldn't crawl pdf file as text itself.\n In my nutch-site.xml, I'm giving http.agent.name,http.robots.name,http.proxy.host alone..Is there anything should I add...\n In my plugins I have only parse-tika, Is there anything to add...If so suggest me the link...</p>\n\n<p>I can crawl .html but for .pdf file no parsetext....</p>\n\n<p>Error:\n parse.ParseUtil - Unable to successfully parse content <a href=\"http://nutch.apache.orgmailing_lists.pdf\" rel=\"nofollow\">http://nutch.apache.orgmailing_lists.pdf</a> of type application/pdf\n parse.ParseSegment - Error parsing: <a href=\"http://nutch.apache.org/mailing_lists.pdf\" rel=\"nofollow\">http://nutch.apache.org/mailing_lists.pdf</a>: failed(2,200): org.apache.nutch.parse.ParseException: Unable to successfully parse content</p>\n\n<p>Thanks in advance....</p>\n", "creation_date": 1375849793, "score": 3},
{"title": "Extracking Multiple Documents from one Page using Nutch", "view_count": 100, "is_answered": false, "question_id": 17933546, "tags": ["api", "parsing", "nutch", "web-crawler"], "answer_count": 0, "link": "http://stackoverflow.com/questions/17933546/extracking-multiple-documents-from-one-page-using-nutch", "last_activity_date": 1375381487, "owner": {"user_id": 91324, "answer_count": 10, "creation_date": 1239830824, "accept_rate": 32, "view_count": 128, "reputation": 153}, "body": "<p>I'm using Nutch to Crawl APIs and Index the Data.</p>\n\n<p>Using APIs, I can multiple \"Pages\" of data in one go.  For example, lets say I was indexing Movies.</p>\n\n<p>I could query the top level and get a list of Categories like Action, Drama, Comedy, etc.  Then, I could query each category and get a list of Movies.  At this point, I can insert each movie as an outlink and have nutch crawl the details of each movie.</p>\n\n<p>However, the Category call already gives me the details of say 10 movies at a time.</p>\n\n<p>I want to be able to create the 10 entries in Nutch without having to crawl each of them.  Can this be done?</p>\n", "creation_date": 1375129594, "score": 2},
{"title": "Nutch : org.apache.hadoop.mapreduce.lib.input.InvalidInputException: Input path does not exist", "view_count": 1146, "is_answered": false, "answers": [{"question_id": 17959597, "owner": {"user_id": 1038826, "link": "http://stackoverflow.com/users/1038826/zsxwing", "user_type": "registered", "reputation": 9192}, "body": "<p>If you want to use <code>-dir crawl</code>, you need to create the folder <code>file:/C:/cygwin/usr/local/apache-nutch-2.2.1/runtime/local/crawl</code> at first.</p>\n", "creation_date": 1375240286, "is_accepted": false, "score": 0, "last_activity_date": 1375240286, "answer_id": 17960932}], "question_id": 17959597, "tags": ["hadoop", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/17959597/nutch-org-apache-hadoop-mapreduce-lib-input-invalidinputexception-input-path", "last_activity_date": 1375377416, "owner": {"user_id": 1663662, "answer_count": 26, "creation_date": 1347385562, "accept_rate": 75, "view_count": 143, "location": "MEX", "reputation": 827}, "body": "<p>When i execute nutch command to create crawldb folders and contents:</p>\n\n<pre><code>soporte@CNEOSYLAP /usr/local/apache-nutch-2.2.1/runtime/local\n$ bin/nutch crawl urls -dir crawl -depth 3 -topN 5\n</code></pre>\n\n<p>I get this error:</p>\n\n<pre><code>InjectorJob: Using class org.apache.gora.hbase.store.HBaseStore as the Gora storage class.\nException in thread \"main\" org.apache.hadoop.mapreduce.lib.input.InvalidInputException: Input path does not exist: file:/C:/cygwin/usr/local/apache-nutch-2.2.1/runtime/local/crawl\n        at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.listStatus(FileInputFormat.java:224)\n        at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.getSplits(FileInputFormat.java:241)\n        at org.apache.hadoop.mapred.JobClient.writeNewSplits(JobClient.java:885)\n        at org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:779)\n        at org.apache.hadoop.mapreduce.Job.submit(Job.java:432)\n        at org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:447)\n        at org.apache.nutch.util.NutchJob.waitForCompletion(NutchJob.java:50)\n        at org.apache.nutch.crawl.InjectorJob.run(InjectorJob.java:233)\n        at org.apache.nutch.crawl.Crawler.runTool(Crawler.java:68)\n        at org.apache.nutch.crawl.Crawler.run(Crawler.java:136)\n        at org.apache.nutch.crawl.Crawler.run(Crawler.java:250)\n        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n        at org.apache.nutch.crawl.Crawler.main(Crawler.java:257)\n</code></pre>\n\n<p>I am using apache-nutch-2.2.1, hadoop-0.20.2-core.jar, hbase-0.90.4.jar and CygWin setup 2.774.</p>\n\n<p>I have not hadoop installed, only hadoop libary within nutch installation hence is not a distributed but local nutch setup.</p>\n\n<p>Any idea?\nThanks in advance!</p>\n\n<p>EDIT:</p>\n\n<p>When manually create the dir , i get another error:</p>\n\n<pre><code>soporte@CNEOSYLAP /usr/local/apache-nutch-2.2.1/runtime/local\n$ mkdir crawl\n\nsoporte@CNEOSYLAP /usr/local/apache-nutch-2.2.1/runtime/local\n$ chmod 777 crawl\n\nsoporte@CNEOSYLAP /usr/local/apache-nutch-2.2.1/runtime/local\n$ bin/nutch crawl urls -dir crawl -depth 3 -topN 5\ncygpath: can't convert empty path\nInjectorJob: Using class org.apache.gora.hbase.store.HBaseStore as the Gora storage class.\nException in thread \"main\" java.lang.RuntimeException: job failed: name=inject crawl, jobid=null\n        at org.apache.nutch.util.NutchJob.waitForCompletion(NutchJob.java:54)\n        at org.apache.nutch.crawl.InjectorJob.run(InjectorJob.java:233)\n        at org.apache.nutch.crawl.Crawler.runTool(Crawler.java:68)\n        at org.apache.nutch.crawl.Crawler.run(Crawler.java:136)\n        at org.apache.nutch.crawl.Crawler.run(Crawler.java:250)\n        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n        at org.apache.nutch.crawl.Crawler.main(Crawler.java:257)\n</code></pre>\n", "creation_date": 1375229791, "score": 1},
{"title": "How to crawl urls having space using Apache Nutch?", "view_count": 368, "is_answered": true, "answers": [{"question_id": 17806536, "owner": {"user_id": 2586595, "link": "http://stackoverflow.com/users/2586595/mohsen89z", "user_type": "registered", "reputation": 128}, "body": "<p>I had the same problem but with more characters so I changed Fetcher.java!\nNew URLs add to Queue in \"feeding\" section!\nyou have to find this line:</p>\n\n<pre><code>nURL.set(url.toString());\n</code></pre>\n\n<p>and replace it with this:</p>\n\n<pre><code>nURL.set(URIUtil.encodeQuery(url.toString()));\n</code></pre>\n", "creation_date": 1374662262, "is_accepted": false, "score": 1, "last_activity_date": 1374662262, "answer_id": 17831581}, {"question_id": 17806536, "owner": {"user_id": 2623052, "accept_rate": 45, "link": "http://stackoverflow.com/users/2623052/allan-macmillan", "user_type": "registered", "reputation": 866}, "body": "<p>I had the same problem and added this to my regex-normalize.xml</p>\n\n<pre><code>&lt;regex&gt; \n   &lt;pattern&gt;&amp;#x20;&lt;/pattern&gt; \n   &lt;substitution&gt;%20&lt;/substitution&gt; \n&lt;/regex&gt; \n</code></pre>\n", "creation_date": 1375107373, "is_accepted": false, "score": 1, "last_activity_date": 1375107373, "answer_id": 17926398}], "question_id": 17806536, "tags": ["nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/17806536/how-to-crawl-urls-having-space-using-apache-nutch", "last_activity_date": 1375107373, "owner": {"user_id": 1474659, "answer_count": 6, "creation_date": 1340364399, "accept_rate": 48, "view_count": 118, "reputation": 321}, "body": "<p>I am using nutch for crawling but it is getting failed on urls which have space.  I have gone through this link <a href=\"http://lucene.472066.n3.nabble.com/URL-with-Space-td619127.html\" rel=\"nofollow\">http://lucene.472066.n3.nabble.com/URL-with-Space-td619127.html</a> but did not get satisfactory answer.</p>\n\n<p>It works for URL in the seed.txt file but wont work for URLs in the parsed content of a page </p>\n\n<p>I used a URL that has spaces in the conf/seed.txt file and it replaces the space with %20 and I was able to crawl the page.\nI have added following in regex-normalize.xml </p>\n\n<pre><code>&lt;regex&gt; \n &lt;pattern&gt; &lt;/pattern&gt; \n &lt;substitution&gt;%20&lt;/substitution&gt; \n&lt;/regex&gt;                                                                    \n</code></pre>\n\n<p>Also, I added the reference of regex-normalize.xml in nutch-site.xml. But still I am facing the same problem.</p>\n", "creation_date": 1374572125, "score": 0},
{"title": "Nutch - why are my url exclusions not excluding those urls?", "view_count": 241, "owner": {"user_id": 340648, "answer_count": 11, "creation_date": 1273781720, "accept_rate": 94, "view_count": 119, "location": "New York, NY, United States", "reputation": 695}, "is_answered": true, "answers": [{"last_edit_date": 1374773701, "owner": {"user_id": 907642, "link": "http://stackoverflow.com/users/907642/okke-klein", "user_type": "registered", "reputation": 2221}, "body": "<p>My guess is that the url is accepted by first regex and the second one isn't checked anymore. If you want to deny URLs, put their regexes first in list.</p>\n", "question_id": 17751275, "creation_date": 1374324777, "is_accepted": true, "score": 1, "last_activity_date": 1374773701, "answer_id": 17762476}], "question_id": 17751275, "tags": ["regex", "apache", "solr", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/17751275/nutch-why-are-my-url-exclusions-not-excluding-those-urls", "last_activity_date": 1374773701, "accepted_answer_id": 17762476, "body": "<p>Surprise! I have another Apache Nutch v1.5 question. So in crawling and indexing our site to Solr via Nutch, we need to be able to exclude any content that falls under a certain path. </p>\n\n<p>So say we have our site: <code>http://oursite.com/</code> and we have a path that we don't want to index at <code>http://oursite.com/private/</code></p>\n\n<p>I have <code>http://oursite.com/</code> in the <code>seed.txt</code> file and <code>+^http://www.oursite.com/([a-z0-9\\-A-Z]*\\/)*</code> in the <code>regex-urlfilter.txt</code> file</p>\n\n<p>I thought that putting: <code>-.*/private/.*</code> also in the <code>regex-urlfilter.txt</code> file would exclude that path and anything under it, but the crawler is still fetching and indexing content under the <code>/private/</code> path. </p>\n\n<p>Is there some kind of restart I need to do on the server, like Solr? Or is my regex not actually the right way to do this?</p>\n\n<p>thanks</p>\n", "creation_date": 1374252021, "score": 0},
{"title": "How do I configure nutch and solr to search for video files?", "view_count": 497, "is_answered": false, "question_id": 17803159, "tags": ["video", "solr", "indexing", "nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/17803159/how-do-i-configure-nutch-and-solr-to-search-for-video-files", "last_activity_date": 1374561862, "owner": {"user_id": 2404797, "view_count": 0, "answer_count": 0, "creation_date": 1369127849, "reputation": 6}, "body": "<p>I've installed Nutch 1.7 and Solr 3.6.2 and able to search and index xls, doc, pdf &amp; zip files. Now I want to index video files like .avi, .mov</p>\n\n<p>I've edited regex-urlfilter.txt to remove those extension types but the only files that are able to be indexed are .flv files. I know that's what Tika says it supports but I don't need metadata indexing of video files, I just want the filenames to be indexed. </p>\n\n<p>How can I enable that? </p>\n\n<h1>regex-urlfilter.txt</h1>\n\n<pre><code># skip image and other suffixes we can't yet parse\n-\\.(gif|GIF|jpg|JPG|png|PNG|ico|ICO|css|sit|eps|wmf|gz|rpm|tgz|exe|jpeg|JPEG|bmp|BMP)$\n</code></pre>\n\n<h1>nutch-site.xml</h1>\n\n<pre><code>&lt;configuration&gt;\n\n&lt;property&gt;\n     &lt;name&gt;http.agent.name&lt;/name&gt;\n      &lt;value&gt;crawler&lt;/value&gt;\n&lt;/property&gt;\n\n&lt;property&gt;\n      &lt;name&gt;http.robots.agents&lt;/name&gt;\n      &lt;value&gt;crawler,*&lt;/value&gt;\n&lt;/property&gt;\n\n&lt;property&gt;\n      &lt;name&gt;http.accept.language&lt;/name&gt;\n      &lt;value&gt;zh-cn, ja-jp, en-us,en-gb,en;q=0.7,*;q=0.3&lt;/value&gt;\n      &lt;description&gt;Value of the \u201cAccept-Language\u201d request header field.\n      This allows selecting non-English language as default one to retrieve.\n      It is a useful setting for search engines build for certain national group.\n      &lt;/description&gt;\n&lt;/property&gt;\n\n&lt;property&gt;\n      &lt;name&gt;parser.character.encoding.default&lt;/name&gt;\n      &lt;value&gt;utf-8&lt;/value&gt;\n      &lt;description&gt;The character encoding to fall back to when no other information\n      is available&lt;/description&gt;\n&lt;/property&gt;\n\n&lt;property&gt;\n      &lt;name&gt;http.content.limit&lt;/name&gt;\n      &lt;value&gt;10000000&lt;/value&gt;\n      &lt;description&gt;The length limit for downloaded content, in bytes.\n  If this value is nonnegative (&gt;=0), content longer than it will be truncated;\n  otherwise, no truncation at all.\n      &lt;/description&gt;\n&lt;/property&gt;\n\n&lt;property&gt;\n      &lt;name&gt;file.content.limit&lt;/name&gt;\n      &lt;value&gt;10000000&lt;/value&gt;\n      &lt;description&gt;The length limit for downloaded content, in bytes.\n       If this value is nonnegative (&gt;=0), content longer than it will be      truncated; otherwise, no truncation at all.\n      &lt;/description&gt;\n&lt;/property&gt;\n\n&lt;property&gt;\n      &lt;name&gt;plugin.includes&lt;/name&gt;\n      &lt;value&gt;protocol-http|urlfilter-regex|parse-(html|tika|metatags|zip)|index-(basic|anchor|metadata)|summary-basic|scoring-opic|urlnormalizer-(pass|regex|basic)&lt;/value&gt;\n&lt;/property&gt; \n\n&lt;property&gt;\n      &lt;name&gt;metatags.names&lt;/name&gt;\n      &lt;value&gt;*&lt;/value&gt;\n      &lt;description&gt; Names of the metatags to extract, separated by;.\n  Use '*' to extract all metatags. Prefixes the names with 'metatag.'\n  in the parse-metadata. For instance to index description and keywords,\n  you need to activate the plugin index-metadata and set the value of the\n  parameter 'index.parse.md' to 'metatag.description;metatag.keywords'.\n      &lt;/description&gt;\n&lt;/property&gt;\n\n&lt;property&gt;\n      &lt;name&gt;index.parse.md&lt;/name&gt;\n      &lt;value&gt;metatag.description,metatag.keywords&lt;/value&gt;\n      &lt;description&gt; Comma-separated list of keys to be taken from the parse metadata to generate fields.  Can be used e.g. for 'description' or 'keywords' provided that these values are generated by a parser (see parse-metatags plugin)\n      &lt;/description&gt;\n&lt;/property&gt;\n\n&lt;/configuration&gt;\n</code></pre>\n", "creation_date": 1374561862, "score": 1},
{"title": "How to set up regex in nutch for filtering URL of techcrunch?", "view_count": 689, "is_answered": false, "answers": [{"question_id": 17797853, "owner": {"user_id": 2255089, "link": "http://stackoverflow.com/users/2255089/casimir-et-hippolyte", "user_type": "registered", "reputation": 61179}, "body": "<p>I don't know nutch but do you try:</p>\n\n<pre><code>+^http://www.techcrunch.com/2013/[0-9]{2}/[0-9]{2}.*$\n</code></pre>\n\n<p>or</p>\n\n<pre><code>+^http://www.techcrunch.com/2013/[0-9]+/[0-9]+.*$\n</code></pre>\n", "creation_date": 1374529079, "is_accepted": false, "score": 0, "last_activity_date": 1374529079, "answer_id": 17797893}, {"last_edit_date": 1374534155, "owner": {"user_id": 2517733, "accept_rate": 50, "link": "http://stackoverflow.com/users/2517733/racso", "user_type": "registered", "reputation": 1280}, "body": "<p>The following expressions will match the URLs you need:</p>\n\n<p><strong>Without groups</strong></p>\n\n<pre><code>http:\\/\\/www.techcrunch.com\\/\\d{4}\\/\\d{2}\\/\\d{2}\\/\\w+\n</code></pre>\n\n<p><strong>With groups</strong></p>\n\n<pre><code>http:\\/\\/www.techcrunch.com\\/(\\d{4})\\/(\\d{2})\\/(\\d{2})\\/(\\w+)\n</code></pre>\n\n<p><em>I didn't put anchors (<code>^$</code>), but you can put them if you need them for the filtering.</em></p>\n\n<p>Try them to see if any of them work.</p>\n\n<p>I don't know how nutch works, but a couple of suggestions about your regex that may apply: the <code>/</code> in the regexp should be escaped; the <code>dd</code> parts should be <code>\\d\\d</code> so they match two digits.</p>\n\n<p>About setting up the regex, check out <a href=\"http://stackoverflow.com/questions/13884249/nutch-regex-urlfilter-syntax\">this answer</a> to see if it helps you.</p>\n", "question_id": 17797853, "creation_date": 1374533821, "is_accepted": false, "score": 0, "last_activity_date": 1374534155, "answer_id": 17798845}], "question_id": 17797853, "tags": ["regex", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/17797853/how-to-set-up-regex-in-nutch-for-filtering-url-of-techcrunch", "last_activity_date": 1374557024, "owner": {"age": 23, "answer_count": 7, "creation_date": 1373650315, "user_id": 2577337, "accept_rate": 73, "view_count": 41, "location": "Mumbai, India", "reputation": 158}, "body": "<p>I want to crawl the pages of Techcrunch uploaded after the 1 Jan of 2013.The website follows the pattern </p>\n\n<pre><code>             http://www.techcrunch.com/YYYY/MM/DD\n</code></pre>\n\n<p>So my question is how to setup the regex in urlfilter in nutch so that i could crawl only pages which i want.</p>\n\n<pre><code>             +^http://www.techcrunch.com/2013/dd/dd/([a-z0-9\\-A-Z]*\\/)*\n</code></pre>\n", "creation_date": 1374528944, "score": 1},
{"title": "LinkDb: adding segment &amp; SolrIndexer takes lots of time", "view_count": 52, "is_answered": true, "answers": [{"question_id": 17769674, "owner": {"user_id": 2586595, "link": "http://stackoverflow.com/users/2586595/mohsen89z", "user_type": "registered", "reputation": 128}, "body": "<p>If you have a static seedlist then you can delete \"crawl\" folder each time you want to run the nutch! it would save a lot's of time for you!\nevery time you run nutch your segments growth so linkdb gonna take more time!\nAlso you can create a thread and pass this part of job to it, but you have to handle segmenting buy yourself!</p>\n", "creation_date": 1374510070, "is_accepted": false, "score": 1, "last_activity_date": 1374510070, "answer_id": 17792610}], "question_id": 17769674, "tags": ["nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/17769674/linkdb-adding-segment-solrindexer-takes-lots-of-time", "last_activity_date": 1374510070, "owner": {"user_id": 319297, "answer_count": 41, "creation_date": 1271520246, "accept_rate": 62, "view_count": 75, "reputation": 861}, "body": "<p>Below is the command that I am running to index pages.</p>\n\n<pre><code>bin/nutch crawl bin/urls -solr http://localhost:8983/solr/ -dir crawl -depth 2 -topN 15\n</code></pre>\n\n<p>The fetching happens pretty quickly but LinkDb:adding segments and SolrIndexer steps are taking lot of time, as I run above command repeatedly the time increases. My requirement is such that I want to index pages as fast as possible because links disappear pretty quickly (within 2 mins). I want to decrease this time to a very small figure, what should I do to make this possible?</p>\n\n<p>If I only wanted to index URL and title of the page, will doing so do any good to indexing speed?</p>\n\n<p>Thanks</p>\n", "creation_date": 1374387842, "score": 0},
{"title": "Can i use solr by itself", "view_count": 55, "owner": {"user_id": 2555482, "answer_count": 1, "creation_date": 1373081890, "accept_rate": 67, "view_count": 17, "reputation": 161}, "is_answered": true, "answers": [{"question_id": 17761389, "owner": {"user_id": 907642, "link": "http://stackoverflow.com/users/907642/okke-klein", "user_type": "registered", "reputation": 2221}, "body": "<p>You need a crawler to gather the content, so Solr can index it. <a href=\"http://nutch.apache.org/\" rel=\"nofollow\">Nutch</a> and <a href=\"http://www.crawl-anywhere.com/\" rel=\"nofollow\">Crawl Anywhere</a> work well with Solr.</p>\n", "creation_date": 1374318465, "is_accepted": true, "score": 3, "last_activity_date": 1374318465, "answer_id": 17761604}, {"question_id": 17761389, "owner": {"user_id": 2586595, "link": "http://stackoverflow.com/users/2586595/mohsen89z", "user_type": "registered", "reputation": 128}, "body": "<p>Solr is an indexer and Nutch is a Crawler!\nThey can NOT do each others job!\nIf you want to have a search engine you have to have a crawler and an indexer!\nyou can change them or even you can create your own application for this!</p>\n", "creation_date": 1374326499, "is_accepted": false, "score": 1, "last_activity_date": 1374326499, "answer_id": 17762692}], "question_id": 17761389, "tags": ["apache", "solr", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/17761389/can-i-use-solr-by-itself", "last_activity_date": 1374326499, "accepted_answer_id": 17761604, "body": "<p>I'm building a search engine and need to crawl the web, index it and be able to search the data.</p>\n\n<p>Can I use Apache Solr by itself or do I need Nutch first to crawl the web?</p>\n", "creation_date": 1374316939, "score": 0},
{"title": "Nutch how to crawl but not index the site navigation (w/ Solr)", "view_count": 306, "is_answered": false, "question_id": 17705017, "tags": ["solr", "nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/17705017/nutch-how-to-crawl-but-not-index-the-site-navigation-w-solr", "last_activity_date": 1374077605, "owner": {"user_id": 340648, "answer_count": 11, "creation_date": 1273781720, "accept_rate": 94, "view_count": 119, "location": "New York, NY, United States", "reputation": 695}, "body": "<p>While I feel this should be a standard procedure, I am not really finding anything via searching that gives a good indication of how one might exlude site navigation menu content from the content that Nutch indexes to Solr during a crawl.</p>\n\n<p>That is, I am seeing the navigation menu text in all content that is getting indexed and this damages search because then all content will have the same text in it. Obviously I want to keep using the site navigation for crawling, but I don't want it indexed. Is there a best practice for accomplishing this with Nutch? Like a way to wrap the navigation in some kind of tag <code>&lt;!-- NO_NUTCH_IDX --&gt;</code>, for example?</p>\n\n<p>I am new to Nutch (obviously) so I don't know the best place that this would be accomplished.</p>\n\n<p>thanks very much.</p>\n", "creation_date": 1374077605, "score": 1},
{"title": "nutch 1.5.1 solrindex java.io.IOException: Job failed", "view_count": 2577, "is_answered": true, "answers": [{"question_id": 13177637, "owner": {"user_id": 1526859, "accept_rate": 82, "link": "http://stackoverflow.com/users/1526859/alexander-chepurnoy", "user_type": "registered", "reputation": 474}, "body": "<ol>\n<li>Check whether schema is the same on Nutch and SOLR side</li>\n<li>Change  to  in schema.xml (second \".\" causes problem - bug of 1.5.1)</li>\n<li>If error still exists check download and use SOLR 3.4.0 as Nutch 1.5.1 uses SOLR 3.4.0 client jar.</li>\n</ol>\n", "creation_date": 1352019443, "is_accepted": false, "score": 1, "last_activity_date": 1352019443, "answer_id": 13217289}, {"question_id": 13177637, "owner": {"user_id": 2589206, "link": "http://stackoverflow.com/users/2589206/user2589206", "user_type": "registered", "reputation": 11}, "body": "<p>If you scroll down, you can see it's complaining of a schema.xml error. You can post your schema.xml file and I will look for problems. I came here looking for the solution to the hadoop native platform error.</p>\n", "creation_date": 1374012334, "is_accepted": false, "score": 1, "last_activity_date": 1374012334, "answer_id": 17687826}], "question_id": 13177637, "tags": ["solr", "import", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/13177637/nutch-1-5-1-solrindex-java-io-ioexception-job-failed", "last_activity_date": 1374012334, "owner": {"user_id": 1565249, "answer_count": 7, "creation_date": 1343726275, "accept_rate": 52, "view_count": 41, "location": "Germany", "reputation": 155}, "body": "<p>When i am trying to use the following command on my sles 11 system:</p>\n\n<pre><code>bin/nutch solrindex http://127.0.0.1:8983/solr/ crawl/crawldb -linkdb crawl/linkdb crawl/segments/*\n</code></pre>\n\n<p>I recieve this error:\njava.io.IOException: Job failed!</p>\n\n<p>I am using Nutch 1.5.1 and Solr 1.6.0.</p>\n\n<p>The only log i could find was the hadoop.log, which shows the following the me:</p>\n\n<pre><code>2012-11-01 13:42:38,375 INFO  solr.SolrIndexer - SolrIndexer: starting at 2012-11-01 13:42:38\n2012-11-01 13:42:38,915 INFO  indexer.IndexerMapReduce - IndexerMapReduce: crawldb: crawl/crawldb\n2012-11-01 13:42:38,915 INFO  indexer.IndexerMapReduce - IndexerMapReduce: linkdb: crawl/linkdb\n2012-11-01 13:42:38,915 INFO  indexer.IndexerMapReduce - IndexerMapReduces: adding segment: crawl/segments/20121101124801\n2012-11-01 13:42:39,558 INFO  indexer.IndexerMapReduce - IndexerMapReduces: adding segment: crawl/segments/20121101125604\n2012-11-01 13:42:39,599 INFO  indexer.IndexerMapReduce - IndexerMapReduces: adding segment: crawl/segments/20121101130601\n2012-11-01 13:42:40,083 WARN  util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n2012-11-01 13:42:43,811 INFO  plugin.PluginRepository - Plugins: looking in: /srv/apache-nutch-1.5.1/plugins\n2012-11-01 13:42:44,760 INFO  plugin.PluginRepository - Plugin Auto-activation mode: [true]\n2012-11-01 13:42:44,760 INFO  plugin.PluginRepository - Registered Plugins:\n2012-11-01 13:42:44,770 INFO  plugin.PluginRepository -         the nutch core extension points (nutch-extensionpoints)\n2012-11-01 13:42:44,770 INFO  plugin.PluginRepository -         Basic URL Normalizer (urlnormalizer-basic)\n2012-11-01 13:42:44,770 INFO  plugin.PluginRepository -         Html Parse Plug-in (parse-html)\n2012-11-01 13:42:44,770 INFO  plugin.PluginRepository -         Basic Indexing Filter (index-basic)\n2012-11-01 13:42:44,770 INFO  plugin.PluginRepository -         HTTP Framework (lib-http)\n2012-11-01 13:42:44,770 INFO  plugin.PluginRepository -         Pass-through URL Normalizer (urlnormalizer-pass)\n2012-11-01 13:42:44,770 INFO  plugin.PluginRepository -         Regex URL Filter (urlfilter-regex)\n2012-11-01 13:42:44,770 INFO  plugin.PluginRepository -         Http Protocol Plug-in (protocol-http)\n2012-11-01 13:42:44,770 INFO  plugin.PluginRepository -         Regex URL Normalizer (urlnormalizer-regex)\n2012-11-01 13:42:44,770 INFO  plugin.PluginRepository -         Tika Parser Plug-in (parse-tika)\n2012-11-01 13:42:44,770 INFO  plugin.PluginRepository -         OPIC Scoring Plug-in (scoring-opic)\n2012-11-01 13:42:44,770 INFO  plugin.PluginRepository -         CyberNeko HTML Parser (lib-nekohtml)\n2012-11-01 13:42:44,770 INFO  plugin.PluginRepository -         Anchor Indexing Filter (index-anchor)\n2012-11-01 13:42:44,770 INFO  plugin.PluginRepository -         Regex URL Filter Framework (lib-regex-filter)\n2012-11-01 13:42:44,770 INFO  plugin.PluginRepository - Registered Extension-Points:\n2012-11-01 13:42:44,770 INFO  plugin.PluginRepository -         Nutch URL Normalizer (org.apache.nutch.net.URLNormalizer)\n2012-11-01 13:42:44,770 INFO  plugin.PluginRepository -         Nutch Protocol (org.apache.nutch.protocol.Protocol)\n2012-11-01 13:42:44,770 INFO  plugin.PluginRepository -         Nutch Segment Merge Filter (org.apache.nutch.segment.SegmentMergeFilter)\n2012-11-01 13:42:44,770 INFO  plugin.PluginRepository -         Nutch URL Filter (org.apache.nutch.net.URLFilter)\n2012-11-01 13:42:44,770 INFO  plugin.PluginRepository -         Nutch Indexing Filter (org.apache.nutch.indexer.IndexingFilter)\n2012-11-01 13:42:44,770 INFO  plugin.PluginRepository -         HTML Parse Filter (org.apache.nutch.parse.HtmlParseFilter)\n2012-11-01 13:42:44,770 INFO  plugin.PluginRepository -         Nutch Content Parser (org.apache.nutch.parse.Parser)\n2012-11-01 13:42:44,771 INFO  plugin.PluginRepository -         Nutch Scoring (org.apache.nutch.scoring.ScoringFilter)\n2012-11-01 13:42:44,815 INFO  indexer.IndexingFilters - Adding org.apache.nutch.indexer.basic.BasicIndexingFilter\n2012-11-01 13:42:44,822 INFO  anchor.AnchorIndexingFilter - Anchor deduplication is: off\n2012-11-01 13:42:44,822 INFO  indexer.IndexingFilters - Adding org.apache.nutch.indexer.anchor.AnchorIndexingFilter\n2012-11-01 13:42:54,725 INFO  indexer.IndexingFilters - Adding org.apache.nutch.indexer.basic.BasicIndexingFilter\n2012-11-01 13:42:54,725 INFO  anchor.AnchorIndexingFilter - Anchor deduplication is: off\n2012-11-01 13:42:54,725 INFO  indexer.IndexingFilters - Adding org.apache.nutch.indexer.anchor.AnchorIndexingFilter\n2012-11-01 13:43:03,827 INFO  indexer.IndexingFilters - Adding org.apache.nutch.indexer.basic.BasicIndexingFilter\n2012-11-01 13:43:03,827 INFO  anchor.AnchorIndexingFilter - Anchor deduplication is: off\n2012-11-01 13:43:03,827 INFO  indexer.IndexingFilters - Adding org.apache.nutch.indexer.anchor.AnchorIndexingFilter\n2012-11-01 13:43:12,518 INFO  indexer.IndexingFilters - Adding org.apache.nutch.indexer.basic.BasicIndexingFilter\n2012-11-01 13:43:12,518 INFO  anchor.AnchorIndexingFilter - Anchor deduplication is: off\n2012-11-01 13:43:12,518 INFO  indexer.IndexingFilters - Adding org.apache.nutch.indexer.anchor.AnchorIndexingFilter\n2012-11-01 13:43:24,757 INFO  indexer.IndexingFilters - Adding org.apache.nutch.indexer.basic.BasicIndexingFilter\n2012-11-01 13:43:24,758 INFO  anchor.AnchorIndexingFilter - Anchor deduplication is: off\n2012-11-01 13:43:24,758 INFO  indexer.IndexingFilters - Adding org.apache.nutch.indexer.anchor.AnchorIndexingFilter\n2012-11-01 13:43:34,697 INFO  indexer.IndexingFilters - Adding org.apache.nutch.indexer.basic.BasicIndexingFilter\n2012-11-01 13:43:34,698 INFO  anchor.AnchorIndexingFilter - Anchor deduplication is: off\n2012-11-01 13:43:34,698 INFO  indexer.IndexingFilters - Adding org.apache.nutch.indexer.anchor.AnchorIndexingFilter\n2012-11-01 13:43:44,882 INFO  indexer.IndexingFilters - Adding org.apache.nutch.indexer.basic.BasicIndexingFilter\n2012-11-01 13:43:44,882 INFO  anchor.AnchorIndexingFilter - Anchor deduplication is: off\n2012-11-01 13:43:44,882 INFO  indexer.IndexingFilters - Adding org.apache.nutch.indexer.anchor.AnchorIndexingFilter\n2012-11-01 13:43:50,458 INFO  indexer.IndexingFilters - Adding org.apache.nutch.indexer.basic.BasicIndexingFilter\n2012-11-01 13:43:50,458 INFO  anchor.AnchorIndexingFilter - Anchor deduplication is: off\n2012-11-01 13:43:50,458 INFO  indexer.IndexingFilters - Adding org.apache.nutch.indexer.anchor.AnchorIndexingFilter\n\n2012-11-01 13:43:59,148 INFO  indexer.IndexingFilters - Adding org.apache.nutch.indexer.basic.BasicIndexingFilter\n2012-11-01 13:43:59,148 INFO  anchor.AnchorIndexingFilter - Anchor deduplication is: off\n2012-11-01 13:43:59,148 INFO  indexer.IndexingFilters - Adding org.apache.nutch.indexer.anchor.AnchorIndexingFilter\n2012-11-01 13:44:04,299 INFO  indexer.IndexingFilters - Adding org.apache.nutch.indexer.basic.BasicIndexingFilter\n2012-11-01 13:44:04,299 INFO  anchor.AnchorIndexingFilter - Anchor deduplication is: off\n2012-11-01 13:44:04,299 INFO  indexer.IndexingFilters - Adding org.apache.nutch.indexer.anchor.AnchorIndexingFilter\n2012-11-01 13:44:11,093 INFO  indexer.IndexingFilters - Adding org.apache.nutch.indexer.basic.BasicIndexingFilter\n2012-11-01 13:44:11,093 INFO  anchor.AnchorIndexingFilter - Anchor deduplication is: off\n2012-11-01 13:44:11,093 INFO  indexer.IndexingFilters - Adding org.apache.nutch.indexer.anchor.AnchorIndexingFilter\n2012-11-01 13:44:19,633 INFO  indexer.IndexingFilters - Adding org.apache.nutch.indexer.basic.BasicIndexingFilter\n2012-11-01 13:44:19,633 INFO  anchor.AnchorIndexingFilter - Anchor deduplication is: off\n2012-11-01 13:44:19,633 INFO  indexer.IndexingFilters - Adding org.apache.nutch.indexer.anchor.AnchorIndexingFilter\n2012-11-01 13:44:30,885 INFO  indexer.IndexingFilters - Adding org.apache.nutch.indexer.basic.BasicIndexingFilter\n2012-11-01 13:44:30,885 INFO  anchor.AnchorIndexingFilter - Anchor deduplication is: off\n2012-11-01 13:44:30,885 INFO  indexer.IndexingFilters - Adding org.apache.nutch.indexer.anchor.AnchorIndexingFilter\n2012-11-01 13:44:39,637 INFO  indexer.IndexingFilters - Adding org.apache.nutch.indexer.basic.BasicIndexingFilter\n2012-11-01 13:44:39,637 INFO  anchor.AnchorIndexingFilter - Anchor deduplication is: off\n2012-11-01 13:44:39,637 INFO  indexer.IndexingFilters - Adding org.apache.nutch.indexer.anchor.AnchorIndexingFilter\n2012-11-01 13:44:47,905 INFO  indexer.IndexingFilters - Adding org.apache.nutch.indexer.basic.BasicIndexingFilter\n2012-11-01 13:44:47,906 INFO  anchor.AnchorIndexingFilter - Anchor deduplication is: off\n2012-11-01 13:44:47,906 INFO  indexer.IndexingFilters - Adding org.apache.nutch.indexer.anchor.AnchorIndexingFilter\n2012-11-01 13:44:48,104 INFO  solr.SolrMappingReader - source: content dest: content\n2012-11-01 13:44:48,105 INFO  solr.SolrMappingReader - source: title dest: title\n2012-11-01 13:44:48,105 INFO  solr.SolrMappingReader - source: host dest: host\n2012-11-01 13:44:48,105 INFO  solr.SolrMappingReader - source: segment dest: segment\n2012-11-01 13:44:48,106 INFO  solr.SolrMappingReader - source: boost dest: boost\n2012-11-01 13:44:48,106 INFO  solr.SolrMappingReader - source: digest dest: digest\n2012-11-01 13:44:48,106 INFO  solr.SolrMappingReader - source: tstamp dest: tstamp\n2012-11-01 13:44:48,106 INFO  solr.SolrMappingReader - source: url dest: id\n2012-11-01 13:44:48,107 INFO  solr.SolrMappingReader - source: url dest: url\n2012-11-01 13:44:48,398 INFO  solr.SolrWriter - Indexing 11 documents\n2012-11-01 13:44:49,082 WARN  mapred.LocalJobRunner - job_local_0001\norg.apache.solr.common.SolrException: Severe errors in solr configuration.  Check your log files for more detailed information on what may be wrong.  If you want solr to continue after configuration errors, change:    &lt;abortOnConfigurationError&gt;false&lt;/abortOnConfigurationError&gt;  in solr.xml  ------------------------------------------------------------- org.apache.solr.common.SolrException: Schema Parsing Failed: multiple points         at org.apache.solr.schema.IndexSchema.readSchema(IndexSchema.java:688)  at org.apache.solr.schema.IndexSchema.&lt;init&gt;(IndexSchema.java:123)      at org.apache.solr.core.CoreContainer.create(CoreContainer.java:481)    at org.apache.solr.core.CoreContainer.load(CoreContainer.java:335)      at org.apache.solr.core.CoreContainer.load(CoreContainer.java:219)      at org.apache.solr.core.CoreContainer$Initializer.initialize(CoreContainer.java:161)    at org.apache.solr.servlet.SolrDispatchFilter.init(SolrDispatchFilter.java:96)  at org.mortbay.jetty.servlet.FilterHolder.doStart(FilterHolder.java:97)         at org.mortbay.component.AbstractLifeCycle.start(AbstractLifeCycle.java:50)     at org.mortbay.jetty.servlet.ServletHandler.initialize(ServletHandler.java:713)         at org.mortbay.jetty.servlet.Context.startContext(Context.java:140)     at org.mortbay.jetty.webapp.WebAppContext.startContext(WebAppContext.java:1282)         at org.mortbay.jetty.handler.ContextHandler.doStart(ContextHandler.java:518)    at org.mortbay.jetty.webapp.WebAppContext.doStart(WebAppContext.java:499)       at org.mortbay.component.AbstractLifeCycle.start(AbstractLifeCycle.java:50)     at org.mortbay.jetty.handler.HandlerCollection.doStart(HandlerCollection.java:152)      at org.mortbay.jetty.handler.ContextHandlerCollection.doStart(ContextHandlerCollection.java:156)        at org.mortbay.component.AbstractLifeCycle.start(AbstractLifeCycle.java:50)     at org.mortbay.jetty.handler.HandlerCollection.doStart(HandlerCollection.java:152)      at org.mortbay.component.AbstractLifeCycle.start(AbstractLifeCycle.java:50)     at org.mortbay.jetty.handler.HandlerWrapper.doStart(HandlerWrapper.java:130)    at org.mortbay.jetty.Server.doSt\n\nSevere errors in solr configuration.  Check your log files for more detailed information on what may be wrong.  If you want solr to continue after configuration errors, change:    &lt;abortOnConfigurationError&gt;false&lt;/abortOnConfigurationError&gt;  in solr.xml  ------------------------------------------------------------- org.apache.solr.common.SolrException: Schema Parsing Failed: multiple points       at org.apache.solr.schema.IndexSchema.readSchema(IndexSchema.java:688)  at org.apache.solr.schema.IndexSchema.&lt;init&gt;(IndexSchema.java:123)      at org.apache.solr.core.CoreContainer.create(CoreContainer.java:481)    at org.apache.solr.core.CoreContainer.load(CoreContainer.java:335)      at org.apache.solr.core.CoreContainer.load(CoreContainer.java:219)      at org.apache.solr.core.CoreContainer$Initializer.initialize(CoreContainer.java:161)    at org.apache.solr.servlet.SolrDispatchFilter.init(SolrDispatchFilter.java:96)  at org.mortbay.jetty.servlet.FilterHolder.doStart(FilterHolder.java:97)         at org.mortbay.component.AbstractLifeCycle.start(AbstractLifeCycle.java:50)     at org.mortbay.jetty.servlet.ServletHandler.initialize(ServletHandler.java:713)         at org.mortbay.jetty.servlet.Context.startContext(Context.java:140)     at org.mortbay.jetty.webapp.WebAppContext.startContext(WebAppContext.java:1282)         at org.mortbay.jetty.handler.ContextHandler.doStart(ContextHandler.java:518)    at org.mortbay.jetty.webapp.WebAppContext.doStart(WebAppContext.java:499)       at org.mortbay.component.AbstractLifeCycle.start(AbstractLifeCycle.java:50)     at org.mortbay.jetty.handler.HandlerCollection.doStart(HandlerCollection.java:152)      at org.mortbay.jetty.handler.ContextHandlerCollection.doStart(ContextHandlerCollection.java:156)        at org.mortbay.component.AbstractLifeCycle.start(AbstractLifeCycle.java:50)     at org.mortbay.jetty.handler.HandlerCollection.doStart(HandlerCollection.java:152)      at org.mortbay.component.AbstractLifeCycle.start(AbstractLifeCycle.java:50)     at org.mortbay.jetty.handler.HandlerWrapper.doStart(HandlerWrapper.java:130)    at org.mortbay.jetty.Server.doSt\n\nrequest: http://localhost:8983/solr/update?wt=javabin&amp;version=2\n        at org.apache.solr.client.solrj.impl.CommonsHttpSolrServer.request(CommonsHttpSolrServer.java:430)\n        at org.apache.solr.client.solrj.impl.CommonsHttpSolrServer.request(CommonsHttpSolrServer.java:244)\n        at org.apache.solr.client.solrj.request.AbstractUpdateRequest.process(AbstractUpdateRequest.java:105)\n        at org.apache.nutch.indexer.solr.SolrWriter.close(SolrWriter.java:142)\n        at org.apache.nutch.indexer.IndexerOutputFormat$1.close(IndexerOutputFormat.java:48)\n        at org.apache.hadoop.mapred.ReduceTask$OldTrackingRecordWriter.close(ReduceTask.java:466)\n        at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:530)\n        at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:420)\n        at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:260)\n2012-11-01 13:45:01,864 ERROR solr.SolrIndexer - java.io.IOException: Job failed!\n</code></pre>\n\n<p>As it said i checked for a solr log, but i couldnt find one :/\nAny idea about that?</p>\n\n<p>Greets</p>\n", "creation_date": 1351774798, "score": 0},
{"title": "nutch 1.2 solr 3.1 bad request issue", "view_count": 199, "is_answered": false, "question_id": 17526427, "tags": ["solr", "nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/17526427/nutch-1-2-solr-3-1-bad-request-issue", "last_activity_date": 1373996955, "owner": {"user_id": 2526784, "answer_count": 0, "creation_date": 1372313344, "accept_rate": 64, "view_count": 53, "reputation": 77}, "body": "<p>I have crawled a site successfully using NUTCH 1.2 .Now I want to integrate this with solr 3.1 . Problem is when I am issuing command $ bin/nutch solrindex localhost:8080/solr/ crawl/crawldb crawl/linkdb cra wl/segments/* an error occurs. I am attaching my nutch logs</p>\n\n<p>Please help me to solve this issue</p>\n\n<p>Bad Request</p>\n\n<p>request: //localhost:8080/solr/update?wt=javabin&amp;version=2 at org.apache.solr.client.solrj.impl.CommonsHttpSolrServer.request(CommonsHttpSolrServer.java:436) at org.apache.solr.client.solrj.impl.CommonsHttpSolrServer.request(CommonsHttpSolrServer.java:245) at org.apache.solr.client.solrj.request.AbstractUpdateRequest.process(AbstractUpdateRequest.java:105) at org.apache.solr.client.solrj.SolrServer.add(SolrServer.java:49) at org.apache.nutch.indexer.solr.SolrWriter.close(SolrWriter.java:75) at org.apache.nutch.indexer.IndexerOutputFormat$1.close(IndexerOutputFormat.java:48) at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:474) at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:411) at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:216) 2013-07-08 17:38:47,577 ERROR solr.SolrIndexer - java.io.IOException: Job failed!</p>\n", "creation_date": 1373285899, "score": 1},
{"title": "nutch: searching with key word", "view_count": 185, "is_answered": false, "answers": [{"question_id": 7937065, "owner": {"user_id": 2586595, "link": "http://stackoverflow.com/users/2586595/mohsen89z", "user_type": "registered", "reputation": 128}, "body": "<p>If you want to crawl this specific urls you should include following line in crawl-urlfilter.txt</p>\n\n<pre><code>-.*\n</code></pre>\n\n<p>this command will exclude all other urls!</p>\n", "creation_date": 1373969013, "is_accepted": false, "score": 0, "last_activity_date": 1373969013, "answer_id": 17673624}], "question_id": 7937065, "tags": ["search", "keyword", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/7937065/nutch-searching-with-key-word", "last_activity_date": 1373969013, "owner": {"user_id": 935376, "answer_count": 1, "creation_date": 1315501981, "accept_rate": 57, "view_count": 163, "reputation": 400}, "body": "<p>Using Nutch, I would like to crawl all <a href=\"http://www.amazon.com/\" rel=\"nofollow\">http://www.amazon.com/</a> webpages which have ipod in their url.</p>\n\n<p>For e.g if my search for ipod in their search box, I get</p>\n\n<p><a href=\"http://www.amazon.com/s/ref=nb_sb_noss?url=search-alias%3Daps&amp;field-keywords=ipod&amp;x=0&amp;y=0\" rel=\"nofollow\">http://www.amazon.com/s/ref=nb_sb_noss?url=search-alias%3Daps&amp;field-keywords=ipod&amp;x=0&amp;y=0</a></p>\n\n<p>This webpage shows a bunch of links for ipod. I would like to crawl each link which is related to ipod and get it.</p>\n\n<p>The first link that shows under the search results is </p>\n\n<p><a href=\"http://rads.stackoverflow.com/amzn/click/B001FA1O0O\" rel=\"nofollow\">http://www.amazon.com/Apple-iPod-touch-8GB-Generation/dp/B001FA1O0O/ref=sr_1_1?ie=UTF8&amp;qid=1319863311&amp;sr=8-</a></p>\n\n<p>Should I use below?</p>\n\n<pre><code>+^http://([a-z0-9\\-A-Z]*\\.)*www.amazon.com/*ipod*\n</code></pre>\n", "creation_date": 1319863653, "score": 2},
{"title": "Use Nutch with newest Elasticsearch", "view_count": 2907, "owner": {"user_id": 832056, "answer_count": 32, "creation_date": 1309971839, "accept_rate": 84, "view_count": 163, "reputation": 859}, "is_answered": true, "answers": [{"question_id": 17542878, "owner": {"user_id": 832056, "accept_rate": 84, "link": "http://stackoverflow.com/users/832056/pqn", "user_type": "registered", "reputation": 859}, "body": "<p>I think I found the solution. You need to modify all references to the version number, both in <code>ivy/ivy.xml</code> and <code>pom.xml</code> (which is the file I forgot to change). Changing both <code>0.19.4</code>s to <code>0.90.2</code>s should do the trick. Also, you need to change <code>item.failed()</code> in <code>src/java/org/apache/nutch/indexer/elastic/ElasticWriter.java</code> to <code>item.isFailed()</code> to match the newer Elasticsearch refactoring.</p>\n", "creation_date": 1373937546, "is_accepted": true, "score": 3, "last_activity_date": 1373937546, "answer_id": 17666523}], "question_id": 17542878, "tags": ["apache", "solr", "version", "elasticsearch", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/17542878/use-nutch-with-newest-elasticsearch", "last_activity_date": 1373937546, "accepted_answer_id": 17666523, "body": "<p>Based on <a href=\"http://stackoverflow.com/questions/16729940/outofmemoryerror-for-bin-nutch-elasticindex-cluser-all-nutch-2-1\">this question</a> I have managed to successfully integrate Nutch and Elasticsearch, albeit by downgrading my Elasticsearch version. How can I modify the Nutch source code to accommodate the latest version of Elasticsearch (0.90.2+)? I have tried modifying the Ivy dependency to this version of Elasticsearch and also modified Nutch's Elasticsearch compatibility code so it would build properly, but I end up with an error as Nutch times out waiting for a response from Elasticsearch; the two are unable to communicate.</p>\n", "creation_date": 1373356478, "score": 2},
{"title": "Nutch - Which version to use for Apache Solr", "view_count": 137, "is_answered": false, "question_id": 17654837, "tags": ["solr", "nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/17654837/nutch-which-version-to-use-for-apache-solr", "last_activity_date": 1373893092, "owner": {"age": 26, "answer_count": 34, "creation_date": 1364721047, "user_id": 2228912, "accept_rate": 88, "view_count": 141, "reputation": 446}, "body": "<p>I understand that <code>Nutch 1.x</code> is based on Hadoop (map and reduce), and that <code>Nutch 2.x</code> can be used on NoSQL <code>Apache Gora</code>.  </p>\n\n<p>But which version is the best to use with Apache Solr ? or will it depend on the website crawled size ?</p>\n", "creation_date": 1373893092, "score": 0},
{"title": "error while running solr index", "view_count": 548, "owner": {"user_id": 1456249, "answer_count": 28, "creation_date": 1339678274, "accept_rate": 100, "view_count": 80, "location": "Hyderabad, India", "reputation": 510}, "is_answered": true, "answers": [{"question_id": 13802036, "owner": {"user_id": 1456249, "accept_rate": 100, "link": "http://stackoverflow.com/users/1456249/swamy", "user_type": "registered", "reputation": 510}, "body": "<p>Added the following line in solr_xx_xx/example/solr/conf/schema.xml</p>\n\n<pre><code>&lt;field name=\"cache\" type=\"string\" stored=\"true\" indexed=\"false\"/&gt;\n</code></pre>\n\n<p>and it worked fine.</p>\n", "creation_date": 1373889805, "is_accepted": true, "score": 0, "last_activity_date": 1373889805, "answer_id": 17653828}], "question_id": 13802036, "tags": ["solr", "hadoop", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/13802036/error-while-running-solr-index", "last_activity_date": 1373889805, "accepted_answer_id": 17653828, "body": "<p>I am running solrindex on nutch crawled data using the following command:</p>\n\n<pre><code>bin/nutch solrindex &lt;prep&gt;&lt;code&gt;http://127.0.0.1:8983/solr/ /app/hadoop/tmp/crawled_pages/crawldb -linkdb /app/hadoop/tmp/crawled_pages/linkdb /app/hadoop/tmp/crawled_pages/segments/*\n</code></pre>\n\n<p>I am getting the below error and I am not able to root cause for this issue. </p>\n\n<pre><code>org.apache.solr.common.SolrException: ERROR: [doc=http://www.bbc.co.uk/portugueseafrica/arquivo/index.shtml] unknown field 'cache'\n\nERROR: [doc=http://www.bbc.co.uk/portugueseafrica/arquivo/index.shtml] unknown field 'cache'\n\nrequest: &lt;prep&gt;&lt;code&gt;http://127.0.0.1:8983/solr/update?wt=javabin&amp;version=2\n        at org.apache.solr.client.solrj.impl.CommonsHttpSolrServer.request(CommonsHttpSolrServer.java:430)\n        at org.apache.solr.client.solrj.impl.CommonsHttpSolrServer.request(CommonsHttpSolrServer.java:244)\n        at org.apache.solr.client.solrj.request.AbstractUpdateRequest.process(AbstractUpdateRequest.java:105)\n        at org.apache.nutch.indexer.solr.SolrWriter.write(SolrWriter.java:124)\n        at org.apache.nutch.indexer.IndexerOutputFormat$1.write(IndexerOutputFormat.java:55)\n        at org.apache.nutch.indexer.IndexerOutputFormat$1.write(IndexerOutputFormat.java:44)\n        at org.apache.hadoop.mapred.ReduceTask$OldTrackingRecordWriter.write(ReduceTask.java:457)\n        at org.apache.hadoop.mapred.ReduceTask$3.collect(ReduceTask.java:497)\n        at org.apache.nutch.indexer.IndexerMapReduce.reduce(IndexerMapReduce.java:195)\n        at org.apache.nutch.indexer.IndexerMapReduce.reduce(IndexerMapReduce.java:51)\n        at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:519)\n        at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:420)\n        at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:260)\n2012-12-10 10:05:49,198 ERROR solr.SolrIndexer - java.io.IOException: Job failed!\n</code></pre>\n\n<p>Did anybody got similar problem? </p>\n\n<p>I don't understand what is the root cause for this below error..</p>\n\n<pre><code>org.apache.solr.common.SolrException: ERROR: [doc=http://www.bbc.co.uk/portugueseafrica/arquivo/index.shtml] unknown field 'cache'\n</code></pre>\n", "creation_date": 1355146536, "score": 1},
{"title": "How to configure Apache Nutch to ignore certain url patterns", "view_count": 839, "owner": {"user_id": 1474659, "answer_count": 6, "creation_date": 1340364399, "accept_rate": 48, "view_count": 118, "reputation": 321}, "is_answered": true, "answers": [{"question_id": 17571141, "owner": {"user_id": 1474659, "accept_rate": 48, "link": "http://stackoverflow.com/users/1474659/abhijeet", "user_type": "registered", "reputation": 321}, "body": "<p>I followed following url and found many  useful examples</p>\n\n<p><a href=\"https://scm.thm.de/pharus/nutch-config/blobs/66fba7d3dc015974b5c194e7ba49da60fe3c3199/Nutch-Config/conf/regex-urlfilter.txt\" rel=\"nofollow\">https://scm.thm.de/pharus/nutch-config/blobs/66fba7d3dc015974b5c194e7ba49da60fe3c3199/Nutch-Config/conf/regex-urlfilter.txt</a></p>\n", "creation_date": 1373463613, "is_accepted": true, "score": 0, "last_activity_date": 1373463613, "answer_id": 17572173}], "question_id": 17571141, "tags": ["nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/17571141/how-to-configure-apache-nutch-to-ignore-certain-url-patterns", "last_activity_date": 1373463613, "accepted_answer_id": 17572173, "body": "<p>I am crawling a website using Apache Nutch. While crawling, I want nutch to ignore multiple url patterns like <a href=\"http://www.youtube.com/..so\" rel=\"nofollow\">http://www.youtube.com/..so</a> on..., <a href=\"http://www.twitter.com/so\" rel=\"nofollow\">http://www.twitter.com/so</a> on.., etc. </p>\n\n<p>I know how to configure regex-urlfilter.txt file to crawl specific url. </p>\n\n<p>But I dont know how to configure nutch to ignore certain url patterns? </p>\n", "creation_date": 1373460747, "score": 0},
{"title": "Nutch Crawling Result as JSON", "view_count": 787, "is_answered": true, "answers": [{"last_edit_date": 1373441702, "owner": {"user_id": 502950, "accept_rate": 90, "link": "http://stackoverflow.com/users/502950/mikrodel", "user_type": "registered", "reputation": 4513}, "body": "<p>The feature you are looking for is already implemented: <a href=\"https://issues.apache.org/jira/browse/NUTCH-932\" rel=\"nofollow\">Nutch-932 retrieve crawl results as JSON </a></p>\n\n<p>There are also examples how to use it in the link above.</p>\n", "question_id": 17227993, "creation_date": 1373437325, "is_accepted": false, "score": 1, "last_activity_date": 1373441702, "answer_id": 17563715}], "question_id": 17227993, "tags": ["java", "apache", "web-crawler", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/17227993/nutch-crawling-result-as-json", "last_activity_date": 1373441702, "owner": {"user_id": 1577467, "answer_count": 1, "creation_date": 1344177440, "accept_rate": 52, "view_count": 286, "reputation": 800}, "body": "<p>I am using <code>apache-nutch-2.1</code> to crawl. Is it possible to get the crawled result as json? I wrote a java program to crawl using apache-nutch-2.1. How Would i retrive the crawled result as JSON. It may be a silly question but when i google it. There is no related to that.</p>\n\n<p>How would add i get the crawled result s json?</p>\n", "creation_date": 1371791022, "score": 4},
{"title": "Run apache Nutch 2.2.1", "view_count": 1335, "owner": {"user_id": 1577467, "answer_count": 1, "creation_date": 1344177440, "accept_rate": 52, "view_count": 286, "reputation": 800}, "is_answered": true, "answers": [{"question_id": 17523001, "owner": {"user_id": 1830069, "accept_rate": 63, "link": "http://stackoverflow.com/users/1830069/sunskin", "user_type": "registered", "reputation": 543}, "body": "<p>You might want to follow this link <a href=\"http://nlp.solutions.asia/?p=362\" rel=\"nofollow\">http://nlp.solutions.asia/?p=362</a> but thats for Nutch 2.2 not 2.2.1. You can take a look at that link on atleast how to get started with a src file. Hope this helps!</p>\n", "creation_date": 1373403943, "is_accepted": true, "score": 2, "last_activity_date": 1373403943, "answer_id": 17558184}], "question_id": 17523001, "tags": ["java", "apache", "web-crawler", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/17523001/run-apache-nutch-2-2-1", "last_activity_date": 1373403943, "accepted_answer_id": 17558184, "body": "<p>I want to use the <code>Apache Nutch 2.2.1</code> for web page crawling. It has src file only. How would i run this. Is  there any tutorial available for that. I want the result as Json.  apache 1.6 is working perfectly in my system.</p>\n\n<p>Please anybody can help me?</p>\n", "creation_date": 1373275117, "score": 1},
{"title": "Getting No Urls to Fetch error on Nutch, even though there are Urls to fetch", "view_count": 2760, "owner": {"user_id": 340648, "answer_count": 11, "creation_date": 1273781720, "accept_rate": 94, "view_count": 119, "location": "New York, NY, United States", "reputation": 695}, "is_answered": true, "answers": [{"question_id": 17458155, "owner": {"user_id": 340648, "accept_rate": 94, "link": "http://stackoverflow.com/users/340648/roy", "user_type": "registered", "reputation": 695}, "body": "<p>Folks this is embarrassing. But the old nutch-not-crawling-because-it's-dismissining-urls addage of 'check your <code>*-urlfilter.txt</code>' file applies here. </p>\n\n<p>In my case i had an extra <code>/</code> in the url regex:</p>\n\n<p><code>+^http://([a-z0-9\\-A-Z]*\\.)*nutch.apache.org//([a-z0-9\\-A-Z]*\\/)*</code></p>\n\n<p>should have been <code>+^http://([a-z0-9\\-A-Z]*\\.)*nutch.apache.org/([a-z0-9\\-A-Z]*\\/)*</code></p>\n", "creation_date": 1373383936, "is_accepted": true, "score": 1, "last_activity_date": 1373383936, "answer_id": 17552285}], "question_id": 17458155, "tags": ["solr", "web-crawler", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/17458155/getting-no-urls-to-fetch-error-on-nutch-even-though-there-are-urls-to-fetch", "last_activity_date": 1373383936, "accepted_answer_id": 17552285, "body": "<p>I am still getting used to Nutch. I managed to get a test crawl going using <code>bin/nutch crawl urls -dir crawl -depth 6 -topN 10</code> over <code>nutch.apache.org</code> as well as indexing it to solr using: <code>bin/nutch crawl urls -solr http://&lt;domain&gt;:&lt;port&gt;/solr/core1/ -depth 4 -topN 7</code></p>\n\n<p>Not even mentioning that it times out on my own site, I can't seem to get it to crawl again, or crawl any other sites (e.g. wiki.apache.org). I have deleted all of the crawl directories in the nutch home directory and I still get the following error (stating that there are no more URLs to crawl):</p>\n\n<pre><code>&lt;user&gt;@&lt;domain&gt;:/usr/share/nutch$ sudo sh nutch-test.sh\nsolrUrl is not set, indexing will be skipped...\ncrawl started in: crawl \nrootUrlDir = urls\nthreads = 10\ndepth = 6\nsolrUrl=null\ntopN = 10\nInjector: starting at 2013-07-03 15:56:47\nInjector: crawlDb: crawl/crawldb\nInjector: urlDir: urls\nInjector: Converting injected urls to crawl db entries.\nInjector: total number of urls rejected by filters: 1\nInjector: total number of urls injected after normalization and filtering: 0\nInjector: Merging injected urls into crawl db.\nInjector: finished at 2013-07-03 15:56:50, elapsed: 00:00:03\nGenerator: starting at 2013-07-03 15:56:50\nGenerator: Selecting best-scoring urls due for fetch.\nGenerator: filtering: true\nGenerator: normalizing: true\nGenerator: topN: 10\nGenerator: jobtracker is 'local', generating exactly one partition.\nGenerator: 0 records selected for fetching, exiting ...\nStopping at depth=0 - no more URLs to fetch.\nNo URLs to fetch - check your seed list and URL filters.\ncrawl finished: crawl\n</code></pre>\n\n<p>My <code>urls/seed.txt</code> file has <code>http://nutch.apache.org/</code> in it.</p>\n\n<p>My <code>regex-urlfilter.txt</code> has <code>+^http://([a-z0-9\\-A-Z]*\\.)*nutch.apache.org//([a-z0-9\\-A-Z]*\\/)*</code> in it. </p>\n\n<p>I have also increase the <code>-depth</code> and <code>topN</code> to specify that there is more to index, but it always gives the error after the first crawl. How do I reset it so that it crawls again? Is there some cache of URLs that needs to be cleaned out somewhere in Nutch?</p>\n\n<p><strong>UPDATE</strong>: It seems the problem with our site was that I was not using <code>www</code>, it did not resolve without <code>www</code>. By a <code>ping</code>, www.ourdomain.org does resolve. </p>\n\n<p>But i have put this into the necessary files and there is still a problem. Primarily it looks like <code>Injector: total number of urls rejected by filters: 1</code> is the problem across the board, but was not on the first crawl. Why and what filter is rejecting the URL, it should not be.</p>\n", "creation_date": 1372885735, "score": 2},
{"title": "nutch 1.2 solr 3.1 integration issue", "view_count": 73, "owner": {"user_id": 2526784, "answer_count": 0, "creation_date": 1372313344, "accept_rate": 64, "view_count": 53, "reputation": 77}, "is_answered": true, "answers": [{"question_id": 17525861, "owner": {"user_id": 176569, "accept_rate": 71, "link": "http://stackoverflow.com/users/176569/bpgergo", "user_type": "registered", "reputation": 11357}, "body": "<p>You'll need to add following Apache Commons library to the classpath: <code>commons-httpclient.jar</code> (you would put it in the same folder where other JARs reside that are used by your nutch installation). </p>\n\n<p>You can find the current version of HttpClient here <a href=\"http://hc.apache.org/httpcomponents-client-ga/\" rel=\"nofollow\">http://hc.apache.org/httpcomponents-client-ga/</a></p>\n\n<p>Please note that it is possible that your Nutch version uses an older version of the HttpClient and the current version of the HttpClient is not backward compatible with that older version. In this case you'll need to download that older version of the HttpClient and include that older version within your libs.</p>\n", "creation_date": 1373284646, "is_accepted": true, "score": 0, "last_activity_date": 1373284646, "answer_id": 17526014}], "question_id": 17525861, "tags": ["solr", "nutch", "java.lang.class"], "answer_count": 1, "link": "http://stackoverflow.com/questions/17525861/nutch-1-2-solr-3-1-integration-issue", "last_activity_date": 1373285581, "accepted_answer_id": 17526014, "body": "<p>I have crawled a site successfully using NUTCH 1.2 .Now I want to integrate this with solr 3.1 . Problem is when I am issuing command $ bin/nutch solrindex localhost:8080/solr/ crawl/crawldb crawl/linkdb cra wl/segments/* an error  occurs. I am attaching my nutch logs</p>\n\n<p>Please help me to solve this issue</p>\n\n<p>Bad Request</p>\n\n<p>request: //localhost:8080/solr/update?wt=javabin&amp;version=2\n    at org.apache.solr.client.solrj.impl.CommonsHttpSolrServer.request(CommonsHttpSolrServer.java:436)\n    at org.apache.solr.client.solrj.impl.CommonsHttpSolrServer.request(CommonsHttpSolrServer.java:245)\n    at org.apache.solr.client.solrj.request.AbstractUpdateRequest.process(AbstractUpdateRequest.java:105)\n    at org.apache.solr.client.solrj.SolrServer.add(SolrServer.java:49)\n    at org.apache.nutch.indexer.solr.SolrWriter.close(SolrWriter.java:75)\n    at org.apache.nutch.indexer.IndexerOutputFormat$1.close(IndexerOutputFormat.java:48)\n    at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:474)\n    at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:411)\n    at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:216)\n2013-07-08 17:38:47,577 ERROR solr.SolrIndexer - java.io.IOException: Job failed!</p>\n", "creation_date": 1373284142, "score": 0},
{"title": "not able to use webgraph command in nutch 1.2", "view_count": 40, "is_answered": false, "question_id": 17519772, "tags": ["nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/17519772/not-able-to-use-webgraph-command-in-nutch-1-2", "last_activity_date": 1373261886, "owner": {"user_id": 2526784, "answer_count": 0, "creation_date": 1372313344, "accept_rate": 64, "view_count": 53, "reputation": 77}, "body": "<p>I am quite new to nutch . Thing is I have crawled a site successfully using nutch 1.2 .Now using cygwin I am working on crawldb and segments . Problem is when I am using webgraphdb command it is showing \"Error: Could not find or load main class WebGraph\". Please suggest me that what I need to do to use this command properly.</p>\n", "creation_date": 1373261886, "score": 1},
{"title": "Nutch showing following errors, what to do", "view_count": 4315, "is_answered": true, "answers": [{"last_edit_date": 1335631755, "owner": {"user_id": 1150329, "accept_rate": 89, "link": "http://stackoverflow.com/users/1150329/tejas-patil", "user_type": "registered", "reputation": 4793}, "body": "<blockquote>\n  <p>but when i run nutch from terminal it show</p>\n</blockquote>\n\n<p>This verifies that the <code>NUTCH_HOME/bin/nutch</code> script is present at the correct location.</p>\n\n<p>Please export <code>NUTCH_HOME</code> and <code>NUTCH_CONF_DIR</code></p>\n\n<p>Which mode of nutch are you trying to use ?</p>\n\n<ol>\n<li><strong>local mode</strong> : jobs run without hadoop. you need to have nutch jar inside NUTCH_HOME/lib. Its named after the version that you are using . eg. for nutch release 1.3, the jar name is nutch-1.3.jar.</li>\n<li><strong>hadoop mode</strong> : jobs run on hadoop cluster. you need to have nutch job file inside NUTCH_HOME. its named after the release version eg. nutch-1.3.job</li>\n</ol>\n\n<p>If you happen to have these files (corresponding to the mode), then extract those and see if the Crawl.class file is indeed present inside it. </p>\n\n<p>If Crawl.class file is not present, then obtain the new jar/job file by compiling the nutch source.</p>\n\n<p><strong>EDIT:</strong></p>\n\n<ol>\n<li>Dont use <code>ant jar</code>. Use <code>ant clean runtime</code> instead. The output gets generated inside <code>NUTCH_INSTALLATION_DIR/runtime/local</code> directory. Run nutch from there. That will be your <code>NUTCH_HOME</code></li>\n<li>Export the required variables <code>JAVA_HOME, NUTCH_HOME and NUTCH_CONF_DIR</code> before running.</li>\n<li>I am getting a feeling that the Crawl.class file is not present in the jar. Please extract the jar and check it out. FYI: Command to extract a jar file is <code>jar -xvf &lt;filename&gt;</code></li>\n<li>If after #2, you see that class file aint present in the jar, then see if the nutch source code that you downloaded has the java file. ie. <code>nutch-1.x\\src\\java\\org\\apache\\nutch\\crawl\\Crawl.java</code> If not present, get it from internet and rebuild nutch jar.</li>\n<li>If after #2, the jar file has class file and you see the issue again, then something is wrong with the environment. Try out some other command like <a href=\"http://wiki.apache.org/nutch/bin/nutch_inject\" rel=\"nofollow\">inject</a>. Look for some errors in the hadoop.log file. Let me know what you see.</li>\n</ol>\n", "question_id": 10267376, "creation_date": 1335094542, "is_accepted": false, "score": 1, "last_activity_date": 1335631755, "answer_id": 10267584}, {"question_id": 10267376, "owner": {"user_id": 1359760, "link": "http://stackoverflow.com/users/1359760/claudius", "user_type": "registered", "reputation": 21}, "body": "<p>It seems that in Nutch version 2.x the name of the Crawl class has changed to Crawler.\nI'm using Hadoop to run Nutch, so I use the following command for crawling:</p>\n\n<p>hadoop jar apache-nutch-2.2.1.job org.apache.nutch.crawl.Crawler urls -solr http://:8983 -depth 2</p>\n\n<p>If you crawl using Nutch on its own, the nutch script should reference the new class name.</p>\n", "creation_date": 1373189476, "is_accepted": false, "score": 2, "last_activity_date": 1373189476, "answer_id": 17510848}], "question_id": 10267376, "tags": ["nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/10267376/nutch-showing-following-errors-what-to-do", "last_activity_date": 1373189476, "owner": {"age": 26, "answer_count": 0, "creation_date": 1334056673, "user_id": 1323904, "view_count": 16, "location": "New Delhi", "reputation": 7}, "body": "<pre><code>enter code here\n\nnpun@nipun:~$ nutch crawl urls -dir crawl -depth 2 -topN 10\nException in thread \"main\" java.lang.NoClassDefFoundError: org/apache/nutch/crawl/Crawl\nCaused by: java.lang.ClassNotFoundException: org.apache.nutch.crawl.Crawl\n    at java.net.URLClassLoader$1.run(URLClassLoader.java:202)\n    at java.security.AccessController.doPrivileged(Native Method)\n    at java.net.URLClassLoader.findClass(URLClassLoader.java:190)\n    at java.lang.ClassLoader.loadClass(ClassLoader.java:307)\n    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301)\n    at java.lang.ClassLoader.loadClass(ClassLoader.java:248)\nCould not find the main class: org.apache.nutch.crawl.Crawl.  Program will exit.\n</code></pre>\n\n<p>but when i run nutch from terminal it show<p> \nUsage: nutch [-core] COMMAND<br>\nwhere COMMAND is one of:<br>\n  crawl             one-step crawler for intranets\n<br>\netc etc.....</p>\n\n<p>please tell me what to do</p>\n\n<p>Hey Tejasp i did what u told me, i changed the NUTCH_HOME=/nutch/runtime/local/bin also the crawl.java file is there but when i did this</p>\n\n<pre><code> npun@nipun:~$ nutch crawl urls -dir crawl -depth 2 -topN 10\n [Fatal Error] nutch-site.xml:6:6: The processing instruction target matching \"[xX]           [mM][lL]\" is not allowed.\n Exception in thread \"main\" java.lang.RuntimeException: org.xml.sax.SAXParseException:     The processing instruction target matching \"[xX][mM][lL]\" is not allowed.\n    at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:1168)\nat org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:1040)\nat org.apache.hadoop.conf.Configuration.getProps(Configuration.java:980)\nat org.apache.hadoop.conf.Configuration.set(Configuration.java:405)\nat org.apache.hadoop.conf.Configuration.setBoolean(Configuration.java:585)\nat org.apache.hadoop.util.GenericOptionsParser.processGeneralOptions(GenericOptionsParser.java:290)\nat org.apache.hadoop.util.GenericOptionsParser.parseGeneralOptions(GenericOptionsParser.java:375)\nat org.apache.hadoop.util.GenericOptionsParser.&lt;init&gt;(GenericOptionsParser.java:153)\nat org.apache.hadoop.util.GenericOptionsParser.&lt;init&gt;(GenericOptionsParser.java:138)\nat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:59)\nat org.apache.nutch.crawl.Crawl.main(Crawl.java:55)\n    Caused by: org.xml.sax.SAXParseException: The processing instruction target matching \"[xX][mM][lL]\" is not allowed.\nat org.apache.xerces.parsers.DOMParser.parse(Unknown Source)\nat org.apache.xerces.jaxp.DocumentBuilderImpl.parse(Unknown Source)\nat javax.xml.parsers.DocumentBuilder.parse(DocumentBuilder.java:180)\nat org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:1079)\n... 10 more\n</code></pre>\n\n<p>it showed me this result now what...?</p>\n\n<p>also i checked nutch-site.xml file i have done the following edits in it</p>\n\n<pre><code> &lt;configuration&gt;\n &lt;property&gt;\n &lt;name&gt;http.agent.name&lt;/name&gt;\n &lt;value&gt;PARAM_TEST&lt;/value&gt;&lt;!-- Your crawler name here --&gt;\n &lt;/property&gt;\n &lt;/configuration&gt; \n</code></pre>\n\n<p>Sir, i did as you told me, this time i compiled nutch with 'ant clean runtime' and nutch home is</p>\n\n<pre><code>    NUTCH_HOME=/nutch/runtime/deploy/bin\n\n    NUTCH_CONF_DIR=/nutch/runtime/local/conf\n</code></pre>\n\n<p>and now when i run the same command it is giving me this error</p>\n\n<pre><code>  npun@nipun:~$ nutch crawl urls -dir crawl -depth 2 -topN 10\n  Can't find Hadoop executable. Add HADOOP_HOME/bin to the path or run in local mode.\n</code></pre>\n\n<p>All i want to create a search engine which can search certain thing from certain websites, for my final year project.... </p>\n", "creation_date": 1335092529, "score": 0},
{"title": "Step by step running apache Nutch 2.2.1", "view_count": 1633, "is_answered": false, "answers": [{"question_id": 17499100, "owner": {"user_id": 62576, "accept_rate": 100, "link": "http://stackoverflow.com/users/62576/ken-white", "user_type": "registered", "reputation": 94719}, "body": "<p>The error message clearly indicates the problem (and where to look for a solution):</p>\n\n<pre><code>SLF4J: Class path contains multiple SLF4J bindings.\nSLF4J: Found binding in [jar:file:/Users/hung/.m2/repository/org/slf4j/slf4j-log4j12/1.6.1/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]\nSLF4J: Found binding in [jar:file:/Users/hung/.m2/repository/org/slf4j/slf4j-jdk14/1.6.1/slf4j-jdk14-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]\nSLF4J: Found binding in [jar:file:/Users/hung/.m2/repository/org/slf4j/slf4j-simple/1.6.1/slf4j-simple-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]\nSLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n</code></pre>\n", "creation_date": 1373079030, "is_accepted": false, "score": 0, "last_activity_date": 1373079030, "answer_id": 17499199}], "question_id": 17499100, "tags": ["nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/17499100/step-by-step-running-apache-nutch-2-2-1", "last_activity_date": 1373092817, "owner": {"user_id": 953261, "view_count": 1, "answer_count": 1, "creation_date": 1316455277, "reputation": 11}, "body": "<p>I have config plugin.folders in nutch-default.xml but when I run Nutch via Eclipse &amp; Netbeans,</p>\n\n<pre><code>Main class: org.apache.nutch.crawl.InjectorJob\nArguments: /MY_DATA_SOURCE/HR_PROJECTS/JSearch/Apache_Nutch/RELEASE/release-2.2.1/urls\nVM Options: -Dhadoop.log.dir=logs -Dhadoop.log.file=hadoop.log \n</code></pre>\n\n<p>THe errors like below:</p>\n\n<pre><code>cd /MY_DATA_SOURCE/HR_PROJECTS/JSearch/Apache_Nutch/RELEASE/release-2.2.1; JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk1.7.0_25.jdk/Contents/Home \"/Applications/NetBeans/NetBeans 7.3.app/Contents/Resources/NetBeans/java/maven/bin/mvn\" \"-Dexec.args=-Dhadoop.log.dir=logs -Dhadoop.log.file=hadoop.log -classpath %classpath org.apache.nutch.crawl.InjectorJob /MY_DATA_SOURCE/HR_PROJECTS/JSearch/Apache_Nutch/RELEASE/release-2.2.1/urls\" -Dexec.executable=/Library/Java/JavaVirtualMachines/jdk1.7.0_25.jdk/Contents/Home/bin/java process-classes org.codehaus.mojo:exec-maven-plugin:1.2.1:exec\nScanning for projects...\n\n------------------------------------------------------------------------\nBuilding Apache Nutch 2.2.1\n------------------------------------------------------------------------\n\n[resources:resources]\n[debug] execute contextualize\nUsing platform encoding (US-ASCII actually) to copy filtered resources, i.e. build is platform dependent!\nskip non existing resourceDirectory /MY_DATA_SOURCE/HR_PROJECTS/JSearch/Apache_Nutch/RELEASE/release-2.2.1/src/main/resources\n\n[compiler:compile]\nNothing to compile - all classes are up to date\n\n[exec:exec]\nSLF4J: Class path contains multiple SLF4J bindings.\nSLF4J: Found binding in [jar:file:/Users/hung/.m2/repository/org/slf4j/slf4j-log4j12/1.6.1/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]\nSLF4J: Found binding in [jar:file:/Users/hung/.m2/repository/org/slf4j/slf4j-jdk14/1.6.1/slf4j-jdk14-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]\nSLF4J: Found binding in [jar:file:/Users/hung/.m2/repository/org/slf4j/slf4j-simple/1.6.1/slf4j-simple-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]\nSLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n13/07/06 08:55:18 INFO crawl.InjectorJob: InjectorJob: starting at 2013-07-06 08:55:18\n13/07/06 08:55:18 INFO crawl.InjectorJob: InjectorJob: Injecting urlDir: /MY_DATA_SOURCE/HR_PROJECTS/JSearch/Apache_Nutch/RELEASE/release-2.2.1/urls\n2013-07-06 08:55:18.420 java[1206:1c03] Unable to load realm info from SCDynamicStore\n13/07/06 08:55:18 WARN store.DataStoreFactory: gora.properties not found, properties will be empty.\n13/07/06 08:55:18 WARN store.DataStoreFactory: gora.properties not found, properties will be empty.\n13/07/06 08:55:19 INFO crawl.InjectorJob: InjectorJob: Using class org.apache.gora.sql.store.SqlStore as the Gora storage class.\n13/07/06 08:55:19 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n13/07/06 08:55:19 WARN mapred.JobClient: No job jar file set.  User classes may not be found. See JobConf(Class) or JobConf#setJar(String).\n13/07/06 08:55:19 INFO input.FileInputFormat: Total input paths to process : 1\n13/07/06 08:55:19 WARN snappy.LoadSnappy: Snappy native library not loaded\n13/07/06 08:55:19 INFO mapred.JobClient: Running job: job_local226390157_0001\n13/07/06 08:55:19 INFO mapred.LocalJobRunner: Waiting for map tasks\n13/07/06 08:55:19 INFO mapred.LocalJobRunner: Starting task: attempt_local226390157_0001_m_000000_0\n13/07/06 08:55:19 INFO mapred.Task:  Using ResourceCalculatorPlugin : null\n13/07/06 08:55:19 INFO mapred.MapTask: Processing split: file:/MY_DATA_SOURCE/HR_PROJECTS/JSearch/Apache_Nutch/RELEASE/release-2.2.1/urls/seed.txt:0+20\n13/07/06 08:55:19 WARN store.DataStoreFactory: gora.properties not found, properties will be empty.\n13/07/06 08:55:19 INFO mapreduce.GoraRecordWriter: gora.buffer.write.limit = 10000\n13/07/06 08:55:19 INFO mapred.LocalJobRunner: Map task executor complete.\n13/07/06 08:55:19 WARN mapred.FileOutputCommitter: Output path is null in cleanup\n13/07/06 08:55:19 WARN mapred.LocalJobRunner: job_local226390157_0001\njava.lang.Exception: java.lang.IllegalArgumentException: plugin.folders is not defined\n    at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:354)\nCaused by: java.lang.IllegalArgumentException: plugin.folders is not defined\n    at org.apache.nutch.plugin.PluginManifestParser.parsePluginFolder(PluginManifestParser.java:78)\n    at org.apache.nutch.plugin.PluginRepository.&lt;init&gt;(PluginRepository.java:69)\n    at org.apache.nutch.plugin.PluginRepository.get(PluginRepository.java:97)\n    at org.apache.nutch.net.URLNormalizers.&lt;init&gt;(URLNormalizers.java:117)\n    at org.apache.nutch.crawl.InjectorJob$UrlMapper.setup(InjectorJob.java:99)\n    at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:142)\n    at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:764)\n    at org.apache.hadoop.mapred.MapTask.run(MapTask.java:364)\n    at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:223)\n    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)\n    at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)\n    at java.util.concurrent.FutureTask.run(FutureTask.java:166)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n    at java.lang.Thread.run(Thread.java:724)\n13/07/06 08:55:20 INFO mapred.JobClient:  map 0% reduce 0%\n13/07/06 08:55:20 INFO mapred.JobClient: Job complete: job_local226390157_0001\n13/07/06 08:55:20 INFO mapred.JobClient: Counters: 0\n13/07/06 08:55:20 ERROR crawl.InjectorJob: InjectorJob: java.lang.RuntimeException: job failed: name=inject /MY_DATA_SOURCE/HR_PROJECTS/JSearch/Apache_Nutch/RELEASE/release-2.2.1/urls, jobid=job_local226390157_0001\n    at org.apache.nutch.util.NutchJob.waitForCompletion(NutchJob.java:54)\n    at org.apache.nutch.crawl.InjectorJob.run(InjectorJob.java:233)\n    at org.apache.nutch.crawl.InjectorJob.inject(InjectorJob.java:251)\n    at org.apache.nutch.crawl.InjectorJob.run(InjectorJob.java:273)\n    at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n    at org.apache.nutch.crawl.InjectorJob.main(InjectorJob.java:282)\n\n------------------------------------------------------------------------\nBUILD FAILURE\n------------------------------------------------------------------------\nTotal time: 6.572s\nFinished at: Sat Jul 06 08:55:20 ICT 2013\nFinal Memory: 11M/236M\n------------------------------------------------------------------------\nFailed to execute goal org.codehaus.mojo:exec-maven-plugin:1.2.1:exec (default-cli) on project nutch: Command execution failed. Process exited with an error: 255 (Exit value: 255) -&gt; [Help 1]\n\nTo see the full stack trace of the errors, re-run Maven with the -e switch.\nRe-run Maven using the -X switch to enable full debug logging.\n\nFor more information about the errors and possible solutions, please read the following articles:\n[Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException\n</code></pre>\n", "creation_date": 1373077911, "score": 2},
{"title": "How to limit nutch to fetch only N pages from each host?", "view_count": 260, "is_answered": true, "answers": [{"question_id": 17480277, "owner": {"user_id": 313885, "accept_rate": 53, "link": "http://stackoverflow.com/users/313885/dennis-yurichev", "user_type": "registered", "reputation": 860}, "body": "<p>I've found: option called <em>generate.max.count</em>.</p>\n", "creation_date": 1373018260, "is_accepted": false, "score": 1, "last_activity_date": 1373018260, "answer_id": 17486064}], "question_id": 17480277, "tags": ["nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/17480277/how-to-limit-nutch-to-fetch-only-n-pages-from-each-host", "last_activity_date": 1373018260, "owner": {"user_id": 313885, "answer_count": 20, "creation_date": 1270986717, "accept_rate": 53, "view_count": 431, "location": "Kiev, Ukraine", "reputation": 860}, "body": "<p>How to limit nutch 2.x to fetch only N (5-10) pages from each host or domain? I fail to figure it out from config files.</p>\n", "creation_date": 1372991378, "score": 0},
{"title": "Nutch crawling external links from a web page", "view_count": 212, "is_answered": false, "question_id": 17471963, "tags": ["nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/17471963/nutch-crawling-external-links-from-a-web-page", "last_activity_date": 1372945603, "owner": {"user_id": 1474659, "answer_count": 6, "creation_date": 1340364399, "accept_rate": 48, "view_count": 118, "reputation": 321}, "body": "<p>I am using Apache Nutch for crawling websites. Nutch is not crawling links for external websites.</p>\n\n<p>I have gone through this link <a href=\"http://stackoverflow.com/questions/4019115/how-do-you-crawl-external-links-on-a-found-page\">How do you crawl external links on a found page?</a> but it did not produce intended result.</p>\n", "creation_date": 1372945603, "score": 0},
{"title": "Using Solr for indexing HTML tags with attributes", "view_count": 1265, "is_answered": true, "answers": [{"last_edit_date": 1372332355, "owner": {"user_id": 1373711, "accept_rate": 85, "link": "http://stackoverflow.com/users/1373711/jhs", "user_type": "registered", "reputation": 5236}, "body": "<p>You could use the <code>HTMLStripCharFilterFactory</code> in your analyzer before tokenizing.</p>\n\n<p>This filter <code>strips HTML from the input stream</code>. For more info have a look <a href=\"http://wiki.apache.org/solr/AnalyzersTokenizersTokenFilters#solr.HTMLStripCharFilterFactory\" rel=\"nofollow\">here</a></p>\n", "question_id": 17341072, "creation_date": 1372331985, "is_accepted": false, "score": 1, "last_activity_date": 1372332355, "answer_id": 17341742}, {"question_id": 17341072, "owner": {"user_id": 81194, "link": "http://stackoverflow.com/users/81194/claude", "user_type": "registered", "reputation": 827}, "body": "<p>You can implement a Nutch filter (I like <a href=\"http://jericho.htmlparser.net/docs/index.html\" rel=\"nofollow\">Jericho HTML Parser</a>) to extract only the parts of the page you need to index using DOM manipulation. You can use the <a href=\"http://jericho.htmlparser.net/docs/javadoc/net/htmlparser/jericho/TextExtractor.html\" rel=\"nofollow\">TextExtractor</a> class to grab clean text (sans HTML tags) to be used in your index. I usually save that data in custom fields.</p>\n", "creation_date": 1372460585, "is_accepted": false, "score": 1, "last_activity_date": 1372460585, "answer_id": 17374899}], "question_id": 17341072, "tags": ["solr", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/17341072/using-solr-for-indexing-html-tags-with-attributes", "last_activity_date": 1372460585, "owner": {"user_id": 1474659, "answer_count": 6, "creation_date": 1340364399, "accept_rate": 48, "view_count": 118, "reputation": 321}, "body": "<p>I have crawled websites using Nutch and I have pushed crawled data to solr. Now I want to search content between specific tag with specific attribute value. For example,</p>\n\n<pre><code> &lt;h&gt;&lt;title&gt; title to search &lt;/title&gt;&lt;/h&gt;\n &lt;div id=\"abc\"&gt;\n     content to search\n &lt;/div&gt;\n &lt;div class=\"efg\"&gt;\n     other content to search\n &lt;/div&gt;\n</code></pre>\n\n<p>I have seen this question(<a href=\"http://stackoverflow.com/questions/12338967/how-to-parse-html-with-nutch-and-index-specific-tag-to-solr\">how to parse html with nutch and index specific tag to solr?</a>) but this does not have enough clarity.</p>\n\n<p>I want to know that whether there is any plugin available or i need to write a customized plugin altogether. If i have to write a plugin, i just need few directions for handling html tags and attributes. </p>\n", "creation_date": 1372329984, "score": 1},
{"title": "How to add dependency lib to nutch project", "view_count": 953, "owner": {"age": 27, "answer_count": 19, "creation_date": 1331644819, "user_id": 1266556, "accept_rate": 75, "view_count": 41, "location": "Shanghai, China", "reputation": 362}, "is_answered": true, "answers": [{"question_id": 10386598, "owner": {"user_id": 1150329, "accept_rate": 89, "link": "http://stackoverflow.com/users/1150329/tejas-patil", "user_type": "registered", "reputation": 4793}, "body": "<p>Add the hbase jars to buildpath in your eclipse project and ensure that the jar is copied in the lib folder inside nutch.</p>\n", "creation_date": 1335871864, "is_accepted": false, "score": 0, "last_activity_date": 1335871864, "answer_id": 10397047}, {"last_edit_date": 1335935507, "owner": {"user_id": 1266556, "accept_rate": 75, "link": "http://stackoverflow.com/users/1266556/freedom", "user_type": "registered", "reputation": 362}, "body": "<p>Eventually, I made it, by adding a dependency declaration in ivy.xml file. And the library description can be fond in <a href=\"http://search.maven.org/\" rel=\"nofollow\">http://search.maven.org/</a>. Then just run ant in command shell.(for my case, running ant in Eclipse does not work. I don't know why.)<br/>\nThe question may be trivial, but I hope this helpful for the beginners like me.</p>\n", "question_id": 10386598, "creation_date": 1335889141, "is_accepted": true, "score": 0, "last_activity_date": 1335935507, "answer_id": 10400711}, {"question_id": 10386598, "owner": {"user_id": 2212239, "link": "http://stackoverflow.com/users/2212239/user2212239", "user_type": "registered", "reputation": 1}, "body": "<p>Let me give this in detail:</p>\n\n<p>Add the following line to ivysettings.xml of Nutch:</p>\n\n<pre><code>&lt;module organisation=\"org.apache.giraph\" name=\".*\" resolver=\"internal\"/&gt;\n</code></pre>\n\n<p>Add the following lines to ivy.xml of Nutch:</p>\n\n<pre><code>    &lt;dependency org=\"org.apache.giraph\" name=\"giraph\" rev=\"1.1.0-SNAPSHOT\"\n      conf=\"*-&gt;default\" /&gt;\n    &lt;dependency org=\"org.apache.giraph\" name=\"giraph-hbase\" rev=\"1.1.0-SNAPSHOT\"\n      conf=\"*-&gt;default\" /&gt;\n    &lt;dependency org=\"org.apache.giraph\" name=\"giraph-examples\" rev=\"1.1.0-SNAPSHOT\"\n      conf=\"*-&gt;default\" /&gt;\n</code></pre>\n\n<p>Now create put jar files under .ivy2 subfolders:</p>\n\n<pre><code>/home/emre/.ivy2/local/org.apache.giraph/giraph/1.1.0-SNAPSHOT/jars/giraph.jar\n/home/emre/.ivy2/local/org.apache.giraph/giraph-hbase/1.1.0-SNAPSHOT/jars/giraph-hbase.jar\n/home/emre/.ivy2/local/org.apache.giraph/giraph-examples/1.1.0-SNAPSHOT/jars/giraph-examples.jar  \n</code></pre>\n", "creation_date": 1372426545, "is_accepted": false, "score": 0, "last_activity_date": 1372426545, "answer_id": 17365971}], "question_id": 10386598, "tags": ["maven", "ivy", "nutch"], "answer_count": 3, "link": "http://stackoverflow.com/questions/10386598/how-to-add-dependency-lib-to-nutch-project", "last_activity_date": 1372426545, "accepted_answer_id": 10400711, "body": "<p>Recently I want to modify nutch's code(Fetcher.java) to save the raw html page into Hbase. I have import the project into Eclipse. But I'm not familiar with ant,ivy,maven or buildfile. How do I add the Hbase library jar file into the project so that I can use ant to build it? Thanks.  </p>\n", "creation_date": 1335801055, "score": 0},
{"title": "Running Apache Nutch on windows", "view_count": 1209, "is_answered": false, "answers": [{"question_id": 11081852, "owner": {"user_id": 1204751, "accept_rate": 76, "link": "http://stackoverflow.com/users/1204751/crs", "user_type": "registered", "reputation": 283}, "body": "<p>It is better to run Nutch on Unix. But if you want to run it on windows then probably you can download 1.2 version of Nutch which comes with Hadoop version which does not have this issue.</p>\n", "creation_date": 1340085860, "is_accepted": false, "score": 0, "last_activity_date": 1340085860, "answer_id": 11095264}], "question_id": 11081852, "tags": ["solr", "web-crawler", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/11081852/running-apache-nutch-on-windows", "last_activity_date": 1372420150, "owner": {"user_id": 1463504, "view_count": 0, "answer_count": 0, "creation_date": 1340018587, "reputation": 6}, "body": "<p>I am trying to run Apache Nutch on Windows for web crawling.I have installed cygwin and set its Path .But I am getting the following exception :</p>\n\n<pre><code>Exception in thread \"main\" java.io.IOException: Failed to set permissions of path:    \\tmp\\hadoop-cjindal\\mapred\\staging\\cjindal-330065706\\.staging to 0700\nat org.apache.hadoop.fs.FileUtil.checkReturnValue(FileUtil.java:682)\nat org.apache.hadoop.fs.FileUtil.setPermission(FileUtil.java:655)\nat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:509)\nat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:344)\nat org.apache.hadoop.fs.FilterFileSystem.mkdirs(FilterFileSystem.java:189)\nat org.apache.hadoop.mapreduce.JobSubmissionFiles.getStagingDir(JobSubmissionFiles.java:116)\nat org.apache.hadoop.mapred.JobClient$2.run(JobClient.java:856)\nat org.apache.hadoop.mapred.JobClient$2.run(JobClient.java:850)\nat java.security.AccessController.doPrivileged(Native Method)\nat javax.security.auth.Subject.doAs(Subject.java:396)\nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1083)\nat org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:850)\nat org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:824)\nat org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:1261)\nat org.apache.nutch.crawl.Injector.inject(Injector.java:217)\nat org.apache.nutch.crawl.Crawl.run(Crawl.java:127)\nat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\nat org.apache.nutch.crawl.Crawl.main(Crawl.java:55)\n</code></pre>\n\n<p>I have not installed hadoop. Please help.</p>\n", "creation_date": 1340018929, "score": 1},
{"title": "Integrating Apache Nutch with MySQL on Windows", "view_count": 1087, "is_answered": true, "answers": [{"question_id": 17123095, "owner": {"user_id": 463911, "link": "http://stackoverflow.com/users/463911/robert", "user_type": "registered", "reputation": 568}, "body": "<p>I have using Nutch2 on both windows and Linux. Just to run it on Windows you need this Haddop 1.0.3 patch installed: <a href=\"https://github.com/congainc/patch-hadoop_7682-1.0.x-win\" rel=\"nofollow\">https://github.com/congainc/patch-hadoop_7682-1.0.x-win</a>. </p>\n", "creation_date": 1372420066, "is_accepted": false, "score": 1, "last_activity_date": 1372420066, "answer_id": 17364011}], "question_id": 17123095, "tags": ["java", "mysql", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/17123095/integrating-apache-nutch-with-mysql-on-windows", "last_activity_date": 1372420066, "owner": {"user_id": 1474659, "answer_count": 6, "creation_date": 1340364399, "accept_rate": 48, "view_count": 118, "reputation": 321}, "body": "<p>I am trying to integrate Apache Nutch 2.1 with Mysql server on Windows 8 platform. I am following tutorial <a href=\"http://nlp.solutions.asia/?p=180\" rel=\"nofollow\">http://nlp.solutions.asia/?p=180</a>. I have made following changes to the apache-nutch-2.1.</p>\n\n<ol>\n<li>I downloaded apache-nutch-2.1-src.zip and extracted.</li>\n<li><p>Uncommented following in ivy/ivy.xml</p>\n\n<pre><code> &lt;dependency org=\"mysql\" name=\"mysql-connector-java\" rev=\"5.1.18\" conf=\"*-&gt;default\"/&gt;\n</code></pre></li>\n<li><p>commented sql properties for and added gora properties for mysql conf/gora.properties.</p>\n\n<pre><code>gora.sqlstore.jdbc.driver=com.mysql.jdbc.Driver\ngora.sqlstore.jdbc.url=jdbc:mysql://localhost:3306/nutch?\n                                 createDatabaseIfNotExist=true\ngora.sqlstore.jdbc.user=root\ngora.sqlstore.jdbc.password=root \n</code></pre></li>\n<li>Added properties to conf/nutch-site.xml</li>\n<li>executed ant runtime command from command prompt. It created /runtime directory.</li>\n<li>Added seeds.txt file inside /runtime/local/urls directory with www.apache.nutch.org value.</li>\n<li>added +^http://([a-z0-9]*.)*nutch.org/  to both domain-urlfilter.txt and regex-urlfilter.txt files inside /runtime/local/conf directory.</li>\n</ol>\n\n<p>When I am running command for start crawling through cygwin terminal..following exception is occurring,</p>\n\n<pre><code>   Exception in thread \"main\" java.io.IOException: Failed to set permissions of path: \\tmp\\hadoop-Abhijeet\\mapred\\staging\\Abhijeet530509219\\.staging to 0700\n    at org.apache.hadoop.fs.FileUtil.checkReturnValue(FileUtil.java:689)\n    at org.apache.hadoop.fs.FileUtil.setPermission(FileUtil.java:662)\n    at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:509)\n    at org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:344)\n    at org.apache.hadoop.fs.FilterFileSystem.mkdirs(FilterFileSystem.java:189)\n    at org.apache.hadoop.mapreduce.JobSubmissionFiles.getStagingDir(JobSubmissionFiles.java:116)\n    at org.apache.hadoop.mapred.JobClient$2.run(JobClient.java:856)\n    at org.apache.hadoop.mapred.JobClient$2.run(JobClient.java:850)\n    at java.security.AccessController.doPrivileged(Native Method)\n    at javax.security.auth.Subject.doAs(Subject.java:396)\n    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1121)\n    at org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:850)\n    at org.apache.hadoop.mapreduce.Job.submit(Job.java:500)\n    at org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:530)\n    at org.apache.nutch.util.NutchJob.waitForCompletion(NutchJob.java:50)\n    at org.apache.nutch.crawl.InjectorJob.run(InjectorJob.java:219)\n    at org.apache.nutch.crawl.Crawler.runTool(Crawler.java:68)\n    at org.apache.nutch.crawl.Crawler.run(Crawler.java:136)\n    at org.apache.nutch.crawl.Crawler.run(Crawler.java:250)\n    at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n    at org.apache.nutch.crawl.Crawler.main(Crawler.java:257)\n</code></pre>\n\n<p>I have searched over internet that Hadoop does not work with Windows which is alright as I my not using Hadoop for storing data. I am using Mysql.</p>\n\n<p>Can anybody suggest What am i doing wrong ?</p>\n", "creation_date": 1371295390, "score": 0},
{"title": "Modify crawled URL before indexing it", "view_count": 62, "owner": {"age": 27, "answer_count": 0, "creation_date": 1367929486, "user_id": 2358303, "view_count": 2, "location": "Mumbai, India", "reputation": 3}, "is_answered": true, "answers": [{"question_id": 17340912, "owner": {"user_id": 950178, "link": "http://stackoverflow.com/users/950178/jayendra", "user_type": "registered", "reputation": 34896}, "body": "<p>There is no out of the box way to modify the value fed to solr unless you write a custom plugin to do so.<br>\nHowever, this can be easily handled at client side before the results are displayed to the User.</p>\n", "creation_date": 1372414404, "is_accepted": true, "score": 0, "last_activity_date": 1372414404, "answer_id": 17362331}], "question_id": 17340912, "tags": ["apache", "solr", "lucene", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/17340912/modify-crawled-url-before-indexing-it", "last_activity_date": 1372414404, "accepted_answer_id": 17362331, "body": "<p>I am using nutch 1.4. I want to manipulate the crawled url before indexing it.</p>\n\n<p>For example, if my URL is <a href=\"http://xyz.com/home/xyz.aspx\" rel=\"nofollow\">http://xyz.com/home/xyz.aspx</a> then I want to modify the URL to <a href=\"http://xyz.com/index.aspx?role=xyz\" rel=\"nofollow\">http://xyz.com/index.aspx?role=xyz</a> and only the latter field should be indexed in SOLR. The reason is I don't want to expose the first URL. The 2nd URL will ultimately redirect it to same page.</p>\n\n<p>Do we have a provision in Nutch to manipulate the crawled URL's before indexing it to SOLR??</p>\n", "creation_date": 1372329485, "score": 0},
{"title": "Nutch In Eclipse", "view_count": 1235, "owner": {"user_id": 1162512, "answer_count": 27, "creation_date": 1327163131, "accept_rate": 58, "view_count": 772, "reputation": 2192}, "is_answered": true, "answers": [{"question_id": 11273683, "owner": {"user_id": 1270457, "link": "http://stackoverflow.com/users/1270457/amas", "user_type": "registered", "reputation": 544}, "body": "<p>if you are still working on it. This is a super annoying step which you have to add src/test and src/java folder of <strong>each</strong> individual plugin to your class path.</p>\n", "creation_date": 1342005075, "is_accepted": true, "score": 2, "last_activity_date": 1342005075, "answer_id": 11431381}, {"question_id": 11273683, "owner": {"user_id": 1150329, "accept_rate": 89, "link": "http://stackoverflow.com/users/1150329/tejas-patil", "user_type": "registered", "reputation": 4793}, "body": "<p>The updated steps are here: <a href=\"https://wiki.apache.org/nutch/RunNutchInEclipse\" rel=\"nofollow\">https://wiki.apache.org/nutch/RunNutchInEclipse</a>.\n Now, you wont have to manually add each src/test and src.java directory to classpath :)</p>\n", "creation_date": 1370954356, "is_accepted": false, "score": 1, "last_activity_date": 1370954356, "answer_id": 17044319}], "question_id": 11273683, "tags": ["eclipse", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/11273683/nutch-in-eclipse", "last_activity_date": 1372400846, "accepted_answer_id": 11431381, "body": "<p>While configuring nutch ,there's a step under section \"Establish the Eclipse environment for Nutch\" on <a href=\"http://wiki.apache.org/nutch/RunNutchInEclipse\" rel=\"nofollow\">http://wiki.apache.org/nutch/RunNutchInEclipse</a> which i am unable to understand.</p>\n\n<p>The step no. 3 says</p>\n\n<p>\"In additon, we must maunally add EVERY individual plugin src/java and src/test folder, although this takes some time it is absolutely essential that this is done.\"</p>\n\n<p>Which plugins is it talking of and how to add these plugins???</p>\n\n<p>Need help as i am trying to run nutch on eclipse from past 2 days and still not able to get hold of it..</p>\n\n<p>Thanks,</p>\n", "creation_date": 1341055357, "score": 0},
{"title": "indexing different record types in one single solr schema", "view_count": 318, "owner": {"user_id": 340648, "answer_count": 11, "creation_date": 1273781720, "accept_rate": 94, "view_count": 119, "location": "New York, NY, United States", "reputation": 695}, "is_answered": true, "answers": [{"question_id": 17351822, "owner": {"user_id": 1333610, "accept_rate": 90, "link": "http://stackoverflow.com/users/1333610/arun", "user_type": "registered", "reputation": 4680}, "body": "<p>See <a href=\"https://wiki.apache.org/solr/MultipleIndexes#Flattening_Data_Into_a_Single_Index\" rel=\"nofollow\">https://wiki.apache.org/solr/MultipleIndexes#Flattening_Data_Into_a_Single_Index</a> and  <a href=\"https://wiki.apache.org/solr/UniqueKey\" rel=\"nofollow\">https://wiki.apache.org/solr/UniqueKey</a></p>\n\n<p>Solr does not need a uniqueKey. If you do not specify a unique key, then you need to do the following - when you post a new doc that has the same key as an existing doc, the new doc will not replace the old one, so you will have to delete the old one first manually and then add the new one (and commit, of course).</p>\n\n<p>If you need a unique key, then append a prefix to the IDs which is based on the type. Then you can have two other fields like id and type. So, for example:</p>\n\n<pre><code>uniquekey: P1\nproduct_code: 1\ntype: product\n\nuniquekey: J1\njob_id: 1\ntype: job\n</code></pre>\n", "creation_date": 1372366581, "is_accepted": true, "score": 1, "last_activity_date": 1372366581, "answer_id": 17353021}], "question_id": 17351822, "tags": ["solr", "nutch", "unique-key"], "answer_count": 1, "link": "http://stackoverflow.com/questions/17351822/indexing-different-record-types-in-one-single-solr-schema", "last_activity_date": 1372366581, "accepted_answer_id": 17353021, "body": "<p>I am struggling with the overall view of how (whether possible) one might be able to index multiple different types of records in one single Solr core. <em>Multiple records meaning that they have different unique keys</em>. </p>\n\n<p>We are inclined to want to use a single core because we want to be able to, at certain levels, search everything all at once and not have to cobble cores together.</p>\n\n<p>So, for example, I have <strong>products</strong> that have the fields:</p>\n\n<pre><code>product_code &lt;--- unique key\nproduct_title \nproduct_description \netc...\n</code></pre>\n\n<p>then there are <strong>job listings</strong> that have the fields: <br/></p>\n\n<pre><code>job_id &lt;---- unique key\njob_description\njob_title\netc... \n</code></pre>\n\n<p>there are multiple other entities, including a <a href=\"http://nutch.apache.org/\" rel=\"nofollow\">Nutch</a> search index, which will have a unique id of 'id'</p>\n\n<p>is it possible to include in the schema.xml more than one unique key? so that id do not have to send each different kind of record to a different solr core?</p>\n\n<p>The main concern I have is that in identifying the <code>&lt;uniqueKey&gt;</code>s at least one of them has to be required, but not all records sent to the solr index will have the required key.</p>\n\n<p>Is there an accepted way to get around this problem in Solr?</p>\n", "creation_date": 1372361988, "score": 0},
{"title": "How to push Nutch clrawled data from Mysql to Solr for indexing", "view_count": 342, "is_answered": false, "question_id": 17314709, "tags": ["mysql", "solr", "nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/17314709/how-to-push-nutch-clrawled-data-from-mysql-to-solr-for-indexing", "last_activity_date": 1372233883, "owner": {"user_id": 1474659, "answer_count": 6, "creation_date": 1340364399, "accept_rate": 48, "view_count": 118, "reputation": 321}, "body": "<p>I have crawled website using Nutch and stored data into Mysql database following tutorial provided on the <a href=\"http://nlp.solutions.asia/?p=180\" rel=\"nofollow\">http://nlp.solutions.asia/?p=180</a>. Now I want to push crawled data to solr for indexing. I have executed following command from terminal  {APACHE_NUTCH_HOME}/runtime/local/, </p>\n\n<pre><code>  bin/nutch solrindex http://localhost:8983/solr/ -reindex\n</code></pre>\n\n<p>this process takes hardly 5 seconds to get completed without any error, although Mysql table has 111 rows.</p>\n\n<p>After this, i opened </p>\n\n<pre><code> http://localhost:8983/solr/#/collection1/query for executing queries. \n</code></pre>\n\n<p>Irrespective of the string inside \"q\" box, I am getting same response for every executed query which is following,</p>\n\n<pre><code>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;response&gt;\n  &lt;lst name=\"responseHeader\"&gt;\n     &lt;int name=\"status\"&gt;0&lt;/int&gt;\n     &lt;int name=\"QTime\"&gt;1&lt;/int&gt;\n     &lt;lst name=\"params\"&gt;\n         &lt;str name=\"indent\"&gt;true&lt;/str&gt;\n         &lt;str name=\"q\"&gt;content&lt;/str&gt;\n         &lt;str name=\"_\"&gt;1372233303649&lt;/str&gt;\n         &lt;str name=\"wt\"&gt;xml&lt;/str&gt;\n     &lt;/lst&gt;\n &lt;/lst&gt;\n &lt;result name=\"response\" numFound=\"0\" start=\"0\"&gt;\n &lt;/result&gt;\n&lt;/response&gt;\n</code></pre>\n\n<p>What is the possible reason behind this or what am i doing wrong?</p>\n", "creation_date": 1372233883, "score": 1},
{"title": "How to search for key words using Solr from crawled web pages by nutch?", "view_count": 429, "is_answered": false, "question_id": 17296720, "tags": ["solr", "nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/17296720/how-to-search-for-key-words-using-solr-from-crawled-web-pages-by-nutch", "last_activity_date": 1372161490, "owner": {"user_id": 1474659, "answer_count": 6, "creation_date": 1340364399, "accept_rate": 48, "view_count": 118, "reputation": 321}, "body": "<p>I have an application which crawls over the websites using Apache Nutch 2.1 and persisting data to the MySQL. I have to integrate Nutch and Solr which is not a problem as enough documentation is available on the internet.</p>\n\n<p>After storing content from webpages, i want to add a search functionality based on Solr. I need to search for key words in the webpages. For example, if i am crawling websites which are movies related and i want to search for any specific movie(as a key word) from the crawled data, what are the changes i need to make to the Solr configurations. Do i need to write a separate plugin altogether or i can use existing plugins?What type of indexing i have to add to the solr configurations?</p>\n", "creation_date": 1372161490, "score": 1},
{"title": "Nutch segments folder grows every day", "view_count": 1500, "owner": {"age": 46, "answer_count": 107, "creation_date": 1309523111, "user_id": 824846, "accept_rate": 82, "view_count": 176, "location": "London", "reputation": 2015}, "is_answered": true, "answers": [{"question_id": 17238813, "owner": {"user_id": 824846, "accept_rate": 82, "link": "http://stackoverflow.com/users/824846/marco-altieri", "user_type": "registered", "reputation": 2015}, "body": "<p>It is necessary to delete from the nutchdb the segments that are older than the db.default.fetch.interval. This interval defines when a page should be refetched. </p>\n\n<p>If the page has been refetched, the old segments can be deleted.</p>\n\n<p>If the segments are not deleted the step solrindexer has to read too many segments and becomes very slow (in my case one hour instead of 4 minutes).</p>\n\n<p>Regards,\nMarco</p>\n", "creation_date": 1372160265, "is_accepted": true, "score": 2, "last_activity_date": 1372160265, "answer_id": 17296259}], "question_id": 17238813, "tags": ["performance", "solr", "nutch", "segments"], "answer_count": 1, "link": "http://stackoverflow.com/questions/17238813/nutch-segments-folder-grows-every-day", "last_activity_date": 1372160265, "accepted_answer_id": 17296259, "body": "<p>I have configured nutch/solr 1.6 to crawl/index every 12 hours an intranet with about 4000 documents and html pages.</p>\n\n<p>If I execute the crawler with an empty database the process takes about 30 minutes.\nWhen the crawling is executed for several days, it becomes very slow.\nLooking the log file it seems that this night the last step (SolrIndexer) started after 1 hour and 20 minutes and it took a bit more than 1 hour.</p>\n\n<p>Because the number of documents indexed doesn't grow, I'm wondering why it is so slow now.</p>\n\n<p>Nutch is executed with the following command:</p>\n\n<pre><code>bin/nutch crawl -urlDir urls -solr http://localhost:8983/solr -dir nutchdb -depth 15 -topN 3000\n</code></pre>\n\n<p>The nutch-site.xml contains:</p>\n\n<pre><code>&lt;?xml version=\"1.0\"?&gt;\n&lt;?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?&gt;\n\n&lt;!-- Put site-specific property overrides in this file. --&gt;\n\n&lt;configuration&gt;\n    &lt;property&gt;\n        &lt;name&gt;http.agent.name&lt;/name&gt;\n        &lt;value&gt;Internet Site Agent&lt;/value&gt;\n    &lt;/property&gt;\n    &lt;property&gt;\n        &lt;name&gt;plugin.includes&lt;/name&gt;\n        &lt;value&gt;protocol-http|urlfilter-regex|parse-(tika|metatags)|index-(basic|anchor|metadata|more|http-header)|scoring-opic|urlnormalizer-(pass|regex|basic)&lt;/value&gt;\n    &lt;/property&gt;\n    &lt;!-- Used only if plugin parse-metatags is enabled. --&gt;\n    &lt;property&gt;\n        &lt;name&gt;metatags.names&lt;/name&gt;\n        &lt;value&gt;description;keywords;published;modified&lt;/value&gt;\n        &lt;description&gt; Names of the metatags to extract, separated by;.\n            Use '*' to extract all metatags. Prefixes the names with 'metatag.'\n            in the parse-metadata. For instance to index description and keywords,\n            you need to activate the plugin index-metadata and set the value of the\n            parameter 'index.parse.md' to 'metatag.description;metatag.keywords'.\n        &lt;/description&gt;\n    &lt;/property&gt;\n    &lt;property&gt;\n        &lt;name&gt;index.parse.md&lt;/name&gt;\n        &lt;value&gt;metatag.description,metatag.keywords,metatag.published,metatag.modified&lt;/value&gt;\n        &lt;description&gt; Comma-separated list of keys to be taken from the parse metadata to generate fields.\n            Can be used e.g. for 'description' or 'keywords' provided that these values are generated\n            by a parser (see parse-metatags plugin)\n        &lt;/description&gt;\n    &lt;/property&gt;       \n    &lt;property&gt;\n    &lt;name&gt;db.ignore.external.links&lt;/name&gt;\n    &lt;value&gt;true&lt;/value&gt;\n    &lt;description&gt;Set this to false if you start crawling your website from\n       for example http://www.example.com but you would like to crawl\n       xyz.example.com. Set it to true otherwise if you want to exclude external links\n    &lt;/description&gt;\n    &lt;/property&gt;\n    &lt;property&gt;\n        &lt;name&gt;http.content.limit&lt;/name&gt;\n        &lt;value&gt;10000000&lt;/value&gt;\n        &lt;description&gt;The length limit for downloaded content using the http\n            protocol, in bytes. If this value is nonnegative (&gt;=0), content longer\n            than it will be truncated; otherwise, no truncation at all. Do not\n            confuse this setting with the file.content.limit setting.\n        &lt;/description&gt;\n    &lt;/property&gt; \n\n    &lt;property&gt;\n        &lt;name&gt;fetcher.max.crawl.delay&lt;/name&gt;\n        &lt;value&gt;1&lt;/value&gt;\n        &lt;description&gt;\n            If the Crawl-Delay in robots.txt is set to greater than this value (in\n            seconds) then the fetcher will skip this page, generating an error report.\n            If set to -1 the fetcher will never skip such pages and will wait the\n            amount of time retrieved from robots.txt Crawl-Delay, however long that\n            might be.\n        &lt;/description&gt;\n    &lt;/property&gt;\n\n    &lt;property&gt;\n        &lt;name&gt;fetcher.threads.fetch&lt;/name&gt;\n        &lt;value&gt;10&lt;/value&gt;\n        &lt;description&gt;The number of FetcherThreads the fetcher should use.\n        This is also determines the maximum number of requests that are\n        made at once (each FetcherThread handles one connection). The total\n        number of threads running in distributed mode will be the number of\n        fetcher threads * number of nodes as fetcher has one map task per node.\n        &lt;/description&gt;\n    &lt;/property&gt;\n\n    &lt;property&gt;\n        &lt;name&gt;fetcher.threads.fetch&lt;/name&gt;\n        &lt;value&gt;10&lt;/value&gt;\n        &lt;description&gt;The number of FetcherThreads the fetcher should use.\n            This is also determines the maximum number of requests that are\n            made at once (each FetcherThread handles one connection). The total\n            number of threads running in distributed mode will be the number of\n            fetcher threads * number of nodes as fetcher has one map task per node.\n        &lt;/description&gt;\n    &lt;/property&gt;\n\n    &lt;property&gt;\n        &lt;name&gt;fetcher.server.delay&lt;/name&gt;\n        &lt;value&gt;1.0&lt;/value&gt;\n        &lt;description&gt;The number of seconds the fetcher will delay between\n            successive requests to the same server.&lt;/description&gt;\n    &lt;/property&gt;\n\n    &lt;property&gt;\n        &lt;name&gt;http.redirect.max&lt;/name&gt;\n        &lt;value&gt;0&lt;/value&gt;\n        &lt;description&gt;The maximum number of redirects the fetcher will follow when\n            trying to fetch a page. If set to negative or 0, fetcher won't immediately\n            follow redirected URLs, instead it will record them for later fetching.\n        &lt;/description&gt;\n    &lt;/property&gt;\n\n    &lt;property&gt;\n        &lt;name&gt;fetcher.threads.per.queue&lt;/name&gt;\n        &lt;value&gt;2&lt;/value&gt;\n        &lt;description&gt;This number is the maximum number of threads that\n           should be allowed to access a queue at one time. Replaces\n           deprecated parameter 'fetcher.threads.per.host'.\n        &lt;/description&gt;\n    &lt;/property&gt;\n\n    &lt;property&gt;\n        &lt;name&gt;link.delete.gone&lt;/name&gt;\n        &lt;value&gt;true&lt;/value&gt;\n        &lt;description&gt;Whether to delete gone pages from the web graph.&lt;/description&gt;\n   &lt;/property&gt;\n\n   &lt;property&gt;\n       &lt;name&gt;link.loops.depth&lt;/name&gt;\n       &lt;value&gt;20&lt;/value&gt;\n       &lt;description&gt;The depth for the loops algorithm.&lt;/description&gt;\n   &lt;/property&gt;\n\n&lt;!-- moreindexingfilter plugin properties --&gt;\n\n    &lt;property&gt;\n      &lt;name&gt;moreIndexingFilter.indexMimeTypeParts&lt;/name&gt;\n      &lt;value&gt;false&lt;/value&gt;\n      &lt;description&gt;Determines whether the index-more plugin will split the mime-type\n      in sub parts, this requires the type field to be multi valued. Set to true for backward\n      compatibility. False will not split the mime-type.\n      &lt;/description&gt;\n    &lt;/property&gt;\n\n    &lt;property&gt;\n      &lt;name&gt;moreIndexingFilter.mapMimeTypes&lt;/name&gt;\n      &lt;value&gt;false&lt;/value&gt;\n      &lt;description&gt;Determines whether MIME-type mapping is enabled. It takes a\n      plain text file with mapped MIME-types. With it the user can map both\n      application/xhtml+xml and text/html to the same target MIME-type so it\n      can be treated equally in an index. See conf/contenttype-mapping.txt.\n      &lt;/description&gt;\n    &lt;/property&gt;\n\n    &lt;!-- Fetch Schedule Configuration --&gt; \n    &lt;property&gt;\n      &lt;name&gt;db.fetch.interval.default&lt;/name&gt;\n              &lt;!-- for now always re-fetch everything --&gt;\n      &lt;value&gt;10&lt;/value&gt;\n      &lt;description&gt;The default number of seconds between re-fetches of a page (less than 1 day).\n      &lt;/description&gt;\n    &lt;/property&gt;\n\n    &lt;property&gt;\n      &lt;name&gt;db.fetch.interval.max&lt;/name&gt;\n              &lt;!-- for now always re-fetch everything --&gt;\n      &lt;value&gt;10&lt;/value&gt;\n      &lt;description&gt;The maximum number of seconds between re-fetches of a page\n      (less than one day). After this period every page in the db will be re-tried, no\n       matter what is its status.\n      &lt;/description&gt;\n    &lt;/property&gt;\n\n    &lt;!--property&gt;\n      &lt;name&gt;db.fetch.schedule.class&lt;/name&gt;\n      &lt;value&gt;org.apache.nutch.crawl.AdaptiveFetchSchedule&lt;/value&gt;\n      &lt;description&gt;The implementation of fetch schedule. DefaultFetchSchedule simply\n      adds the original fetchInterval to the last fetch time, regardless of\n      page changes.&lt;/description&gt;\n    &lt;/property&gt;\n\n    &lt;property&gt;\n      &lt;name&gt;db.fetch.schedule.adaptive.inc_rate&lt;/name&gt;\n      &lt;value&gt;0.4&lt;/value&gt;\n      &lt;description&gt;If a page is unmodified, its fetchInterval will be\n      increased by this rate. This value should not\n      exceed 0.5, otherwise the algorithm becomes unstable.&lt;/description&gt;\n    &lt;/property&gt;\n\n    &lt;property&gt;\n      &lt;name&gt;db.fetch.schedule.adaptive.dec_rate&lt;/name&gt;\n      &lt;value&gt;0.2&lt;/value&gt;\n      &lt;description&gt;If a page is modified, its fetchInterval will be\n      decreased by this rate. This value should not\n      exceed 0.5, otherwise the algorithm becomes unstable.&lt;/description&gt;\n    &lt;/property&gt;\n\n    &lt;property&gt;\n      &lt;name&gt;db.fetch.schedule.adaptive.min_interval&lt;/name&gt;\n      &lt;value&gt;60.0&lt;/value&gt;\n      &lt;description&gt;Minimum fetchInterval, in seconds.&lt;/description&gt;\n    &lt;/property&gt;\n\n    &lt;property&gt;\n      &lt;name&gt;db.fetch.schedule.adaptive.max_interval&lt;/name&gt;\n      &lt;value&gt;31536000.0&lt;/value&gt;\n      &lt;description&gt;Maximum fetchInterval, in seconds (365 days).\n      NOTE: this is limited by db.fetch.interval.max. Pages with\n      fetchInterval larger than db.fetch.interval.max\n      will be fetched anyway.&lt;/description&gt;\n    &lt;/property&gt;\n\n    &lt;property&gt;\n      &lt;name&gt;db.fetch.schedule.adaptive.sync_delta&lt;/name&gt;\n      &lt;value&gt;true&lt;/value&gt;\n      &lt;description&gt;If true, try to synchronize with the time of page change.\n      by shifting the next fetchTime by a fraction (sync_rate) of the difference\n      between the last modification time, and the last fetch time.&lt;/description&gt;\n    &lt;/property&gt;\n\n    &lt;property&gt;\n      &lt;name&gt;db.fetch.schedule.adaptive.sync_delta_rate&lt;/name&gt;\n      &lt;value&gt;0.3&lt;/value&gt;\n      &lt;description&gt;See sync_delta for description. This value should not\n      exceed 0.5, otherwise the algorithm becomes unstable.&lt;/description&gt;\n    &lt;/property&gt;\n\n    &lt;property&gt;\n      &lt;name&gt;db.fetch.schedule.adaptive.sync_delta_rate&lt;/name&gt;\n      &lt;value&gt;0.3&lt;/value&gt;\n      &lt;description&gt;See sync_delta for description. This value should not\n      exceed 0.5, otherwise the algorithm becomes unstable.&lt;/description&gt;\n    &lt;/property--&gt;\n\n    &lt;property&gt;\n      &lt;name&gt;fetcher.threads.fetch&lt;/name&gt;\n      &lt;value&gt;1&lt;/value&gt;\n      &lt;description&gt;The number of FetcherThreads the fetcher should use.\n         This is also determines the maximum number of requests that are\n         made at once (each FetcherThread handles one connection). The total\n         number of threads running in distributed mode will be the number of\n         fetcher threads * number of nodes as fetcher has one map task per node.\n      &lt;/description&gt;\n    &lt;/property&gt;\n\n    &lt;property&gt;\n       &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;\n       &lt;value&gt;/opt/apache-nutch/tmp/&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;!-- Boilerpipe --&gt;\n    &lt;property&gt;\n      &lt;name&gt;tika.boilerpipe&lt;/name&gt;\n      &lt;value&gt;true&lt;/value&gt;\n    &lt;/property&gt;\n    &lt;property&gt;\n      &lt;name&gt;tika.boilerpipe.extractor&lt;/name&gt;\n      &lt;value&gt;ArticleExtractor&lt;/value&gt;\n    &lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre>\n\n<p>As you can see, I have configured nutch to always refetch all the documents.\nBecause the site is small, it should be ok for now to refetch everything (the first time takes only 30 minutes...).</p>\n\n<p>I have noticed that in the folder crawldb/segments every day more or less 40 new segments are created.\nthe disk size of the database of course is growing very fast.</p>\n\n<p>Is this the expected behaviour ? Is there something wrong with the configuration?</p>\n", "creation_date": 1371827993, "score": 2},
{"title": "Nutch Keyword search cannot retrieve all the pages contain that keyword", "view_count": 97, "owner": {"user_id": 1487305, "answer_count": 2, "creation_date": 1340849264, "accept_rate": 86, "view_count": 7, "reputation": 59}, "is_answered": true, "answers": [{"question_id": 17204192, "owner": {"user_id": 1487305, "accept_rate": 86, "link": "http://stackoverflow.com/users/1487305/404", "user_type": "registered", "reputation": 59}, "body": "<p>I just solved the problem . </p>\n\n<pre><code>Query query = Query.parse(searchQuery, conf);\nQueryParams queryParams = new QueryParams();\nqueryParams.setMaxHitsPerDup(100);\nqueryParams.setNumHits(100);\nquery.setParams(queryParams);\nHits hits = bean.search(query);\nlong allResultsCount =**hits.getTotal());**\n</code></pre>\n\n<p>I changed this into </p>\n\n<pre><code>long allResultsCount =**hits.getLength());**\n</code></pre>\n", "creation_date": 1372129247, "is_accepted": true, "score": 0, "last_activity_date": 1372129247, "answer_id": 17288461}], "question_id": 17204192, "tags": ["java", "lucene", "nutch", "keyword-search"], "answer_count": 1, "link": "http://stackoverflow.com/questions/17204192/nutch-keyword-search-cannot-retrieve-all-the-pages-contain-that-keyword", "last_activity_date": 1372129247, "accepted_answer_id": 17288461, "body": "<p>I am using Nutch and Lucene (java API) for my website keyword search . My problem is that Nutch cannot allocate all the pages that contains the desired keywords. For eg. I have the product named \"Luxury Bag\" and If I search with \"Luxury\" I don't get the product in results and if I find with \"Luxury Bag\" I found it . I spent over a week for this errors and no idea at all. In addition , I've tested my crawled data with Luke Program . It perfectly work with Luke but not with my java codes. Can anyone kindly suggest me . Any suggestion is welcome and appreciated . Thanks.</p>\n", "creation_date": 1371693875, "score": 0},
{"title": "Template based Indexing/Extraction with Apache Nutch &amp; Solr", "view_count": 491, "owner": {"user_id": 2452071, "answer_count": 1, "creation_date": 1370354387, "accept_rate": 62, "view_count": 24, "reputation": 116}, "is_answered": true, "answers": [{"question_id": 16920092, "owner": {"user_id": 2225199, "link": "http://stackoverflow.com/users/2225199/nimeshjm", "user_type": "registered", "reputation": 1137}, "body": "<p>You could use the xpath filter plugin to segregate crawled content into two different fields.\n<a href=\"http://www.atlantbh.com/precise-data-extraction-with-apache-nutch/\" rel=\"nofollow\">http://www.atlantbh.com/precise-data-extraction-with-apache-nutch/</a></p>\n\n<p>Content in class=\"post\" would go to field A, content in class=\"commentlist\" would go to field B.</p>\n\n<p>In your search page logic, you query Solr on field A so your search results are only from your blog post, not the comments.</p>\n\n<p>Comments data is still saved against the document, but not searchable.</p>\n", "creation_date": 1371401594, "is_accepted": true, "score": 0, "last_activity_date": 1371401594, "answer_id": 17135587}], "question_id": 16920092, "tags": ["apache", "solr", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/16920092/template-based-indexing-extraction-with-apache-nutch-solr", "last_activity_date": 1371401594, "accepted_answer_id": 17135587, "body": "<p>I am new to Apache Nutch/Solr family of products. I have setup basic Nutch (1.6) with Solr (4.3) and have successfully crawled a site and Solr has indexed my crawled data as well. </p>\n\n<p>Now my question is if I crawl a web blog like where user can give their comments (e.g <a href=\"http://blogs.alliedtechnique.com/2009/04/16/setting-global-environment-variables-in-centos/\" rel=\"nofollow\">http://blogs.alliedtechnique.com/2009/04/16/setting-global-environment-variables-in-centos/</a>), how can I make sure Nutch consider user's comments and main blog as separate document, So when I search for keyword , it returns me main blog and comments as separate results and later I could use that data for sentiment analysis as well.  </p>\n\n<p>I would greatly appreciate any help here.</p>\n\n<p>Thanks.\nTony</p>\n", "creation_date": 1370355053, "score": 0},
{"title": "How to use Apache Nutch through a Java application?", "view_count": 3375, "is_answered": true, "answers": [{"question_id": 17102607, "owner": {"user_id": 2082437, "accept_rate": 60, "link": "http://stackoverflow.com/users/2082437/abhinav", "user_type": "registered", "reputation": 767}, "body": "<p>Nutch will be your backend to do crawling. Then you will use solr indexing and then your front end will search on this solr index. See this link here <a href=\"http://www.building-blocks.com/thinking/building-a-search-engine-with-nutch-and-solr-in-10-minutes\" rel=\"nofollow\">http://www.building-blocks.com/thinking/building-a-search-engine-with-nutch-and-solr-in-10-minutes</a></p>\n", "creation_date": 1371192733, "is_accepted": false, "score": -1, "last_activity_date": 1371192733, "answer_id": 17102703}, {"question_id": 17102607, "owner": {"user_id": 81194, "link": "http://stackoverflow.com/users/81194/claude", "user_type": "registered", "reputation": 827}, "body": "<p>Apache Nutch is only going to help you crawl for data, but you need to index what it finds into a search server. This is where Apache Solr comes in. Then you can write your Java application to interact with Solr using <a href=\"http://wiki.apache.org/solr/Solrj\" rel=\"nofollow\">SolrJ</a>.</p>\n\n<p>See <a href=\"http://stackoverflow.com/questions/16571463/latest-compatible-versions-of-nutch-and-solr\">this thread</a> for what versions of Nutch and Solr work best together.</p>\n", "creation_date": 1371219113, "is_accepted": false, "score": 1, "last_activity_date": 1371219113, "answer_id": 17110506}], "question_id": 17102607, "tags": ["java", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/17102607/how-to-use-apache-nutch-through-a-java-application", "last_activity_date": 1371301406, "owner": {"user_id": 1474659, "answer_count": 6, "creation_date": 1340364399, "accept_rate": 48, "view_count": 118, "reputation": 321}, "body": "<p>I have to design a Java/Java EE based search engine using apache nutch. I have searched over the internet and I found many articles regarding installation of apache nutch but unable to find any article/tutorial which deals with the java program to access or control apache nutch for crawling.</p>\n", "creation_date": 1371192333, "score": 2},
{"title": "Crawl Desired/precise data from site using nutch-solr", "view_count": 515, "is_answered": false, "question_id": 17082418, "tags": ["search", "solr", "web-crawler", "fetch", "nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/17082418/crawl-desired-precise-data-from-site-using-nutch-solr", "last_activity_date": 1371120738, "owner": {"user_id": 2530846, "answer_count": 0, "creation_date": 1359713735, "accept_rate": 0, "view_count": 50, "reputation": 94}, "body": "<p>While running nutch, I am getting all the data. This is what I do not want. I wants to fetch data according to div class and div id. I mean that I do not want to fetch all data but data according to my need .Is it possible ??</p>\n", "creation_date": 1371111645, "score": 0},
{"title": "Apache Nutch crawler how to exclude static folders like; cgi-bin, images, css exclude from nutch crawler?", "view_count": 169, "owner": {"user_id": 2430823, "view_count": 3, "answer_count": 0, "creation_date": 1369795120, "reputation": 3}, "is_answered": true, "answers": [{"last_edit_date": 1369926836, "owner": {"user_id": 1150329, "accept_rate": 89, "link": "http://stackoverflow.com/users/1150329/tejas-patil", "user_type": "registered", "reputation": 4793}, "body": "<p>Add reject rules to the <code>conf/regex-urlfilter.txt</code> file. </p>\n\n<pre><code>-cgi-bin\n-images\n-css\n</code></pre>\n\n<p>Note that this must be added before the accept all rule ie. <code>+.</code> in the regex file.</p>\n", "question_id": 16805284, "creation_date": 1369877012, "is_accepted": true, "score": 0, "last_activity_date": 1369926836, "answer_id": 16826484}], "question_id": 16805284, "tags": ["apache", "hadoop", "lucene", "mapreduce", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/16805284/apache-nutch-crawler-how-to-exclude-static-folders-like-cgi-bin-images-css-ex", "last_activity_date": 1370382085, "accepted_answer_id": 16826484, "body": "<p>When we run the crawler we see static folders like; /cgi-bin, /images, /css etc. popup in the crawler jobs, we want to exclude them from crawling (not that they end up in indexer) and we don\u00b4t want them in indexer, but how we can exclude them in the crawler so it is not occupied with these static folders? Any help is appreciated. Does it help performance, excluding them? as now we see it fetches them for some reason or another. Nutch crawler 1.2, Lucene indexer.</p>\n", "creation_date": 1369800134, "score": 0},
{"title": "Where can I find documentation about Nutch status codes?", "view_count": 1085, "owner": {"age": 31, "answer_count": 4, "creation_date": 1308680775, "user_id": 809043, "accept_rate": 89, "view_count": 37, "location": "Sweden", "reputation": 122}, "is_answered": true, "answers": [{"question_id": 16853155, "owner": {"user_id": 1150329, "accept_rate": 89, "link": "http://stackoverflow.com/users/1150329/tejas-patil", "user_type": "registered", "reputation": 4793}, "body": "<p>There is no official documentation but I could pull this one from the <a href=\"http://svn.apache.org/viewvc/nutch/trunk/src/java/org/apache/nutch/crawl/CrawlDatum.java?view=markup\">CrawlDatum</a> class:</p>\n\n<pre><code>  /** Page was not fetched yet. */\n  public static final byte STATUS_DB_UNFETCHED      = 0x01;\n\n  /** Page was successfully fetched. */\n  public static final byte STATUS_DB_FETCHED        = 0x02;\n\n  /** Page no longer exists. */\n  public static final byte STATUS_DB_GONE           = 0x03;\n\n  /** Page temporarily redirects to other page. */\n  public static final byte STATUS_DB_REDIR_TEMP     = 0x04;\n\n  /** Page permanently redirects to other page. */\n  public static final byte STATUS_DB_REDIR_PERM     = 0x05;\n\n  /** Page was successfully fetched and found not modified. */\n  public static final byte STATUS_DB_NOTMODIFIED    = 0x06;\n</code></pre>\n", "creation_date": 1370061896, "is_accepted": true, "score": 6, "last_activity_date": 1370061896, "answer_id": 16869165}], "question_id": 16853155, "tags": ["nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/16853155/where-can-i-find-documentation-about-nutch-status-codes", "last_activity_date": 1370061896, "accepted_answer_id": 16869165, "body": "<p>Nutch has a couple of status codes which are used to classify crawled documents.</p>\n\n<p>Examples of codes which Nutch uses are:</p>\n\n<pre><code>db_unfetched\ndb_fetched\ndb_gone\ndb_redir_perm\ndb_redir_temp\ndb_notmodified\n</code></pre>\n\n<p>Where can I find a clear explanation what the codes means?</p>\n\n<p>Reading forum posts and answerers here at Stackoverflow gives a good understanding of the codes. Also this page gives some good input: <a href=\"http://wiki.apache.org/nutch/CrawlDatumStates\" rel=\"nofollow\">http://wiki.apache.org/nutch/CrawlDatumStates</a> But I'm looking for a page which describes the meaning of each status code.</p>\n", "creation_date": 1369989377, "score": 3},
{"title": "How is an aggregator built?", "view_count": 9140, "is_answered": true, "answers": [{"question_id": 928433, "owner": {"user_id": 114054, "link": "http://stackoverflow.com/users/114054/gabriel", "user_type": "registered", "reputation": 1113}, "body": "<p>For a basic look - check out this: <a href=\"http://en.wikipedia.org/wiki/Aggregator\" rel=\"nofollow\">http://en.wikipedia.org/wiki/Aggregator</a> </p>\n\n<p>It will give you an overview of aggregators in general.</p>\n\n<p>In terms of how to build your own aggregator if you're looking for something out of the box that can get you content that YOU want - I'd suggest this: <a href=\"http://dailyme.com/\" rel=\"nofollow\">http://dailyme.com/</a></p>\n\n<p>If you're looking for a codebase / architecture to BUILD your own aggregator-service - I'd suggest looking at something straight forward - like: Open Reddit from <a href=\"http://www.reddit.com/\" rel=\"nofollow\">http://www.reddit.com/</a></p>\n", "creation_date": 1243639071, "is_accepted": false, "score": 1, "last_activity_date": 1243639071, "answer_id": 928521}, {"last_edit_date": 1254984948, "owner": {"user_id": 80701, "accept_rate": 87, "link": "http://stackoverflow.com/users/80701/monksy", "user_type": "registered", "reputation": 9803}, "body": "<p>This all depends on the aggregator you are looking for. </p>\n\n<h3>Types:</h3>\n\n<ul>\n<li>Losely defined - Generially this requires for you datasource to be very flexible about determining the type of information gathers (answers the question of is this site/information Travel Related? Humour? Business related? )</li>\n<li>Specific - This relaxes a requirement in the data storage that all of the data is specificially travel related requires for flights, hotel prices, etc.</li>\n</ul>\n\n<h3>Typcially an aggregator is a system of sub programs:</h3>\n\n<ol>\n<li>Grabber, this searches and grabs all of the content that is needed to be summarized</li>\n<li>Summerization- this is typically done through queries to the db and can be adjusted based on user preferences [through programming logic]</li>\n<li>View - this formats the information for what the user would like to see and can respond to feedback on the user's likes or dislikes of the item suggested.</li>\n</ol>\n", "question_id": 928433, "creation_date": 1254980473, "is_accepted": false, "score": 6, "last_activity_date": 1254984948, "answer_id": 1535805}, {"question_id": 928433, "owner": {"user_id": 194031, "accept_rate": 84, "link": "http://stackoverflow.com/users/194031/chad", "user_type": "registered", "reputation": 1722}, "body": "<p>You need to define what your application is going to do.  Building your own web crawler is a huge task as you tend to keep adding new features as you find you need them... only to complicate your design, etc...</p>\n\n<p>Building an aggregator is much different.  <strong>Whereas a crawler simply retrieves data to be processed later, an aggregator takes already defined sets of data and puts them together.</strong>  If you use an aggregator, you will probably want to look for already defined travel feeds, financial feeds, travel data, etc...  An aggregator is easier to build IMO, but it's more constrained.</p>\n\n<p>If you, instead, want to build a crawler you'll need to define starting pages, define ending conditions (crawl depth, time, etc...) and so on and then still process the data afterwards (that is aggregate, summarize and so on).</p>\n", "creation_date": 1280881234, "is_accepted": false, "score": 1, "last_activity_date": 1280881234, "answer_id": 3401721}], "question_id": 928433, "tags": ["web-services", "aggregation", "web-crawler", "nutch"], "answer_count": 3, "link": "http://stackoverflow.com/questions/928433/how-is-an-aggregator-built", "last_activity_date": 1369903243, "owner": {"user_type": "does_not_exist"}, "body": "<p>Let's say I want to aggregate information related to a specific niche from many sources (could be travel, technology, or whatever).\nHow would I do that?</p>\n\n<p>Have a spider/crawler who will crawl the web for finding the information I need (how would I tell the crawler what to crawl because I don't want to get the whole web?)?\nThen have an indexing system to index and organize the information I crawled and also be a search engine?</p>\n\n<p>Are systems like Nutch lucene.apache.org/nutch OK to be used for what I want? Do you recommend something else? </p>\n\n<p>Or can you recommend another approach? </p>\n\n<p>For example, how Techmeme.com is built? (it's an aggregator of technology news and it's completely automated - only recently they added some human intervention).\nWhat would it take to build such a service?</p>\n\n<p>Or how do Kayak.com aggregate their data? (It's a travel aggregator service.)</p>\n", "creation_date": 1243636574, "score": 11},
{"title": "How to include previously excluded URLS in a nutch crawl", "view_count": 70, "owner": {"user_id": 859955, "answer_count": 66, "creation_date": 1311484167, "accept_rate": 91, "view_count": 167, "reputation": 1052}, "is_answered": true, "answers": [{"question_id": 16770319, "owner": {"user_id": 1150329, "accept_rate": 89, "link": "http://stackoverflow.com/users/1150329/tejas-patil", "user_type": "registered", "reputation": 4793}, "body": "<p><a href=\"http://www.mail-archive.com/user@nutch.apache.org/msg09626.html\" rel=\"nofollow\">This post</a> over nutch user group might help you do that.</p>\n", "creation_date": 1369877108, "is_accepted": true, "score": 1, "last_activity_date": 1369877108, "answer_id": 16826501}], "question_id": 16770319, "tags": ["hadoop", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/16770319/how-to-include-previously-excluded-urls-in-a-nutch-crawl", "last_activity_date": 1369901275, "accepted_answer_id": 16826501, "body": "<p>Using Nutch 2.1</p>\n\n<p>During initial crawling, I had excluded some urls to limit the number of pages to be crawled. Now when I remove those reject rules from regex-urlfilter.txt and run these command, I dont get those filtered urls:</p>\n\n<pre><code>bin/nutch updatedb\nbin/nutch generate\nbin/nutch fetch -all\n</code></pre>\n\n<p>What am I missing?</p>\n", "creation_date": 1369647480, "score": 0},
{"title": "Searching Images in Solr", "view_count": 1580, "is_answered": false, "question_id": 16741813, "tags": ["image", "search", "solr", "nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/16741813/searching-images-in-solr", "last_activity_date": 1369421683, "owner": {"user_id": 2406613, "view_count": 7, "answer_count": 0, "creation_date": 1369157741, "reputation": 11}, "body": "<p>I am new to Solr. I need to be able to search on image meta data in Solr. Also, I need to have the simple search and view ability to view the image in Solr.</p>\n\n<p>Currently, 1) I crawl the sites with apache Nutch and then run indexsolr command to send the data to Solr.\n2) I added there </p>\n\n<pre><code>&lt;requestHandler name=\"/update/extract\" class=\"solr.extraction.ExtractingRequestHandler\" &gt;\n&lt;lst name=\"defaults\"&gt;\n &lt;str name=\"fmap.content\"&gt;text&lt;/str&gt;\n &lt;str name=\"lowernames\"&gt;true&lt;/str&gt;\n &lt;str name=\"uprefix\"&gt;attr_&lt;/str&gt;\n &lt;str name=\"captureAttr\"&gt;true&lt;/str&gt;\n</code></pre>\n\n<p>\n   </p>\n\n<p>and </p>\n\n<pre><code>&lt;lib dir=\"/path-to-solr/example/solr/collection1/extract\" regex=\".*\\.jar\" /&gt;\n</code></pre>\n\n<p>(I created a folder extract and move all the jars from solar dist folder in there as well as tika related jars)\nto solrconfig.xml</p>\n\n<p>3) added to schema.xml the following fields</p>\n\n<pre><code>&lt;field name=\"host\" type=\"string\" stored=\"false\" indexed=\"true\"/&gt;\n&lt;field name=\"segment\" type=\"string\" stored=\"true\" indexed=\"false\"/&gt;\n&lt;field name=\"digest\" type=\"string\" stored=\"true\" indexed=\"false\"/&gt;  enter code here\n&lt;field name=\"boost\" type=\"float\" stored=\"true\" indexed=\"false\"/&gt;\n\n\n&lt;field name=\"tstamp\" type=\"date\" stored=\"true\" indexed=\"false\"/&gt;\n&lt;!--  fields for index-anchor plugin  --&gt;\n&lt;field name=\"anchor\" type=\"string\" stored=\"true\" indexed=\"true\" multiValued=\"true\"/&gt;\n&lt;!--  fields for index-more plugin  --&gt;\n&lt;field name=\"type\" type=\"string\" stored=\"true\" indexed=\"true\" multiValued=\"true\"/&gt;\n&lt;field name=\"contentLength\" type=\"long\" stored=\"true\" indexed=\"false\"/&gt;\n&lt;field name=\"lastModified\" type=\"date\" stored=\"true\" indexed=\"false\"/&gt;\n&lt;field name=\"date\" type=\"date\" stored=\"true\" indexed=\"true\"/&gt;\n&lt;field name=\"aperture\" type=\"double\" indexed=\"true\" stored=\"true\"/&gt;\n&lt;field name=\"exposure\" type=\"string\" indexed=\"true\" stored=\"true\"/&gt;\n&lt;field name=\"exposure_time\" type=\"double\" indexed=\"true\" stored=\"true\"/&gt;\n&lt;field name=\"focal\" type=\"string\" indexed=\"true\" stored=\"true\"/&gt;\n&lt;field name=\"focal_35\" type=\"string\" indexed=\"true\" stored=\"true\"/&gt;\n\n&lt;!-- for indexing and parsing metadata plugin --&gt;\n&lt;field name=\"metatag.description\" type=\"string\" stored=\"true\" indexed=\"true\"/&gt;\n&lt;field name=\"metatag.keywords\" type=\"string\" stored=\"true\" indexed=\"true\"/&gt;\n</code></pre>\n\n<p>Looks like I am still missing something, since I cannot see the images in Solr.</p>\n\n<p>Do I need to explicitly add each image via something like:</p>\n\n<pre><code>curl \"http://127.0.0.1:8983/solr/update/extract?literal.id=n1&amp;commit=true\" -F   \"myfile=@my_image.jpg\"\n</code></pre>\n\n<p>?\nThanks</p>\n", "creation_date": 1369421683, "score": 0},
{"title": "Separate Nutch regex files to crawl and index to multiple Solr cores", "view_count": 579, "owner": {"user_id": 1788590, "answer_count": 1, "creation_date": 1351687889, "accept_rate": 89, "view_count": 37, "location": "Turin, Italy", "reputation": 80}, "is_answered": true, "answers": [{"question_id": 16715348, "owner": {"user_id": 950178, "link": "http://stackoverflow.com/users/950178/jayendra", "user_type": "registered", "reputation": 34896}, "body": "<p>You can try having multiple regex files and the main file as a link and just re pointing it before you start nutch indexing </p>\n", "creation_date": 1369386609, "is_accepted": true, "score": 2, "last_activity_date": 1369386609, "answer_id": 16731353}], "question_id": 16715348, "tags": ["solr", "nutch", "solr4"], "answer_count": 1, "link": "http://stackoverflow.com/questions/16715348/separate-nutch-regex-files-to-crawl-and-index-to-multiple-solr-cores", "last_activity_date": 1369386609, "accepted_answer_id": 16731353, "body": "<p>My setup is: Nutch 1.6 and Solr 4.3.0 on Ubuntu Server 12.04 LTS</p>\n\n<p>I need to crawl and index the content of a big website and would like doing this using separate cores.</p>\n\n<p>I've configured Solr and started it this way:</p>\n\n<pre><code>java -Dsolr.solr.home=multicore -jar start.jar\n</code></pre>\n\n<p>Then I configured and launched Nutch two times, one for each source urls folder and index destination (core0, core1):</p>\n\n<pre><code>bin/nutch crawl urlsNewsArticles -dir crawlNewsArticles -solr http://localhost:8983/solr/core1 -depth 10 -topN 100000\n\nbin/nutch crawl urlsPictureGalleries -dir crawlPictureGalleries -solr http://localhost:8983/solr/core0 -depth 10 -topN 100000\n</code></pre>\n\n<p>The result is perfect but, in order to filter away undesired URLs patterns, I had to specify some regex expressions in the regex-urlfilter.txt file.\nBeing those regex sets different for the two crawling sessions, I had to edit the regex-urlfilter.txt file before running the second crawl.</p>\n\n<p><strong>Question</strong>: is there a way to prepare two separate regex-urlfilter.txt files and specify the proper one on each /bin/nutch command line ?</p>\n\n<p>Please consider that I started my experimental configuration with 2 url sets and cores, but I will have to configure at least 5 and they should be configured for automatic re-crawling without manual editing sessions in between.... </p>\n", "creation_date": 1369316078, "score": 0},
{"title": "java benchmarking : More time for lesser input", "view_count": 81, "owner": {"user_id": 1150329, "answer_count": 207, "creation_date": 1326629590, "accept_rate": 89, "view_count": 969, "location": "California", "reputation": 4793}, "is_answered": true, "answers": [{"question_id": 16726146, "owner": {"user_id": 321356, "accept_rate": 82, "link": "http://stackoverflow.com/users/321356/jan-doerrenhaus", "user_type": "registered", "reputation": 4459}, "body": "<p>At the beginning, the regex is compiled. So the first run uses extra time. Also, there is probably a run-time optimization going on.</p>\n", "creation_date": 1369356303, "is_accepted": false, "score": 1, "last_activity_date": 1369356303, "answer_id": 16726169}, {"question_id": 16726146, "owner": {"user_id": 1150329, "accept_rate": 89, "link": "http://stackoverflow.com/users/1150329/tejas-patil", "user_type": "registered", "reputation": 4793}, "body": "<p>This is due to the standard micro-benchmark issues with Java.</p>\n\n<p><strong>JVM warmup</strong>: Due to several parameters, the code is first often slow and becomes faster and faster when the execution time grows until it goes to steady-state.</p>\n\n<p><strong>Class loading</strong>: The first time you launch a benchmark, all the used classes must be loaded, increasing the execution time.</p>\n\n<p><strong>Just In Time Compiler</strong>: When the JVM identify a hot part of the code</p>\n\n<p><strong>Garbage Collector</strong>: A garbage collection can happen during the benchmark and with that the time can increase a lot.</p>\n\n<p>One good <a href=\"http://www.ibm.com/developerworks/java/library/j-benchmark1/index.html\" rel=\"nofollow\">read</a>.</p>\n", "creation_date": 1369359783, "is_accepted": true, "score": 2, "last_activity_date": 1369359783, "answer_id": 16726493}], "question_id": 16726146, "tags": ["java", "regex", "performance", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/16726146/java-benchmarking-more-time-for-lesser-input", "last_activity_date": 1369359783, "accepted_answer_id": 16726493, "body": "<p>After running some benchmarking on 2 regex library implementations, below is the results that we got:</p>\n\n<pre><code>inputs   automaton      regex\n 50      343ms          210ms\n100      48ms           187ms\n200      65ms           363ms\n400      100ms          692ms\n800      165ms         1385ms\n</code></pre>\n\n<p>Why the first run (with 50 inputs) is super expensive for both implementations ?</p>\n\n<p>FYI: <code>automation</code> refers to <a href=\"http://www.brics.dk/automaton\" rel=\"nofollow\">http://www.brics.dk/automaton</a> and \n'regex' refers to the java regex library.</p>\n", "creation_date": 1369356184, "score": 0},
{"title": "Nutch - Are the -depth and -topN still available in 1.6", "view_count": 564, "owner": {"age": 26, "answer_count": 34, "creation_date": 1364721047, "user_id": 2228912, "accept_rate": 88, "view_count": 141, "reputation": 446}, "is_answered": true, "answers": [{"question_id": 16714774, "owner": {"user_id": 950178, "link": "http://stackoverflow.com/users/950178/jayendra", "user_type": "registered", "reputation": 34896}, "body": "<p>For the description :-</p>\n\n<ul>\n<li><p><code>depth</code> depth indicates the link depth from the root page that should be crawled.<br>\ne.g. you can have links in you root page scan which in turn would have links in it and so on.\nThis may lead to exponential scanning of links. The depth param restricts the hierarchy of links that would be scanned from the root page.</p></li>\n<li><p><code>topN</code> N determines the maximum number of pages that will be retrieved at each level up to the depth.<br>\ne.g. You may have 100 links on the root page. topN would limit the number of links to be scanned on each level.</p></li>\n</ul>\n\n<p>So basically the Number of links max that should be scanned would be Root Page * Depth * topN</p>\n\n<p>Also, Don't see in the documentation that they have been removed or deprecated. So I assume they are available.  </p>\n", "creation_date": 1369314972, "is_accepted": true, "score": 0, "last_activity_date": 1369314972, "answer_id": 16714959}], "question_id": 16714774, "tags": ["solr", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/16714774/nutch-are-the-depth-and-topn-still-available-in-1-6", "last_activity_date": 1369314972, "accepted_answer_id": 16714959, "body": "<p>I want to know if the parameters <code>-depth</code> &amp; <code>-topN</code> are still available nutch 1.6.<br>\nI dont even know what's the difference between these parameters and the <code>limit</code> parameter in /bin/crawl bash script?</p>\n", "creation_date": 1369314374, "score": 0},
{"title": "Nutch regex for crawl", "view_count": 237, "owner": {"age": 27, "answer_count": 125, "creation_date": 1313859912, "user_id": 903907, "accept_rate": 50, "view_count": 306, "location": "Chennai, India", "reputation": 2981}, "is_answered": true, "answers": [{"question_id": 16711801, "owner": {"user_id": 1609187, "link": "http://stackoverflow.com/users/1609187/shadow", "user_type": "registered", "reputation": 136}, "body": "<p>In my memory nutch got an extra setting for cutting off url parameters like ?q=bill+gates.\nI'll think this setting is located in automaton-urlfilter.txt:</p>\n\n<pre><code># skip URLs containing certain characters as probable queries, etc.\n-.*[?*!@=].*\n</code></pre>\n\n<p>So you got to change this line.</p>\n\n<p>Hope I could help you</p>\n", "creation_date": 1369306437, "is_accepted": true, "score": 1, "last_activity_date": 1369306437, "answer_id": 16712086}], "question_id": 16711801, "tags": ["java", "web-crawler", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/16711801/nutch-regex-for-crawl", "last_activity_date": 1369306437, "accepted_answer_id": 16712086, "body": "<p>I am using Apache Nutch to crawl the web page. I want to crawl the web  page when i search for particular name like \n    if i search bill gates i want to get the results links of that search result. I have url like </p>\n\n<pre><code>www.mysite.com/search?name=bill+gates\n</code></pre>\n\n<p>but in crawling it displays no more url to fetch. actually it does not fetch any results.</p>\n\n<p>Is there any option to crawl that page? i have added in regex-urlfilter.txt to accept everything.\nHow would i crawl  the link? Thanks in advance.</p>\n", "creation_date": 1369305636, "score": 0},
{"title": "Solr &amp; Nutch - Indexing only certain urls", "view_count": 297, "owner": {"age": 26, "answer_count": 34, "creation_date": 1364721047, "user_id": 2228912, "accept_rate": 88, "view_count": 141, "reputation": 446}, "is_answered": true, "answers": [{"question_id": 16709250, "owner": {"user_id": 950178, "link": "http://stackoverflow.com/users/950178/jayendra", "user_type": "registered", "reputation": 34896}, "body": "<p>If you don't need the pages, it would be best to filter out from the indexing itself.<br>\nHowever, If you have a pattern that you can filter upon in Solr, you can use <a href=\"http://wiki.apache.org/solr/CommonQueryParameters#fq\" rel=\"nofollow\">filter queries</a> to do the filtering.</p>\n", "creation_date": 1369299756, "is_accepted": true, "score": 1, "last_activity_date": 1369299756, "answer_id": 16709690}], "question_id": 16709250, "tags": ["solr", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/16709250/solr-nutch-indexing-only-certain-urls", "last_activity_date": 1369302759, "accepted_answer_id": 16709690, "body": "<p>I'm using nutch 1.6 for crawling, and solr 3.6.2 for indexing the urls crawled.\nBut, I want to keep only urls containing <code>details</code>.<br>\nWhat I have done is to add many (a lot of) filters to <code>nutch/conf/regex-urlfilter.txt</code>.  </p>\n\n<p>I want to know if there is a better solution even if I should crawl all data (urls), and then filter only the important once in Solr (in <code>Solrindex</code> command).</p>\n", "creation_date": 1369298278, "score": 1},
{"title": "Getting error while running nutch 1.6 with hadoop 0.20.2 in eclips in windows environment", "view_count": 1390, "owner": {"user_id": 1602236, "view_count": 2, "answer_count": 0, "creation_date": 1345091143, "reputation": 8}, "is_answered": true, "answers": [{"question_id": 16595822, "owner": {"user_id": 1736002, "accept_rate": 60, "link": "http://stackoverflow.com/users/1736002/cguzel", "user_type": "registered", "reputation": 576}, "body": "<p><code>2013-05-17 00:44:18,611 WARN  plugin.PluginRepository (PluginManifestParser.java:getPluginFolder(123)) -</code> <strong><code>Plugins: directory not found: plugins</code></strong>.</p>\n\n<p>You check plugins directory. And add plugin folders path in <code>nutch-site.xml</code>.\nYou must add your plugin path in the <code>&lt;value&gt;&lt;/value&gt;</code> tag as follows:</p>\n\n<pre><code>&lt;property&gt;\n    &lt;name&gt;plugin.folders&lt;/name&gt;\n    &lt;value&gt;/home/YOUR-USER/nutch/build/plugins&lt;/value&gt;\n&lt;/property&gt;\n</code></pre>\n", "creation_date": 1369057304, "is_accepted": true, "score": 2, "last_activity_date": 1369057304, "answer_id": 16650857}], "question_id": 16595822, "tags": ["nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/16595822/getting-error-while-running-nutch-1-6-with-hadoop-0-20-2-in-eclips-in-windows-en", "last_activity_date": 1369057304, "accepted_answer_id": 16650857, "body": "<p>I configure my nutch code in eclips as per this site <a href=\"http://wiki.apache.org/nutch/RunNutchInEclipse\" rel=\"nofollow\">http://wiki.apache.org/nutch/RunNutchInEclipse</a>\nbut getting error \"java.io.IOException: Failed to set permissions of path: \\tmp\\hadoop \"\nso i moved to hadoop 0.20.2 but after change hadoop jar I am stuck on this error:-</p>\n\n<pre><code>2013-05-17 00:44:17,742 WARN  crawl.Crawl (Crawl.java:run(97)) - solrUrl is not set, indexing will be skipped...\n2013-05-17 00:44:17,876 INFO  crawl.Crawl (Crawl.java:run(108)) - crawl started in: crawl\n2013-05-17 00:44:17,876 INFO  crawl.Crawl (Crawl.java:run(109)) - rootUrlDir = urls\n2013-05-17 00:44:17,876 INFO  crawl.Crawl (Crawl.java:run(110)) - threads = 10\n2013-05-17 00:44:17,876 INFO  crawl.Crawl (Crawl.java:run(111)) - depth = 3\n2013-05-17 00:44:17,877 INFO  crawl.Crawl (Crawl.java:run(112)) - solrUrl=null\n2013-05-17 00:44:17,877 INFO  crawl.Crawl (Crawl.java:run(114)) - topN = 50\n2013-05-17 00:44:17,888 INFO  crawl.Injector (Injector.java:inject(257)) - Injector: starting at 2013-05-17 00:44:17\n2013-05-17 00:44:17,888 INFO  crawl.Injector (Injector.java:inject(258)) - Injector: crawlDb: crawl/crawldb\n2013-05-17 00:44:17,888 INFO  crawl.Injector (Injector.java:inject(259)) - Injector: urlDir: urls\n2013-05-17 00:44:17,936 INFO  crawl.Injector (Injector.java:inject(269)) - Injector: Converting injected urls to crawl db entries.\n2013-05-17 00:44:17,961 INFO  jvm.JvmMetrics (JvmMetrics.java:init(71)) - Initializing JVM Metrics with processName=JobTracker, sessionId=\n2013-05-17 00:44:18,144 WARN  mapred.JobClient (JobClient.java:configureCommandLineOptions(661)) - No job jar file set.  User classes may not be found. See JobConf(Class) or JobConf#setJar(String).\n2013-05-17 00:44:18,176 INFO  mapred.FileInputFormat (FileInputFormat.java:listStatus(192)) - Total input paths to process : 1\n2013-05-17 00:44:18,519 INFO  mapred.JobClient (JobClient.java:monitorAndPrintJob(1275)) - Running job: job_local_0001\n2013-05-17 00:44:18,521 INFO  mapred.FileInputFormat (FileInputFormat.java:listStatus(192)) - Total input paths to process : 1\n2013-05-17 00:44:18,573 INFO  mapred.MapTask (MapTask.java:runOldMapper(347)) - numReduceTasks: 1\n2013-05-17 00:44:18,578 INFO  mapred.MapTask (MapTask.java:&lt;init&gt;(776)) - io.sort.mb = 100\n2013-05-17 00:44:18,601 INFO  mapred.MapTask (MapTask.java:&lt;init&gt;(788)) - data buffer = 79691776/99614720\n2013-05-17 00:44:18,601 INFO  mapred.MapTask (MapTask.java:&lt;init&gt;(789)) - record buffer = 262144/327680\n2013-05-17 00:44:18,611 WARN  plugin.PluginRepository (PluginManifestParser.java:getPluginFolder(123)) - Plugins: directory not found: plugins\n2013-05-17 00:44:18,612 INFO  plugin.PluginRepository (PluginRepository.java:displayStatus(313)) - Plugin Auto-activation mode: [true]\n2013-05-17 00:44:18,612 INFO  plugin.PluginRepository (PluginRepository.java:displayStatus(314)) - Registered Plugins:\n2013-05-17 00:44:18,612 INFO  plugin.PluginRepository (PluginRepository.java:displayStatus(317)) -  NONE\n2013-05-17 00:44:18,613 INFO  plugin.PluginRepository (PluginRepository.java:displayStatus(324)) - Registered Extension-Points:\n2013-05-17 00:44:18,613 INFO  plugin.PluginRepository (PluginRepository.java:displayStatus(326)) -  NONE\n2013-05-17 00:44:18,615 WARN  mapred.LocalJobRunner (LocalJobRunner.java:run(256)) - job_local_0001\njava.lang.RuntimeException: Error in configuring object\n    at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:93)\n    at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:64)\n    at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:117)\n    at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:354)\n    at org.apache.hadoop.mapred.MapTask.run(MapTask.java:307)\n    at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:177)\nCaused by: java.lang.reflect.InvocationTargetException\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\n    at java.lang.reflect.Method.invoke(Unknown Source)\n    at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:88)\n    ... 5 more\nCaused by: java.lang.RuntimeException: Error in configuring object\n    at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:93)\n    at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:64)\n    at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:117)\n    at org.apache.hadoop.mapred.MapRunner.configure(MapRunner.java:34)\n    ... 10 more\nCaused by: java.lang.reflect.InvocationTargetException\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\n    at java.lang.reflect.Method.invoke(Unknown Source)\n    at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:88)\n    ... 13 more\nCaused by: java.lang.RuntimeException: x point org.apache.nutch.net.URLNormalizer not found.\n    at org.apache.nutch.net.URLNormalizers.&lt;init&gt;(URLNormalizers.java:123)\n    at org.apache.nutch.crawl.Injector$InjectMapper.configure(Injector.java:74)\n    ... 18 more\n2013-05-17 00:44:19,520 INFO  mapred.JobClient (JobClient.java:monitorAndPrintJob(1288)) -  map 0% reduce 0%\n2013-05-17 00:44:19,523 INFO  mapred.JobClient (JobClient.java:monitorAndPrintJob(1343)) - Job complete: job_local_0001\n2013-05-17 00:44:19,524 INFO  mapred.JobClient (Counters.java:log(514)) - Counters: 0\nException in thread \"main\" java.io.IOException: Job failed!\n    at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:1252)\n    at org.apache.nutch.crawl.Injector.inject(Injector.java:281)\n    at org.apache.nutch.crawl.Crawl.run(Crawl.java:132)\n    at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n    at org.apache.nutch.crawl.Crawl.main(Crawl.java:55)\n</code></pre>\n\n<p>I searched a lot but did not find  any working solution. \nPlease suggest.</p>\n", "creation_date": 1368732405, "score": 1},
{"title": "How to filter URLs in Nutch 2.1 solrindex command", "view_count": 437, "owner": {"user_id": 1737710, "view_count": 1, "answer_count": 2, "creation_date": 1349950324, "reputation": 41}, "is_answered": true, "answers": [{"question_id": 16422182, "owner": {"user_id": 2225199, "link": "http://stackoverflow.com/users/2225199/nimeshjm", "user_type": "registered", "reputation": 1137}, "body": "<p>What is your exclusion criteria?</p>\n\n<p>Can you exclude them at the fetch/parse stages?</p>\n", "creation_date": 1368561450, "is_accepted": false, "score": 0, "last_activity_date": 1368561450, "answer_id": 16551792}, {"question_id": 16422182, "owner": {"user_id": 1737710, "link": "http://stackoverflow.com/users/1737710/thomas", "user_type": "registered", "reputation": 41}, "body": "<p>It seems that an index filter plugin should be used.</p>\n\n<p>I followed <a href=\"http://florianhartl.com/nutch-plugin-tutorial.html\" rel=\"nofollow\">Hartl's tutorial</a> to organize and link the source code.</p>\n\n<p>Here is the interesting part of the code:</p>\n\n<pre><code>public NutchDocument filter(NutchDocument doc, String url, WebPage page)\n    throws IndexingException {\n    String input_url = url;\n    try {\n        url = urlNormalizers.normalize(url, URLNormalizers.SCOPE_DEFAULT);\n        url = urlFilters.filter(url); // filter the url\n    } catch (Exception e) {\n        LOG.warn(\"Skipping \" + input_url + \":\" + e);\n        return null;\n    }  \n    if (url == null) {\n        LOG.info(\"Skipping (filter): \" + input_url);\n        return null;\n    }  \n    return doc;\n}\n\npublic void setConf(Configuration conf) {\n    this.conf = conf;\n    this.urlFilters = new URLFilters(conf);\n    urlNormalizers = new URLNormalizers(conf, URLNormalizers.SCOPE_INJECT);\n}  \n</code></pre>\n", "creation_date": 1369048159, "is_accepted": true, "score": 1, "last_activity_date": 1369048159, "answer_id": 16648183}], "question_id": 16422182, "tags": ["java", "solr", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/16422182/how-to-filter-urls-in-nutch-2-1-solrindex-command", "last_activity_date": 1369048364, "accepted_answer_id": 16648183, "body": "<p>How do I prevent some pages I fetched from getting indexed when I run a <code>bin/nutch solrindex</code> command? I would like to define and use a regex-urlfilter.txt like I did at fetch time.</p>\n\n<p>I'm using Nutch 2.1 and according to <a href=\"http://wiki.apache.org/nutch/bin/nutch%20solrindex\" rel=\"nofollow\">http://wiki.apache.org/nutch/bin/nutch%20solrindex</a> the option just disappeared. I checked the source code and can't find any clue about the matter.</p>\n\n<p>Are there any workarounds?</p>\n", "creation_date": 1367938467, "score": 1},
{"title": "how to switch off / on indexing in a web page", "view_count": 160, "owner": {"user_id": 1788590, "answer_count": 1, "creation_date": 1351687889, "accept_rate": 89, "view_count": 37, "location": "Turin, Italy", "reputation": 80}, "is_answered": true, "answers": [{"question_id": 16606229, "owner": {"user_id": 2393399, "link": "http://stackoverflow.com/users/2393399/alfeliz", "user_type": "registered", "reputation": 1}, "body": "<p>There is a text file, \"robots.txt\" that provide information to the search engines about which html pages the program is allowed or not to look for content. In the link <a href=\"http://www.robotstxt.org/faq/prevent.html\" rel=\"nofollow\">FAQ robots.txt: How to stop indexing</a> you will find all the information.</p>\n", "creation_date": 1368784941, "is_accepted": false, "score": 0, "last_activity_date": 1368784941, "answer_id": 16606361}, {"question_id": 16606229, "owner": {"user_id": 167980, "accept_rate": 100, "link": "http://stackoverflow.com/users/167980/paige-cook", "user_type": "registered", "reputation": 18612}, "body": "<p>You wil need to create a custom plugin for Nutch to be able to accomplish this behavior. Below are some relevant links with examples.</p>\n\n<ul>\n<li><a href=\"http://stackoverflow.com/questions/8576735/apache-nutch-manipulating-the-dom-before-parsing\">Apache nutch: Manipulating the DOM before parsing</a></li>\n<li><a href=\"http://www.atlantbh.com/precise-data-extraction-with-apache-nutch/\" rel=\"nofollow\">Precise data extraction with Apache Nutch</a></li>\n</ul>\n", "creation_date": 1368791097, "is_accepted": true, "score": 3, "last_activity_date": 1368791097, "answer_id": 16608198}], "question_id": 16606229, "tags": ["solr", "nutch", "apache-tika"], "answer_count": 2, "link": "http://stackoverflow.com/questions/16606229/how-to-switch-off-on-indexing-in-a-web-page", "last_activity_date": 1368791097, "accepted_answer_id": 16608198, "body": "<p>I'm using Nutch 1.6 and Solr 4.3 on Ubuntu Server 12.04\nI would like to switch on and off content indexing. Is there a way to specify this behaviour in my HTML pages so that Solr can behave accordingly ?</p>\n\n<p>As an example, when using Google Search Appliance I would use \"googleon\" - \"googleoff\" tags around the content on the page that i don't want indexed (headers, footers, copyright strings, etc ). </p>\n\n<p>thank you</p>\n", "creation_date": 1368784516, "score": 0},
{"title": "How to control the way Nutch parses and Solr indexes a URL when its HTML structure is unknown?", "view_count": 775, "is_answered": true, "answers": [{"last_edit_date": 1368564628, "owner": {"user_id": 2225199, "link": "http://stackoverflow.com/users/2225199/nimeshjm", "user_type": "registered", "reputation": 1137}, "body": "<p>You can use the plugin below to extract content based on XPath queries.\nIf your content is in a specific div, you can use this plugin to extract the content you want from that specific section.</p>\n\n<p><a href=\"http://www.atlantbh.com/precise-data-extraction-with-apache-nutch/\" rel=\"nofollow\">Filter xpath</a></p>\n", "question_id": 15953751, "creation_date": 1368564299, "is_accepted": false, "score": 1, "last_activity_date": 1368564628, "answer_id": 16552504}], "question_id": 15953751, "tags": ["apache", "solr", "web-crawler", "nutch", "solr4"], "answer_count": 1, "link": "http://stackoverflow.com/questions/15953751/how-to-control-the-way-nutch-parses-and-solr-indexes-a-url-when-its-html-structu", "last_activity_date": 1368564628, "owner": {"user_id": 1830069, "answer_count": 10, "creation_date": 1353080229, "accept_rate": 63, "view_count": 103, "reputation": 543}, "body": "<p>I am trying to crawl some sites which has poorly maintained HTML structure and I have no control over it to change it. When I look at the nutch crawled data indexed by Solr, the field 'title' looks okay where as the 'content' field includes lot of junk as it grabbed all the text from the html banner with its drop down menu and worked down into the left side menu, navigation, footer etc. </p>\n\n<p>In my case, I am interested to just grab the \"Description:\" information which is defined in a paragragh on HTML page into 'content' field.</p>\n\n<p>Example: (raw html):</p>\n\n<pre><code> &lt;p&gt;&lt;strong&gt;Description:&lt;/strong&gt; Apache Nutch is an open source Web crawler written in Java. By using it, we can find Web page hyperlinks in an automated manner, reduce lots of maintenance work, for example checking broken links, and create a copy of all the visited pages for searching over. \n</code></pre>\n\n<p>How can I filter the junk out of the 'content' field and only have the information I am interested in?</p>\n", "creation_date": 1365696674, "score": 0},
{"title": "Nutch Parse out HTML5 header tag", "view_count": 246, "is_answered": false, "answers": [{"question_id": 15976561, "owner": {"user_id": 2225199, "link": "http://stackoverflow.com/users/2225199/nimeshjm", "user_type": "registered", "reputation": 1137}, "body": "<p>I have used this plugin successfully with Nutch 1.6.</p>\n\n<p><a href=\"http://www.atlantbh.com/precise-data-extraction-with-apache-nutch/\" rel=\"nofollow\">Nutch filter xpath</a></p>\n\n<p>I did have the reverse requirement, I wanted to only include a specific div. :)</p>\n\n<p>With this plugin you can configure which sections of the HTML document you want to extract, using XPath queries.</p>\n", "creation_date": 1368563883, "is_accepted": false, "score": 0, "last_activity_date": 1368563883, "answer_id": 16552421}], "question_id": 15976561, "tags": ["solr", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/15976561/nutch-parse-out-html5-header-tag", "last_activity_date": 1368563883, "owner": {"user_id": 1226505, "answer_count": 4, "creation_date": 1329932308, "accept_rate": 44, "view_count": 48, "location": "Boston, MA", "reputation": 465}, "body": "<p>I am trying to parse HTML5 pages using Nutch 1.2 and running into problems with the HTML5 tag 'header' being included in the index.</p>\n\n<p>I used to exclude the HTML4 header with 'parser.html.divIDsToExclude' but this will no longer meet my requirements.</p>\n\n<p>Is there a similar plugin which can exclude configured HTML5 tags</p>\n\n<p>Thanks!</p>\n", "creation_date": 1365784513, "score": 0},
{"title": "Scoring Nutch results", "view_count": 336, "is_answered": false, "answers": [{"question_id": 16042729, "owner": {"user_id": 2225199, "link": "http://stackoverflow.com/users/2225199/nimeshjm", "user_type": "registered", "reputation": 1137}, "body": "<p>What is your criteria for boosting the results?</p>\n\n<p>SOLR already does a good job at calculating a documents relevancy based on the frequency the terms are present.</p>\n\n<p>What are your specific requirements that are not covered by the default set up?</p>\n", "creation_date": 1368563144, "is_accepted": false, "score": 0, "last_activity_date": 1368563144, "answer_id": 16552232}], "question_id": 16042729, "tags": ["nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/16042729/scoring-nutch-results", "last_activity_date": 1368563144, "owner": {"user_id": 1471836, "answer_count": 3, "creation_date": 1340275475, "accept_rate": 11, "view_count": 24, "reputation": 51}, "body": "<p>I am crawling using Nutch 1.2 by providing seed links to it for travel domain. Next i am indexing using Solr 3.1. I am getting the search results in my serach engine. But now i want to score the indexed results and display them in the search engine.\nI have reffered the URLS: 1) <a href=\"http://wiki.apache.org/solr/QueryElevationComponent\" rel=\"nofollow\">http://wiki.apache.org/solr/QueryElevationComponent</a> which is basically for boosting the queries.\n2) <a href=\"http://wiki.apache.org/solr/SolrRelevancyFAQ#index-time_boosts\" rel=\"nofollow\">http://wiki.apache.org/solr/SolrRelevancyFAQ#index-time_boosts</a> which is for boosting the documents.\nHow do i boost the results at the index time and retrieve the results??</p>\n\n<p>Thanks in advance!</p>\n", "creation_date": 1366131170, "score": 0},
{"title": "Nutch can crawl all website.Is there any rule to crawl specific site.Does need permission before start crawling from specific site.?", "view_count": 347, "is_answered": false, "answers": [{"question_id": 16493857, "owner": {"user_id": 2225199, "link": "http://stackoverflow.com/users/2225199/nimeshjm", "user_type": "registered", "reputation": 1137}, "body": "<p>In Nutch you can configure how many concurrent requests you can send to a specific host.</p>\n\n<p>You can override this property in your conf/nutch-site.xml file.\nBy default Nutch will only send a request every 5 seconds to a server.</p>\n\n<pre><code>&lt;property&gt;\n  &lt;name&gt;fetcher.server.delay&lt;/name&gt;\n  &lt;value&gt;5.0&lt;/value&gt;\n  &lt;description&gt;The number of seconds the fetcher will delay between \n   successive requests to the same server.&lt;/description&gt;\n&lt;/property&gt;\n</code></pre>\n\n<p>In the robots.txt file you configure the exclusions of the website for specific user agent strings, not the other way around. \nIf the site owner has not specifically disabled site access to search engines, you can crawl it with Nutch.</p>\n\n<p>e.g. Exclusion of all search bots:</p>\n\n<pre><code>User-Agent: *\nDisallow: /\n</code></pre>\n", "creation_date": 1368561281, "is_accepted": false, "score": 0, "last_activity_date": 1368561281, "answer_id": 16551754}], "question_id": 16493857, "tags": ["web-crawler", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/16493857/nutch-can-crawl-all-website-is-there-any-rule-to-crawl-specific-site-does-need-p", "last_activity_date": 1368561281, "owner": {"user_id": 2372130, "view_count": 0, "answer_count": 0, "creation_date": 1368247105, "reputation": 6}, "body": "<p>Using crawler we can search any specific site but this will slow the bandwidth of that site.Is there any rule before crawl specific site or using nutch we can search any site without an issue.I would like  to create vertical search using Nutch.Can anyone help me out from this question on the basis of above question section?</p>\n\n<p>If specific site have not allowed Nutch bot in robots.txt then how can search that site using Nutch?Do we need to take permission before.</p>\n", "creation_date": 1368247756, "score": 0},
{"title": "unable to parse flv and epub file contents using nutch", "view_count": 163, "owner": {"user_id": 2353439, "answer_count": 0, "creation_date": 1367816898, "accept_rate": 85, "view_count": 23, "reputation": 81}, "is_answered": true, "answers": [{"question_id": 16518004, "owner": {"user_id": 2225199, "link": "http://stackoverflow.com/users/2225199/nimeshjm", "user_type": "registered", "reputation": 1137}, "body": "<p>Your second command is going to insert your crawled content into the SOLR index.</p>\n\n<p>You need to run a search in SOLR to get the crawled content.\nSomething similar to:</p>\n\n<pre><code>http://127.0.0.1:8983/solr/select/?q=*%3A*&amp;version=2.2&amp;start=0&amp;rows=10&amp;indent=on\n</code></pre>\n\n<p>Can you change the \"content\" field in the schema.xml to stored=\"true\", re-crawl, re-index and post the results of the SOLR search?</p>\n", "creation_date": 1368560825, "is_accepted": true, "score": 0, "last_activity_date": 1368560825, "answer_id": 16551645}], "question_id": 16518004, "tags": ["linux", "parsing", "solr", "web-crawler", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/16518004/unable-to-parse-flv-and-epub-file-contents-using-nutch", "last_activity_date": 1368560825, "accepted_answer_id": 16551645, "body": "<p>i am working with apache nutch and solr, my requirement is to parse the contents of flv and epub files, i am using below command to parse the files</p>\n\n<pre><code>bin/nutch crawl urls -solr http://localhost:8983/solr/ \n</code></pre>\n\n<p>i have kept the file urls in urls folder of nutch. the above command is working but when i tried to view the parsed content using solr with the following command its is just displaying the url of the files.</p>\n\n<pre><code>bin/nutch solrindex http://127.0.0.1:8983/solr/ crawl/crawldb -linkdb crawl/linkdb crawl/segments/*\n</code></pre>\n\n<p>please suggest me....</p>\n\n<p>Thanks</p>\n", "creation_date": 1368434786, "score": 0},
{"title": "how to extract contents from flv file using any web crawler?", "view_count": 266, "owner": {"user_id": 2353439, "answer_count": 0, "creation_date": 1367816898, "accept_rate": 85, "view_count": 23, "reputation": 81}, "is_answered": true, "answers": [{"question_id": 16539620, "owner": {"user_id": 2225199, "link": "http://stackoverflow.com/users/2225199/nimeshjm", "user_type": "registered", "reputation": 1137}, "body": "<p>Using Nutch you can parse and extract metadata from the FLV file.\nIf the text has been added into the file as part of the metadata you can retrieve it with Nutch and put it into a database.</p>\n\n<p>But you probably should be looking at a combination of wget (to download the content) + \"a FLV stream extraction tool\" to achieve what you require.</p>\n\n<p><a href=\"http://wiki.apache.org/nutch\" rel=\"nofollow\">Nutch</a></p>\n\n<p><a href=\"http://lifehacker.com/5086682/winwget-makes-automated-downloads-a-breeze\" rel=\"nofollow\">Wget</a></p>\n\n<p><a href=\"http://www.buraks.com/flvmdv/\" rel=\"nofollow\">FLV metadata</a></p>\n", "creation_date": 1368560324, "is_accepted": true, "score": 0, "last_activity_date": 1368560324, "answer_id": 16551480}], "question_id": 16539620, "tags": ["parsing", "flv", "web-crawler", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/16539620/how-to-extract-contents-from-flv-file-using-any-web-crawler", "last_activity_date": 1368560324, "accepted_answer_id": 16551480, "body": "<p>My requirement is to extract text and audio from a flv file. please suggest me how can i achieve this using any web crawler. if it is not possible with web crawler please suggest me any other tool.</p>\n\n<p>Thankyou</p>\n", "creation_date": 1368523883, "score": 0},
{"title": "Apache Nutch Command Unable to Execute", "view_count": 848, "is_answered": false, "answers": [{"question_id": 16521582, "owner": {"user_id": 1736002, "accept_rate": 60, "link": "http://stackoverflow.com/users/1736002/cguzel", "user_type": "registered", "reputation": 576}, "body": "<p>you can try as follow:</p>\n\n<p>First of all, build nutch via ant. </p>\n\n<p><code>cd nutch-1.x.x/runtime/local/</code> </p>\n\n<p><code>mkdir urls</code> (for seed list directory)</p>\n\n<p><code>mkdir crawl</code> (for <code>-dir</code> option)</p>\n\n<p><code>vim urls/seed</code> , then you add one or more than one url (ex:<a href=\"http://www.examplesite.com\" rel=\"nofollow\">http://www.examplesite.com</a>)</p>\n\n<p><code>bin/nutch crawl urls</code> --or-- <code>bin/nutch crawl urls -dir crawl -depth 3 -topN 5</code></p>\n", "creation_date": 1368534559, "is_accepted": false, "score": 0, "last_activity_date": 1368534559, "answer_id": 16543171}, {"question_id": 16521582, "owner": {"user_id": 1462968, "accept_rate": 60, "link": "http://stackoverflow.com/users/1462968/debopam", "user_type": "registered", "reputation": 825}, "body": "<p>After some research I figured out that I forgot to set the NUTCH_JAVA_HOME.\nHere is the step:</p>\n\n<pre><code>set NUTCH_JAVA_HOME=/System/Library/Frameworks/JavaVM.framework/Versions/1.6/Home\nexport NUTCH_JAVA_HOME\n</code></pre>\n\n<p>And yes I reset the JAVA_HOME as well:</p>\n\n<pre><code>set JAVA_HOME=/System/Library/Frameworks/JavaVM.framework/Versions/1.6/Home\nexport JAVA_HOME\n</code></pre>\n", "creation_date": 1368548753, "is_accepted": false, "score": 0, "last_activity_date": 1368548753, "answer_id": 16548260}], "question_id": 16521582, "tags": ["apache", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/16521582/apache-nutch-command-unable-to-execute", "last_activity_date": 1368548753, "owner": {"age": 28, "answer_count": 18, "creation_date": 1340003403, "user_id": 1462968, "accept_rate": 60, "view_count": 154, "location": "Kolkata, India", "reputation": 825}, "body": "<p>I followed each and every step in the <a href=\"http://wiki.apache.org/nutch/NutchTutorial\" rel=\"nofollow\" title=\"Apache Nutch Wiki\">Apache Nutch Wiki</a>. I am using MacOSX 10.8.3, my <code>JAVA_HOME</code> is perfectly set and can even see various command options when <code>bin/nutch</code> is executed (according to the wiki). </p>\n\n<p>But when I use <code>bin/nutch crawl urls -dir crawl -depth 3 -topN 5</code>, I get the following error: </p>\n\n<pre><code>bin/nutch: line 104: [: too many arguments\nError: Could not find or load main class Engines\n</code></pre>\n\n<p>FYI: I have already created a <code>urls</code> directory in <code>apache-nutch-1.6/urls</code></p>\n\n<p>Can any one tell what might be the problem?</p>\n", "creation_date": 1368446518, "score": 1},
{"title": "What is link graph database", "view_count": 508, "is_answered": false, "answers": [{"question_id": 16477252, "owner": {"user_id": 1736002, "accept_rate": 60, "link": "http://stackoverflow.com/users/1736002/cguzel", "user_type": "registered", "reputation": 576}, "body": "<p>Nutch is open source web-search software. It builds on Lucene Java, adding web-specifics, <strong>such as</strong> a crawler, <strong><code>a link-graph database</code></strong>, parsers for HTML and other document formats, etc. Its main feature include...</p>\n\n<p>Nutch work together with a link-graph database. If you don't know anything about graph database, you can examine <a href=\"http://www.seguetech.com/blog/2013/02/04/what-are-the-differences-between-relational-and-graph-databases\" rel=\"nofollow\">here</a>. There are more than one graph database. Ex : <a href=\"http://www.neo4j.org/\" rel=\"nofollow\">neo4j</a> ,<a href=\"http://infogrid.org/trac/\" rel=\"nofollow\">infogrid</a></p>\n", "creation_date": 1368198040, "is_accepted": false, "score": 0, "last_activity_date": 1368198040, "answer_id": 16485231}], "question_id": 16477252, "tags": ["web-crawler", "nutch", "graph-databases", "apache-tika"], "answer_count": 1, "link": "http://stackoverflow.com/questions/16477252/what-is-link-graph-database", "last_activity_date": 1368198040, "owner": {"user_id": 2235661, "answer_count": 15, "creation_date": 1364898381, "accept_rate": 62, "view_count": 143, "reputation": 473}, "body": "<p>While checking documentation of some of the open source web crawlers like Apache Nutch, Apache Tika; I frequently encountered \"link graph database\" term. Can anyone summarize about it? </p>\n", "creation_date": 1368170989, "score": 0},
{"title": "tika installation", "view_count": 2878, "owner": {"user_id": 1535518, "view_count": 16, "answer_count": 0, "creation_date": 1342629273, "reputation": 25}, "is_answered": true, "answers": [{"last_edit_date": 1343831376, "owner": {"user_id": 633239, "accept_rate": 90, "link": "http://stackoverflow.com/users/633239/javanna", "user_type": "registered", "reputation": 38114}, "body": "<p>There are basically two ways to index binary documents within Solr, both with Tika:</p>\n\n<ol>\n<li>Using Tika on the client side to extract information from binary files and then manually indexing the extracted text within Solr</li>\n<li>Using <a href=\"http://wiki.apache.org/solr/ExtractingRequestHandler\" rel=\"nofollow\">ExtractingRequestHandler</a> through which you can upload the binary file to the Solr server so that Solr can do the work for you. This way tika is not required on the client side.</li>\n</ol>\n\n<p>In both cases you need to have the binary documents on the client side. While crawling, nutch should be able to download binary files, use Tika to generate text content out of them and then index data in Solr as it'd normally do with text documents. Nutch already uses <a href=\"https://issues.apache.org/jira/browse/NUTCH-766\" rel=\"nofollow\">Tika</a>, I guess it's just a matter of configuring the type of documents you want to index changing the regex-urlfilter.txt nutch config file by removing from the following lines the file extensions that you want to index.</p>\n\n<pre><code># skip some suffixes\n-\\.(swf|SWF|doc|DOC|mp3|MP3|WMV|wmv|txt|TXT|rtf|RTF|avi|AVI|m3u|M3U|flv|FLV|WAV|wav|mp4|MP4|avi|AVI|rss|RSS|xml|XML|pdf|PDF|js|JS|gif|GIF|jpg|JPG|png|PNG|ico|ICO|css|sit|eps|wmf|zip|ppt|mpg|xls|gz|rpm|tgz|mov|MOV|exe|jpeg|JPEG|bmp|BMP)$\n</code></pre>\n\n<p>This way you would use the first option I mentioned. Then you need to enable the Tika plugin on nutch within your nutch-site.xml, have a look at <a href=\"http://lucene.472066.n3.nabble.com/Using-Tika-to-crawl-doc-pdf-etc-tt603220.html#a603223\" rel=\"nofollow\">this discussion</a> from the nutch mailing list.</p>\n\n<p>This should theoretically work, let me know if it doesn't.</p>\n", "question_id": 11756852, "creation_date": 1343820147, "is_accepted": true, "score": 3, "last_activity_date": 1343831376, "answer_id": 11758148}], "question_id": 11756852, "tags": ["solr", "nutch", "apache-tika"], "answer_count": 1, "link": "http://stackoverflow.com/questions/11756852/tika-installation", "last_activity_date": 1368197200, "accepted_answer_id": 11758148, "body": "<p>I integrated Tika with Solr following the instructions provided in <a href=\"http://www.abcseo.com/tech/search/integrating-solr-and-tika\" rel=\"nofollow\">this link</a>  <br> <br></p>\n\n<p><strong>Correct me if I am wrong</strong>, it seems to me that it can index the document files(pdf,doc,audio) located on my own system (given the path of directory in which those files are stored), but cannot index those files, located on internet, when I crawl some sites using nutch.\n<br><br>\nCan I index the documents files(pdf,audio,doc,zip) located on the web using Tika?</p>\n", "creation_date": 1343815051, "score": 1},
{"title": "Apache Nutch 2.1 - Skipping http://someurl.com/something.html; different batch id (null)", "view_count": 178, "is_answered": false, "answers": [{"question_id": 16195904, "owner": {"user_id": 1736002, "accept_rate": 60, "link": "http://stackoverflow.com/users/1736002/cguzel", "user_type": "registered", "reputation": 576}, "body": "<p>I think, the message is not problem. batch_id not assigned to all of url. So, if batch_id is null , skip url. Generate url when batch_id assined for url.</p>\n\n<p>There is a similar question <a href=\"http://stackoverflow.com/questions/14828438/apache-nutch-2-1-different-batch-id-null/16079418#16079418\">here</a>.</p>\n", "creation_date": 1367105029, "is_accepted": false, "score": 0, "last_activity_date": 1367105029, "answer_id": 16257918}], "question_id": 16195904, "tags": ["java", "apache", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/16195904/apache-nutch-2-1-skipping-http-someurl-com-something-html-different-batch-i", "last_activity_date": 1367677624, "owner": {"age": 27, "answer_count": 17, "creation_date": 1332257106, "user_id": 1281305, "accept_rate": 88, "view_count": 129, "location": "Prague, Czech Republic", "reputation": 494}, "body": "<p>I crawl few sites with Apache Nutch 2.1.</p>\n\n<p>While crawling I see the following message on lot of pages:\nex. Skipping <a href=\"http://videos.arte.tv/fr/videos/x-enius--7453284.html\" rel=\"nofollow\">http://videos.arte.tv/fr/videos/x-enius--7453284.html</a>; different batch id (null)</p>\n\n<p>What causes this error? It is while parsing phase...\nHow can I resolve this problem, I use HBase to store the pages.</p>\n\n<p>My urls in regex-urlfilter.txt looks like</p>\n\n<ul>\n<li>+^http://([a-z0-9]*.)*videos.arte.tv/</li>\n<li>+^http://([a-z0-9]*.)*rbb-online.de/</li>\n</ul>\n\n<p>but I also tried with same effect</p>\n\n<ul>\n<li>+^<a href=\"http://videos.arte.tv/\" rel=\"nofollow\">http://videos.arte.tv/</a></li>\n<li>+^<a href=\"http://www.rbb-online.de/\" rel=\"nofollow\">http://www.rbb-online.de/</a></li>\n</ul>\n\n<p><strong>EDIT:</strong> It was added to track by maintainer of mailing list like bug, you can read here <a href=\"http://lucene.472066.n3.nabble.com/Nutch-2-1-different-batch-id-null-td4040592.html#a4059636\" rel=\"nofollow\">http://lucene.472066.n3.nabble.com/Nutch-2-1-different-batch-id-null-td4040592.html#a4059636</a></p>\n", "creation_date": 1366816936, "score": 1},
{"title": "Using SearchQuerySet haystack API without django models and nutch", "view_count": 527, "is_answered": false, "question_id": 11084729, "tags": ["django", "nutch", "django-haystack"], "answer_count": 0, "link": "http://stackoverflow.com/questions/11084729/using-searchqueryset-haystack-api-without-django-models-and-nutch", "last_activity_date": 1367576710, "owner": {"user_id": 1443094, "answer_count": 1, "creation_date": 1339098158, "accept_rate": 75, "view_count": 22, "reputation": 25}, "body": "<p>Morning all,</p>\n\n<p>This past week I have been experimenting with different search engines and indexing tools and have come across a problem. I have a django site that is all static pages. I have used nutch to scrape the site and send the results to a solr index. I would like to use haystack for getting the results of the search, but I have run into a few problems.</p>\n\n<p>The first problem is that I do not have any models in the site, and therefor the tutorial that haystack has set up is useless. I already have the index set up in solr and would just like to use the SearchQuerySet API to search it.</p>\n\n<p>The first thing I tried was adding these lines to the solr index to get it to work:</p>\n\n<pre><code>    &lt;field name=\"id\" type=\"string\" stored=\"true\" indexed=\"true\" multiValued=\"false\" required=\"true\"/&gt;\n    &lt;field name=\"django_ct\" type=\"string\" indexed=\"true\" stored=\"true\" multiValued=\"false\"/&gt;\n    &lt;field name=\"django_id\" type=\"string\" indexed=\"true\" stored=\"true\" multiValued=\"false\"/&gt;\n</code></pre>\n\n<p>However, after that, when I try to use the API to search, I get the following error on the django page:</p>\n\n<pre><code>   \"KeyError at /search/\" 'django_ct'\n</code></pre>\n\n<p>I've heard from people, that you have to add those keys to the schema.xml in solr in order to be able to use the SearchQuerySet without having models.</p>\n\n<p>If someone could provide further information or help on what I should do next, that would be great.</p>\n\n<p>Thanks</p>\n\n<p>P.S here is my traceback</p>\n\n<pre><code>  Traceback:\n  File \"/home/dave/virtualenvs/stuff/local/lib/python2.7/site-packages/django/core/handlers/base.py\" in get_response\n  136.                     response = response.render()\n  File \"/home/dave/virtualenvs/stuff/local/lib/python2.7/site-packages/django/template/response.py\" in render\n  104.             self._set_content(self.rendered_content)\n  File \"/home/dave/virtualenvs/stuff/local/lib/python2.7/site-packages/django/template/response.py\" in rendered_content\n  81.         content = template.render(context)\n  File \"/home/dave/virtualenvs/stuff/local/lib/python2.7/site-packages/django/template/base.py\" in render\n  140.             return self._render(context)\n  File \"/home/dave/virtualenvs/stuff/local/lib/python2.7/site-packages/django/template/base.py\" in _render\n  134.         return self.nodelist.render(context)\n  File \"/home/dave/virtualenvs/stuff/local/lib/python2.7/site-packages/django/template/base.py\" in render\n  823.                 bit = self.render_node(node, context)\n  File \"/home/dave/virtualenvs/stuff/local/lib/python2.7/site-packages/django/template/base.py\" in render_node\n  837.         return node.render(context)\n  File \"/home/dave/virtualenvs/stuff/local/lib/python2.7/site-packages/django/template/loader_tags.py\" in render\n  123.         return compiled_parent._render(context)\n  File \"/home/dave/virtualenvs/stuff/local/lib/python2.7/site-packages/django/template/base.py\" in _render\n  134.         return self.nodelist.render(context)\n  File \"/home/dave/virtualenvs/stuff/local/lib/python2.7/site-packages/django/template/base.py\" in render\n  823.                 bit = self.render_node(node, context)\n  File \"/home/dave/virtualenvs/stuff/local/lib/python2.7/site-packages/django/template/base.py\" in render_node\n  837.         return node.render(context)\n  File \"/home/dave/virtualenvs/stuff/local/lib/python2.7/site-packages/django/template/loader_tags.py\" in render\n  123.         return compiled_parent._render(context)\n  File \"/home/dave/virtualenvs/stuff/local/lib/python2.7/site-packages/django/template/base.py\" in _render\n  134.         return self.nodelist.render(context)\n  File \"/home/dave/virtualenvs/stuff/local/lib/python2.7/site-packages/django/template/base.py\" in render\n  823.                 bit = self.render_node(node, context)\n  File \"/home/dave/virtualenvs/stuff/local/lib/python2.7/site-packages/django/template/base.py\" in render_node\n  837.         return node.render(context)\n  File \"/home/dave/virtualenvs/stuff/local/lib/python2.7/site-packages/django/template/loader_tags.py\" in render\n  62.             result = block.nodelist.render(context)\n  File \"/home/dave/virtualenvs/stuff/local/lib/python2.7/site-packages/django/template/base.py\" in render\n  823.                 bit = self.render_node(node, context)\n  File \"/home/dave/virtualenvs/stuff/local/lib/python2.7/site-packages/django/template/base.py\" in render_node\n  837.         return node.render(context)\n  File \"/home/dave/virtualenvs/stuff/local/lib/python2.7/site-packages/django/template/loader_tags.py\" in render\n  62.             result = block.nodelist.render(context)\n  File \"/home/dave/virtualenvs/stuff/local/lib/python2.7/site-packages/django/template/base.py\" in render\n  823.                 bit = self.render_node(node, context)\n  File \"/home/dave/virtualenvs/stuff/local/lib/python2.7/site-packages/django/template/base.py\" in render_node\n  837.         return node.render(context)\n  File \"/home/dave/virtualenvs/stuff/local/lib/python2.7/site-packages/django/template/loader_tags.py\" in render\n  62.             result = block.nodelist.render(context)\n  File \"/home/dave/virtualenvs/stuff/local/lib/python2.7/site-packages/django/template/base.py\" in render\n  823.                 bit = self.render_node(node, context)\n  File \"/home/dave/virtualenvs/stuff/local/lib/python2.7/site-packages/django/template/base.py\" in render_node\n  837.         return node.render(context)\n  File \"/home/dave/virtualenvs/stuff/local/lib/python2.7/site-packages/django/template/defaulttags.py\" in render\n  145.         len_values = len(values)\n  File \"/home/dave/virtualenvs/stuff/src/stuff/haystack/query.py\" in __len__\n  84.             self._result_count = self.query.get_count()\n  File \"/home/dave/virtualenvs/stuff/src/stuff/haystack/backends/__init__.py\" in get_count\n  459.                 self.run()\n  File \"/home/dave/virtualenvs/stuff/src/stuff/haystack/backends/solr_backend.py\" in run\n  667.         results = self.backend.search(final_query, **search_kwargs)\n  File \"/home/dave/virtualenvs/stuff/src/stuff/haystack/backends/__init__.py\" in wrapper\n  27.             return func(obj, query_string, *args, **kwargs)\n  File \"/home/dave/virtualenvs/stuff/src/stuff/haystack/backends/solr_backend.py\" in search\n  257.         return self._process_results(raw_results, highlight=highlight, result_class=result_class, distance_point=distance_point)\n  File \"/home/dave/virtualenvs/stuff/src/stuff/haystack/backends/solr_backend.py\" in _process_results\n  355.             app_label, model_name = raw_result[DJANGO_CT].split('.')\n</code></pre>\n\n<p>Exception Type: KeyError at /search/\nException Value: 'django_ct'</p>\n", "creation_date": 1340028860, "score": 2},
{"title": "Nutch didn&#39;t crawl all URLs from the seed.txt", "view_count": 2275, "is_answered": true, "answers": [{"question_id": 13063337, "owner": {"user_id": 950178, "link": "http://stackoverflow.com/users/950178/jayendra", "user_type": "registered", "reputation": 34896}, "body": "<p>Check out the property <code>db.max.outlinks.per.page</code> in the nutch configuration files.<br>\nThe default value for this property is 100 and hence only 100 urls will be picked up from the seeds.txt and rest would be skipped.<br>\nChange this value to a higher number to have all the urls scanned and indexed.</p>\n", "creation_date": 1351148988, "is_accepted": false, "score": 3, "last_activity_date": 1351148988, "answer_id": 13063385}, {"question_id": 13063337, "owner": {"user_id": 1977874, "link": "http://stackoverflow.com/users/1977874/davseq", "user_type": "registered", "reputation": 1}, "body": "<p>topN indicates how many of the generated links should be fetched. You could have 100 links which have been generated , but if you set topN as 12, then only 12 of those links will get fetched, parsed and indexed.</p>\n", "creation_date": 1367480241, "is_accepted": false, "score": 0, "last_activity_date": 1367480241, "answer_id": 16332727}], "question_id": 13063337, "tags": ["solr", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/13063337/nutch-didnt-crawl-all-urls-from-the-seed-txt", "last_activity_date": 1367480241, "owner": {"user_id": 1773304, "answer_count": 0, "creation_date": 1351146931, "accept_rate": 0, "view_count": 3, "reputation": 16}, "body": "<p>I am new to Nutch and Solr. Currently I would like to crawl a website and its content is </p>\n\n<p>generated by ASP. Since the content is not static, I created a seed.txt which </p>\n\n<p>contained all the URLs I would like to crawl. For example:</p>\n\n<pre><code>http://us.abc.com/product/10001\nhttp://us.abc.com/product/10002\nhttp://jp.abc.com/product/10001\nhttp://jp.abc.com/product/10002\n...\n</code></pre>\n\n<p>The regex-urlfilter.txt has this filter:</p>\n\n<pre><code># accept anything else\n#+.\n+^http://([a-z0-9]*\\.)*abc.com/\n</code></pre>\n\n<p>I used this command to start the crawling:</p>\n\n<pre><code>/bin/nutch crawl urls -solr http://abc.com:8983/solr/ -dir crawl -depth 10 -topN 10\n</code></pre>\n\n<p>The seed.txt content 40,000+ URLs. However, I found that many of the URLs content are not </p>\n\n<p>able to be found by Solr.</p>\n\n<p>Question:</p>\n\n<ol>\n<li><p>Is this approach for a large seed.txt workable ?</p></li>\n<li><p>How can I check a URL was being crawlered ?</p></li>\n<li><p>Is seed.txt has a size limitation ?</p></li>\n</ol>\n\n<p>Thank you !</p>\n", "creation_date": 1351148797, "score": 2},
{"title": "Nutch does not crawl all links in form", "view_count": 1008, "is_answered": true, "answers": [{"last_edit_date": 1367442321, "owner": {"user_id": 1539623, "accept_rate": 100, "link": "http://stackoverflow.com/users/1539623/robin-rieger", "user_type": "registered", "reputation": 764}, "body": "<p><strong>Sorry, too low rep to post comment!!!</strong></p>\n\n<p>Have you got a link.</p>\n\n<p>Also are the drop downs ajax or something fancy. Nutch from memory will only crawl what is on the page. I.e. if you load the first 10 on page load and the only load the rest with a service when the user scrolls I believe it can't find that. </p>\n\n<p>Some more info would be good re the page.... </p>\n\n<p>Cheers\nRobin</p>\n", "question_id": 12796858, "creation_date": 1350095905, "is_accepted": false, "score": 1, "last_activity_date": 1367442321, "answer_id": 12869476}, {"question_id": 12796858, "owner": {"user_id": 1731155, "accept_rate": 0, "link": "http://stackoverflow.com/users/1731155/hayk", "user_type": "registered", "reputation": 291}, "body": "<p>thanks for your answer. This is the [link] (auto.am/en), after crawl I have only around 100 makes and not all models from car makes that I have. ... I hope that after you have got a link you will suggest the solution to crawl all cars makes and models :). Thanks.</p>\n", "creation_date": 1350373442, "is_accepted": false, "score": 0, "last_activity_date": 1350373442, "answer_id": 12909795}], "question_id": 12796858, "tags": ["apache", "solr", "nutch", "web-crawler"], "answer_count": 2, "link": "http://stackoverflow.com/questions/12796858/nutch-does-not-crawl-all-links-in-form", "last_activity_date": 1367442321, "owner": {"user_id": 1731155, "answer_count": 2, "creation_date": 1349772488, "accept_rate": 0, "view_count": 21, "reputation": 291}, "body": "<p>I have problem to crawling my site...there is a form with two drop-down lists....and when I start crawl , the crawler fetch only part of links from form....from first drop-down list it takes part of options, as from second drop-down....I try  change some configurations in nutch-defaults.xml file, but everything is the same...</p>\n\n<pre><code>I change \nfetcher.threads.per.queue  1 - 10         \ndb.ignore.internal.links true - false  \ndb.ignore.external.links false - true  \nhttp.content.limit    65536 - 65536000  \nfile.content.limit    65536 - 65536000  \ndb.update.max.inlinks  10.000 - 100.000\n</code></pre>\n\n<p>is there any other option, that can help me to crawl all options in my form......??\nThanks for answers.</p>\n", "creation_date": 1349774780, "score": 5},
{"title": "Properly importing Apache nutch to eclipse EE juno using SVN", "view_count": 431, "is_answered": false, "answers": [{"question_id": 15321408, "owner": {"user_id": 1977874, "link": "http://stackoverflow.com/users/1977874/davseq", "user_type": "registered", "reputation": 1}, "body": "<p>Refer to <a href=\"http://wiki.apache.org/nutch/RunNutchInEclipse\" rel=\"nofollow\">http://wiki.apache.org/nutch/RunNutchInEclipse</a></p>\n\n<p>These should be followed to the \"T\" and it works fine. </p>\n\n<p>Specially, pay attention to the manual configuration  of the build paths for the plugins.</p>\n\n<p>You may also notice that some library dependancies are not set even after setting the build path,within certain plugins. .. you will need to manually add the ivey dependancies for these plugins into the build patch as well.</p>\n", "creation_date": 1367320359, "is_accepted": false, "score": 0, "last_activity_date": 1367320359, "answer_id": 16298484}], "question_id": 15321408, "tags": ["java", "eclipse", "apache", "svn", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/15321408/properly-importing-apache-nutch-to-eclipse-ee-juno-using-svn", "last_activity_date": 1367320359, "owner": {"user_id": 739093, "answer_count": 1, "creation_date": 1304564494, "accept_rate": 61, "view_count": 47, "reputation": 157}, "body": "<p>I have imported apache nutch from <a href=\"http://svn.apache.org/repos/asf/nutch/trunk/\" rel=\"nofollow\">http://svn.apache.org/repos/asf/nutch/trunk/</a> through SVN on eclipse; however, the project I have imported has 10k+ java errors:</p>\n\n<ol>\n<li><p>Some errors are due to source files being in a package different from what is specified in their source, (e.g. classes in java.org.apache... but it is declared in source that they should be in org.apache..., this is observed in many other packages)</p></li>\n<li><p>Unimported classes, for example the Class \"Context\" is used in many of the classes however when looking at the import list, \"Context\" is not declared there.</p></li>\n<li><p>Missing classes, though imported, classes still not resolved to a type, most probably because my imported project lacks many of the needed libraries such as hadoop,gora,witty,etc.</p></li>\n<li><p>(Not familiar with this) Bound mismatch errors like: <strong>The generic method createDataStore(Configuration, Class, Class) of type StorageUtils is not applicable for the arguments (Configuration, Class, Class). The inferred type WebPage is not a valid substitute for the bounded parameter </strong></p></li>\n</ol>\n\n<p>What is the correct way of importing a nutch through eclipse without encountering  the errors specified above? Thanks!</p>\n", "creation_date": 1362912619, "score": 1},
{"title": "Can any one explain briefly about the link analysis in nutch", "view_count": 149, "is_answered": false, "answers": [{"question_id": 9889606, "owner": {"user_id": 868892, "link": "http://stackoverflow.com/users/868892/ajay-prakash", "user_type": "registered", "reputation": 640}, "body": "<p>The Nutch Link analysis details can be found here\n<a href=\"https://wiki.apache.org/nutch/NewScoring\" rel=\"nofollow\">https://wiki.apache.org/nutch/NewScoring</a></p>\n", "creation_date": 1367107193, "is_accepted": false, "score": 0, "last_activity_date": 1367107193, "answer_id": 16258148}], "question_id": 9889606, "tags": ["nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/9889606/can-any-one-explain-briefly-about-the-link-analysis-in-nutch", "last_activity_date": 1367107193, "owner": {"user_id": 989562, "answer_count": 1, "creation_date": 1318338439, "accept_rate": 56, "view_count": 198, "reputation": 742}, "body": "<p>Can any one explain briefly about the link analysis in nutch and give some use full links regarding link analysis.</p>\n", "creation_date": 1332851671, "score": 1},
{"title": "Nutch regex doesn&#39;t crawl the way I want it to", "view_count": 1097, "owner": {"age": 31, "answer_count": 22, "creation_date": 1336360349, "user_id": 1378880, "accept_rate": 62, "view_count": 324, "location": "Irvine, CA", "reputation": 620}, "is_answered": true, "answers": [{"last_edit_date": 1366393248, "owner": {"user_id": 1181624, "link": "http://stackoverflow.com/users/1181624/koda", "user_type": "registered", "reputation": 477}, "body": "<p>You've got a / at the end of both of your regexes but your URL doesn't.</p>\n\n<p><code>http://tigerdirect.com/</code> will match, <code>http://tigerdirect.com</code> will not.</p>\n\n<p><code>+^http://tigerdirect.com/([a-z0-9]*\\.)*</code>\nTry moving that tailing slash inside the parens\n<code>+^http://tigerdirect.com(/[a-z0-9]*\\.)*</code></p>\n", "question_id": 16109633, "creation_date": 1366392944, "is_accepted": false, "score": 1, "last_activity_date": 1366393248, "answer_id": 16110405}, {"question_id": 16109633, "owner": {"user_id": 453596, "accept_rate": 77, "link": "http://stackoverflow.com/users/453596/kamaci", "user_type": "registered", "reputation": 21412}, "body": "<p>According to your comments I see that you have crawled something before and this is why your Nutch starts to crawl Wikipedia. </p>\n\n<p>When you crawl something with Nutch it records some metada at a table (if you use Hbase it is a table named webpage) When you finish a crawling and start a new one that table is scanned and if there is a record that has a metada says \"this record can be fetched again because next fetch time is passed\" Nutch starts to fetch that urls and also your new urls.</p>\n\n<p>So if you want to have just <a href=\"http://www.tigerdirect.com/\" rel=\"nofollow\">http://www.tigerdirect.com/</a> crawled at your system you have to clean up that table first. If you use Hbase start shell:</p>\n\n<pre><code>./bin/hbase shell\n</code></pre>\n\n<p>and disable table:</p>\n\n<pre><code>disable 'webpage'\n</code></pre>\n\n<p>and finally drop it:</p>\n\n<pre><code>drop 'webpage'\n</code></pre>\n\n<p>I could truncate that table but removed it.</p>\n\n<p>Next thing is putting that into your seed.txt:</p>\n\n<pre><code>http://www.tigerdirect.com/\n</code></pre>\n\n<p>open regex-urlfilter.txt that is located at:</p>\n\n<pre><code>nutch/runtime/local/conf\n</code></pre>\n\n<p>write that line into it:</p>\n\n<pre><code>+^http://([a-z0-9]*\\.)*www.tigerdirect.com/([a-z0-9]*\\.)*\n</code></pre>\n\n<p>you will put that line instead of <code>+.</code></p>\n\n<p>I have indicated to crawl subdomains of tigerdirect, it is up to you.</p>\n\n<p>After that you can send it into solr to index and make a search on it. I have tried it and works however you may have some errors at Nutch side but it is another topic to talk about.</p>\n", "creation_date": 1366715223, "is_accepted": true, "score": 4, "last_activity_date": 1366715223, "answer_id": 16167769}], "question_id": 16109633, "tags": ["regex", "search-engine", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/16109633/nutch-regex-doesnt-crawl-the-way-i-want-it-to", "last_activity_date": 1366715223, "accepted_answer_id": 16167769, "body": "<p>Ok, I asked this already, but I guess I didn't ask it to the way stackoverflow expects. Hopefully I will get more luck this time and an answer.</p>\n\n<p>I am trying to run nutch to crawl this site: <a href=\"http://www.tigerdirect.com/\" rel=\"nofollow\">http://www.tigerdirect.com/</a></p>\n\n<p>I want it to crawl that site and all sublinks.</p>\n\n<p>The problem is its not working. In my reg-ex file I tried a couple of things, but none of them worked:</p>\n\n<pre><code>+^http://([a-z0-9]*\\.)*tigerdirect.com/\n\n+^http://tigerdirect.com/([a-z0-9]*\\.)*\n</code></pre>\n\n<p>my urls.txt is:</p>\n\n<pre><code>http://tigerdirect.com\n</code></pre>\n\n<p>Basically what I am trying to accomplish is to crawl all the product pages on their website so I can create a search engine (I am using solr) of electronic products. Eventually I want to crawl bestbuy.com, newegg.com and other sites as well. </p>\n\n<p>BTW, I followed the tutorial from here: <a href=\"http://wiki.apache.org/nutch/NutchTutorial\" rel=\"nofollow\">http://wiki.apache.org/nutch/NutchTutorial</a> and I am using the script mentioned in session 3.3 (after fixing a bug it had).</p>\n\n<p>I have a background in java and android and bash so this is a little new to me. I used to do regex in perl 5 years ago, but that is all forgotten.</p>\n\n<p>Thanks!</p>\n", "creation_date": 1366390009, "score": 2},
{"title": "nutch crawler: db_unfetched a large number and no fetching is done", "view_count": 136, "is_answered": false, "question_id": 15874079, "tags": ["nutch", "web-crawler"], "answer_count": 0, "link": "http://stackoverflow.com/questions/15874079/nutch-crawler-db-unfetched-a-large-number-and-no-fetching-is-done", "last_activity_date": 1366597285, "owner": {"user_id": 1240022, "answer_count": 6, "creation_date": 1330512055, "accept_rate": 95, "view_count": 65, "reputation": 273}, "body": "<p>I am using nutch to crawl and download content from wikipedia from a certain domain. In the log I see that around 3000 urls are fetched But when I look at the stats, db_fetched is just 81 while db_unfetched is ~2900.\nIn the log no exception occurs while downloading but somehow the urls are not downloaded to my computer. I have set the topN to 50000 and depth to 50.</p>\n", "creation_date": 1365408010, "score": 1},
{"title": "nutch vs solr indexing", "view_count": 3585, "owner": {"user_id": 1204751, "answer_count": 10, "creation_date": 1329026289, "accept_rate": 76, "view_count": 49, "reputation": 283}, "is_answered": true, "answers": [{"last_edit_date": 1366498389, "owner": {"user_id": 633239, "accept_rate": 90, "link": "http://stackoverflow.com/users/633239/javanna", "user_type": "registered", "reputation": 38114}, "body": "<p>Having a look <a href=\"http://wiki.apache.org/nutch/NutchTutorial#A3.2_Using_Individual_Commands_for_Whole-Web_Crawling\" rel=\"nofollow\">here</a> might be useful.\nWhen you run the first command: </p>\n\n<pre><code>bin/nutch crawl urls -dir crawl -depth 3 -topN 5\n</code></pre>\n\n<p>you're crawling, which means that nutch will create its own internal data, composed of:</p>\n\n<ul>\n<li>the crawldb</li>\n<li>the linkdb</li>\n<li>a set of segments</li>\n</ul>\n\n<p>you can see them in the following directories, which are created while you run the crawl command:</p>\n\n<ul>\n<li>crawl/crawldb</li>\n<li>crawl/linkdb</li>\n<li>crawl/segments</li>\n</ul>\n\n<p>You can think of that data as some kind of database where nutch stores crawled data. That doesn't have anything to do with an inverted index.</p>\n\n<p>After the crawl process you can index your data on a Solr instance. You can crawl and then index running a single command, which is the second command from your question:</p>\n\n<pre><code>bin/nutch crawl urls -solr http://localhost:8983/solr/ -depth 3 -topN 5\n</code></pre>\n\n<p>Otherwise you can run a second command after the crawl command, specific for indexing to Solr, but you have to provide the path of your crawldb, linkdb and segments:</p>\n\n<pre><code>bin/nutch solrindex http://localhost:8983/solr/ crawldb -linkdb crawldb/linkdb crawldb/segments/*\n</code></pre>\n", "question_id": 10844792, "creation_date": 1338543531, "is_accepted": true, "score": 5, "last_activity_date": 1366498389, "answer_id": 10847888}, {"question_id": 10844792, "owner": {"user_id": 1454079, "link": "http://stackoverflow.com/users/1454079/john-reece", "user_type": "registered", "reputation": 46}, "body": "<p>You may be getting confused by legacy Nutch versions and associated online documentation.  Originally it created its own index and had its own web search interface.  Using Solr became an option requiring extra configuration and fiddling. Starting with 1.3 the indexing and server parts were stripped out and now it's assumed Nutch will be using Solr.</p>\n", "creation_date": 1351656630, "is_accepted": false, "score": 3, "last_activity_date": 1351656630, "answer_id": 13151150}], "question_id": 10844792, "tags": ["solr", "lucene", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/10844792/nutch-vs-solr-indexing", "last_activity_date": 1366498389, "accepted_answer_id": 10847888, "body": "<p>I have recently started working on nutch and I am trying to understand how it works. As far as I know Nutch is basically used to crawl the web and solr/Lucene is used to index and search. But when I read documentation on nutch, it says that nutch also does inverted indexing. Does it uses Lucene internally to do indexing or does it have some other library for indexing? If it uses solr/lucene for indexing then why is it necessary to configure solr with nutch as the nutch tutorial says?</p>\n\n<p>Is the indexing done by default. I mean I run this command to start crawling. Is indexing happening here?</p>\n\n<pre><code>bin/nutch crawl urls -dir crawl -depth 3 -topN 5\n</code></pre>\n\n<p>Or does indexing happen only in this case. (According to tutorial:  If you have a Solr core already set up and wish to index to it, you are required to add the -solr  parameter to your crawl command e.g.)</p>\n\n<pre><code>bin/nutch crawl urls -solr http://localhost:8983/solr/ -depth 3 -topN 5\n</code></pre>\n", "creation_date": 1338527903, "score": 8},
{"title": "Apache Nutch 2.1 different batch id (null)", "view_count": 533, "is_answered": true, "answers": [{"last_edit_date": 1366282632, "owner": {"user_id": 1736002, "accept_rate": 60, "link": "http://stackoverflow.com/users/1736002/cguzel", "user_type": "registered", "reputation": 576}, "body": "<p>I think, the message is not problem. batch_id not assigned to all of url. So, if batch_id is null , skip url. Generate url when batch_id assined for url.</p>\n", "question_id": 14828438, "creation_date": 1366277836, "is_accepted": false, "score": 1, "last_activity_date": 1366282632, "answer_id": 16079418}], "question_id": 14828438, "tags": ["apache", "nutch", "web-crawler"], "answer_count": 1, "link": "http://stackoverflow.com/questions/14828438/apache-nutch-2-1-different-batch-id-null", "last_activity_date": 1366282632, "owner": {"user_id": 1999894, "answer_count": 7, "creation_date": 1358849350, "accept_rate": 33, "view_count": 88, "reputation": 456}, "body": "<p>I crawl few sites with Apache Nutch 2.1.</p>\n\n<p>While crawling I see the following message on lot of pages:<br>\nex. Skipping <a href=\"http://www.domainname.com/news/subcategory/111111/index.html\">http://www.domainname.com/news/subcategory/111111/index.html</a>; different batch id (null).</p>\n\n<p>What causes this error ?<br>\nHow can I resolve this problem, because the pages with different batch id (null) are not stored in database.</p>\n\n<p>The site that I crawled is based on drupal, but i have tried on many others non drupal sites.</p>\n", "creation_date": 1360657981, "score": 8},
{"title": "Sites are crawled even when the URL is removed from seed.txt (Nutch 2.1)", "view_count": 647, "is_answered": true, "answers": [{"question_id": 16044906, "owner": {"user_id": 81194, "link": "http://stackoverflow.com/users/81194/claude", "user_type": "registered", "reputation": 827}, "body": "<p>Your crawl database contains a list of URLs to crawl. Unless you delete the original crawl directory or create a new one as part of your new crawl, the original list of URLs will be used and extended with the new URL.</p>\n", "creation_date": 1366215890, "is_accepted": false, "score": 2, "last_activity_date": 1366215890, "answer_id": 16065297}], "question_id": 16044906, "tags": ["nutch", "web-crawler"], "answer_count": 1, "link": "http://stackoverflow.com/questions/16044906/sites-are-crawled-even-when-the-url-is-removed-from-seed-txt-nutch-2-1", "last_activity_date": 1366215890, "owner": {"user_id": 1830069, "answer_count": 10, "creation_date": 1353080229, "accept_rate": 63, "view_count": 103, "reputation": 543}, "body": "<p>I performed a successful crawl with url-1 in seed.txt and I could see the crawled data in MySQL database. Now when I tried to perform another fresh crawl by replacing url-1 with url-2 in seed.txt, the new crawl started with fetching step and the urls it was trying to fetch is of the old replaced url in seed.txt. I am not sure from where it picked up the old url.</p>\n\n<p>I tried to check for hidden seed files, I didn't find any and there is only one folder urls/seed.txt in NUTCH_HOME/runtime/local where I run my crawl command. Please advise what might be the issue?</p>\n", "creation_date": 1366138595, "score": 0},
{"title": "org.apache.solr.common.SolrException: ERROR: [doc=SOMEURL] unknown field &#39;&#39;", "view_count": 1500, "is_answered": false, "answers": [{"last_edit_date": 1365928441, "owner": {"user_id": 1337352, "accept_rate": 32, "link": "http://stackoverflow.com/users/1337352/babu", "user_type": "registered", "reputation": 623}, "body": "<p>Well, I had to be blind. I found where was the problem. For someone who will have the similar problem here is the reason:</p>\n\n<p>In my <strong>solrindex-mapping.xml</strong> I had this:</p>\n\n<pre><code>&lt;field dest=\"video_og_title\" source=\"video_og_title\" /&gt;\n&lt;field dest=\"video_og_type\" source=\"video_og_type\"/&gt;\n&lt;field dest=\"video_og_image\" source=\"video_og_image\" /&gt;\n&lt;field name=\"video_og_url\" source=\"video_og_url\"/&gt;\n&lt;field name=\"video_og_description\" source=\"video_og_description\" /&gt;\n&lt;field name=\"video_og_video\" source=\"video_og_video\" /&gt;\n</code></pre>\n\n<p>I didn't see the field has attribute <code>name</code> and not <code>dest</code> so Solr represent the <code>dest</code> attribute, which it uses for mapping like empty field ' '.</p>\n", "question_id": 15914033, "creation_date": 1365928261, "is_accepted": false, "score": 0, "last_activity_date": 1365928441, "answer_id": 15997163}], "question_id": 15914033, "tags": ["solr", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/15914033/org-apache-solr-common-solrexception-error-doc-someurl-unknown-field", "last_activity_date": 1365928441, "owner": {"user_id": 1337352, "answer_count": 21, "creation_date": 1334613000, "accept_rate": 32, "view_count": 155, "location": "Prague, Czech Republic", "reputation": 623}, "body": "<p>Hi I am getting this exception and I've exhausted all the possible settings that I could think of.</p>\n\n<pre><code>org.apache.solr.common.SolrException: ERROR: [doc=SOMEURL] unknown field ''\n</code></pre>\n\n<p>The problem is <strong>field ''</strong> - the quotation marks are empty so I don't know what causes the problem.</p>\n\n<p>Does anybody had the same problem? I will help me a lot.</p>\n\n<p>Some informations:</p>\n\n<ul>\n<li>Nutch version 2.1 </li>\n<li>Solr version 1.5</li>\n<li>Hbase as a data storage\n-Tomcat6 for Solr running</li>\n</ul>\n\n<p>In code have just this: </p>\n\n<pre><code>nutchDocument.add(\"my_key\",stringValue);\n</code></pre>\n\n<p>I have checked Solr's schema.xml, Nutch's schema.xml and also Nutch solr-mapping.xml (I am sure in the right directories) in each is \"my_key\" written in the right way.</p>\n\n<p>Thanks for help</p>\n", "creation_date": 1365548288, "score": 0},
{"title": "Nutch bin/crawl script is failing - Manual steps work fine", "view_count": 2012, "is_answered": true, "answers": [{"question_id": 15970263, "owner": {"user_id": 682754, "accept_rate": 74, "link": "http://stackoverflow.com/users/682754/carlton", "user_type": "registered", "reputation": 1953}, "body": "<p>Looks like there was a bug with the <a href=\"https://issues.apache.org/jira/browse/NUTCH-1500\" rel=\"nofollow\">bin/crawl</a> script</p>\n\n<pre><code>-  $bin/nutch solrindex $SOLRURL $CRAWL_PATH/crawldb -linkdb $CRAWL_PATH/linkdb $SEGMENT\n+  $bin/nutch solrindex $SOLRURL $CRAWL_PATH/crawldb -linkdb $CRAWL_PATH/linkdb $CRAWL_PATH/segments/$SEGMENT\n</code></pre>\n", "creation_date": 1365766847, "is_accepted": false, "score": 1, "last_activity_date": 1365766847, "answer_id": 15970440}], "question_id": 15970263, "tags": ["apache", "shell", "solr", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/15970263/nutch-bin-crawl-script-is-failing-manual-steps-work-fine", "last_activity_date": 1365766847, "owner": {"user_id": 682754, "answer_count": 67, "creation_date": 1301427459, "accept_rate": 74, "view_count": 102, "reputation": 1953}, "body": "<p>I'm trying to run the script provided in Nutch 1.6 \"bin/crawl\" which does all of the manual steps below required to go off and spider a site.</p>\n\n<p>When I run these steps manually everything works fine and my page is indexed as expected (albeit only one page but will look into this)</p>\n\n<p>created text file containing a URL @ seeds/urls.txt</p>\n\n<pre><code>bin/nutch inject crawl_test/crawldb seeds/\n\nbin/nutch generate crawl_test/crawldb crawl_test/segments\n\nexport SEGMENT=crawl_test/segments/`ls -tr crawl_test/segments|tail -1`\n\nbin/nutch fetch $SEGMENT -noParsing\n\nbin/nutch parse $SEGMENT\n\nbin/nutch updatedb crawl_test/crawldb $SEGMENT -filter -normalize\n\nbin/nutch invertlinks crawl_test/linkdb -dir crawl_test/segments\n\nbin/nutch solrindex http://dev:8080/solr/ crawl_test/crawldb -linkdb crawl_test/linkdb crawl_test/segments/*\n</code></pre>\n\n<p>The bin/crawl script gives this error...</p>\n\n<blockquote>\n<pre><code>Indexing 20130412115759 on SOLR index -&gt; someurl:8080/solr/\nSolrIndexer: starting at 2013-04-12 11:58:47\nSolrIndexer: deleting gone documents: false\nSolrIndexer: URL filtering: false\nSolrIndexer: URL normalizing: false\norg.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/opt/nutch/20130412115759/crawl_fetch\nInput path does not exist: file:/opt/nutch/20130412115759/crawl_parse\nInput path does not exist: file:/opt/nutch/20130412115759/parse_data\nInput path does not exist: file:/opt/nutch/20130412115759/parse_text\n</code></pre>\n</blockquote>\n\n<p>Any idea why this script isn't working? I think it must be an error in the script itself rather then my config as the path it is looking for doesn't exist and not sure why it would even be looking there.</p>\n", "creation_date": 1365766376, "score": 0},
{"title": "Apache Nutch and Solr integration", "view_count": 3772, "owner": {"user_id": 682754, "answer_count": 67, "creation_date": 1301427459, "accept_rate": 74, "view_count": 102, "reputation": 1953}, "is_answered": true, "answers": [{"question_id": 15945927, "owner": {"user_id": 1628375, "link": "http://stackoverflow.com/users/1628375/femtorgon", "user_type": "registered", "reputation": 22137}, "body": "<p>Looks like <code>EnglishPorterFilterFactory</code> is no longer around in 4.x.  See the note in <a href=\"http://lucene.apache.org/solr/api-3_6_0/org/apache/solr/analysis/EnglishPorterFilterFactory.html\">it's 3.6.0 documentation</a>:</p>\n\n<pre><code>Deprecated.\n  Use SnowballPorterFilterFactory with language=\"English\" instead\n</code></pre>\n\n<p>A lot of Deprecated stuff went away in 4.0.  I'd do what it says, see the <a href=\"http://lucene.apache.org/core/4_2_0/analyzers-common/org/apache/lucene/analysis/snowball/SnowballPorterFilterFactory.html\">documentation for SnowballPorterFilterFactory</a>.</p>\n", "creation_date": 1365694226, "is_accepted": true, "score": 11, "last_activity_date": 1365694226, "answer_id": 15952879}], "question_id": 15945927, "tags": ["linux", "solr", "lucene", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/15945927/apache-nutch-and-solr-integration", "last_activity_date": 1365756571, "accepted_answer_id": 15952879, "body": "<p>I've tried to follow the <a href=\"http://wiki.apache.org/nutch/NutchTutorial#A4._Setup_Solr_for_search\">nutch tutorial</a> but having a bit of a problem with the schema.xml file.</p>\n\n<p>I was told to the nutch provided schema to my project, essentially this...</p>\n\n<pre><code>cp ${NUTCH_RUNTIME_HOME}/conf/schema.xml ${APACHE_SOLR_HOME}/example/solr/conf/\n</code></pre>\n\n<p>I have deployed my solr file in Tomcat and the error I get when I go to the Solr dashboard is</p>\n\n<pre><code>collection1: org.apache.solr.common.SolrException:org.apache.solr.common.SolrException:\nPlugin init failure for [schema.xml] fieldType \"text\": \nPlugin init failure for [schema.xml] analyzer/filter:\nError loading class 'solr.EnglishPorterFilterFactory'\n</code></pre>\n\n<p>Which relates to this element in my solrconfig.xml file (I can comment this out but not sure how important this is yet)</p>\n\n<pre><code>&lt;filter class=\"solr.EnglishPorterFilterFactory\" protected=\"protwords.txt\"/&gt;\n</code></pre>\n\n<p>I have edited my solrconfig.xml to try and included a range of jar files that come with solr, specifically</p>\n\n<pre><code>&lt;lib path=\"/etc/solr/collection1/libs/dist/solr-core-4.2.1.jar\" /&gt;\n&lt;lib path=\"/etc/solr/collection1/libs/dist/solr-analysis-extras-4.2.1.jar\" /&gt;\n</code></pre>\n\n<p>But I don't think they contain the missing class \"solr.EnglishPorterFilterFactory\"</p>\n\n<p>Does anyone have idea why this might not be working or if I have missed something?\nI'm not a Java developer btw so no doubt it will be something simple :)</p>\n\n<p><strong>UPDATE</strong>\nAfter finding out that the schema had some old classes being referenced I had another look in the nutch/conf and tt looks like there is a ${NUTCH_RUNTIME_HOME}/conf/schema-solr4.xml file which seems to work.</p>\n\n<p>Not 100% if this is correct but hey...</p>\n", "creation_date": 1365674565, "score": 10},
{"title": "How to fetch information in XML format from Nutch spidered webpages database", "view_count": 137, "owner": {"user_id": 2235661, "answer_count": 15, "creation_date": 1364898381, "accept_rate": 62, "view_count": 143, "reputation": 473}, "is_answered": true, "answers": [{"last_edit_date": 1365707414, "owner": {"user_id": 223478, "accept_rate": 90, "link": "http://stackoverflow.com/users/223478/neil-mcguigan", "user_type": "registered", "reputation": 23381}, "body": "<p>It depends on how structured the data is.</p>\n\n<p>I assume you are crawling mostly HTML pages.</p>\n\n<p>Oftentimes you can use <a href=\"https://developer.mozilla.org/en/docs/XPath\" rel=\"nofollow\">XPath</a> to grab portions of the page, such as \"//div[@class='books']/a/text()\"</p>\n\n<p>If much of the text is unstructured (no structured HTML patterns to grab), then you are going to have to use regular expressions or information extraction.</p>\n\n<p>If you're lucky, you can do some/most of it using regular expressions.</p>\n\n<p>For some more complicated structures, you'd need to use information extraction / named entity recognition.</p>\n\n<p>You would have to train an IE tool, such as <a href=\"http://nlp.stanford.edu/software/corenlp.shtml\" rel=\"nofollow\">Stanford's CoreNLP</a> to recognize, say, book titles and annotate them in your documents. Also check out <a href=\"http://brat.nlplab.org/index.html\" rel=\"nofollow\">BRAT Rapid Annotation Tool</a>.</p>\n\n<p>Services like Mozenda can do the crawling and X-Path work for you, but I haven't seen a company that provides IE services. </p>\n", "question_id": 15909558, "creation_date": 1365621103, "is_accepted": true, "score": 0, "last_activity_date": 1365707414, "answer_id": 15934396}], "question_id": 15909558, "tags": ["xml", "nutch", "aggregation", "text-mining"], "answer_count": 1, "link": "http://stackoverflow.com/questions/15909558/how-to-fetch-information-in-xml-format-from-nutch-spidered-webpages-database", "last_activity_date": 1365707414, "accepted_answer_id": 15934396, "body": "<p>I'm trying to build books aggregation portal. Nutch provides me excellent web crawler, but I want very specific information like, book title, book price, ISBN, author etc. How to extract that information from the crawled pages? I would like to fetch this information in XML format if possible.</p>\n\n<p>In addition to the above, I would like to ask if this is the right approach! Can it be done in better way with other open source software?</p>\n", "creation_date": 1365531697, "score": 1},
{"title": "nutch to extract only pdf files", "view_count": 711, "owner": {"age": 27, "answer_count": 12, "creation_date": 1286472479, "user_id": 469396, "accept_rate": 29, "view_count": 160, "reputation": 660}, "is_answered": true, "answers": [{"question_id": 15853628, "owner": {"user_id": 938312, "accept_rate": 78, "link": "http://stackoverflow.com/users/938312/neeraj-narang", "user_type": "registered", "reputation": 254}, "body": "<p><code>content.getContent()</code> will return the content in bytes.<br/>\nJust write the bytes to a file using <code>BufferedOutputStream</code> and save it as a pdf</p>\n", "creation_date": 1365546009, "is_accepted": true, "score": 1, "last_activity_date": 1365546009, "answer_id": 15913603}], "question_id": 15853628, "tags": ["apache", "hadoop", "search-engine", "web-crawler", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/15853628/nutch-to-extract-only-pdf-files", "last_activity_date": 1365626897, "accepted_answer_id": 15913603, "body": "<p>Is there any way to perform a urlfilter from level 1-5 and a different urlfilter from 5 onwards. I need to extract pdf files which will be only after a given level (just to experiment).</p>\n\n<p>The pdf files will be stored in a binary format in the crawl/segment folder. I would like to extract these pdf files and store all in 1 folder. I have been able to write a java program to identify a pdf file. I cant figure how to make a pdf file with its content having <strong>same font, page #, images</strong> etc.</p>\n\n<ol>\n<li>perform crawl </li>\n<li>merge segment data</li>\n<li>run makePDF.java</li>\n</ol>\n\n<p>this only identifies pdf files:</p>\n\n<pre><code>    String uri = \"/usr/local/nutch/framework/apache-nutch-1.6/merged572/20130407131335\";\n    Configuration conf = new Configuration();\n    FileSystem fs = FileSystem.get(URI.create(uri), conf);\n    Path path = new Path(uri, Content.DIR_NAME + \"/part-00000/data\");\n\n    SequenceFile.Reader reader = null;\n    try {\n      reader = new SequenceFile.Reader(fs, path, conf);\n      Text key = new Text();\n      Content content = new Content();\n      while (reader.next(key, content)) {\n          String contentType = content.getContentType();\n          if (contentType.equalsIgnoreCase(\"application/pdf\")) {\n            //System.out.write( content.getContent(), 0, content.getContent().length );\n            System.out.println(key);\n          }\n      }\n      reader.close();\n    } \n        finally {\n        fs.close();\n    }\n</code></pre>\n", "creation_date": 1365267786, "score": 1},
{"title": "Nutch - fetch new discovered domains", "view_count": 168, "owner": {"age": 29, "answer_count": 13, "creation_date": 1300893182, "user_id": 673308, "accept_rate": 46, "view_count": 199, "reputation": 608}, "is_answered": true, "answers": [{"question_id": 15880927, "owner": {"user_id": 673308, "accept_rate": 46, "link": "http://stackoverflow.com/users/673308/tugcem", "user_type": "registered", "reputation": 608}, "body": "<p>Got it.</p>\n\n<p>You can add suffixes to domain-urlfilter.txt like \"gov.uk\" as <a href=\"http://javasourcecode.org/html/open-source/nutch/nutch-2.0/org/apache/nutch/urlfilter/domain/DomainURLFilter.java.html\" rel=\"nofollow\">DomainURLFilter source code</a> on lines 186-189:</p>\n\n<pre><code>  if (domainSet.contains(suffix) || domainSet.contains(domain)\n    || domainSet.contains(host)) {\n    return url;\n  }\n</code></pre>\n\n<p>it checks for suffix, domain and host.</p>\n\n<p>Also, you may keep domain urls in an HBase table and manage them via your own filter plugin instead of using DomainURLFilter.</p>\n", "creation_date": 1365504164, "is_accepted": true, "score": 0, "last_activity_date": 1365504164, "answer_id": 15899791}], "question_id": 15880927, "tags": ["java", "configuration", "hadoop", "fetch", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/15880927/nutch-fetch-new-discovered-domains", "last_activity_date": 1365504164, "accepted_answer_id": 15899791, "body": "<p>We're using nutch 1.6 to crawl web. According to nutch configuration, one should give the seedlist and domain url-filter to traverse across specified domains. However, we want to fetch newly discovered urls if their extension is, let's say, co.uk (only for this extension) We can manage it by adding newly discovered url's domains to a file - or db, whatever -, stop crawler, update domain url-filters and seedlist, then restart it. But how can we do it dynamically, w/o stopping the crawler?</p>\n\n<p>Thanks in advance. </p>\n\n<p>P.S : co.uk domain extension is just an example, we also could add more than one extension to allow.</p>\n", "creation_date": 1365429188, "score": 0},
{"title": "How can I crawl pdf files that are served on internet using Nutch-1.0 using http protocol", "view_count": 1502, "is_answered": false, "answers": [{"last_edit_date": 1300540738, "owner": {"user_id": 159136, "accept_rate": 77, "link": "http://stackoverflow.com/users/159136/sunil", "user_type": "registered", "reputation": 1173}, "body": "<p>add this property in the nutch-site.xml file then you will crawl the pdf files</p>\n\n<pre><code>&lt;property&gt;\n&lt;name&gt;plugin.includes&lt;/name&gt;\n&lt;value&gt;protocol-httpclient|urlfilter-regex|parse-(html|text|pdf)|index-(basic|anchor)|query-(basic|site|url)|response-(json|xml)|summary-basic|scoring-opic|urlnormalizer-(pass|regex|basic)&lt;/value&gt;\n&lt;description&gt;protocol-httpclient|urlfilter-regex|parse-(html|text|pdf)|index-(basic|anchor)|query-(basic|site|url)|response-(json|xml)|summary-basic|scoring-opic|urlnormalizer-(pass|regex|basic)&lt;/description&gt;\n&lt;/property&gt;\n</code></pre>\n", "question_id": 1544422, "creation_date": 1256805849, "is_accepted": false, "score": 0, "last_activity_date": 1300540738, "answer_id": 1642329}], "question_id": 1544422, "tags": ["filesystems", "nutch", "web-crawler"], "answer_count": 1, "link": "http://stackoverflow.com/questions/1544422/how-can-i-crawl-pdf-files-that-are-served-on-internet-using-nutch-1-0-using-http", "last_activity_date": 1365480121, "owner": {"user_type": "does_not_exist"}, "body": "<p>I want to know How can I crawl pdf files that are served on internet using Nutch-1.0 using http protocol</p>\n\n<p>I am able to do it on local file systems using file:// protocol but not http protocol</p>\n", "creation_date": 1255101083, "score": 1},
{"title": "How to read urls from different files and set different depth for crawling?", "view_count": 87, "owner": {"age": 27, "answer_count": 28, "creation_date": 1340810733, "user_id": 1486136, "accept_rate": 89, "view_count": 130, "location": "Bitola, Macedonia (FYROM)", "reputation": 2255}, "is_answered": true, "answers": [{"question_id": 14377712, "owner": {"user_id": 1830069, "accept_rate": 63, "link": "http://stackoverflow.com/users/1830069/sunskin", "user_type": "registered", "reputation": 543}, "body": "<blockquote>\n  <p><em><strong>I want to have two files seed.txt and seed2.txt and in each file to have different urls</em></strong></p>\n</blockquote>\n\n<p>You need to maintain the seed file name as is; do not rename it to seed2 etc. Instead, You can create two seperate urls directory with a seed file in each containing different set of urls. Ex. folder 'urls1' will have one seed.txt and another folder 'urls2' will have another seed.txt with a different set of urls. But also make sure to create seperate crawl directories where the crawl data would go to (ex. create a 'crawl1' directory for seed.txt in 'urls1' folder and 'crawl2' directory for the 'seeds.txt' in 'urls2'.</p>\n\n<blockquote>\n  <p><em><strong>In seed.txt the depth for crawling i want to be for ex. 2 and in seed2.txt the depth to be 3.</em></strong></p>\n</blockquote>\n\n<p>You should specify the depth value in your crawl command not in the seed.txt. In your case, run the following commands in seperate terminals if running on the same machine (provided your nutch/hadoop configuration supports running multiple crawl jobs in parallel. </p>\n\n<ul>\n<li><p>bin/nutch crawl urls1 -dir crawl1 -depth 2  </p></li>\n<li><p>bin/nutch crawl urls2 -dir crawl2 -depth 3</p></li>\n</ul>\n\n<p>Hope this helped! </p>\n", "creation_date": 1364846518, "is_accepted": true, "score": 1, "last_activity_date": 1364846518, "answer_id": 15750848}], "question_id": 14377712, "tags": ["apache", "lucene", "nutch", "web-crawler"], "answer_count": 1, "link": "http://stackoverflow.com/questions/14377712/how-to-read-urls-from-different-files-and-set-different-depth-for-crawling", "last_activity_date": 1364846518, "accepted_answer_id": 15750848, "body": "<p>I want to have two files seed.txt and seed2.txt and in each file to have different urls. In seed.txt the depth for crawling i want to be for ex. 2 and in seed2.txt the depth to be 3. <br> Is there any solution or workaround to do this?? <br/></p>\n", "creation_date": 1358420861, "score": 2},
{"title": "IOException during #Crawl.run() -&gt; #JobClient.runJob()", "view_count": 141, "is_answered": false, "answers": [{"question_id": 15731486, "owner": {"user_id": 2225199, "link": "http://stackoverflow.com/users/2225199/nimeshjm", "user_type": "registered", "reputation": 1137}, "body": "<p>I didn't have much success when I tried running nutch 1.6 on Windows.\nI downloaded the latest version known to run in Windows (nutch 1.2) and didn't have any problems with that.</p>\n\n<p>Having said that, can you share the output of your logs/hadoop.log file?\nThat may give more hints on what is going wrong.</p>\n", "creation_date": 1364772068, "is_accepted": false, "score": 0, "last_activity_date": 1364772068, "answer_id": 15735689}], "question_id": 15731486, "tags": ["eclipse", "apache", "hadoop", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/15731486/ioexception-during-crawl-run-jobclient-runjob", "last_activity_date": 1364772068, "owner": {"user_id": 739093, "answer_count": 1, "creation_date": 1304564494, "accept_rate": 61, "view_count": 47, "reputation": 157}, "body": "<p>I am pretty new with nutch so bear with me. I have been encountering an IOException during one of my test crawls. I am using <strong>nutch 1.6 with hadoop 0.20.2</strong> (chose this version for windows compatibiliy in setting file access rights). </p>\n\n<p>I am running nutch through eclipse. I followed this guide in importing nutch from an SVN: <a href=\"http://wiki.apache.org/nutch/RunNutchInEclipse\" rel=\"nofollow\">http://wiki.apache.org/nutch/RunNutchInEclipse</a></p>\n\n<p>My crawler's code is from this website: <a href=\"http://cmusphinx.sourceforge.net/2012/06/building-a-java-application-with-apache-nutch-and-solr/\" rel=\"nofollow\">http://cmusphinx.sourceforge.net/2012/06/building-a-java-application-with-apache-nutch-and-solr/</a></p>\n\n<p>Here is the system exception log: </p>\n\n<p>solrUrl is not set, indexing will be skipped... <br/>\ncrawl started in: crawl <br/>\nrootUrlDir = urls <br/>\nthreads = 1 <br/>\ndepth = 1 <br/>\nsolrUrl=null <br/>\ntopN = 1 <br/>\nInjector: starting at 2013-03-31 23:51:11 <br/>\nInjector: crawlDb: crawl/crawldb <br/>\nInjector: urlDir: urls <br/>\nInjector: Converting injected urls to crawl db entries. <br/></p>\n\n<pre><code>java.io.IOException: Job failed! \n    at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:1252) \n    at org.apache.nutch.crawl.Injector.inject(Injector.java:\n    at org.apache.nutch.crawl.Crawl.run(Crawl.java:\n    at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:\n    at rjpb.sp.crawler.CrawlerTest.main(CrawlerTest.java:51)\n</code></pre>\n\n<p>I see these calls involving paths before #Injector.inject() in Crawl.java </p>\n\n<pre><code>Path crawlDb = new Path(dir + \"/crawldb\"); \nPath linkDb = new Path(dir + \"/linkdb\"); \nPath segments = new Path(dir + \"/segments\"); \nPath indexes = new Path(dir + \"/indexes\"); \nPath index = new Path(dir + \"/index\");\n</code></pre>\n\n<p>Currently I my eclipse project does not include the folders crawldb,linkdb,segments... I think my problem is I have not set all the necessary files for crawling. I have only set <strong>nutch-site.xml,regex-urlfilter.txt, and urls/seed.txt</strong>. Any advice on the matter will be of great help. Thanks!</p>\n", "creation_date": 1364746349, "score": 0},
{"title": "Is there anyway to log the list of urls &#39;ignored&#39; in Nutch crawl?", "view_count": 250, "owner": {"user_id": 1830069, "answer_count": 10, "creation_date": 1353080229, "accept_rate": 63, "view_count": 103, "reputation": 543}, "is_answered": true, "answers": [{"question_id": 15453097, "owner": {"user_id": 2204324, "link": "http://stackoverflow.com/users/2204324/manisha-verma", "user_type": "registered", "reputation": 38}, "body": "<p>The links can be found by using the following command</p>\n\n<p>bin/nutch readdb PATH_TO_CRAWL_DB -stats -sort -dump DUMP_FOLDER -format csv </p>\n\n<p>this will generate part-00000 file in dump_folder which will contain the url list and their status respectively.  </p>\n\n<p>Those with the status of db_unfetched have been ignored by the crawler.</p>\n", "creation_date": 1364124101, "is_accepted": true, "score": 1, "last_activity_date": 1364124101, "answer_id": 15597804}], "question_id": 15453097, "tags": ["apache", "solr", "web-crawler", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/15453097/is-there-anyway-to-log-the-list-of-urls-ignored-in-nutch-crawl", "last_activity_date": 1364124101, "accepted_answer_id": 15597804, "body": "<p>I am using Nutch to crawl a list of URLS specified in the seed file with depth 100 and topN 10,000 to ensure a full crawl. Also, I am trying to ignore urls with repeated strings in their path using regex-urlfilter <a href=\"http://rubular.com/r/oSkwqGHrri\" rel=\"nofollow\">http://rubular.com/r/oSkwqGHrri</a> </p>\n\n<p>However, I am curious to know which urls have been ignored during crawling. Is there anyway i can log the list of urls \"ignored\" by Nutch while it crawls?</p>\n", "creation_date": 1363459003, "score": 0},
{"title": "How to find depth and score of unfetched URLS in Nutch", "view_count": 335, "is_answered": false, "question_id": 15597765, "tags": ["nutch", "web-crawler"], "answer_count": 0, "link": "http://stackoverflow.com/questions/15597765/how-to-find-depth-and-score-of-unfetched-urls-in-nutch", "last_activity_date": 1364123878, "owner": {"user_id": 2204324, "view_count": 14, "answer_count": 2, "creation_date": 1364122625, "reputation": 38}, "body": "<p>Hi I had crawled a website using nutch 1.5.1 with following parameters. -depth=20 and -topN=800. Now I have a fairly large number of URLS (approx 6K) that are with status db_unfetched. I want to identify 2 things: </p>\n\n<ol>\n<li><p>Why is their status unfetched. Is there some way to trace the reason why a URL was ignored while crawling. I checked the URL filters (they are all in place).  </p></li>\n<li><p>Can I find out at what depth each URL was discovered by Nutch. Are all the unfetched pages (all these pages have content so no 404 error) found at depth of 20 or some pages were ignored even before it reached that depth. </p></li>\n</ol>\n\n<p>I cannot afford to crawl that website again, Are there any commands that I can use to traceback scoring of URLS and identifying their depths. </p>\n", "creation_date": 1364123878, "score": 0},
{"title": "What is a good Java-based crawler for an academic project regarding building a search engine?", "view_count": 757, "is_answered": false, "answers": [{"question_id": 14603330, "owner": {"user_id": 1143887, "accept_rate": 67, "link": "http://stackoverflow.com/users/1143887/kich", "user_type": "registered", "reputation": 169}, "body": "<p>AFAIK, Apache Nutch suits most of your requirements. Nutch also has a plugin architecture which is helpful to write your own if you need. You can go through the wiki [0] and ask in the mailing list if you have any questions</p>\n\n<p>[0] <a href=\"http://wiki.apache.org/nutch/FrontPage\" rel=\"nofollow\">http://wiki.apache.org/nutch/FrontPage</a></p>\n", "creation_date": 1363975595, "is_accepted": false, "score": 0, "last_activity_date": 1363975595, "answer_id": 15577093}], "question_id": 14603330, "tags": ["java", "multithreading", "web-crawler", "nutch", "heritrix"], "answer_count": 1, "link": "http://stackoverflow.com/questions/14603330/what-is-a-good-java-based-crawler-for-an-academic-project-regarding-building-a-s", "last_activity_date": 1363975595, "owner": {"age": 27, "answer_count": 0, "creation_date": 1359543705, "user_id": 2025042, "view_count": 5, "location": "Netherlands", "reputation": 11}, "body": "<p>Okay, so I have been looking for the last two days for a crawler that suits my needs. I want to build a search engine and I want to do the indexing myself. This will be part of an academic project. Although I do not have the processing power to crawl the entire web, I would like to use a crawler that is actually capable of doing this. So what I am looking for is a crawler that:</p>\n\n<ol>\n<li>supports multithreading</li>\n<li>doesn't miss many links</li>\n<li>gives me the opportunity to (override a method so that I can) access the content of the pages crawled so that I can save it, parse it etc.</li>\n<li>obeys robots.txt files</li>\n<li>crawls html pages (also php,jsp etc.).</li>\n<li>recognizes pages with same content and only returns one.</li>\n</ol>\n\n<p>What it doesn't (necessarily) have to do is:</p>\n\n<ol>\n<li>supporting pageranking.</li>\n<li>index results.</li>\n<li>crawl images/audio/video/pdf etc.</li>\n</ol>\n\n<p>I found a few libraries/projects that came very close to my needs, but as far as I know they don't support everything I need:</p>\n\n<ol>\n<li>First I came across <a href=\"http://code.google.com/p/crawler4j/\" rel=\"nofollow\">crawler4j</a>. The only problem with this one is that it doesn't support politeness interval per host. Therefore, by setting the politeness level to a decent value of 1000ms, makes the crawler terribly slow.</li>\n<li>I also found <a href=\"http://code.google.com/p/flaxcrawler/\" rel=\"nofollow\">flaxcrawler</a>. This did support multithreading but it appears to have problems with finding and following links in webpages.</li>\n</ol>\n\n<p>I also looked at more complete and complex 'crawlers' such as Heritrix and Nutch. Although I am not that good with more complex stuff but I am definitely willing to use it if I am sure that it would be able to do what I need it to do: crawl the web and give me all the pages so that I can read them.</p>\n\n<p>Long story short: I am looking for a crawler that goes very fast through all pages on the web and gives me the opportunity to do something with them.</p>\n", "creation_date": 1359546689, "score": 2},
{"title": "Focused crawling from nutch", "view_count": 386, "is_answered": false, "answers": [{"question_id": 14640563, "owner": {"user_id": 1143887, "accept_rate": 67, "link": "http://stackoverflow.com/users/1143887/kich", "user_type": "registered", "reputation": 169}, "body": "<p>Please go through the wiki [0] first. There is a lot of documentation available and you can customize it based on your requirements.</p>\n\n<p>[0] - <a href=\"http://wiki.apache.org/nutch/FrontPage\" rel=\"nofollow\">http://wiki.apache.org/nutch/FrontPage</a></p>\n", "creation_date": 1363975409, "is_accepted": false, "score": 0, "last_activity_date": 1363975409, "answer_id": 15577041}], "question_id": 14640563, "tags": ["solr", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/14640563/focused-crawling-from-nutch", "last_activity_date": 1363975409, "owner": {"user_id": 1471836, "answer_count": 3, "creation_date": 1340275475, "accept_rate": 11, "view_count": 24, "reputation": 51}, "body": "<p>I am using nutch 1.2 and solr 3.1.0 and a NUTCH-828-1-20100608 patch for fetch filter.</p>\n\n<p>I don't understand how to filter the fetch, nor do I understand how the patch works. <a href=\"http://lucene.472066.n3.nabble.com/Focused-crawling-with-nutch-td3656343.html\" rel=\"nofollow\">This link</a> helped me patch the file.</p>\n", "creation_date": 1359697984, "score": 0},
{"title": "Getting the number of facebook likes from a certain page with Nutch 2.1", "view_count": 515, "is_answered": false, "answers": [{"question_id": 14378083, "owner": {"user_id": 2082437, "accept_rate": 60, "link": "http://stackoverflow.com/users/2082437/abhinav", "user_type": "registered", "reputation": 767}, "body": "<p>As far as I know you cannot crawl Facbook Using nutch.  <a href=\"https://www.facebook.com/robots.txt\" rel=\"nofollow\">https://www.facebook.com/robots.txt</a> specifies that content inside facebook is not available for crawling. </p>\n", "creation_date": 1363766437, "is_accepted": false, "score": 0, "last_activity_date": 1363766437, "answer_id": 15518166}], "question_id": 14378083, "tags": ["facebook", "apache", "indexing", "web-crawler", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/14378083/getting-the-number-of-facebook-likes-from-a-certain-page-with-nutch-2-1", "last_activity_date": 1363766437, "owner": {"user_id": 1878786, "view_count": 5, "answer_count": 2, "creation_date": 1354704920, "reputation": 135}, "body": "<p>I want to ask if there is any way I can get number of times the URL was posted on public facebook pages / number of times the posted URL and comments were liked on Facebook using Nutch 2.1 crawler?</p>\n", "creation_date": 1358422125, "score": 4},
{"title": "Regular expression to match a URL with 6 or more levels", "view_count": 1022, "owner": {"user_id": 1830069, "answer_count": 10, "creation_date": 1353080229, "accept_rate": 63, "view_count": 103, "reputation": 543}, "is_answered": true, "answers": [{"question_id": 15505406, "owner": {"user_id": 112335, "accept_rate": 78, "link": "http://stackoverflow.com/users/112335/saulo-silva", "user_type": "registered", "reputation": 665}, "body": "<p>Try the following:</p>\n\n<pre><code>^http:\\/\\/([a-zA-Z\\.-]*)(\\/[\\w\\.]+){6,}\n</code></pre>\n\n<p><a href=\"http://rubular.com/r/QZlidUqheq\" rel=\"nofollow\">http://rubular.com/r/QZlidUqheq</a></p>\n", "creation_date": 1363711601, "is_accepted": false, "score": 2, "last_activity_date": 1363711601, "answer_id": 15505526}, {"last_edit_date": 1363744947, "owner": {"user_id": 20938, "link": "http://stackoverflow.com/users/20938/alan-moore", "user_type": "registered", "reputation": 51234}, "body": "<p>I think this is what you were trying for:\n</p>\n\n<pre class=\"lang-none prettyprint-override\"><code>^http://([a-zA-Z.-]+)/(?:[^/]+/){6,}.*$\n</code></pre>\n\n<p>This matches six or more levels, which is what you said you wanted in the question.  However in the question's title you phrased it \"more than six\".  If that's what you really want, change the quantifier from <code>{6,}</code> to <code>{7,}</code>.</p>\n\n<p>On a side note, the forward slash (<code>/</code>) has no special meaning in regexes, and doesn't need to be escaped.  Rubular forces you to escape the slash because that's what it uses as the regex delimiter.  Nutch uses Java's built-in regexes, so you should use a tester that the same flavor, like <a href=\"http://www.regexplanet.com/advanced/java/index.html\" rel=\"nofollow\">this one</a>.</p>\n", "question_id": 15505406, "creation_date": 1363716565, "is_accepted": true, "score": 1, "last_activity_date": 1363744947, "answer_id": 15507246}], "question_id": 15505406, "tags": ["java", "regex", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/15505406/regular-expression-to-match-a-url-with-6-or-more-levels", "last_activity_date": 1363744947, "accepted_answer_id": 15507246, "body": "<p>I am trying to match a URL with 6 or more than 6 levels or sub-paths</p>\n\n\n\n<pre class=\"lang-none prettyprint-override\"><code>http://www.domain.com/level1/level2/level3/level4/level5/level6/level7/level8/level9/level10/level11/level12.html\n</code></pre>\n\n<p>I came up with an expression </p>\n\n<pre class=\"lang-none prettyprint-override\"><code>^http:\\/\\/([a-zA-Z\\.-]*)\\W(\\b\\w+\\b) \n</code></pre>\n\n<p>...which matches level1 (<a href=\"http://rubular.com/r/JzIJjD7OUP\" rel=\"nofollow\">demo</a>)</p>\n\n<p>However, when I am trying to match a URL with six or more levels it doesn't seem to work. </p>\n\n<pre class=\"lang-none prettyprint-override\"><code>^http:\\/\\/([a-zA-Z\\.-]*)\\W(\\b\\w+\\b){6,}\n</code></pre>\n\n<p>(<a href=\"http://rubular.com/r/ZFWSGXP23j\" rel=\"nofollow\">demo</a>)</p>\n", "creation_date": 1363711295, "score": 0},
{"title": "Malformed URL: &#39;&#39;, skipping (java.net.MalformedURLException", "view_count": 1895, "is_answered": true, "answers": [{"last_edit_date": 1323990345, "owner": {"user_id": 976878, "accept_rate": 67, "link": "http://stackoverflow.com/users/976878/ha-sh", "user_type": "registered", "reputation": 405}, "body": "<p>According to the docs.\n\"MalformedURLException is thrown to indicate that a malformed URL has occurred. Either no legal protocol could be found in a specification string or the string could not be parsed.\"</p>\n\n<p>The thing to be noted here is that this exception is not thrown when the server is down or when the path points to a missing file. It occurs only when URL cannot be parsed. </p>\n\n<p>The error indicates that there is <i>no protocol</i>. and also the crawler does not see any URL, </p>\n\n<p>Malformed URL: <b>'' </b>, skipping (java.net.MalformedURLException: no protocol:</p>\n\n<p>Here is interesting article that I came across, have a look <a href=\"http://www.symphonious.net/2007/03/29/javaneturl-or-javaneturi/\" rel=\"nofollow\">http://www.symphonious.net/2007/03/29/javaneturl-or-javaneturi/</a></p>\n\n<p>What is the exact URL you are trying to parse? </p>\n", "question_id": 8527696, "creation_date": 1323989733, "is_accepted": false, "score": 2, "last_activity_date": 1323990345, "answer_id": 8527782}, {"last_edit_date": 1363516629, "owner": {"user_id": 2179000, "link": "http://stackoverflow.com/users/2179000/afra-mehrparvar", "user_type": "registered", "reputation": 204}, "body": "<p>After having set all setting with <code>regex-urlfilter.txt</code> and <code>seed.txt</code> try this command:</p>\n\n<pre><code>./nutch plugin protocol-file org.apache.nutch.protocol.file.File file:\\\\\\e:\\\\test.html\n</code></pre>\n\n<p>(if the file is located at <code>e:\\test.htm</code> in my example.</p>\n\n<p>Before this, I always ran this</p>\n\n<pre><code>./nutch plugin protocol-file org.apache.nutch.protocol.file.File \\\\\\e:\\test.html\n</code></pre>\n\n<p>and got this error, because the protocol <code>file:</code> was missing:</p>\n\n<blockquote>\n  <p>java.netMalformedURLException : no protocol : \\\\e:\\test.html   </p>\n</blockquote>\n", "question_id": 8527696, "creation_date": 1363514990, "is_accepted": false, "score": 1, "last_activity_date": 1363516629, "answer_id": 15459585}, {"question_id": 8527696, "owner": {"user_id": 246622, "accept_rate": 92, "link": "http://stackoverflow.com/users/246622/tomsv", "user_type": "registered", "reputation": 4207}, "body": "<pre><code>Malformed URL: ''\n</code></pre>\n\n<p>means that the URL was empty instead of being something like <a href=\"http://www.google.com\" rel=\"nofollow\">http://www.google.com</a>.</p>\n", "creation_date": 1363515122, "is_accepted": false, "score": 0, "last_activity_date": 1363515122, "answer_id": 15459603}], "question_id": 8527696, "tags": ["nutch", "web-crawler"], "answer_count": 3, "link": "http://stackoverflow.com/questions/8527696/malformed-url-skipping-java-net-malformedurlexception", "last_activity_date": 1363516629, "owner": {"user_id": 1045194, "answer_count": 0, "creation_date": 1321258730, "accept_rate": 0, "view_count": 19, "reputation": 14}, "body": "<p>i crawl sites with nutch 1.3. i see this exception in my log when nutch crawl my sites:</p>\n\n<pre><code>Malformed URL: '', skipping (java.net.MalformedURLException: no protocol: \nat java.net.URL.&lt;init&gt;(URL.java:567)\nat java.net.URL.&lt;init&gt;(URL.java:464)\nat java.net.URL.&lt;init&gt;(URL.java:413)\nat org.apache.nutch.crawl.Generator$Selector.reduce(Generator.java:247)\nat org.apache.nutch.crawl.Generator$Selector.reduce(Generator.java:109)\nat org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:463)\nat org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:411)\nat org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:216)\n</code></pre>\n\n<p>)</p>\n\n<p>how can i solve this? help me.</p>\n", "creation_date": 1323989113, "score": 0},
{"title": "Showing Links Going To A Domain Or Page Using Nutch", "view_count": 44, "is_answered": false, "answers": [{"question_id": 15456837, "owner": {"user_id": 1734954, "accept_rate": 45, "link": "http://stackoverflow.com/users/1734954/ajameswolf", "user_type": "registered", "reputation": 763}, "body": "<p>I noticed that inside of the database inlinks were stored so executing the following query would get indexed inbound links to creativecommons.org domain after parsing the inlinks from each row:</p>\n\n<p>SELECT *\nFROM webpage\nWHERE inlinks LIKE '%creativecommons.org%'</p>\n", "creation_date": 1363495480, "is_accepted": false, "score": 0, "last_activity_date": 1363495480, "answer_id": 15457612}], "question_id": 15456837, "tags": ["nutch", "web-crawler"], "answer_count": 1, "link": "http://stackoverflow.com/questions/15456837/showing-links-going-to-a-domain-or-page-using-nutch", "last_activity_date": 1363495480, "owner": {"age": 29, "answer_count": 48, "creation_date": 1349874263, "user_id": 1734954, "accept_rate": 45, "view_count": 93, "location": "Menomonie, WI", "reputation": 763}, "body": "<p>I am using nutch and solr on ubuntu. I would like to use php to query the database or other methods to return an array of links from indexed pages that go to any particular url or domain. Please point me in the right direction.</p>\n\n<p>I used this tutorial to set up the spider <a href=\"http://nlp.solutions.asia/?p=180\" rel=\"nofollow\">http://nlp.solutions.asia/?p=180</a></p>\n\n<p>I would also like to note that the preference is for a php language option or an api interface with the nutch or sorl application via php curl or command line interface.\nThanks</p>\n", "creation_date": 1363485868, "score": -1},
{"title": "Apache Nutch at Amazon Web Services or Local", "view_count": 503, "owner": {"user_id": 453596, "answer_count": 127, "creation_date": 1285053991, "accept_rate": 77, "view_count": 2042, "reputation": 21412}, "is_answered": true, "answers": [{"question_id": 15278745, "owner": {"user_id": 81194, "link": "http://stackoverflow.com/users/81194/claude", "user_type": "registered", "reputation": 827}, "body": "<p>It sounds like your facing a steep learning curve.</p>\n\n<p>For one, you admit that you're just learning Nutch, so I would recommend you install CentOS on a physical box at home and play around there.</p>\n\n<p>On the other hand, you are pondering the use of a micro AWS instance, which will not be useful in running a CPU/memory intensive application like Nutch. <strong>Read about <a href=\"http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/concepts_micro_instances.html\" rel=\"nofollow\">AWS micro instances here</a>.</strong></p>\n\n<p>My suggestion is to stick to a single physical box solution at home and work on scripting your solution before moving on to an AWS instance.</p>\n", "creation_date": 1362815225, "is_accepted": true, "score": 1, "last_activity_date": 1362815225, "answer_id": 15308486}], "question_id": 15278745, "tags": ["amazon-web-services", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/15278745/apache-nutch-at-amazon-web-services-or-local", "last_activity_date": 1362815225, "accepted_answer_id": 15308486, "body": "<p>I want to learn Apache Nutch and I have an account at Amazon Web Services (AWS). I have three machines at AWS and one of them is micro sized, other one is small and the other one is medium. I want to start with small sized and I will install Nutch, Hadoop and Hbase on it. I have Centos 6 at my machines. </p>\n\n<p>There is a question here but not I ask: <a href=\"http://stackoverflow.com/questions/14139296/nutch-2-1-hbase-solr-with-amazon-web-services\">Nutch 2.1 (HBase, SOLR) with Amazon Web Services</a></p>\n\n<p>I want to learn that which approach is better. I want to install them on small size machine. After that I want to add micro sized. On the other hand I don't have any experience about Nutch maybe I should work on local or is there a possibility using my machine and AWS both (does it charge more i.e. copying data from AWS may be charged.)</p>\n\n<p>When I want to implement a wrapper into my Nutch, should I install it on my local(to have source codes) and run it on AWS. </p>\n\n<p>Any ideas?</p>\n", "creation_date": 1362680276, "score": 0},
{"title": "Integrating Solr + Hadoop and Nutch + Hbase on Amazon Web Services and Local", "view_count": 712, "is_answered": false, "answers": [{"question_id": 15303987, "owner": {"user_id": 81194, "link": "http://stackoverflow.com/users/81194/claude", "user_type": "registered", "reputation": 827}, "body": "<p>If you installed Nutch you already have Hadoop, which handled your crawling and parsing.\nYou can take the data produced by Nutch and push it into to Solr for indexing.</p>\n", "creation_date": 1362814432, "is_accepted": false, "score": 0, "last_activity_date": 1362814432, "answer_id": 15308408}], "question_id": 15303987, "tags": ["hadoop", "solr", "amazon-web-services", "hbase", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/15303987/integrating-solr-hadoop-and-nutch-hbase-on-amazon-web-services-and-local", "last_activity_date": 1362814432, "owner": {"user_id": 453596, "answer_count": 127, "creation_date": 1285053991, "accept_rate": 77, "view_count": 2042, "reputation": 21412}, "body": "<p>I have a machine on Amazon Web Services and I am trying some search features on it. I have installed Nutch and Hbase on my AWS machine.</p>\n\n<p>I have crawled wikipedia.org as an example and the crawled data is stored at Hbase at cloud.</p>\n\n<p>I want to install Solr and make a search on it. I did not installed my system as a cluster because I just want to make some research i.e. how can I use it etc.</p>\n\n<p>My question is where Hadoop stands at this system(I did not install Hadoop yet)? Is it logical to install Hadoop  and Solr my local computer. Also my next challenge will be how can I communicate my local computer with AWS computer. </p>\n", "creation_date": 1362779648, "score": 0},
{"title": "How to make Apache Nutch indexing while crawling", "view_count": 2971, "is_answered": true, "answers": [{"last_edit_date": 1362513518, "owner": {"user_id": 1159331, "link": "http://stackoverflow.com/users/1159331/jake-the-dweeb", "user_type": "registered", "reputation": 101}, "body": "<p>(If I had enough rep I would post this as a comment).</p>\n\n<p>Remember that the -depth switch refers to EACH CRAWL, and not the overall depth of the site it will crawl. That means that the second run of depth=1 will descend one MORE level from the already indexed data and stop at topN documents. </p>\n\n<p>So, If you aren't in a hurry to fully populate the data, I've had a lot of success in a similar situation by performing a large number of repeated shallow <em>nutch crawl</em> statements (using smallish -depth (3-5) and -topN (100-200) variables) from a large seed list. This will ensure that only (depth * topN) pages get indexed in each batch, and the index will start delivering URLs within a few minutes. </p>\n\n<p>Then, I typically set up the crawl to fire off every (1.5*initial crawl time average) seconds and let it rip. Understandably, at only 1,000 documents per crawl, it can take a lot of time to get through a large infrastructure, and (after indexing, the paused time and other overhead) the method can multiply the time to crawl the whole stack.</p>\n\n<p>First few times through the infrastructure, it's a pretty bad slog. As the adaptive crawling algo starts to kick in, however, and the recrawl times start to approach reasonable values : the package starts really delivering.</p>\n\n<p>(This is somewhat similar to the \"<a href=\"http://wiki.apache.org/nutch/NutchTutorial\" rel=\"nofollow\" title=\"Whole Web Crawling\">whole web crawling</a>\" method you mention in the nutch wiki, which advises you to break the data into 1,000 page segments, but much more terse and understandable for a beginner.)</p>\n", "question_id": 11717179, "creation_date": 1362513156, "is_accepted": false, "score": 3, "last_activity_date": 1362513518, "answer_id": 15232849}], "question_id": 11717179, "tags": ["nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/11717179/how-to-make-apache-nutch-indexing-while-crawling", "last_activity_date": 1362513518, "owner": {"user_id": 1482243, "answer_count": 4, "creation_date": 1340702748, "accept_rate": 86, "view_count": 11, "reputation": 66}, "body": "<p>I started using Apache Nutch (v1.5.1) to index all the website under some certain domain.\nThere is huge number of websites (in the order of milions) in my domains and I need to index them step by step instead of waiting the end of the whole process.</p>\n\n<p>I found this in nutch wiki (here <a href=\"http://wiki.apache.org/nutch/NutchTutorial/#A3.2_Using_Individual_Commands_for_Whole-Web_Crawling\" rel=\"nofollow\">http://wiki.apache.org/nutch/NutchTutorial/#A3.2_Using_Individual_Commands_for_Whole-Web_Crawling</a>) something that should work. The idea is to make a script witch calls every single step of my process (crawl, fetch, parse, ...) on a certain amount of data (for example 1000 URL) cyclically.</p>\n\n<pre><code>bin/nutch inject crawl/crawldb crawl/seed.txt\n\nbin/nutch generate crawl/crawldb crawl/segments -topN 25\ns1=`ls -d crawl/segments/2* | tail -1`\necho $s1\n\nbin/nutch fetch $s1\nbin/nutch parse $s1\nbin/nutch updatedb crawl/crawldb $s1\n\nbin/nutch generate crawl/crawldb crawl/segments -topN 25\ns2=`ls -d crawl/segments/2* | tail -1`\necho $s2\n\nbin/nutch fetch $s2\nbin/nutch parse $s2\nbin/nutch updatedb crawl/crawldb $s2\n\n...\n\nbin/nutch invertlinks crawl/linkdb -dir crawl/segments\nbin/nutch index crawl/indexes crawl/crawldb crawl/linkdb crawl/segments/*\n</code></pre>\n\n<p>My question is: is there any way to specify this setting directly into Nutch and make him do this stuff in a parallel and more trasparent way? For example on separated threds?</p>\n\n<p>Thank for answering.</p>\n\n<p><strong>UPDATE</strong></p>\n\n<p>I tried to create the script (the code is above) but unfortunatlly I get an error on the invert link phases. This is the output:</p>\n\n<pre><code>LinkDb: starting at 2012-07-30 11:04:58\nLinkDb: linkdb: crawl/linkdb\nLinkDb: URL normalize: true\nLinkDb: URL filter: true\nLinkDb: internal links will be ignored.\nLinkDb: adding segment: file:/home/apache-nutch-1.5-bin/crawl/segments/20120730102927\nLinkDb: adding segment: file:/home/apache-nutch-1.5-bin/crawl/segments/20120704094625\n...\nLinkDb: adding segment: file:/home/apache-nutch-1.5-bin/crawl/segments/20120704095730\n\nLinkDb: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist:\nfile:/home/apache-nutch-1.5-bin/crawl/segments/20120730102927/parse_data\n\nInput path does not exist:\nfile:/home/apache-nutch-1.5-bin/crawl/segments/20120704094625/parse_data\n...\n</code></pre>\n\n<p>Thanks for your help.</p>\n", "creation_date": 1343633912, "score": 2},
{"title": "How to range query the document from 20 to 40?", "view_count": 61, "owner": {"user_id": 1136700, "answer_count": 2, "creation_date": 1325995339, "accept_rate": 48, "view_count": 738, "reputation": 5755}, "is_answered": true, "answers": [{"question_id": 15169725, "owner": {"user_id": 1502563, "link": "http://stackoverflow.com/users/1502563/oakninja", "user_type": "registered", "reputation": 1507}, "body": "<p>You can paginate your search result, see this link for more information about implementing pagination: <a href=\"http://hrycan.com/2010/02/10/paginating-lucene-search-results/\" rel=\"nofollow\">http://hrycan.com/2010/02/10/paginating-lucene-search-results/</a></p>\n", "creation_date": 1362212876, "is_accepted": true, "score": 1, "last_activity_date": 1362212876, "answer_id": 15172431}], "question_id": 15169725, "tags": ["lucene.net", "nutch", "lucene"], "answer_count": 1, "link": "http://stackoverflow.com/questions/15169725/how-to-range-query-the-document-from-20-to-40", "last_activity_date": 1362212876, "accepted_answer_id": 15172431, "body": "<p>I am wondering how to query the index and only get the result from 20 to 40, skip the first 20 results, for exampe, any API supports this operation? </p>\n\n<p>I only find the <code>TopDocs search(Query query, int n).</code></p>\n", "creation_date": 1362188676, "score": 0},
{"title": "Disable robots.txt check in nutch", "view_count": 979, "owner": {"age": 26, "answer_count": 14, "creation_date": 1346181938, "user_id": 1631282, "accept_rate": 81, "view_count": 109, "location": "Macedonia", "reputation": 614}, "is_answered": true, "answers": [{"question_id": 14897058, "owner": {"user_id": 1631282, "accept_rate": 81, "link": "http://stackoverflow.com/users/1631282/emrah-mehmedov", "user_type": "registered", "reputation": 614}, "body": "<p>as far as i understand we are not able to disable robots.txt in nutch.</p>\n", "creation_date": 1362173305, "is_accepted": true, "score": 1, "last_activity_date": 1362173305, "answer_id": 15167175}], "question_id": 14897058, "tags": ["web-crawler", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/14897058/disable-robots-txt-check-in-nutch", "last_activity_date": 1362173305, "accepted_answer_id": 15167175, "body": "<p>I wanna disable robots.txt checking in <a href=\"http://nutch.apache.org/\" rel=\"nofollow\">Nutch</a> and crawl everything from websites.\n<code>Disable</code> means before fetching or parsing any website, skip checking robot.txt.\nIs this possible?</p>\n", "creation_date": 1360939223, "score": 4},
{"title": "Nutch .Net implementation", "view_count": 986, "owner": {"user_id": 294085, "answer_count": 11, "creation_date": 1268668922, "accept_rate": 50, "view_count": 65, "reputation": 453}, "is_answered": true, "answers": [{"last_edit_date": 1313937718, "owner": {"user_id": 636354, "link": "http://stackoverflow.com/users/636354/yavuz", "user_type": "registered", "reputation": 48}, "body": "<p>I don't experince with it, but you can give it a try - <a href=\"http://arachnode.net/\" rel=\"nofollow\">http://arachnode.net/</a>\nas it indicates, its data is stored in Lucene.Net indexes.</p>\n", "question_id": 7110807, "creation_date": 1313890982, "is_accepted": false, "score": 2, "last_activity_date": 1313937718, "answer_id": 7135833}, {"question_id": 7110807, "owner": {"user_id": 254385, "link": "http://stackoverflow.com/users/254385/arachnode-net", "user_type": "registered", "reputation": 795}, "body": "<p>There was an effort for nutch.net for a while but it was abandoned.</p>\n", "creation_date": 1362019447, "is_accepted": true, "score": 1, "last_activity_date": 1362019447, "answer_id": 15126554}], "question_id": 7110807, "tags": [".net", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/7110807/nutch-net-implementation", "last_activity_date": 1362019447, "accepted_answer_id": 15126554, "body": "<p>We are building an enterprise search application and we use Lucene.net for indexing. We are looking to have a crawler.Is there a .Net implementation of Nutch - Web crawler? or is there any ongoing efforts for the same?</p>\n", "creation_date": 1313684720, "score": -1},
{"title": "Creating different tables in hbase for every diferent url in my seed file in nutch?", "view_count": 375, "is_answered": false, "answers": [{"question_id": 12674965, "owner": {"user_id": 1711935, "link": "http://stackoverflow.com/users/1711935/%d0%94%d0%b5%d1%8f%d0%bd-%d0%93%d1%8e%d1%80%d0%b4%d0%b6%d0%b5%d0%ba%d0%bb%d0%b8%d0%b5%d0%b2", "user_type": "registered", "reputation": 6}, "body": "<p>I found it.In nutch 2.0 threre is the so-called crawlId for the prefix of the name of the table :).</p>\n", "creation_date": 1349340534, "is_accepted": false, "score": 0, "last_activity_date": 1349340534, "answer_id": 12723578}, {"question_id": 12674965, "owner": {"user_id": 985880, "link": "http://stackoverflow.com/users/985880/4d6176", "user_type": "registered", "reputation": 3}, "body": "<p>I was searching for a similar functionality, but failed to understand how crawlId could be used without some hacking. </p>\n\n<p>What wasn't immediately clear is that inject can take a -crawlId  parameter (eg. nutch inject seed.txt -crawlId firstSeed)...This will then create a separate table in HBase called firstSeed_webpage for that entire .txt file. </p>\n\n<p>For the above use case it would be fairly straightforward to write a bash script which could read off the .txt line by line and give each URL a new crawlId.</p>\n\n<p>!!!!! (Note: For all commands related to that crawl you should include the -crawlId  flag to denote which table you intend on using.)</p>\n", "creation_date": 1362006130, "is_accepted": false, "score": 0, "last_activity_date": 1362006130, "answer_id": 15124228}], "question_id": 12674965, "tags": ["hbase", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/12674965/creating-different-tables-in-hbase-for-every-diferent-url-in-my-seed-file-in-nut", "last_activity_date": 1362006130, "owner": {"user_id": 1711935, "view_count": 19, "answer_count": 1, "creation_date": 1349099541, "reputation": 6}, "body": "<p>I'm using nutch 2.1 integrated with Hbase 0.92.1. When I fetch data from websites all of the data is written in only one  table in Hbase and this is my problem. The table's name is \"webpage\". </p>\n\n<p>Is there a way for every different URL in my seed file to be created a new table?</p>\n", "creation_date": 1349100712, "score": 0},
{"title": "Solr XSLTResponseWriter", "view_count": 321, "is_answered": false, "answers": [{"question_id": 15109643, "owner": {"user_id": 950178, "link": "http://stackoverflow.com/users/950178/jayendra", "user_type": "registered", "reputation": 34896}, "body": "<p>Add the parameters to the request handler </p>\n\n<pre><code>&lt;str name=\"wt\"&gt;json&lt;/str&gt;\n&lt;str name=\"tr\"&gt;example.xsl&lt;/str&gt;\n</code></pre>\n", "creation_date": 1361978272, "is_accepted": false, "score": 0, "last_activity_date": 1361978272, "answer_id": 15115661}], "question_id": 15109643, "tags": ["html", "xslt", "solr", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/15109643/solr-xsltresponsewriter", "last_activity_date": 1361978272, "owner": {"user_id": 1471836, "answer_count": 3, "creation_date": 1340275475, "accept_rate": 11, "view_count": 24, "reputation": 51}, "body": "<p>Am using Nutch and Solr for crawling web. when queried from solr interface, the result is in XML format. i want it to be displayed in normal html output.</p>\n\n<p>I have tried to attach the xslt steelsheet to the response of SOLR with\npassing this 2 variables wt=xslt&amp;tr=example.xsl referred from \n <a href=\"http://grokbase.com/t/nutch/user/103xcghta8/getting-solr-response-in-html-format-htmlresponsewriter\" rel=\"nofollow\">http://grokbase.com/t/nutch/user/103xcghta8/getting-solr-response-in-html-format-htmlresponsewriter</a></p>\n\n<p>then it works to some expectation but this is a manual input which everytime i have to write when querying. </p>\n\n<p>Can this be done automatically by modifying certain files in solr and nutch?</p>\n", "creation_date": 1361960642, "score": 0},
{"title": "Nutch does not crawl multiple sites", "view_count": 1673, "is_answered": true, "answers": [{"question_id": 10847893, "owner": {"user_id": 655312, "accept_rate": 75, "link": "http://stackoverflow.com/users/655312/ravi-singh", "user_type": "registered", "reputation": 1719}, "body": "<p>Try this in regex-urlfilter.txt :</p>\n\n<p>Old Settings:</p>\n\n<pre><code># accept anything else\n#+.\n+^http://1.a.b/*\n+^http://2.a.b/*\n</code></pre>\n\n<p>New Sertting :</p>\n\n<pre><code># accept anything else\n+.\n</code></pre>\n", "creation_date": 1361422508, "is_accepted": false, "score": 1, "last_activity_date": 1361422508, "answer_id": 14994813}], "question_id": 10847893, "tags": ["nutch", "web-crawler", "multiple-sites"], "answer_count": 1, "link": "http://stackoverflow.com/questions/10847893/nutch-does-not-crawl-multiple-sites", "last_activity_date": 1361422508, "owner": {"user_id": 1315894, "answer_count": 2, "creation_date": 1333646440, "accept_rate": 0, "view_count": 19, "reputation": 211}, "body": "<p>I'm trying to crawl multiple sites using Nutch. My seed.txt looks like this:</p>\n\n<pre><code>http://1.a.b/\nhttp://2.a.b/\n</code></pre>\n\n<p>and my regex-urlfilter.txt looks like this:</p>\n\n<pre><code># skip file: ftp: and mailto: urls\n-^(file|ftp|mailto):\n\n# skip image and other suffixes we can't yet parse\n# for a more extensive coverage use the urlfilter-suffix plugin\n-\\.(gif|GIF|jpg|JPG|png|PNG|ico|ICO|css|CSS|sit|SIT|eps|EPS|wmf|WMF|zip|ZIP|ppt|PPT|mpg|MPG|xls|XLS|gz|GZ|rpm|RPM|tgz|TGZ|mov|MOV|exe|EXE|jpeg|JPEG|bmp|BMP|js|JS)$\n\n# skip URLs containing certain characters as probable queries, etc.\n-[?*!@=]\n\n# skip URLs with slash-delimited segment that repeats 3+ times, to break loops\n-.*(/[^/]+)/[^/]+\\1/[^/]+\\1/\n\n# accept anything else\n#+.\n+^http://1.a.b/*\n+^http://2.a.b/*\n</code></pre>\n\n<p>I tried the following for the last part:</p>\n\n<pre><code>+^http://([a-z0-9]*\\.)*a.b/*\n</code></pre>\n\n<p>The only site crawled is the first one. All other configuration is default.</p>\n\n<p>I run the following command:</p>\n\n<pre><code>bin/nutch crawl urls -solr http://localhost:8984/solr/ -dir crawl -depth 10 -topN 10\n</code></pre>\n\n<p>Any ideas?!</p>\n\n<p>Thank you!</p>\n", "creation_date": 1338543538, "score": 0},
{"title": "Read Nutch crawled data from MySQL", "view_count": 347, "owner": {"age": 33, "answer_count": 85, "creation_date": 1299845597, "user_id": 655312, "accept_rate": 75, "view_count": 208, "location": "Pune, India", "reputation": 1719}, "is_answered": true, "answers": [{"question_id": 14985276, "owner": {"user_id": 655312, "accept_rate": 75, "link": "http://stackoverflow.com/users/655312/ravi-singh", "user_type": "registered", "reputation": 1719}, "body": "<p>I missed the content field in <code>MySQL</code> which is <code>LongBlob</code> and stores the image. </p>\n", "creation_date": 1361380775, "is_accepted": true, "score": 0, "last_activity_date": 1361380775, "answer_id": 14985967}], "question_id": 14985276, "tags": ["java", "mysql", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/14985276/read-nutch-crawled-data-from-mysql", "last_activity_date": 1361380775, "accepted_answer_id": 14985967, "body": "<p>I wrote a program to store nutch(2.1) crawled results in MySQL. It works fine. I get the file formats specified for search. I get few <code>jpeg</code> file. I want to fetch these images from DB(I am not sure if they are getting stored). I have a field with image URL. I can go and fetch it from there. But then what is the use of nutch storing so much data in MySQL.  Can you help me with that. <br>\nI am using following code :</p>\n\n<pre><code>    String crawlArg = \"urls  -threads 5\";\n\n    // Run Crawl tool\n\n     try {\n             ToolRunner.run(NutchConfiguration.create(), new org.apache.nutch.crawl.Crawler(),\n                             tokenize(crawlArg));\n     } catch (Exception e) {\n             e.printStackTrace();\n             return;\n     }\n</code></pre>\n\n<p>Please ask any more details that you might need. I am novice in nutch.</p>\n", "creation_date": 1361378689, "score": 1},
{"title": "Solr search - how to prevent duplicates from location hash", "view_count": 167, "owner": {"user_id": 692979, "answer_count": 4, "creation_date": 1302010343, "view_count": 8, "location": "Dublin, Ireland", "reputation": 39}, "is_answered": true, "answers": [{"question_id": 14713508, "owner": {"user_id": 950178, "link": "http://stackoverflow.com/users/950178/jayendra", "user_type": "registered", "reputation": 34896}, "body": "<p>You can filter the url ending with the pattern having # in them.\nYou can specify the patterns you want to include or exclude in <a href=\"http://svn.apache.org/viewvc/nutch/trunk/conf/regex-urlfilter.txt.template?view=markup\" rel=\"nofollow\">regex-urlfilter.txt</a>  </p>\n\n<p>This would allow only the root page to be indexed and the ones with # to be excluded from being indexed.</p>\n", "creation_date": 1360124140, "is_accepted": false, "score": 1, "last_activity_date": 1360124140, "answer_id": 14721434}, {"question_id": 14713508, "owner": {"user_id": 458870, "accept_rate": 58, "link": "http://stackoverflow.com/users/458870/samuel-garc%c3%ada", "user_type": "registered", "reputation": 1474}, "body": "<p>The first option, and best option, is don't crawl any page of this kind. Using, as @Jayendra mentioned, the regex urlfilter</p>\n\n<p>Another option you have is modifying indexed document values using the update chain feature. \nImplementing the interface <a href=\"http://wiki.apache.org/solr/UpdateRequest\" rel=\"nofollow\">UpdateRequestProcessor</a> and parse it by yourself using Java code, modifying the URL to remove the #anchor element.</p>\n\n<p>And the last option I have in mind is to use an <a href=\"http://wiki.apache.org/solr/AnalyzersTokenizersTokenFilters#solr.PatternReplaceCharFilterFactory\" rel=\"nofollow\">PatternReplaceCharFilterFactory</a> in the URL field as follows:</p>\n\n<pre><code>&lt;charFilter class=\"solr.PatternReplaceCharFilterFactory\" pattern=\"(#.*)\" replacement=\"\"/&gt;\n</code></pre>\n\n<p>Using the first solution you only index one page per \"location\". Using the others you will index all pages, but only the last is really indexed as Solr will delete any previous version of this document.</p>\n", "creation_date": 1360364448, "is_accepted": false, "score": 1, "last_activity_date": 1360364448, "answer_id": 14782724}, {"question_id": 14713508, "owner": {"user_id": 692979, "link": "http://stackoverflow.com/users/692979/shano", "user_type": "registered", "reputation": 39}, "body": "<p>OK I got this working, what I did was edit the <strong>regex-normalize.xml</strong> file and told it to ignore URLs with # in them:</p>\n\n<pre><code>&lt;regex&gt;\n  &lt;pattern&gt;#.*&lt;/pattern&gt;\n  &lt;substitution&gt;$1&lt;/substitution&gt;\n&lt;/regex&gt;\n</code></pre>\n\n<p>I needed to add \"urlfilter-regex\" to the plugin.includes property in <strong>nutch-site.xml</strong> to get it to use this file.</p>\n", "creation_date": 1360669354, "is_accepted": true, "score": 0, "last_activity_date": 1360669354, "answer_id": 14831757}], "question_id": 14713508, "tags": ["search", "solr", "lucene", "nutch"], "answer_count": 3, "link": "http://stackoverflow.com/questions/14713508/solr-search-how-to-prevent-duplicates-from-location-hash", "last_activity_date": 1360669354, "accepted_answer_id": 14831757, "body": "<p>I'm getting results back from a crawled internal site. The problem is I'm getting multiple results because of the use of location hashes in the code:</p>\n\n<pre><code>http://site.com/en/personal/refunds.html\nhttp://site.com/en/personal/refunds.html#\nhttp://site.com/en/personal/refunds.html#content\nhttp://site.com/en/personal/refunds.html#section1\n</code></pre>\n\n<p>Although they may all be relevant, it doesn't look good when they're my top four results! \nAny way they can be seen as one result?</p>\n\n<p>It looks like # and #content occur on most pages, so I could apply some rule to filter these out. They're used to skip to content and another to toggle accessibility stylesheet.</p>\n", "creation_date": 1360085779, "score": 0},
{"title": "Is it possible to store in solr full html page source code?", "view_count": 731, "owner": {"age": 27, "answer_count": 20, "creation_date": 1328855341, "user_id": 1201415, "accept_rate": 86, "view_count": 88, "location": "Minsk, Belarus", "reputation": 607}, "is_answered": true, "answers": [{"question_id": 14829631, "owner": {"user_id": 950178, "link": "http://stackoverflow.com/users/950178/jayendra", "user_type": "registered", "reputation": 34896}, "body": "<p>Nutch with Solr is a solution if you want to Crawl websites and have it indexed.<br>\n<a href=\"http://wiki.apache.org/nutch/NutchTutorial\" rel=\"nofollow\">Nutch with Solr Tutorial</a> will get you started.<br>\nHowever, Nutch would not maintain the Original Solr code with html tags.</p>\n\n<p>You would need to develop an custom solution by downloading the html page and then can use <a href=\"http://wiki.apache.org/solr/ExtractingRequestHandler\" rel=\"nofollow\">Solr Extracting Request Handler</a> to feed Solr with the HTML file and extract contents from the html file. e.g. at <a href=\"http://wiki.apache.org/solr/ExtractingRequestHandler#Getting_Started_with_the_Solr_Example\" rel=\"nofollow\">link</a>  </p>\n\n<p>Solr uses <a href=\"http://tika.apache.org/\" rel=\"nofollow\">Apache Tika</a> to extract contents from the <a href=\"http://tika.apache.org/1.2/formats.html#HyperText_Markup_Language\" rel=\"nofollow\">uploaded html file</a>  </p>\n\n<p>You can also check <a href=\"http://wiki.apache.org/solr/AnalyzersTokenizersTokenFilters#solr.HTMLStripCharFilterFactory\" rel=\"nofollow\">HTMLStripCharFilterFactory</a> if you are feeding data as html text.</p>\n", "creation_date": 1360666033, "is_accepted": true, "score": 3, "last_activity_date": 1360666033, "answer_id": 14830713}], "question_id": 14829631, "tags": ["java", "solr", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/14829631/is-it-possible-to-store-in-solr-full-html-page-source-code", "last_activity_date": 1360666033, "accepted_answer_id": 14830713, "body": "<p>In my previous question I got answer that I can store small index (few sites) data in solr without using any data base (<a href=\"http://stackoverflow.com/questions/14811669/is-it-posible-store-data-in-solr\">Is it posible store data in solr?</a>). I wonder, if it is possible to store full html page source code in solr without using any data base?</p>\n", "creation_date": 1360662515, "score": 1},
{"title": "How to modify search result page given by Solr?", "view_count": 1550, "owner": {"user_id": 1731650, "answer_count": 3, "creation_date": 1349783747, "accept_rate": 92, "view_count": 62, "reputation": 341}, "is_answered": true, "answers": [{"question_id": 14728903, "owner": {"user_id": 417864, "accept_rate": 75, "link": "http://stackoverflow.com/users/417864/alexandre-rafalovitch", "user_type": "registered", "reputation": 6649}, "body": "<p>A couple of things:</p>\n\n<ul>\n<li>If you are just starting, do not use Solr 3.6, go straight to latest 4.1+. A bunch of things have changed and a lot of new features are added.</li>\n<li>You seem to be saying that you will expose Solr + UI directly to general web - that's a really bad idea, as Solr is completely unsecured and allows web-based delete queries. You really want a business layer in a middle.</li>\n<li>With Solr 4.1, there is a pretty Admin UI and, also, there is a /browse page that shows how to use Velocity to do the pages backed by Solr. Or have a look at something like <a href=\"http://projectblacklight.org/\" rel=\"nofollow\">Project Blacklight</a> for an example of how to get UI over Solr.</li>\n</ul>\n", "creation_date": 1360154095, "is_accepted": false, "score": 2, "last_activity_date": 1360154095, "answer_id": 14729224}, {"question_id": 14728903, "owner": {"user_id": 1731650, "accept_rate": 92, "link": "http://stackoverflow.com/users/1731650/sushant-gupta", "user_type": "registered", "reputation": 341}, "body": "<p>I found below link \n<a href=\"http://cmusphinx.sourceforge.net/2012/06/building-a-java-application-with-apache-nutch-and-solr/\" rel=\"nofollow\">http://cmusphinx.sourceforge.net/2012/06/building-a-java-application-with-apache-nutch-and-solr/</a>\nwhich answered my query.</p>\n\n<p>I agree after reading the content available on above link, I felt very angry at me. \nSolr package provides all the required objects to query solr. </p>\n\n<p>Infact, the essential jars are just solr-solrj-3.4.0.jar, commons-httpclient-3.1.jar and slf4j-api-1.6.4.jar.</p>\n\n<p>Anyone can build a java search engine using these objects to query the database and have a fancy UI.</p>\n\n<p>Thanks again.</p>\n", "creation_date": 1360393608, "is_accepted": true, "score": 0, "last_activity_date": 1360393608, "answer_id": 14785812}], "question_id": 14728903, "tags": ["java", "solr", "search-engine", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/14728903/how-to-modify-search-result-page-given-by-solr", "last_activity_date": 1360393608, "accepted_answer_id": 14785812, "body": "<p>I intend to make a niche search engine. I am using apache-nutch-1.6 as the crawler and apache-solr-3.6.2 as the searcher. I must say there is very less updated information on web about these technologies.</p>\n\n<p>I followed this tutorial <a href=\"http://wiki.apache.org/nutch/NutchTutorial\" rel=\"nofollow\">http://wiki.apache.org/nutch/NutchTutorial</a> and have successfully installed apache and solr on my ubuntu system. I was also successful in injecting seed url to webdb and perform the crawl. </p>\n\n<p>Using solr interface at <code>http://localhost:8983/solr/admin</code>, I can also query the crawled results. But this is the output I receive. <img src=\"http://i.stack.imgur.com/FMAbq.png\" alt=\"enter image description here\">.</p>\n\n<p>Am I missing something here, the earlier apache-nutch-0.7 had a war which generated a clear html output like this. <img src=\"http://i.stack.imgur.com/5jv2Y.jpg\" alt=\"enter image description here\">. How do I achieve this... Or if anyone could point me to a latest tutorial or guidebook, highly appreciated.</p>\n", "creation_date": 1360153064, "score": 2},
{"title": "Indexing on Amazon S3 using Hadoop/Lucene", "view_count": 647, "is_answered": false, "question_id": 14775388, "tags": ["solr", "hadoop", "lucene", "nutch", "amazon-emr"], "answer_count": 0, "link": "http://stackoverflow.com/questions/14775388/indexing-on-amazon-s3-using-hadoop-lucene", "last_activity_date": 1360335749, "owner": {"user_id": 1355966, "answer_count": 0, "creation_date": 1335351456, "accept_rate": 40, "view_count": 27, "reputation": 8}, "body": "<p>I want to perform indexing on the files present in the S3 repository. I want to create mapper/reducers for it and trigger jobs that would distribute indexing using EMR and then complete it. Can anyone please help me on any demo or tutorials on this. \nI found one link here. But I am not able to download the zip files from here.\n<a href=\"http://aws.amazon.com/code/1119\" rel=\"nofollow\">http://aws.amazon.com/code/1119</a></p>\n\n<p>Also I would like to know if we can use Apache Nutch or Apache Solr. Have anyone experimented with this for indexing on S3 files.</p>\n", "creation_date": 1360335749, "score": 0},
{"title": "error while running solrindexer", "view_count": 1042, "is_answered": false, "answers": [{"question_id": 14709278, "owner": {"user_id": 417864, "accept_rate": 75, "link": "http://stackoverflow.com/users/417864/alexandre-rafalovitch", "user_type": "registered", "reputation": 6649}, "body": "<p>With the logs you showed, I think the answer will be on the Solr side. You should have an exception trace there which will tell you what component stopped the processing. And if it worked two weeks ago, either something changed (jar versions?) or you have a particular document that is a problem. </p>\n\n<p>If the problem happens with a single document (try a couple of different ones) than you probably have some environment (jars, properties, etc) change. If it does not happen with one subset of documents but happens with another, there might be an issue with specific document (e.g. wrong encoding).</p>\n\n<p>Again, Solr-side stack trace will be the first thing to check.</p>\n", "creation_date": 1360176793, "is_accepted": false, "score": 0, "last_activity_date": 1360176793, "answer_id": 14736589}], "question_id": 14709278, "tags": ["solr", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/14709278/error-while-running-solrindexer", "last_activity_date": 1360176793, "owner": {"user_id": 1731155, "answer_count": 2, "creation_date": 1349772488, "accept_rate": 0, "view_count": 21, "reputation": 291}, "body": "<p>now i use 3.6.1 and nutch 1.5 and it worked fine...i crawl my site and index the data into solr and use solr search, but two weeks ago it's started not work...\nWhene i use  ./nutch crawl urls -solr <code>http://localhost:8080/solr/</code> -depth 5 -topN 100  command it's work, but whene i use  ./nutch crawl urls -solr <code>http://localhost:8080/solr/</code> -depth 5 -topN 100000, it's throw an exception, in my log file i found this..     </p>\n\n<pre><code>2013-02-05 17:04:20,697 INFO  solr.SolrWriter - Indexing 250 documents\n2013-02-05 17:04:20,697 INFO  solr.SolrWriter - Deleting 0 documents\n2013-02-05 17:04:21,275 WARN  mapred.LocalJobRunner - job_local_0029\norg.apache.solr.common.SolrException: Internal Server Error\n\nInternal Server Error\n\nrequest: `http://localhost:8080/solr/update?wt=javabin&amp;version=2`\n    at org.apache.solr.client.solrj.impl.CommonsHttpSolrServer.request(CommonsHttpSolrServer.java:430)\n    at org.apache.solr.client.solrj.impl.CommonsHttpSolrServer.request(CommonsHttpSolrServer.java:244)\n    at org.apache.solr.client.solrj.request.AbstractUpdateRequest.process(AbstractUpdateRequest.java:105)\n    at org.apache.nutch.indexer.solr.SolrWriter.write(SolrWriter.java:124)\n    at org.apache.nutch.indexer.IndexerOutputFormat$1.write(IndexerOutputFormat.java:55)\n    at org.apache.nutch.indexer.IndexerOutputFormat$1.write(IndexerOutputFormat.java:44)\n    at org.apache.hadoop.mapred.ReduceTask$OldTrackingRecordWriter.write(ReduceTask.java:457)\n    at org.apache.hadoop.mapred.ReduceTask$3.collect(ReduceTask.java:497)\n    at org.apache.nutch.indexer.IndexerMapReduce.reduce(IndexerMapReduce.java:195)\n    at org.apache.nutch.indexer.IndexerMapReduce.reduce(IndexerMapReduce.java:51)\n    at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:519)\n    at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:420)\n    at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:260)\n2013-02-05 17:04:21,883 ERROR solr.SolrIndexer - java.io.IOException: Job failed!\n2013-02-05 17:04:21,887 INFO  solr.SolrDeleteDuplicates - SolrDeleteDuplicates: starting at 2013-02-05 17:04:21\n2013-02-05 17:04:21,887 INFO  solr.SolrDeleteDuplicates - SolrDeleteDuplicates: Solr url: `http://localhost:8080/solr/`    \n</code></pre>\n\n<p>two weeks ago it works well...\nDid anybody got similar problem? </p>\n\n<p>Hi, i just finish crawling and haw the same exception, but when i look at my log/hadoop.log file, i found this..</p>\n\n<pre><code>    2013-02-06 22:02:14,111 INFO  solr.SolrWriter - Indexing 250 documents\n2013-02-06 22:02:14,111 INFO  solr.SolrWriter - Deleting 0 documents\n2013-02-06 22:02:14,902 WARN  mapred.LocalJobRunner - job_local_0019\norg.apache.solr.common.SolrException: Bad Request\n\nBad Request\n\nrequest: `http://localhost:8080/solr/update?wt=javabin&amp;version=2`\n    at org.apache.solr.client.solrj.impl.CommonsHttpSolrServer.request(CommonsHttpSolrServer.java:430)\n    at org.apache.solr.client.solrj.impl.CommonsHttpSolrServer.request(CommonsHttpSolrServer.java:244)\n    at org.apache.solr.client.solrj.request.AbstractUpdateRequest.process(AbstractUpdateRequest.java:105)\n    at org.apache.nutch.indexer.solr.SolrWriter.write(SolrWriter.java:124)\n    at org.apache.nutch.indexer.IndexerOutputFormat$1.write(IndexerOutputFormat.java:55)\n    at org.apache.nutch.indexer.IndexerOutputFormat$1.write(IndexerOutputFormat.java:44)\n    at org.apache.hadoop.mapred.ReduceTask$OldTrackingRecordWriter.write(ReduceTask.java:457)\n    at org.apache.hadoop.mapred.ReduceTask$3.collect(ReduceTask.java:497)\n    at org.apache.nutch.indexer.IndexerMapReduce.reduce(IndexerMapReduce.java:304)\n    at org.apache.nutch.indexer.IndexerMapReduce.reduce(IndexerMapReduce.java:53)\n    at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:519)\n    at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:420)\n    at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:260)\n2013-02-06 22:02:15,027 ERROR solr.SolrIndexer - java.io.IOException: Job failed!\n2013-02-06 22:02:15,032 INFO  solr.SolrDeleteDuplicates - SolrDeleteDuplicates: starting at 2013-02-06 22:02:15\n2013-02-06 22:02:15,032 INFO  solr.SolrDeleteDuplicates - SolrDeleteDuplicates: Solr url: `http://localhost:8080/solr/`\n2013-02-06 22:02:21,281 WARN  mapred.FileOutputCommitter - Output path is null in cleanup\n2013-02-06 22:02:22,263 INFO  solr.SolrDeleteDuplicates - SolrDeleteDuplicates: finished at 2013-02-06 22:02:22, elapsed: 00:00:07\n2013-02-06 22:02:22,263 INFO  crawl.Crawl - crawl finished: crawl-20130206205733 \n</code></pre>\n\n<p>I hope it will help to understand the problem...</p>\n", "creation_date": 1360072881, "score": 0},
{"title": "Solr index empty after nutch solrindex command", "view_count": 1537, "is_answered": false, "answers": [{"question_id": 6950163, "owner": {"user_id": 384430, "accept_rate": 82, "link": "http://stackoverflow.com/users/384430/kaka", "user_type": "registered", "reputation": 1302}, "body": "<p>Did you issue a commit? without that the data will not be seen</p>\n", "creation_date": 1312512988, "is_accepted": false, "score": 0, "last_activity_date": 1312512988, "answer_id": 6950898}, {"question_id": 6950163, "owner": {"user_id": 18096, "link": "http://stackoverflow.com/users/18096/martin-dorey", "user_type": "registered", "reputation": 1742}, "body": "<p>I've spent much of today retracing your steps.  I eventually resorted to printf debugging in /opt/nutch/src/java/org/apache/nutch/indexer/IndexerMapReduce.java, which showed me that each URL I was trying to index was appearing twice, once starting with file:///var/www/Engineering/, as I'd originally specified, and once starting with file:/u/u60/Engineering/.  On this system, /var/www/Engineering is a symlink to /u/u60/Engineering.  Further, the /var/www/Engineering URLs were rejected because the parseText field wasn't supplied and the /u/u60/Engineering URLs were rejected because the fetchDatum field wasn't supplied.  Specifying the original URLs in the /u/u60/Engineering form solved my problem.  Hope that helps the next sap in this situation.</p>\n", "creation_date": 1359442876, "is_accepted": false, "score": 0, "last_activity_date": 1359442876, "answer_id": 14577363}], "question_id": 6950163, "tags": ["solr", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/6950163/solr-index-empty-after-nutch-solrindex-command", "last_activity_date": 1359442876, "owner": {"user_id": 643597, "answer_count": 4, "creation_date": 1299181531, "accept_rate": 75, "view_count": 19, "reputation": 45}, "body": "<p>I'm using Nutch and Solr to index a file share.</p>\n\n<p>I first issue: bin/nutch crawl urls</p>\n\n<p>Which gives me:</p>\n\n<pre><code>solrUrl is not set, indexing will be skipped...\ncrawl started in: crawl-20110804191414\nrootUrlDir = urls\nthreads = 10\ndepth = 5\nsolrUrl=null\nInjector: starting at 2011-08-04 19:14:14\nInjector: crawlDb: crawl-20110804191414/crawldb\nInjector: urlDir: urls\nInjector: Converting injected urls to crawl db entries.\nInjector: Merging injected urls into crawl db.\nInjector: finished at 2011-08-04 19:14:16, elapsed: 00:00:02\nGenerator: starting at 2011-08-04 19:14:16\nGenerator: Selecting best-scoring urls due for fetch.\nGenerator: filtering: true\nGenerator: normalizing: true\nGenerator: jobtracker is 'local', generating exactly one partition.\nGenerator: Partitioning selected urls for politeness.\nGenerator: segment: crawl-20110804191414/segments/20110804191418\nGenerator: finished at 2011-08-04 19:14:20, elapsed: 00:00:03\nFetcher: Your 'http.agent.name' value should be listed first in 'http.robots.agents' property.\nFetcher: starting at 2011-08-04 19:14:20\nFetcher: segment: crawl-20110804191414/segments/20110804191418\nFetcher: threads: 10\nQueueFeeder finished: total 1 records + hit by time limit :0\n-finishing thread FetcherThread, activeThreads=9\n-finishing thread FetcherThread, activeThreads=8\n-finishing thread FetcherThread, activeThreads=7\n-finishing thread FetcherThread, activeThreads=6\n-finishing thread FetcherThread, activeThreads=5\n-finishing thread FetcherThread, activeThreads=4\n-finishing thread FetcherThread, activeThreads=3\n-finishing thread FetcherThread, activeThreads=2\n-finishing thread FetcherThread, activeThreads=1\nfetching file:///mnt/public/Personal/Reminder Building Security.htm\n-finishing thread FetcherThread, activeThreads=0\n-activeThreads=0, spinWaiting=0, fetchQueues.totalSize=0\n-activeThreads=0\nFetcher: finished at 2011-08-04 19:14:22, elapsed: 00:00:02\nParseSegment: starting at 2011-08-04 19:14:22\nParseSegment: segment: crawl-20110804191414/segments/20110804191418\nParseSegment: finished at 2011-08-04 19:14:23, elapsed: 00:00:01\nCrawlDb update: starting at 2011-08-04 19:14:23\nCrawlDb update: db: crawl-20110804191414/crawldb\nCrawlDb update: segments: [crawl-20110804191414/segments/20110804191418]\nCrawlDb update: additions allowed: true\nCrawlDb update: URL normalizing: true\nCrawlDb update: URL filtering: true\nCrawlDb update: Merging segment data into db.\nCrawlDb update: finished at 2011-08-04 19:14:24, elapsed: 00:00:01\nGenerator: starting at 2011-08-04 19:14:24\nGenerator: Selecting best-scoring urls due for fetch.\nGenerator: filtering: true\nGenerator: normalizing: true\nGenerator: jobtracker is 'local', generating exactly one partition.\nGenerator: 0 records selected for fetching, exiting ...\nStopping at depth=1 - no more URLs to fetch.\nLinkDb: starting at 2011-08-04 19:14:25\nLinkDb: linkdb: crawl-20110804191414/linkdb\nLinkDb: URL normalize: true\nLinkDb: URL filter: true\nLinkDb: adding segment: file:/home/nutch/nutch-1.3/runtime/local/crawl-20110804191414/segments/20110804191418\nLinkDb: finished at 2011-08-04 19:14:26, elapsed: 00:00:01\ncrawl finished: crawl-20110804191414\n</code></pre>\n\n<p>Then I: bin/nutch solrindex <a href=\"http://localhost:8983/solr/\" rel=\"nofollow\">http://localhost:8983/solr/</a> crawl-20110804191414/crawldb crawl-20110804191414/linkdb crawl-20110804191414/segments/*</p>\n\n<p>Which gives me:</p>\n\n<pre><code>SolrIndexer: starting at 2011-08-04 19:17:07\nSolrIndexer: finished at 2011-08-04 19:17:08, elapsed: 00:00:01\n</code></pre>\n\n<p>When I do a <em>:</em> query on solr I get:</p>\n\n<pre><code>&lt;response&gt;\n     &lt;lst name=\"responseHeader\"&gt;\n          &lt;int name=\"status\"&gt;0&lt;/int&gt;\n          &lt;int name=\"QTime\"&gt;2&lt;/int&gt;\n          &lt;lst name=\"params\"&gt;\n               &lt;str name=\"indent\"&gt;on&lt;/str&gt;\n               &lt;str name=\"start\"&gt;0&lt;/str&gt;\n               &lt;str name=\"q\"&gt;*:*&lt;/str&gt;\n               &lt;str name=\"version\"&gt;2.2&lt;/str&gt;\n               &lt;str name=\"rows\"&gt;10&lt;/str&gt;\n          &lt;/lst&gt;\n     &lt;/lst&gt;\n     &lt;result name=\"response\" numFound=\"0\" start=\"0\"/&gt;\n&lt;/response&gt;\n</code></pre>\n\n<p>:(</p>\n\n<p>Note that this worked fine when I tried to use protocol-http to crawl a website but does not work when I use protocol-file to crawl a file system.</p>\n\n<p>---EDIT---\nAfter trying this again today I noticed that files with spaces in the names were causing a 404 error. That's a lot of files on the share I'm indexing. However, the thumbs.db files were making it in ok. This tells me that the problem is not what I thought it was.</p>\n", "creation_date": 1312504165, "score": 2},
{"title": "nutch 2.1 generator job runtime exception job failed", "view_count": 2806, "owner": {"user_id": 328836, "answer_count": 4, "creation_date": 1272541802, "accept_rate": 62, "view_count": 86, "location": "Goa India", "reputation": 500}, "is_answered": true, "answers": [{"question_id": 14182186, "owner": {"user_id": 328836, "accept_rate": 62, "link": "http://stackoverflow.com/users/328836/peter", "user_type": "registered", "reputation": 500}, "body": "<p>The above errors are due to insufficient space on the partition on the server where i have installed . check the answer at <a href=\"http://stackoverflow.com/questions/14290504/insufficient-space-for-shared-memory-file-when-i-try-to-run-nutch-generate-comma/14290628\">Insufficient space for shared memory file when i try to run nutch generate command</a></p>\n", "creation_date": 1359007441, "is_accepted": true, "score": 0, "last_activity_date": 1359007441, "answer_id": 14494954}], "question_id": 14182186, "tags": ["nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/14182186/nutch-2-1-generator-job-runtime-exception-job-failed", "last_activity_date": 1359007441, "accepted_answer_id": 14494954, "body": "<p>I am getting the below error when i try to generate urls using the generate command:</p>\n\n<p>GeneratorJob: java.lang.RuntimeException: job failed: name=generate: 1357474131-234134646, jobid=job_local_0001\n        at org.apache.nutch.util.NutchJob.waitForCompletion(NutchJob.java:54)\n        at org.apache.nutch.crawl.GeneratorJob.run(GeneratorJob.java:191)\n        at org.apache.nutch.crawl.GeneratorJob.generate(GeneratorJob.java:213)\n        at org.apache.nutch.crawl.GeneratorJob.run(GeneratorJob.java:241)\n        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n        at org.apache.nutch.crawl.GeneratorJob.main(GeneratorJob.java:249)</p>\n\n<p>The generate,fetch,parse were working fine but updatedb was giving this error before sometimes:\nException in thread \"main\" java.lang.RuntimeException: job failed: name=update-table, jobid=job_local_0001\n    at org.apache.nutch.util.NutchJob.waitForCompletion(NutchJob.java:54)\n    at org.apache.nutch.crawl.DbUpdaterJob.run(DbUpdaterJob.java:98)\n    at org.apache.nutch.crawl.DbUpdaterJob.updateTable(DbUpdaterJob.java:105)\n    at org.apache.nutch.crawl.DbUpdaterJob.run(DbUpdaterJob.java:119)\n    at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n    at org.apache.nutch.crawl.DbUpdaterJob.main(DbUpdaterJob.java:123)</p>\n\n<p>Now it is continuously giving generate job failed. What might be the issue? can it be mysql issue??</p>\n", "creation_date": 1357475880, "score": 0},
{"title": "Recrawl URL with Nutch just for updated sites", "view_count": 2839, "owner": {"user_id": 1878786, "view_count": 5, "answer_count": 2, "creation_date": 1354704920, "reputation": 135}, "is_answered": true, "answers": [{"last_edit_date": 1357833162, "owner": {"user_id": 1275577, "accept_rate": 100, "link": "http://stackoverflow.com/users/1275577/%c4%b0smet-alkan", "user_type": "registered", "reputation": 3787}, "body": "<p>Simply you can't. You need to recrawl the page to control if it's updated. So according to your needs, prioritize the pages/domains and recrawl them within a time period. For that you need a job scheduler such as <a href=\"http://quartz-scheduler.org/\" rel=\"nofollow\">Quartz</a>. </p>\n\n<p>You need to write a function that compares the pages. However, Nutch originally saves the pages as index files. In other words Nutch generates new binary files to save HTMLs. I don't think it's possible to compare binary files, as Nutch combines all crawl results within a single file. If you want to save pages in raw HTML format to compare, see my answer to <a href=\"http://stackoverflow.com/questions/10007178/how-do-i-save-the-origin-html-file-with-apache-nutch/10060160#10060160\">this question.</a> </p>\n", "question_id": 14261586, "creation_date": 1357832708, "is_accepted": true, "score": 4, "last_activity_date": 1357833162, "answer_id": 14261679}, {"question_id": 14261586, "owner": {"user_id": 950178, "link": "http://stackoverflow.com/users/950178/jayendra", "user_type": "registered", "reputation": 34896}, "body": "<p>You have to Schedule ta Job for Firing the Job<br>\nHowever, Nutch AdaptiveFetchSchedule should enable you to crawl and index pages and detect whether the page is new or updated and you don't have to do it manually.</p>\n\n<p><a href=\"http://pascaldimassimo.com/2010/06/11/how-to-re-crawl-with-nutch/\" rel=\"nofollow\">Article</a> describes the same in detail.</p>\n", "creation_date": 1357884349, "is_accepted": false, "score": 4, "last_activity_date": 1357884349, "answer_id": 14272620}, {"question_id": 14261586, "owner": {"user_id": 1973842, "link": "http://stackoverflow.com/users/1973842/user1973842", "user_type": "registered", "reputation": 20}, "body": "<p>what about <a href=\"http://pascaldimassimo.com/2010/06/11/how-to-re-crawl-with-nutch/\" rel=\"nofollow\">http://pascaldimassimo.com/2010/06/11/how-to-re-crawl-with-nutch/</a></p>\n\n<p>This is discussed on : <a href=\"http://stackoverflow.com/questions/13873694/how-to-recrawle-nutch\">How to recrawle nutch</a></p>\n\n<p>I am wondering if the above mentioned solution will indeed work. I am trying as we speak. I crawl news-sites and they update their frontpage quite frequently, so I need to re-crawl the index/frontpage often and fetch the newly discovered links.</p>\n", "creation_date": 1358070637, "is_accepted": false, "score": 1, "last_activity_date": 1358070637, "answer_id": 14302375}], "question_id": 14261586, "tags": ["apache", "solr", "lucene", "nutch", "web-crawler"], "answer_count": 3, "link": "http://stackoverflow.com/questions/14261586/recrawl-url-with-nutch-just-for-updated-sites", "last_activity_date": 1358070637, "accepted_answer_id": 14261679, "body": "<p>I crawled one URL with Nutch 2.1 and then I want to re-crawl pages after they got updated. How can I do this? How can I know that a page is updated?</p>\n", "creation_date": 1357832429, "score": 8},
{"title": "Nutch How to avoid crawl calendar webpage generated by CGI", "view_count": 260, "owner": {"user_id": 561629, "answer_count": 2, "creation_date": 1294080535, "accept_rate": 61, "view_count": 48, "reputation": 215}, "is_answered": true, "answers": [{"question_id": 9029424, "owner": {"user_id": 1150329, "accept_rate": 89, "link": "http://stackoverflow.com/users/1150329/tejas-patil", "user_type": "registered", "reputation": 4793}, "body": "<p>Add regex patterns to <code>conf/regex-urlfilter.txt</code> to speficy rules to accept or reject urls.</p>\n", "creation_date": 1333466797, "is_accepted": true, "score": 0, "last_activity_date": 1333466797, "answer_id": 9996483}], "question_id": 9029424, "tags": ["web-crawler", "webpage", "nutch", "dynamic-websites"], "answer_count": 1, "link": "http://stackoverflow.com/questions/9029424/nutch-how-to-avoid-crawl-calendar-webpage-generated-by-cgi", "last_activity_date": 1358000491, "accepted_answer_id": 9996483, "body": "<p>I am using Nutch to crawl a large website. </p>\n\n<p>The webpages are generated by CGI program. Most of the webpages' URL contains expressions such as <code>?id=2323&amp;title=foo</code>. </p>\n\n<p>I want to crawl these webpages as they contain many useful information. </p>\n\n<p>However, a problem I'm facing is that this website has a calendar. Some date-like webpages are generated too. That means Nutch will try to crawl some innocent webpages such as <code>year=2030&amp;month=12</code>. </p>\n\n<p>This is quite stupid. </p>\n\n<p>How can I avoid such trap in Nutch? Writing many regex expression?</p>\n", "creation_date": 1327642992, "score": 0},
{"title": "How to index new data with Solr 4.0 when the database is updated?", "view_count": 311, "is_answered": false, "question_id": 14275603, "tags": ["solr", "lucene", "indexing", "nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/14275603/how-to-index-new-data-with-solr-4-0-when-the-database-is-updated", "last_activity_date": 1357898152, "owner": {"age": 27, "answer_count": 28, "creation_date": 1340810733, "user_id": 1486136, "accept_rate": 89, "view_count": 130, "location": "Bitola, Macedonia (FYROM)", "reputation": 2255}, "body": "<p>I have a database populated with fetched and parsed web pages. Does Solr know automatically which of the columns are updated to index only the changes? Or should i make my own java code or something else (like conf. changes) to check if there are any changes?</p>\n", "creation_date": 1357898152, "score": 3},
{"title": "Nutch 2.1 (HBase, SOLR) with Amazon Web Services", "view_count": 903, "owner": {"user_id": 1469138, "answer_count": 2, "creation_date": 1340193155, "accept_rate": 60, "view_count": 11, "reputation": 33}, "is_answered": true, "answers": [{"last_edit_date": 1357435556, "owner": {"user_id": 1150329, "accept_rate": 89, "link": "http://stackoverflow.com/users/1150329/tejas-patil", "user_type": "registered", "reputation": 4793}, "body": "<p>If you have a cluster with same capacity as that of a AWS cluster (that you plan to invest in) then there is no advantage except for #1 below. </p>\n\n<p>Here are several factors that you should think about before switching to AWS:</p>\n\n<ol>\n<li><p><strong>Locality of hosts crawled</strong>: If you are sitting in Europe and the websites that you want to crawl are hosted far away ... say Australia. If you buy AWS nodes located in Australia, it would be much faster for crawling that data rather than crawling from Europe.</p></li>\n<li><p><strong>Cost</strong>: For using AWS machines, you need to pay then on hourly basis. Can you afford that ? If not better use your own machines</p></li>\n<li><p><strong>Current cluster capacity</strong> : does your current cluster has ample capacity and space to handle the  amount of crawled data ? I think there wont be problem in terms of computational speed as Nutch runs on Hadoop which was designed to run on commodity hardware. Can your cluster accommodate entire data that is being fetched by the crawler.</p></li>\n<li><p><strong>Volume of data</strong> : What is a rough estimate of the data that is being crawled ? If its less, then it makes no sense to have an AWS cluster.</p></li>\n<li><p><strong>Time constraints</strong> : Is there any time bound for completion for the crawl ?</p></li>\n</ol>\n\n<p>If you are doing this for a professional project, then these factors must be given a thought. </p>\n\n<p>If you are doing it for fun/hobby/learning, go ahead and use free tier nodes of AWS. Those are low capacity nodes given free by Amazon. Its fun to learn new things :)</p>\n\n<p><strong>Advantages of AWS:</strong> </p>\n\n<ol>\n<li>No need to buy machines for setting up a cluster. get started without having any hardware except a terminal PC.</li>\n<li>Locality</li>\n<li>No need to look after machines. If a node crashes badly, leave it (its not your problem :P). Buy a new one, add it to the cluster and go ahead. </li>\n</ol>\n\n<p><strong>Disadvantages of AWS:</strong> </p>\n\n<ol>\n<li>Costly.</li>\n<li>Copying data to any machine outside AWS cluster is charged.</li>\n<li>Your data is NOT persisted when u give up the procured AWS nodes. If u want to persist it, pay them and use the S3 storage service.</li>\n</ol>\n", "question_id": 14139296, "creation_date": 1357435094, "is_accepted": true, "score": 3, "last_activity_date": 1357435556, "answer_id": 14178310}], "question_id": 14139296, "tags": ["solr", "amazon", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/14139296/nutch-2-1-hbase-solr-with-amazon-web-services", "last_activity_date": 1357435556, "accepted_answer_id": 14178310, "body": "<p>I experienced Nutch 2.1 locally without any difficulty. I have also tried on a 3 machine distributed cluster. We're now discussing whether to run it with Amazon Web Services or not. I do not have much experience with AWS. My question is that, is it possible and neccessary to try Nutch2.1 crawling and indexing parts on the cloud. What possible advantages and disadvantages we will have? </p>\n\n<p>Thanks. </p>\n", "creation_date": 1357217363, "score": 1},
{"title": "Solr&#39;s schema and how it works", "view_count": 197, "is_answered": false, "answers": [{"question_id": 11148746, "owner": {"user_id": 1202349, "accept_rate": 89, "link": "http://stackoverflow.com/users/1202349/krunal", "user_type": "registered", "reputation": 1302}, "body": "<p>You need to define it in Solr schema.xml by declaring all the fields and its field type. You can then query Solr for any field to search.</p>\n\n<p>Refer this: <a href=\"http://wiki.apache.org/solr/SchemaXml\" rel=\"nofollow\">http://wiki.apache.org/solr/SchemaXml</a></p>\n", "creation_date": 1340454620, "is_accepted": false, "score": 0, "last_activity_date": 1340454620, "answer_id": 11169515}, {"question_id": 11148746, "owner": {"user_id": 743245, "accept_rate": 75, "link": "http://stackoverflow.com/users/743245/harmstyler", "user_type": "registered", "reputation": 794}, "body": "<p>Solr will not automatically index content from a website. You need to tell it how to index your content. Solr only knows the content you tell it to know. Extracting phone numbers sounds pretty simple so writing an update script or finding one online should not be an issue. Good luck!</p>\n", "creation_date": 1340459052, "is_accepted": false, "score": 0, "last_activity_date": 1340459052, "answer_id": 11169986}], "question_id": 11148746, "tags": ["solr", "lucene", "indexing", "schema", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/11148746/solrs-schema-and-how-it-works", "last_activity_date": 1356545461, "owner": {"user_id": 1473598, "answer_count": 14, "creation_date": 1340324548, "accept_rate": 78, "view_count": 15, "reputation": 346}, "body": "<p>Hey so I started researching about Solr and have a couple of questions on how Solr works. I know the schema defines what is stored and indexed in the Solr application. But I'm confuse as to how Solr knows that the \"content\" is the content of the site or that the url is the url?</p>\n\n<p>My main goal is I'm trying to extract phone numbers from websites and I want Solr to nicely spit out 1234567890.</p>\n", "creation_date": 1340325066, "score": 0},
{"title": "Nutch regex-urlfilter syntax", "view_count": 6005, "owner": {"user_id": 670595, "answer_count": 4, "creation_date": 1300769486, "accept_rate": 100, "view_count": 114, "location": "Denver, CO", "reputation": 458}, "is_answered": true, "answers": [{"question_id": 13884249, "owner": {"user_id": 1408096, "accept_rate": 82, "link": "http://stackoverflow.com/users/1408096/xhudik", "user_type": "registered", "reputation": 1542}, "body": "<p>Just a guess (I'm not allowed to add remark, so I'm putting my guess here):</p>\n\n<p>According to</p>\n\n<p><a href=\"http://wiki.apache.org/nutch/FAQ#What_happens_if_I_inject_urls_several_times.3F\">http://wiki.apache.org/nutch/FAQ#What_happens_if_I_inject_urls_several_times.3F</a></p>\n\n<p>You can't have multiple URLs (they will be ignored). What about to put ONLY:</p>\n\n<pre><code>    +^http://www.example.com/foo.cfm/(.+)*$\n</code></pre>\n\n<p>(which should cover your first line: <code>+^http://www.example.com/foo.cfm$</code> as well)\nOr, if there are problems with \"/\", try:</p>\n\n<pre><code>    +^http://www.example.com/foo.cfm//?(.+)*$\n</code></pre>\n\n<p>Where <code>//?</code> should stand for character <code>/</code> or <code></code> </p>\n\n<p>(If my guess is completely off - give me a note and I'll delete it)</p>\n", "creation_date": 1355826756, "is_accepted": true, "score": 5, "last_activity_date": 1355826756, "answer_id": 13930996}], "question_id": 13884249, "tags": ["regex", "web-crawler", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/13884249/nutch-regex-urlfilter-syntax", "last_activity_date": 1355826756, "accepted_answer_id": 13930996, "body": "<p>I am running Nutch v. 1.6 and it is crawling specific sites correctly, but I can't seem to get the syntax correct for the file <code>NUTCH_ROOT/conf/regex-urlfilter.txt</code>.</p>\n\n<p>The site I want to crawl has a URL similar to this:</p>\n\n<pre><code>http://www.example.com/foo.cfm\n</code></pre>\n\n<p>On that page there are numerous links that match the following pattern:</p>\n\n<pre><code>http://www.example.com/foo.cfm/Bar_-_Foo/Extra/EX/20817/ID=6976\n</code></pre>\n\n<p>I want to crawl links that match second example above as well. In my <code>regex-urlfilter.txt</code> I have the following:</p>\n\n<pre><code>+^http://www.example.com/foo.cfm$\n+^http://www.example.com/foo.cfm/(.+)*$\n</code></pre>\n\n<p>Nutch matches on the first one and crawls it correctly, but does not seem to pick up links using the other filter. How can I get Nutch to crawl URL's like the second one above?</p>\n\n<p>I have tried the following with no luck:</p>\n\n<pre><code>+^http://www.example.com/foo.cfm/(.+)*$\n+^http://www.example.com/foo.cfm/(.)*$\n+^http://www.example.com/foo.cfm/.+$\n+^http://www.example.com/foo.cfm/(.*)*$\n</code></pre>\n\n<p>In my <code>NUTCH_ROOT/urls/nutch</code> I have:</p>\n\n<pre><code>http://www.example.com/foo.cfm/\n</code></pre>\n", "creation_date": 1355509353, "score": 3},
{"title": "does nutch have web service API?", "view_count": 382, "is_answered": true, "answers": [{"question_id": 13704773, "owner": {"user_id": 1866610, "link": "http://stackoverflow.com/users/1866610/jjh", "user_type": "registered", "reputation": 11}, "body": "<p>In this document (<a href=\"http://www.apache.org/dist/nutch/2.1/CHANGES-2.1.txt\" rel=\"nofollow\">http://www.apache.org/dist/nutch/2.1/CHANGES-2.1.txt</a>) you see that nutch has a REST API.\nhere is what it looks like: <a href=\"http://nutch.apache.org/apidocs-2.0/index.html\" rel=\"nofollow\">http://nutch.apache.org/apidocs-2.0/index.html</a> </p>\n\n<p>What is your specific question?</p>\n", "creation_date": 1354914715, "is_accepted": false, "score": 1, "last_activity_date": 1354914715, "answer_id": 13771034}], "question_id": 13704773, "tags": ["java", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/13704773/does-nutch-have-web-service-api", "last_activity_date": 1354914715, "owner": {"user_id": 1874945, "view_count": 6, "answer_count": 0, "creation_date": 1354607707, "reputation": 6}, "body": "<p>I know that Nutch has commands that can be used in a java project, but does Nutch have web services as API? (for example, like ElasticSearch REST API)</p>\n", "creation_date": 1354630108, "score": 0},
{"title": "Liferay + Solr + Nutch", "view_count": 381, "is_answered": true, "answers": [{"question_id": 13376209, "owner": {"user_id": 1882282, "link": "http://stackoverflow.com/users/1882282/prabodhini", "user_type": "unregistered", "reputation": 11}, "body": "<p>Liferay search portlet needs uid to be the primary key.\nHence, keep the unique key as uid and then copy unique key of nutch into uid by using copyField \n</p>\n", "creation_date": 1354795881, "is_accepted": false, "score": 1, "last_activity_date": 1354795881, "answer_id": 13743371}], "question_id": 13376209, "tags": ["apache", "solr", "liferay", "web-crawler", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/13376209/liferay-solr-nutch", "last_activity_date": 1354795881, "owner": {"user_id": 1820478, "view_count": 13, "answer_count": 1, "creation_date": 1352801131, "reputation": 11}, "body": "<p>Readme: <a href=\"http://stackoverflow.com/questions/11324162/nutch-and-solr-with-liferay\">Nutch and Solr with Liferay</a></p>\n\n<p>I am at the exact same point now. I tried to merge the schema files or just changed the uniquekey but nothing worked. Nutch 1.4 and Solr 3.6.1 run fine when using the same schema, the one from nutch. Liferay 6.1.0 solr-web plugin and Solr 3.6.1 also run fine when both use the schema from solr-web.</p>\n\n<p>Have you solved this problem?</p>\n\n<p>Is there an alternative? We also have a Google Search Appliance. Crawling is no problem, but how to integrate into the Liferay search-portlet? Maybe it is possible to use the Liferay Luscene Search + Google Search Appliance?</p>\n", "creation_date": 1352885746, "score": 0},
{"title": "How to integrate apache nutch with apache solr on linux?", "view_count": 370, "is_answered": true, "answers": [{"question_id": 13723368, "owner": {"user_id": 950178, "link": "http://stackoverflow.com/users/950178/jayendra", "user_type": "registered", "reputation": 34896}, "body": "<p><a href=\"http://wiki.apache.org/nutch/NutchTutorial\" rel=\"nofollow\">NutchTutorial</a> covers step by step instructions for configuring Nutch and Solr Integration</p>\n", "creation_date": 1354712549, "is_accepted": false, "score": 1, "last_activity_date": 1354712549, "answer_id": 13723862}], "question_id": 13723368, "tags": ["solr", "glassfish", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/13723368/how-to-integrate-apache-nutch-with-apache-solr-on-linux", "last_activity_date": 1354712549, "owner": {"age": 26, "answer_count": 0, "creation_date": 1354628713, "user_id": 1875888, "accept_rate": 20, "view_count": 23, "reputation": 48}, "body": "<p>I am using glassfish server and eclips IDE. I already downloaded apache solr and configured into glassfish. Now I stucked integration of nutch with solr.</p>\n\n<p>As I new to these concepts, can any one help me on this?</p>\n", "creation_date": 1354710866, "score": 1},
{"title": "Configure Nutch to only index specific filetypes in Solr", "view_count": 400, "is_answered": false, "answers": [{"question_id": 10820740, "owner": {"user_id": 525590, "accept_rate": 75, "link": "http://stackoverflow.com/users/525590/jpee", "user_type": "registered", "reputation": 111}, "body": "<p>In nutch you can define filters for urls. What about filtering by the name of the fileextension?</p>\n", "creation_date": 1354287738, "is_accepted": false, "score": 0, "last_activity_date": 1354287738, "answer_id": 13647700}, {"question_id": 10820740, "owner": {"user_id": 950178, "link": "http://stackoverflow.com/users/950178/jayendra", "user_type": "registered", "reputation": 34896}, "body": "<p>You can filter the file type according to the extension.<br>\nYou can specify the extensions you want to include or exclude in <a href=\"http://svn.apache.org/viewvc/nutch/trunk/conf/regex-urlfilter.txt.template?view=markup\" rel=\"nofollow\">regex-urlfilter.txt</a>  </p>\n\n<p>e.g. for exclusion (-) :-</p>\n\n<blockquote>\n  <p>#skip image and other suffixes we can't yet parse 29 # for a more extensive coverage use the urlfilter-suffix plugin\n  -.(gif|GIF|jpg|JPG|png|PNG|ico|ICO|css|CSS|sit|SIT|eps|EPS|wmf|WMF|zip|ZIP|ppt|PPT|mpg|MPG|xls|XLS|gz|GZ|rpm|RPM|tgz|TGZ|mov|MOV|exe|EXE|jpeg|JPEG|bmp|BMP|js|JS)$</p>\n</blockquote>\n\n<p>with + you can just specify the inclusion list.</p>\n", "creation_date": 1354296776, "is_accepted": false, "score": 0, "last_activity_date": 1354296776, "answer_id": 13650197}], "question_id": 10820740, "tags": ["solr", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/10820740/configure-nutch-to-only-index-specific-filetypes-in-solr", "last_activity_date": 1354296776, "owner": {"user_id": 594677, "view_count": 32, "answer_count": 16, "creation_date": 1296271241, "reputation": 802}, "body": "<p>I am looking for a way to configure Nutch to crawl the web, but only index certain types of files (XML to be specific) into Solr.  I'm pretty sure a custom plugin would do the job, probably based on the index-more code, but I'd rather not do that unless I have to.  I'm also sure I could suck everything into Solr then delete unwanted content with Solr's API, but this is a bit hacky.  Is there a way to configure Nutch to only index certain filetypes in Solr?</p>\n", "creation_date": 1338396747, "score": 2},
{"title": "What is the role of NUTCH if we are going to make a search engine using Hadoop and Solr?", "view_count": 294, "is_answered": true, "answers": [{"question_id": 12303946, "owner": {"user_id": 525590, "accept_rate": 75, "link": "http://stackoverflow.com/users/525590/jpee", "user_type": "registered", "reputation": 111}, "body": "<p>We are using Nutch as a webcrawler and Solr for searching in some productive environments. So I hope I can give you some information about 3).</p>\n\n<p>How does this work? Nutch has it's own crawling db and some websites where it starts crawling. It has some plugins where you can configure different things like pdf crawling, which fields will be extracted of html sites and so on. When crawling Nutch stores all links extracted from a website and will follow them in the next cycle. All crawling results will be stored in a crawl db. In Nutch you configure an intervall where crawled results will be outdated and the crawler begins from the defined startsites.</p>\n\n<p>The results inside the crawl db will be synchronized to the solr index. So you are searching on the solr index. Nutch is in this constallation only to get data from websites and providing them for solr.</p>\n", "creation_date": 1354287240, "is_accepted": false, "score": 1, "last_activity_date": 1354287240, "answer_id": 13647541}], "question_id": 12303946, "tags": ["solr", "hadoop", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/12303946/what-is-the-role-of-nutch-if-we-are-going-to-make-a-search-engine-using-hadoop-a", "last_activity_date": 1354287240, "owner": {"user_id": 1652512, "answer_count": 5, "creation_date": 1346945794, "accept_rate": 13, "view_count": 53, "reputation": 360}, "body": "<p>I want to make a search engine. In which i want to crawl some sites and stored their indexes and info in Hadoop. And then using Solr search will be done.\n                                 But I am facing lots of issues. If search over google then different people give different suggestions and different configuring ways for setup a hadoop based search engine.\nThese are my some questions :</p>\n\n<p>1) How the crawling will be done? Is there any use of NUTCH for completing the crawling or not? If yes then how Hadoop and NUTCH communicate with each other?</p>\n\n<p>2) What is the use of Solr? If NUTCH done Crawling and stored their crawled indexes and their information into the Hadoop then what's the role of Solr?</p>\n\n<p>3) Can we done searching using Solr and Nutch? If yes then where they will saved their crawled indexes?</p>\n\n<p>4) How Solr communicate with Hadoop?</p>\n\n<p>5) Please explain me one by one steps if possible, that how can i crawl some sites and save their info into DB(Hadoop or any other) and then do search .</p>\n\n<p>I am really really stuck with this. Any help will really appreciated. </p>\n\n<p>A very big Thanks in advance. :) </p>\n\n<p>Please help me to sort out my huge issue please</p>\n", "creation_date": 1346947069, "score": 3},
{"title": "Using HtmlParseFilter with Tika parser in Nutch 1.5", "view_count": 210, "owner": {"user_id": 365719, "answer_count": 218, "creation_date": 1276439286, "accept_rate": 70, "view_count": 3234, "reputation": 5678}, "is_answered": true, "answers": [{"question_id": 13262254, "owner": {"user_id": 1274085, "link": "http://stackoverflow.com/users/1274085/hari", "user_type": "registered", "reputation": 166}, "body": "<p>HtmlParseFilter does handle all content types (that Tika can). It has been renamed to ParseFilter in the 2.x branches to more accurately reflect what it does.</p>\n", "creation_date": 1354085784, "is_accepted": true, "score": 1, "last_activity_date": 1354085784, "answer_id": 13599472}], "question_id": 13262254, "tags": ["java", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/13262254/using-htmlparsefilter-with-tika-parser-in-nutch-1-5", "last_activity_date": 1354085784, "accepted_answer_id": 13599472, "body": "<p>What I'd like to do: write a Nutch plugin that receives the parsed data from every page crawled. I know that <code>HtmlParseFilter</code> does what I need for HTML pages, but I also want to process other types of content. When Tika parses, say, a PDF or Word document, would it pass the results to my registered <code>HtmlParseFilter</code>? If not, is there any other way to intercept Tika's output?</p>\n", "creation_date": 1352253909, "score": 0},
{"title": "Does any open, simply extendible web crawler exists?", "view_count": 2025, "owner": {"age": 37, "answer_count": 42, "creation_date": 1262857345, "user_id": 245402, "accept_rate": 67, "view_count": 178, "location": "Mosonmagyar&#243;v&#225;r, Hungary", "reputation": 3328}, "is_answered": true, "answers": [{"question_id": 2085213, "owner": {"user_id": 5190, "accept_rate": 74, "link": "http://stackoverflow.com/users/5190/vinko-vrsalovic", "user_type": "registered", "reputation": 166536}, "body": "<p>I heartily recommend <a href=\"http://crawler.archive.org\" rel=\"nofollow\">heritrix</a>. It is VERY flexible and I'd argue is the most battle tested freely available open source crawler, as it's the one the Internet Archive uses.</p>\n", "creation_date": 1263810747, "is_accepted": false, "score": 2, "last_activity_date": 1263810747, "answer_id": 2085315}, {"question_id": 2085213, "owner": {"user_id": 113535, "accept_rate": 100, "link": "http://stackoverflow.com/users/113535/kane", "user_type": "registered", "reputation": 10407}, "body": "<p>You should be able to find something that fits your needs <a href=\"http://www.manageability.org/blog/stuff/open-source-web-crawlers-java\" rel=\"nofollow\">here</a>.</p>\n", "creation_date": 1263812584, "is_accepted": false, "score": 2, "last_activity_date": 1263812584, "answer_id": 2085477}, {"question_id": 2085213, "owner": {"user_id": 108452, "accept_rate": 83, "link": "http://stackoverflow.com/users/108452/joseph-salisbury", "user_type": "registered", "reputation": 1043}, "body": "<p>A quick search on <a href=\"http://github.com/\" rel=\"nofollow\">GitHub</a> threw up <a href=\"http://github.com/chriskite/anemone\" rel=\"nofollow\">Anemone</a>, a web spider framework which seems to fit your requirements - particularly extensiblility. Written in Ruby.<br>\nHope it goes well!</p>\n", "creation_date": 1263849896, "is_accepted": true, "score": 2, "last_activity_date": 1263849896, "answer_id": 2089324}, {"question_id": 2085213, "owner": {"user_id": 231762, "link": "http://stackoverflow.com/users/231762/kkrugler", "user_type": "registered", "reputation": 1500}, "body": "<p>I've used Nutch extensively, when I was building the open source project index for my Krugle startup. It's hard to customize, being a fairly monolithic design. There is a plug-in architecture, but the interaction between plug-ins and the system is tricky and fragile.</p>\n\n<p>As a result of that experience, and needing something with more flexibility, I started the Bixo project - a web mining toolkit. <a href=\"http://openbixo.org\" rel=\"nofollow\">http://openbixo.org</a>.</p>\n\n<p>Whether it's right for you depends on the weighting of factors such as:</p>\n\n<ol>\n<li>How much flexibility you need (+)</li>\n<li>How mature it should be (-)</li>\n<li>Whether you need the ability to scale (+)</li>\n<li>If you're comfortable with Java/Hadoop (+)</li>\n</ol>\n", "creation_date": 1264952864, "is_accepted": false, "score": 4, "last_activity_date": 1264952864, "answer_id": 2172265}], "question_id": 2085213, "tags": ["web-scraping", "web-crawler", "nutch"], "answer_count": 4, "link": "http://stackoverflow.com/questions/2085213/does-any-open-simply-extendible-web-crawler-exists", "last_activity_date": 1353935338, "accepted_answer_id": 2089324, "body": "<p>I search for a web crawler solution which can is mature enough and can be simply extended. I am interested in the following features... or possibility to extend the crawler to meet them:</p>\n\n<ul>\n<li>partly just to read the feeds of several sites</li>\n<li>to scrap the content of these sites</li>\n<li>if the site has an archive I would like to crawl and index it as well</li>\n<li>the crawler should be capable to explore part of the Web for me and it should be able to decide which sites matches the given criteria</li>\n<li>should be able to notify me, if things possibly matching my interest were found</li>\n<li>the crawler should not kill the servers by attacking it by too many requests, it should be smart doing crawling</li>\n<li>the crawler should be robust against freak sites and servers</li>\n</ul>\n\n<p>Those things above can be done one by one without any big effort, but I am interested in any solution which provide a customisable, extendible crawler. I heard of Apache Nutch, but very unsure about the project so far. Do you have experiences with it? Can you recommend alternatives?</p>\n", "creation_date": 1263809485, "score": 7},
{"title": "error while running nutch on hadoop multi cluster environment", "view_count": 295, "owner": {"user_id": 1456249, "answer_count": 28, "creation_date": 1339678274, "accept_rate": 100, "view_count": 80, "location": "Hyderabad, India", "reputation": 510}, "is_answered": true, "answers": [{"last_edit_date": 1353910114, "owner": {"user_id": 1456249, "accept_rate": 100, "link": "http://stackoverflow.com/users/1456249/swamy", "user_type": "registered", "reputation": 510}, "body": "<p>I could solve this issue. when copying files from local file system to HDFS destination filesystem, it used to be like this: bin/hadoop dfs -put ~/nutch/urls urls. </p>\n\n<p>However it should be \"bin/hadoop dfs -put ~/nutch/urls/* urls\", here urls/* will allow sub directories.  </p>\n", "question_id": 13514429, "creation_date": 1353634849, "is_accepted": true, "score": 0, "last_activity_date": 1353910114, "answer_id": 13522160}], "question_id": 13514429, "tags": ["hadoop", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/13514429/error-while-running-nutch-on-hadoop-multi-cluster-environment", "last_activity_date": 1353910114, "accepted_answer_id": 13522160, "body": "<p>I am running nutch on hadoop multi cluster environment.  </p>\n\n<p>Hadoop is throwing an error when nutch is being executed using the following command</p>\n\n<p>$ bin/hadoop jar /home/nutch/nutch/runtime/deploy/nutch-1.5.1.job org.apache.nutch.crawl.Crawl urls -dir urls  -depth 1 -topN 5</p>\n\n<p>Error:\nException in thread \"main\" java.io.IOException: Not a file:\nhdfs://master:54310/user/nutch/urls/crawldb\n        at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:170)\n        at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:515)\n        at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:753)\n        at com.bdc.dod.dashboard.BDCQueryStatsViewer.run(BDCQueryStatsViewer.java:829)\n        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n        at com.bdc.dod.dashboard.BDCQueryStatsViewer.main(BDCQueryStatsViewer.java:796)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n        at java.lang.reflect.Method.invoke(Method.java:585)\n        at org.apache.hadoop.util.RunJar.main(RunJar.java:155)</p>\n\n<p>I tried with possible ways of solving this and fixed all the issues like setting http.agent.name in /local/conf path etc. And I installed earlier and it was smooth.</p>\n\n<p>Can anybody suggest a solution?</p>\n\n<p>By the way, I followed <a href=\"http://www.rui-yang.com/develop/build-nutch-1-4-cluster-with-hadoop/\" rel=\"nofollow\">link</a> for installing and running.</p>\n", "creation_date": 1353593216, "score": 0},
{"title": "Stanford NLP installation", "view_count": 650, "owner": {"user_id": 1162512, "answer_count": 27, "creation_date": 1327163131, "accept_rate": 58, "view_count": 772, "reputation": 2192}, "is_answered": true, "answers": [{"question_id": 11277541, "owner": {"user_id": 1822867, "link": "http://stackoverflow.com/users/1822867/harsh", "user_type": "unregistered", "reputation": 86}, "body": "<p>I don't have much experience with nutch. However for NLP, there are several good places you can refer for <a href=\"http://nlp.stanford.edu/software/corenlp.shtml\" rel=\"nofollow\">the stanford nlp library</a>, such as <a href=\"http://rajvardhan.wordpress.com/\" rel=\"nofollow\">this blog</a>. </p>\n", "creation_date": 1352874285, "is_accepted": true, "score": 3, "last_activity_date": 1352874285, "answer_id": 13374003}], "question_id": 11277541, "tags": ["nlp", "nutch", "stanford-nlp"], "answer_count": 1, "link": "http://stackoverflow.com/questions/11277541/stanford-nlp-installation", "last_activity_date": 1352874285, "accepted_answer_id": 13374003, "body": "<p>I am designing a customized web search engine using open source Nutch-1.4 along with Solr and Hadoop.\n<br>The designing of NLP part was supposed to be taken care by my friend, but since he backed out , I need to configure NLP with Nutch.<br>\nWhat I need is when any user fires a query, the Stanford NLP tool should fetch the important terms from the query, matches those words from the index and then returns the result page. Thats what i think NLP is used for. Sorry for my poor knowledge of NLP.<br>\nNow, how can i integrate StanfordNLP core with Nutch?? <br></p>\n\n<p>Your suggestions for making my engine more efficient in case of NLP are also welcomed!!!</p>\n", "creation_date": 1341089132, "score": 0},
{"title": "Field mapping for HTML", "view_count": 129, "is_answered": true, "answers": [{"question_id": 13305999, "owner": {"user_id": 950178, "link": "http://stackoverflow.com/users/950178/jayendra", "user_type": "registered", "reputation": 34896}, "body": "<p>There was work done on <a href=\"https://issues.apache.org/jira/browse/NUTCH-978\" rel=\"nofollow\">extracting element from HTML by XPATH</a>, so you can check on it.  </p>\n\n<p>However, using meta tags can be a better option as the html pages are usually malformed for the xpath expression. You can use <a href=\"http://wiki.apache.org/nutch/IndexMetatags\" rel=\"nofollow\">Index Metatags</a> with Nutch to extract the metatag and populate the fields in Solr.</p>\n", "creation_date": 1352459125, "is_accepted": false, "score": 1, "last_activity_date": 1352459125, "answer_id": 13306901}], "question_id": 13305999, "tags": ["solr", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/13305999/field-mapping-for-html", "last_activity_date": 1352459125, "owner": {"user_id": 1773304, "answer_count": 0, "creation_date": 1351146931, "accept_rate": 0, "view_count": 3, "reputation": 16}, "body": "<p>I am now to Solr and Nutch. I have a question on the field mapping in solr / nutch schema.xml. I want solr/nutch pickup some keywords from one of my website. I know that the schema.xml has a field mapping section to allow me to do so. What I want to know is how should I put the keyword field in the HTML ? Should I just hte HTML hidden field:</p>\n\n<pre><code>&lt;input type=\"hidden\" name=\"keyword\" .... /&gt; \n</code></pre>\n\n<p>Or should I use the </p>\n\n<pre><code>&lt;meta/&gt; \n</code></pre>\n\n<p>tag ?</p>\n", "creation_date": 1352455833, "score": 0},
{"title": "Configuration for running nutch unit tests in eclipse", "view_count": 160, "owner": {"user_id": 1786743, "view_count": 1, "answer_count": 1, "creation_date": 1351627937, "reputation": 18}, "is_answered": true, "answers": [{"question_id": 13146988, "owner": {"user_id": 442945, "accept_rate": 81, "link": "http://stackoverflow.com/users/442945/nathaniel-ford", "user_type": "registered", "reputation": 8904}, "body": "<p>If you have a unit test in your project, you can right click and 'Run-as'. Select 'JUnit test' (or other test framework). This should initialize the unit test module in eclipse.</p>\n", "creation_date": 1351629626, "is_accepted": true, "score": 1, "last_activity_date": 1351629626, "answer_id": 13147253}], "question_id": 13146988, "tags": ["java", "eclipse", "junit", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/13146988/configuration-for-running-nutch-unit-tests-in-eclipse", "last_activity_date": 1351629626, "accepted_answer_id": 13147253, "body": "<p>I have nutch set up for eclipse and can run and debug to my hearts content.  I can compile from eclipse or the command line, using ant.  I can test all modules individually.  However, when running the generic nutch download tests from the command line - e.g. ant test, I get a few failures in some of the (unmodified) modules.  </p>\n\n<p>My question: how do I run the unit tests from within eclipse so I can see where the tests are failing? Thanks for your help!</p>\n", "creation_date": 1351628337, "score": 0},
{"title": "How do I integrate Nutch and Solr to work with Drupal?", "view_count": 661, "owner": {"user_id": 554990, "answer_count": 0, "creation_date": 1293455928, "accept_rate": 50, "view_count": 3, "reputation": 30}, "is_answered": true, "answers": [{"last_edit_date": 1351013050, "owner": {"user_id": 1769040, "link": "http://stackoverflow.com/users/1769040/krlucas", "user_type": "unregistered", "reputation": 26}, "body": "<p>If you are using Drupal 7 the <a href=\"http://drupal.org/project/sarnia\" rel=\"nofollow\">Sarnia Module</a> + <a href=\"http://drupal.org/project/search_api\" rel=\"nofollow\">Search API</a> + <a href=\"http://drupal.org/project/search_api_solr\" rel=\"nofollow\">Search API Solr</a> will allow you to search and present data from any Solr index (using Views). Search API is an alternative to the Apache Solr Search Integration module.  </p>\n", "question_id": 12822152, "creation_date": 1351011809, "is_accepted": true, "score": 1, "last_activity_date": 1351013050, "answer_id": 13035645}], "question_id": 12822152, "tags": ["drupal", "solr", "integration", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/12822152/how-do-i-integrate-nutch-and-solr-to-work-with-drupal", "last_activity_date": 1351013050, "accepted_answer_id": 13035645, "body": "<p>I configured my Drupal website to work with a Solr instance (using Apache Solr search inegration module) in order to have a better search than the one given by Drupal. In addition, I want my website to find results from many other websites when performing a search, and for that reason I configured a Nutch instance to work with the Solr instance (I mentioned above). \nNow, I can see data that was indexed by Drupal and data that was indexed by Nutch through my Solr instance interface. The problem is, though, that I can't see any data that was indexed by Nutch when performing a search in my Drupal website.\nAny ideas? </p>\n", "creation_date": 1349880260, "score": 1},
{"title": "&quot;Fatal Error&quot; following nutch tutorial &quot;markup in the document following the root must be well formed&quot;", "view_count": 498, "owner": {"user_id": 1601666, "answer_count": 1, "creation_date": 1345063406, "accept_rate": 100, "view_count": 6, "reputation": 53}, "is_answered": true, "answers": [{"question_id": 13020015, "owner": {"user_id": 871026, "accept_rate": 76, "link": "http://stackoverflow.com/users/871026/reimeus", "user_type": "registered", "reputation": 123109}, "body": "<p>Your properties must appear inside the configuration tags:</p>\n\n<pre><code>&lt;configuration&gt;\n   &lt;property&gt;\n      &lt;name&gt;http.agent.name&lt;/name&gt;\n      &lt;value&gt;Nutch Spidah&lt;/value&gt;\n   &lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre>\n\n<p>See this <a href=\"http://wiki.apache.org/nutch/nutch-site.xml\" rel=\"nofollow\">example</a>.</p>\n", "creation_date": 1350940680, "is_accepted": true, "score": 4, "last_activity_date": 1350940680, "answer_id": 13020042}, {"question_id": 13020015, "owner": {"user_id": 1708432, "accept_rate": 17, "link": "http://stackoverflow.com/users/1708432/sjramsay", "user_type": "registered", "reputation": 450}, "body": "<p>Try putting the configuration tags around the property tags</p>\n", "creation_date": 1350940715, "is_accepted": false, "score": 0, "last_activity_date": 1350940715, "answer_id": 13020054}], "question_id": 13020015, "tags": ["java", "xml", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/13020015/fatal-error-following-nutch-tutorial-markup-in-the-document-following-the-roo", "last_activity_date": 1350978763, "accepted_answer_id": 13020042, "body": "<p>I'm following the tutorial for Apache Nutch.</p>\n\n<p>I have the conf/nutch-site.xml configured like so: </p>\n\n<pre><code>&lt;property&gt;\n &lt;name&gt;http.agent.name&lt;/name&gt;\n &lt;value&gt;Nutch Spidah&lt;/value&gt;\n&lt;/property&gt;\n\n&lt;configuration&gt;\n\n&lt;/configuration&gt;\n</code></pre>\n\n<p>I'm getting this error:</p>\n\n<p>[Fatal Error] nutch-site.xml:11:2: The markup in the document following the root element must be well-formed.</p>\n\n<p>I know I'm missing something obvious. Any help is appreciated. Thanks!</p>\n", "creation_date": 1350940490, "score": 2},
{"title": "hadoop1.0.3 &amp; nutch1.5.1 can&#39;t update crawlDB", "view_count": 93, "is_answered": false, "question_id": 12988939, "tags": ["hadoop", "nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/12988939/hadoop1-0-3-nutch1-5-1-cant-update-crawldb", "last_activity_date": 1350848713, "owner": {"user_id": 1761552, "view_count": 3, "answer_count": 0, "creation_date": 1350738335, "reputation": 16}, "body": "<p>I try to deploy hadoop on my server then I build nutch1.5.1 using <a href=\"http://wiki.apache.org/nutch/NutchHadoopTutorial\" rel=\"nofollow\">this tutorial</a> to run nutch on hadoop. I hadn't got any error message in log files but the crawlDB can't update any urls so that crawlDB always has the init urls.</p>\n\n<p>In my development ENV I run the org.apache.nutch.crawl.Crawl urls -dir crawl -depth 3 that it's working fine.</p>\n\n<p>In my server side I run the script something like that:</p>\n\n<pre><code>./runtime/deploy/bin/nutch crawl urls -dir crawl -depth 3 -topN 5\n</code></pre>\n\n<p>the urls file I had copied in HDFS.</p>\n\n<p>Do I need to config something?</p>\n", "creation_date": 1350740013, "score": 1},
{"title": "Nutch: Invoke in Java, not command line?", "view_count": 1945, "owner": {"user_id": 156210, "answer_count": 22, "creation_date": 1250216222, "accept_rate": 82, "view_count": 55, "reputation": 882}, "is_answered": true, "answers": [{"question_id": 5421020, "owner": {"user_id": 58099, "accept_rate": 74, "link": "http://stackoverflow.com/users/58099/vojislav-stojkovic", "user_type": "registered", "reputation": 5876}, "body": "<p>If you take a look inside <code>bin/nutch</code> script, you'll see that it invokes a Java class corresponding to your command:</p>\n\n<pre><code># figure out which class to run\nif [ \"$COMMAND\" = \"crawl\" ] ; then\n  CLASS=org.apache.nutch.crawl.Crawl\nelif [ \"$COMMAND\" = \"inject\" ] ; then\n  CLASS=org.apache.nutch.crawl.Injector\nelif [ \"$COMMAND\" = \"generate\" ] ; then\n  CLASS=org.apache.nutch.crawl.Generator\nelif [ \"$COMMAND\" = \"freegen\" ] ; then\n  CLASS=org.apache.nutch.tools.FreeGenerator\nelif [ \"$COMMAND\" = \"fetch\" ] ; then\n  CLASS=org.apache.nutch.fetcher.Fetcher\nelif [ \"$COMMAND\" = \"fetch2\" ] ; then\n  CLASS=org.apache.nutch.fetcher.Fetcher2\nelif [ \"$COMMAND\" = \"parse\" ] ; then\n  CLASS=org.apache.nutch.parse.ParseSegment\nelif [ \"$COMMAND\" = \"readdb\" ] ; then\n  CLASS=org.apache.nutch.crawl.CrawlDbReader\nelif [ \"$COMMAND\" = \"convdb\" ] ; then\n  CLASS=org.apache.nutch.tools.compat.CrawlDbConverter\nelif [ \"$COMMAND\" = \"mergedb\" ] ; then\n  CLASS=org.apache.nutch.crawl.CrawlDbMerger\nelif [ \"$COMMAND\" = \"readlinkdb\" ] ; then\n  CLASS=org.apache.nutch.crawl.LinkDbReader\nelif [ \"$COMMAND\" = \"readseg\" ] ; then\n  CLASS=org.apache.nutch.segment.SegmentReader\nelif [ \"$COMMAND\" = \"segread\" ] ; then\n  echo \"[DEPRECATED] Command 'segread' is deprecated, use 'readseg' instead.\"\n  CLASS=org.apache.nutch.segment.SegmentReader\nelif [ \"$COMMAND\" = \"mergesegs\" ] ; then\n  CLASS=org.apache.nutch.segment.SegmentMerger\nelif [ \"$COMMAND\" = \"updatedb\" ] ; then\n  CLASS=org.apache.nutch.crawl.CrawlDb\nelif [ \"$COMMAND\" = \"invertlinks\" ] ; then\n  CLASS=org.apache.nutch.crawl.LinkDb\nelif [ \"$COMMAND\" = \"mergelinkdb\" ] ; then\n  CLASS=org.apache.nutch.crawl.LinkDbMerger\nelif [ \"$COMMAND\" = \"index\" ] ; then\n  CLASS=org.apache.nutch.indexer.Indexer\nelif [ \"$COMMAND\" = \"solrindex\" ] ; then\n  CLASS=org.apache.nutch.indexer.solr.SolrIndexer\nelif [ \"$COMMAND\" = \"dedup\" ] ; then\n  CLASS=org.apache.nutch.indexer.DeleteDuplicates\nelif [ \"$COMMAND\" = \"solrdedup\" ] ; then\n  CLASS=org.apache.nutch.indexer.solr.SolrDeleteDuplicates\nelif [ \"$COMMAND\" = \"merge\" ] ; then\n  CLASS=org.apache.nutch.indexer.IndexMerger\nelif [ \"$COMMAND\" = \"plugin\" ] ; then\n  CLASS=org.apache.nutch.plugin.PluginRepository\nelif [ \"$COMMAND\" = \"server\" ] ; then\n  CLASS='org.apache.nutch.searcher.DistributedSearch$Server'\nelse\n  CLASS=$COMMAND\nfi\n\n# run it\nexec \"$JAVA\" $JAVA_HEAP_MAX $NUTCH_OPTS -classpath \"$CLASSPATH\" $CLASS \"$@\"\n</code></pre>\n\n<p>From there on, it's only the question of looking at the <a href=\"http://nutch.apache.org/apidocs-1.2/index.html\">API docs</a> and, if necessary, source code for those classes.</p>\n", "creation_date": 1300979888, "is_accepted": true, "score": 6, "last_activity_date": 1300979888, "answer_id": 5421443}], "question_id": 5421020, "tags": ["java", "documentation", "web-crawler", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/5421020/nutch-invoke-in-java-not-command-line", "last_activity_date": 1350614453, "accepted_answer_id": 5421443, "body": "<p>Am I being thick or is there really no way to invoke Apache Nutch through some Java code programmatically? Where is the documentation (or a guide or tutorial) on how to do this? Google has failed me. So I actually tried Bing. (Yes, I know, pathetic.) Ideas? Thanks in advance.</p>\n\n<p>(Also, if Nutch is a crap-shoot any other crawlers written in Java that are proven to be reliable on an internet scale with actual documentation?)</p>\n", "creation_date": 1300978219, "score": 6},
{"title": "Nutch : How to separate url result from the depth : 1 and the result from depth : 2", "view_count": 144, "is_answered": true, "answers": [{"question_id": 12797053, "owner": {"user_id": 1150329, "accept_rate": 89, "link": "http://stackoverflow.com/users/1150329/tejas-patil", "user_type": "registered", "reputation": 4793}, "body": "<p>There is no in-built function in nutch to do this. But simple hack will be to run the nutch command with dept 1, copy the web table and then run again for deth 1. So you will have 2 versions of the nutch web-table corresponding to each round</p>\n", "creation_date": 1350547225, "is_accepted": false, "score": 1, "last_activity_date": 1350547225, "answer_id": 12949811}], "question_id": 12797053, "tags": ["nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/12797053/nutch-how-to-separate-url-result-from-the-depth-1-and-the-result-from-depth", "last_activity_date": 1350547225, "owner": {"user_id": 1731279, "answer_count": 0, "creation_date": 1349775013, "accept_rate": 0, "view_count": 10, "reputation": 9}, "body": "<p>After I run this command in nutch: </p>\n\n<p><code>bin/nutch crawl urls -dir crawl -depth 3 -topN 5</code></p>\n\n<p>I get a list of urls, just say 50 urls , but anyone know to separate all the url by the depth.</p>\n\n<p>So I will get the result:</p>\n\n<p>URL from depth 1 = 5 urls</p>\n\n<ul>\n<li><p>url</p></li>\n<li><p>url</p></li>\n<li><p>url</p></li>\n</ul>\n\n<p>......</p>\n\n<p>URL from depth 2 = 15 urls</p>\n\n<ul>\n<li><p>url</p></li>\n<li><p>url</p></li>\n<li><p>url</p></li>\n</ul>\n\n<p>......</p>\n\n<p>Something like that, is there anyone already solved this problem?</p>\n\n<p>Is there an function in nutch to solved this problem?</p>\n\n<p>Any help will be appreciate.</p>\n", "creation_date": 1349775472, "score": 0},
{"title": "running nutch on the hadoop \uff0cwhere is the nutch logs\uff1f", "view_count": 770, "owner": {"user_id": 1617241, "view_count": 2, "answer_count": 0, "creation_date": 1345645262, "reputation": 8}, "is_answered": true, "answers": [{"question_id": 12908905, "owner": {"user_id": 432844, "link": "http://stackoverflow.com/users/432844/julien-nioche", "user_type": "registered", "reputation": 939}, "body": "<p>If you are running Hadoop in distributed or pseudo distributed mode then you should be able to access the logs per job with the map reduce admin interface (e.g. port 50030 of the master node). See <a href=\"http://hadoop.apache.org/docs/stable/cluster_setup.html#Logging\" rel=\"nofollow\">1</a> for the location of the logs in Hadoop </p>\n", "creation_date": 1350414091, "is_accepted": false, "score": 1, "last_activity_date": 1350414091, "answer_id": 12921666}, {"question_id": 12908905, "owner": {"user_id": 1150329, "accept_rate": 89, "link": "http://stackoverflow.com/users/1150329/tejas-patil", "user_type": "registered", "reputation": 4793}, "body": "<p>If you are running nutch on hadoop, the logs corresponding to each mapper and reducer of each phase is generated. The location of that is <code>{HADOOP_LOG_DIR}/userlogs/&lt;task id&gt;/syslog</code></p>\n", "creation_date": 1350547065, "is_accepted": true, "score": 1, "last_activity_date": 1350547065, "answer_id": 12949775}], "question_id": 12908905, "tags": ["logging", "hadoop", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/12908905/running-nutch-on-the-hadoop-where-is-the-nutch-logs", "last_activity_date": 1350547065, "accepted_answer_id": 12949775, "body": "<p>my nutch running on the hadoop \uff0cI want to check the nutch running logs,\n but cannot find the output logs like the standalone nutch logs.</p>\n", "creation_date": 1350369784, "score": 1},
{"title": "Does Nutch 2.1 still support file segments?", "view_count": 123, "owner": {"user_id": 1647891, "answer_count": 6, "creation_date": 1310693153, "accept_rate": 60, "view_count": 22, "location": "Australia", "reputation": 130}, "is_answered": true, "answers": [{"question_id": 12927583, "owner": {"user_id": 1150329, "accept_rate": 89, "link": "http://stackoverflow.com/users/1150329/tejas-patil", "user_type": "registered", "reputation": 4793}, "body": "<p>You mean the nutch segments which had part files ? According to me, Nutch 2.0 does NOT have that.</p>\n", "creation_date": 1350546823, "is_accepted": true, "score": 1, "last_activity_date": 1350546823, "answer_id": 12949709}], "question_id": 12927583, "tags": ["nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/12927583/does-nutch-2-1-still-support-file-segments", "last_activity_date": 1350546823, "accepted_answer_id": 12949709, "body": "<p>Does Nutch 2.1 still support file segments as the data storage?</p>\n\n<p>I cannot find my answer from the documentation or wiki.</p>\n", "creation_date": 1350451206, "score": 0},
{"title": "Nutch 2.1 cannot setup in Mac", "view_count": 1579, "is_answered": true, "answers": [{"question_id": 12927997, "owner": {"user_id": 1150329, "accept_rate": 89, "link": "http://stackoverflow.com/users/1150329/tejas-patil", "user_type": "registered", "reputation": 4793}, "body": "<pre><code>Caused by: org.apache.hadoop.hbase.MasterNotRunningException\n</code></pre>\n\n<p>this indicates that the clsuter setup is not done correctly. The nutch tutorial page mentions this:</p>\n\n<blockquote>\n  <p>Install and configure HBase. You can get it here (N.B. Gora 0.2 uses\n  HBase 0.90.4, however the setup is known to work with more recent\n  versions of the HBase 0.90.x branch)</p>\n</blockquote>\n\n<p>Have you performed this step correctly ?</p>\n", "creation_date": 1350546619, "is_accepted": false, "score": 2, "last_activity_date": 1350546619, "answer_id": 12949652}], "question_id": 12927997, "tags": ["nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/12927997/nutch-2-1-cannot-setup-in-mac", "last_activity_date": 1350546619, "owner": {"user_id": 1647891, "answer_count": 6, "creation_date": 1310693153, "accept_rate": 60, "view_count": 22, "location": "Australia", "reputation": 130}, "body": "<p>Trying to set up the new Nutch 2.1 in local environments. With the fresh download, then \"ant build\". Following the document from wiki <a href=\"http://wiki.apache.org/nutch/Nutch2Tutorial\" rel=\"nofollow\">http://wiki.apache.org/nutch/Nutch2Tutorial</a> however, it seems that no luck</p>\n\n<p>I got the following errors:</p>\n\n<pre>\njava[1815:1903] Unable to load realm info from SCDynamicStore\nInjectorJob: org.apache.gora.util.GoraException: org.apache.hadoop.hbase.MasterNotRunningException\n    at org.apache.gora.store.DataStoreFactory.createDataStore(DataStoreFactory.java:167)\n    at org.apache.gora.store.DataStoreFactory.createDataStore(DataStoreFactory.java:135)\n    at org.apache.nutch.storage.StorageUtils.createWebStore(StorageUtils.java:75)\n    at org.apache.nutch.crawl.InjectorJob.run(InjectorJob.java:214)\n    at org.apache.nutch.crawl.InjectorJob.inject(InjectorJob.java:228)\n    at org.apache.nutch.crawl.InjectorJob.run(InjectorJob.java:248)\n    at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n    at org.apache.nutch.crawl.InjectorJob.main(InjectorJob.java:258)\nCaused by: org.apache.hadoop.hbase.MasterNotRunningException\n    at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.getMaster(HConnectionManager.java:394)\n    at org.apache.hadoop.hbase.client.HBaseAdmin.(HBaseAdmin.java:94)\n    at org.apache.gora.hbase.store.HBaseStore.initialize(HBaseStore.java:108)\n    at org.apache.gora.store.DataStoreFactory.initializeDataStore(DataStoreFactory.java:102)\n    at org.apache.gora.store.DataStoreFactory.createDataStore(DataStoreFactory.java:161)\n    ... 7 more\n</pre>\n\n<p>Your help is highly appreciated. thanks</p>\n", "creation_date": 1350453439, "score": 0},
{"title": "nutch2.0 hadoop Input path does not exist", "view_count": 1051, "is_answered": false, "answers": [{"question_id": 12781338, "owner": {"user_id": 850593, "link": "http://stackoverflow.com/users/850593/octo", "user_type": "registered", "reputation": 650}, "body": "<p>Looks like you don't have input path. Excactly as hadoop said.\nCheck, that hdfs dfs -ls /user/yuqing/2 return something (2 should be file or directory)</p>\n\n<p>As for the second part, when you remove hadoop configs, hadoop library uses internal configs (you can find them in distribution with names *-default.xml, f.e. core-default.xml), and hadoop functions in 'local' mode. In 'local' mode all paths are local (in local filesystem). \nSo, when you reference file in 'hdfs' mode, f.e. hdfs dfs -ls /some/file, hadoop will lookup file in hdfs (hdfs://namenode.ip/some/file), but in local mode file will be searched in relative (typically file:/home/user/some/file). \nYou can see that in your output: file:/home/yuqing/workspace/nutch2.0/2</p>\n", "creation_date": 1349724477, "is_accepted": false, "score": 0, "last_activity_date": 1349724477, "answer_id": 12788185}], "question_id": 12781338, "tags": ["hadoop", "hbase", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/12781338/nutch2-0-hadoop-input-path-does-not-exist", "last_activity_date": 1349724477, "owner": {"user_id": 1677276, "answer_count": 5, "creation_date": 1347877339, "view_count": 5, "location": "China", "reputation": 4}, "body": "<pre><code>Exception in thread \"main\" org.apache.hadoop.mapreduce.lib.input.InvalidInputException: Input path does not exist: hdfs://yuqing-namenode:9000/user/yuqing/2\nat org.apache.hadoop.mapreduce.lib.input.FileInputFormat.listStatus(FileInputFormat.java:235)\nat org.apache.hadoop.mapreduce.lib.input.FileInputFormat.getSplits(FileInputFormat.java:252)\nat org.apache.hadoop.mapred.JobClient.writeNewSplits(JobClient.java:962)\nat org.apache.hadoop.mapred.JobClient.writeSplits(JobClient.java:979)\nat org.apache.hadoop.mapred.JobClient.access$600(JobClient.java:174)\nat org.apache.hadoop.mapred.JobClient$2.run(JobClient.java:897)\nat org.apache.hadoop.mapred.JobClient$2.run(JobClient.java:850)\nat java.security.AccessController.doPrivileged(Native Method)\nat javax.security.auth.Subject.doAs(Subject.java:415)\nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1121)\nat org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:850)\nat org.apache.hadoop.mapreduce.Job.submit(Job.java:500)\nat org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:530)\nat org.apache.nutch.util.NutchJob.waitForCompletion(NutchJob.java:50)\nat org.apache.nutch.crawl.InjectorJob.run(InjectorJob.java:219)\nat org.apache.nutch.crawl.Crawler.runTool(Crawler.java:68)\nat org.apache.nutch.crawl.Crawler.run(Crawler.java:136)\nat org.apache.nutch.crawl.Crawler.run(Crawler.java:250)\nat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\nat org.apache.nutch.crawl.Crawler.main(Crawler.java:257)\n</code></pre>\n\n<p>When I remove the config file of hadoop from nutch conf, the first line of error become:</p>\n\n<pre><code>Exception in thread \"main\" org.apache.hadoop.mapreduce.lib.input.InvalidInputException: Input path does not exist: file:/home/yuqing/workspace/nutch2.0/2\n</code></pre>\n\n<p>Once I run Nutch2.0 success with hbase, but now the full distribution is not work.\nHbase in full distribution runs normal, I can op it in shell.\nnext I create a folder in nutch2.0, then the crawler can running, but output of console seems unnormal.\nNow I have to have a meal.</p>\n", "creation_date": 1349697736, "score": 0},
{"title": "Drupal + Nutch + Solr", "view_count": 878, "is_answered": true, "answers": [{"question_id": 4179172, "owner": {"user_id": 508727, "link": "http://stackoverflow.com/users/508727/robertdouglass", "user_type": "registered", "reputation": 21}, "body": "<p>I and some others are working on this and have bespoke solutions ready for use. </p>\n\n<p>First, please join the discussion on groups.drupal.org: <a href=\"http://groups.drupal.org/lucene-nutch-and-solr\" rel=\"nofollow\">http://groups.drupal.org/lucene-nutch-and-solr</a>\nThen, check out the nutch module for Drupal: drupal.org/project/nutch</p>\n\n<p>Finally, if you need to have it work with with multiple languages or in a multi-site environment, you'll need code that I've written and that will eventually be released as part of the drupal.org nutch project, but which isn't there yet. Watch the issue queue or contact me directly to get your hands on it.</p>\n\n<p>-Robert</p>\n", "creation_date": 1289853496, "is_accepted": false, "score": 0, "last_activity_date": 1289853496, "answer_id": 4188595}, {"question_id": 4179172, "owner": {"user_id": 493742, "accept_rate": 95, "link": "http://stackoverflow.com/users/493742/gokul-n-k", "user_type": "registered", "reputation": 2082}, "body": "<p>This video can be a starting point for you. <a href=\"http://sf2010.drupal.org/conference/sessions/how-build-jobs-aggregation-search-engine-nutch-apache-solr-and-views-3-about.html\" rel=\"nofollow\">http://sf2010.drupal.org/conference/sessions/how-build-jobs-aggregation-search-engine-nutch-apache-solr-and-views-3-about.html</a></p>\n\n<p>Also the module is live now and you can access it here <a href=\"http://drupal.org/project/nutch\" rel=\"nofollow\">http://drupal.org/project/nutch</a>.</p>\n", "creation_date": 1349699514, "is_accepted": false, "score": 1, "last_activity_date": 1349699514, "answer_id": 12781819}], "question_id": 4179172, "tags": ["drupal", "solr", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/4179172/drupal-nutch-solr", "last_activity_date": 1349699514, "owner": {"age": 35, "answer_count": 13, "creation_date": 1270842308, "user_id": 313127, "accept_rate": 75, "view_count": 424, "location": "Algiers, Algeria", "reputation": 2433}, "body": "<p>We're about to start a project consisting of a search engine website. We need to implement a site that has social functionalities upon it's core search engine solution. Obviously, we need to choose a good web crawler along with a full text search engine. Since our team have good experience developing websites with Drupal; a member of the team came up with this solution: integrating Drupal social functionalities with Nutch as a web crawler and Solr as the FTS search engine.</p>\n\n<p>First question: What do you think of our choice? Is there a way to make Nutch and Solr work seamlessly within Drupal?</p>\n\n<p>Second one: Is there any way some good crawling/indexing/searching solutions that merge well with Drupal?</p>\n\n<p>Third question: (as suggested by a member of the team) What about coding the entire site with a UI Java framework like Wicket and integrate it with Nutch and Solr since all three technologies are Java based?</p>\n\n<p>Thanks</p>\n", "creation_date": 1289762489, "score": 1},
{"title": "Nutch crawl only HTML", "view_count": 434, "owner": {"user_id": 1408676, "answer_count": 14, "creation_date": 1337628687, "accept_rate": 91, "view_count": 55, "reputation": 372}, "is_answered": true, "answers": [{"question_id": 12753095, "owner": {"user_id": 1408676, "accept_rate": 91, "link": "http://stackoverflow.com/users/1408676/gmlvsv", "user_type": "registered", "reputation": 372}, "body": "<p>Edit conf/regex-urlfilter.txt</p>\n\n<p>Set files suffix for ignore: \n    -.(jpg|gif|zip|ico)$ </p>\n", "creation_date": 1349681749, "is_accepted": true, "score": 0, "last_activity_date": 1349681749, "answer_id": 12777218}], "question_id": 12753095, "tags": ["nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/12753095/nutch-crawl-only-html", "last_activity_date": 1349681749, "accepted_answer_id": 12777218, "body": "<p>Is it possible to crawl(fetch) only plain HTML pages via Nutch? Not pictures,video, flash, Excel, exe, PDF or Word files.</p>\n\n<p>How to check Content-Type of the page and fetch only text/html pages via Nutch?</p>\n", "creation_date": 1349466490, "score": 0},
{"title": "web crawling ,ruby,python,cassandra", "view_count": 851, "owner": {"user_id": 54128, "answer_count": 1, "creation_date": 1231758737, "accept_rate": 79, "view_count": 152, "reputation": 534}, "is_answered": true, "answers": [{"last_edit_date": 1293443845, "owner": {"user_id": 554135, "accept_rate": 100, "link": "http://stackoverflow.com/users/554135/amjad-masad", "user_type": "registered", "reputation": 2945}, "body": "<p>Its possible may take some time though depending on your machine's performance and your internet connection.<br>You could use PHP's cURL library to automatically send Web requests and then you could easily parse the data using a library for example :<a href=\"http://simplehtmldom.sourceforge.net/\" rel=\"nofollow\">simplHtmlDOM</a> or using native PHP DOM. But beware of running out of memory, also I highly recommend running the script from shell rather than a web browser. Also consider using multi curl functions, to fasten the process.</p>\n\n<p>This is extreamly easy and fast to implement, although multi-threading would give a huge performance boost in this scenario, so I suggest using one of the other languages you proposed. I know you could do this in Java easily using Apache HttpClient library and manipulate the DOM and extract data using native x-path support, regex or use one of the many third party dom implementations in Java. </p>\n\n<p>I strongly recommend also checking out Java library <a href=\"http://htmlunit.sourceforge.net/\" rel=\"nofollow\">HtmlUnit</a>, where it could make your life much easier, but you could maybe take a performance hit for that. A good multi-threading implementation would give a huge performance boost but a bad one could make your program run worse.</p>\n\n<p>Here is some resources for python:<br/>\n<a href=\"http://docs.python.org/library/httplib.html\" rel=\"nofollow\">http://docs.python.org/library/httplib.html</a> <br/>\n<a href=\"http://www.boddie.org.uk/python/HTML.html\" rel=\"nofollow\">http://www.boddie.org.uk/python/HTML.html</a> <br/>\n<a href=\"http://www.tutorialspoint.com/python/python_multithreading.htm\" rel=\"nofollow\">http://www.tutorialspoint.com/python/python_multithreading.htm</a> <br/></p>\n", "question_id": 4537657, "creation_date": 1293442735, "is_accepted": true, "score": 0, "last_activity_date": 1293443845, "answer_id": 4537771}, {"question_id": 4537657, "owner": {"user_id": 520348, "accept_rate": 74, "link": "http://stackoverflow.com/users/520348/munish-goyal", "user_type": "registered", "reputation": 661}, "body": "<p>I would add a little on crawl side.\nyou said crawl the web. So here the crawling direction (i.e. after fetching a page, which link to visit next becomes very important). But if you already have a list of webpages (called seed URLs list) with you then you simply need to download them and parse out reqd. data. If you just need to parse email addresses, then regex would be your option. Because html does not have any tag for emails, then htmldom parser wouldnt help you.</p>\n", "creation_date": 1293444742, "is_accepted": false, "score": 0, "last_activity_date": 1293444742, "answer_id": 4537958}, {"question_id": 4537657, "owner": {"user_id": 1442874, "accept_rate": 88, "link": "http://stackoverflow.com/users/1442874/chris-gerken", "user_type": "registered", "reputation": 12082}, "body": "<p>You should also look at Apache Nutch and Apache Gora which would do what you're looking for.  Nutch does the actual crawling which Gora stores the results in Cassandra, Hive or MySQL</p>\n", "creation_date": 1349534217, "is_accepted": false, "score": 1, "last_activity_date": 1349534217, "answer_id": 12760734}], "question_id": 4537657, "tags": ["cassandra", "web-crawler", "nutch", "gora"], "answer_count": 3, "link": "http://stackoverflow.com/questions/4537657/web-crawling-ruby-python-cassandra", "last_activity_date": 1349534217, "accepted_answer_id": 4537771, "body": "<p>I need to write a script that insert 1-million records  of username or emails by crawling the web, into database.\nThe script may be any types  like python,ruby,php etc.</p>\n\n<p>Please let me know is it possible ?if possible please provide the information how can I build the script.</p>\n\n<p>Thanks</p>\n", "creation_date": 1293441389, "score": 1},
{"title": "Parsing XML Feeds", "view_count": 84, "owner": {"age": 26, "answer_count": 84, "creation_date": 1318718223, "user_id": 997330, "accept_rate": 100, "view_count": 292, "location": "Mumbai, India", "reputation": 1459}, "is_answered": true, "answers": [{"question_id": 12650281, "owner": {"user_id": 992484, "accept_rate": 67, "link": "http://stackoverflow.com/users/992484/madprogrammer", "user_type": "registered", "reputation": 251705}, "body": "<p>First, I'd have a read through <a href=\"http://docs.oracle.com/javase/tutorial/jaxp/index.html\" rel=\"nofollow\">Java API for XML Processing</a>.</p>\n\n<p>Next, I'd become familiar with some of the networking APIs...</p>\n\n<ul>\n<li><a href=\"http://docs.oracle.com/javase/tutorial/networking/index.html\" rel=\"nofollow\">Custom Networking</a>, in particular, <a href=\"http://docs.oracle.com/javase/tutorial/networking/urls/index.html\" rel=\"nofollow\">Working with URLs</a></li>\n<li><a href=\"http://hc.apache.org/httpclient-3.x/\" rel=\"nofollow\">Apache HTTP Components</a> (nb. This might be a little advance, but I've found it useful)</li>\n</ul>\n\n<p>I'd also become familiar with <a href=\"http://www.w3schools.com/xpath/default.asp\" rel=\"nofollow\">xPath</a> and <a href=\"http://docs.oracle.com/javase/tutorial/jaxp/xslt/xpath.html\" rel=\"nofollow\">How xPath Works</a>, this will be useful for finding parts of the documents you are interested in.</p>\n\n<p>Next, I'd become <a href=\"http://docs.oracle.com/javase/tutorial/jdbc/index.html\" rel=\"nofollow\">JDBC(TM) Database Access</a></p>\n\n<p>That should just about cover the basics.</p>\n\n<p>Once you have a basic idea of the technologies, you'll need to think about the design.  I separate it element into it's own project.</p>\n\n<p>Deal with downloading, parsing, sorting and inserting the data into the database as one project.</p>\n\n<p>Deal with searching and retrieval in another.</p>\n\n<p>This will highlight the overlapping areas and show you where you need to provide common libraries.</p>\n", "creation_date": 1348897986, "is_accepted": true, "score": 2, "last_activity_date": 1348897986, "answer_id": 12650325}], "question_id": 12650281, "tags": ["java", "solr", "lucene", "xml-parsing", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/12650281/parsing-xml-feeds", "last_activity_date": 1348898323, "accepted_answer_id": 12650325, "body": "<p>I want to parse RSS feeds of sites and want to fetch the content ( e.g article) and want to put them into the database. Later i also want to index them based on the search keywords. Can anyone please tell how to proceed? What tools are best to use for this ? \nI searched on Apache lucene, solr and apache nutch. But still i am not clear how to implement it in programming language such as java. \nCan anyone please give more details about the implementation.</p>\n\n<p>Thanks in advance.</p>\n", "creation_date": 1348897512, "score": 0},
{"title": "nutch configuration nutch-site.xml", "view_count": 516, "is_answered": true, "answers": [{"question_id": 12305101, "owner": {"user_id": 12016, "accept_rate": 77, "link": "http://stackoverflow.com/users/12016/mana", "user_type": "registered", "reputation": 2720}, "body": "<p>Your assumption is correct. This is how it works. But keep in mind that each plugin can be assigned a certain content type, or a set of content types. For example the parse-pdf plugin will not parse msword documents.</p>\n", "creation_date": 1348563235, "is_accepted": false, "score": 1, "last_activity_date": 1348563235, "answer_id": 12579412}], "question_id": 12305101, "tags": ["nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/12305101/nutch-configuration-nutch-site-xml", "last_activity_date": 1348563235, "owner": {"user_id": 484999, "answer_count": 1, "creation_date": 1287831318, "accept_rate": 9, "view_count": 21, "reputation": 63}, "body": "<p>In nutch-site.xml under plugin-includes header , when I write parse-(type1|type2) what does it mean.</p>\n\n<p>My understanding : \n  Does this mean for each url being fetched by nutch, nutch parses the content first by using type1 parser and then sequentially invokes the type2 parser.</p>\n\n<p>Please correct me if I am wrong.</p>\n", "creation_date": 1346951476, "score": 0},
{"title": "Crawling redirects later with Nutch", "view_count": 161, "owner": {"user_id": 234901, "answer_count": 411, "creation_date": 1261179837, "accept_rate": 97, "view_count": 1229, "location": "London, United Kingdom", "reputation": 16396}, "is_answered": true, "answers": [{"question_id": 12457444, "owner": {"user_id": 1150329, "accept_rate": 89, "link": "http://stackoverflow.com/users/1150329/tejas-patil", "user_type": "registered", "reputation": 4793}, "body": "<p>The <a href=\"http://wiki.apache.org/nutch/NutchTutorial\" rel=\"nofollow\">topN parameter</a> must be increased so that all the urls will be picked up in the fetchlist. The selection of the urls in the 2nd round is based on the scores of the urls... i think that it cant be modified.</p>\n", "creation_date": 1348424137, "is_accepted": true, "score": 1, "last_activity_date": 1348424137, "answer_id": 12554919}], "question_id": 12457444, "tags": ["nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/12457444/crawling-redirects-later-with-nutch", "last_activity_date": 1348424137, "accepted_answer_id": 12554919, "body": "<p>The nutch-default.xml suggests that there is a way to save redirect destination on the first crawl and crawl them on the next crawl by setting the <code>http.redirect.max</code> to 0.  </p>\n\n<p>The first crawl finished successfully and we could see the redirect response in the segments stored. Then we attempted to update the crawl DB to add the redirect destination to the next fetch list, but we couldn't make them included -- the fetch list seemed to be mostly empty, with just a few URLs that nutch failed to crawl on the first crawl.  </p>\n\n<p>Is there a parameter/config we need to give during parsing/updating/generating?   </p>\n", "creation_date": 1347877881, "score": 0},
{"title": "nutch2.0 with cassandra", "view_count": 631, "is_answered": false, "answers": [{"question_id": 12457658, "owner": {"user_id": 1442874, "accept_rate": 88, "link": "http://stackoverflow.com/users/1442874/chris-gerken", "user_type": "registered", "reputation": 12082}, "body": "<p>I'm not sure if I had the exact same problem, but I found that in the gora-cassandra-mapping.xml file I had to add a keyspace attribute (<strong>keyspace=\"ks1\"</strong>) to the class element:</p>\n\n<pre><code>&lt;keyspace name=\"ks1\" cluster=\"My Cluster\" host=\"1.2.3.4\"&gt;\n    ...\n&lt;/keyspace&gt;\n&lt;class keyspace=\"ks1\" keyClass=\"java.lang.String\" name=\"org.apache.nutch.storage.WebPage\"&gt;\n    ...\n&lt;/class&gt;    \n</code></pre>\n", "creation_date": 1348170801, "is_accepted": false, "score": 0, "last_activity_date": 1348170801, "answer_id": 12519920}], "question_id": 12457658, "tags": ["eclipse", "cassandra", "nutch", "gora"], "answer_count": 1, "link": "http://stackoverflow.com/questions/12457658/nutch2-0-with-cassandra", "last_activity_date": 1348170801, "owner": {"user_id": 1677276, "answer_count": 5, "creation_date": 1347877339, "view_count": 5, "location": "China", "reputation": 4}, "body": "<pre><code>Exception in thread \"main\" org.apache.gora.util.GoraException: java.io.IOException\n    at org.apache.gora.store.DataStoreFactory.createDataStore(DataStoreFactory.java:167)\n    at org.apache.gora.store.DataStoreFactory.createDataStore(DataStoreFactory.java:135)\n    at org.apache.nutch.storage.StorageUtils.createWebStore(StorageUtils.java:75)\n    at org.apache.nutch.crawl.InjectorJob.run(InjectorJob.java:214)\n    at org.apache.nutch.crawl.Crawler.runTool(Crawler.java:68)\n    at org.apache.nutch.crawl.Crawler.run(Crawler.java:136)\n    at org.apache.nutch.crawl.Crawler.run(Crawler.java:250)\n    at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n    at org.apache.nutch.crawl.Crawler.main(Crawler.java:257)\nCaused by: java.io.IOException\n    at org.apache.gora.cassandra.store.CassandraStore.initialize(CassandraStore.java:88)\n    at org.apache.gora.store.DataStoreFactory.initializeDataStore(DataStoreFactory.java:102)\n    at org.apache.gora.store.DataStoreFactory.createDataStore(DataStoreFactory.java:161)\n    ... 8 more\nCaused by: java.lang.NullPointerException\n    at org.apache.gora.cassandra.store.CassandraMapping.&lt;init&gt;(CassandraMapping.java:117)\n    at org.apache.gora.cassandra.store.CassandraMappingManager.get(CassandraMappingManager.java:84)\n    at org.apache.gora.cassandra.store.CassandraClient.initialize(CassandraClient.java:84)\n    at org.apache.gora.cassandra.store.CassandraStore.initialize(CassandraStore.java:85)\n    ... 10 more\n</code></pre>\n\n<p><strong>I just run nutch2.0 on cassandra. It's the output of crawl, and the output of TestGoreStorage is as following:</strong></p>\n\n<pre><code>Starting!\nException in thread \"main\" org.apache.gora.util.GoraException: java.io.IOException\n    at org.apache.gora.store.DataStoreFactory.createDataStore(DataStoreFactory.java:167)\n    at org.apache.gora.store.DataStoreFactory.createDataStore(DataStoreFactory.java:135)\n    at org.apache.nutch.storage.StorageUtils.createWebStore(StorageUtils.java:75)\n    at org.apache.nutch.storage.TestGoraStorage.main(TestGoraStorage.java:204)\nCaused by: java.io.IOException\n    at org.apache.gora.cassandra.store.CassandraStore.initialize(CassandraStore.java:88)\n    at org.apache.gora.store.DataStoreFactory.initializeDataStore(DataStoreFactory.java:102)\n    at org.apache.gora.store.DataStoreFactory.createDataStore(DataStoreFactory.java:161)\n    ... 3 more\nCaused by: java.lang.NullPointerException\n    at org.apache.gora.cassandra.store.CassandraMapping.&lt;init&gt;(CassandraMapping.java:117)\n    at org.apache.gora.cassandra.store.CassandraMappingManager.get(CassandraMappingManager.java:84)\n    at org.apache.gora.cassandra.store.CassandraClient.initialize(CassandraClient.java:84)\n    at org.apache.gora.cassandra.store.CassandraStore.initialize(CassandraStore.java:85)\n    ... 5 more\n</code></pre>\n\n<p><strong>I can connect cassandra with cassandra-cli, and just check out the nutch from svn.\nHere is the effect config in gora.properties:</strong></p>\n\n<pre><code>    gora.datastore.default=org.apache.gora.cassandra.store.CassandraStore\n    gora.sqlstore.jdbc.driver=org.hsqldb.jdbc.JDBCDriver\n    gora.sqlstore.jdbc.url=jdbc:hsqldb:hsql://210.44.138.8/nutchtest\n    gora.sqlstore.jdbc.user=sa\n    gora.sqlstore.jdbc.password=\n    gora.cassandrastore.servers=210.44.138.8:9160\n</code></pre>\n\n<p><strong>and the config in gora-cassandra-mapping:</strong></p>\n\n<pre><code>&lt;keyspace name=\"webpage\" cluster=\"My Cluster\" host=\"210.44.138.8\"&gt;\n    &lt;family name=\"p\"/&gt;\n    &lt;family name=\"f\"/&gt;\n    &lt;family name=\"sc\" type=\"super\"/&gt;\n&lt;/keyspace&gt;\n</code></pre>\n\n<p><strong>210.44.138.8 is a node of my cluster, and the name of cluster is \"My Cluster\",\nmore info: closed firewall, run in eclipse. I'm very pleasure if someone give me any help.</strong></p>\n", "creation_date": 1347878689, "score": 1},
{"title": "Events triggering loading content on facebook&#39;s timeline", "view_count": 262, "is_answered": false, "answers": [{"question_id": 12479358, "owner": {"user_id": 1427878, "link": "http://stackoverflow.com/users/1427878/cbroe", "user_type": "registered", "reputation": 54356}, "body": "<p>Before dealing to your question \u2026 what kind of project are you trying to build there?</p>\n\n<p>Since Apache Nutch is an <em>open source web-search software</em>, I think you are trying to build some kind of search engine, that scrapes Facebook user profiles/feeds to get data and make it searchable on some third-party website?</p>\n\n<p>Well, that would be a violoation of <a href=\"https://developers.facebook.com/policy/\" rel=\"nofollow\">Facebook Platform Policies</a>:</p>\n\n<blockquote>\n  <p>I. Features and Functionality</p>\n  \n  <p><code>12.</code> You must not include data obtained from us in any search engine or directory without our written permission.</p>\n</blockquote>\n\n<p>So, do you have that written permission?</p>\n", "creation_date": 1347980292, "is_accepted": false, "score": 0, "last_activity_date": 1347980292, "answer_id": 12479610}], "question_id": 12479358, "tags": ["facebook", "javascript-events", "htmlunit", "nutch", "facebook-timeline"], "answer_count": 1, "link": "http://stackoverflow.com/questions/12479358/events-triggering-loading-content-on-facebooks-timeline", "last_activity_date": 1347980292, "owner": {"age": 25, "answer_count": 29, "creation_date": 1347978093, "user_id": 1680554, "accept_rate": 0, "view_count": 44, "location": "Poland", "reputation": 517}, "body": "<p>I am working on Apache Nutch modification project. We already swapped Nutch's original module with ours built using HtmlUnit. I need to download whole facebook user site (ex. <a href=\"http://www.facebook.com/profile.php?id=100002517096832\" rel=\"nofollow\">http://www.facebook.com/profile.php?id=100002517096832</a>), which is going to be parsed using our own parser. Unfortunately facebook is using mechanism called BigPipe (http://www.facebook.com/note.php?note_id=389414033919). That's why most of current website is hidden in &lt;.!-- --> tags.\nUsually when we scroll down facebook page, new content is being unpacked every time we are about to hit bottom of the page. I have tried to use javascript that scroll my htmlPage (HtmlPage object from HtmlUnit project), but finally I realized that scrolling is not triggering loading new content on facebook user site.</p>\n\n<p>How can I check, what event on page triggers loading content on current facebook page? Maybe I should approach problem from different side, for example try to extract BigPipe \"things\" on my own? Have you ever did that?</p>\n\n<p>Best regards, igleyy</p>\n", "creation_date": 1347979391, "score": 0},
{"title": "nutch 2.0 fetch page repeatedly when a job failed", "view_count": 837, "is_answered": false, "answers": [{"question_id": 12138288, "owner": {"user_id": 1150329, "accept_rate": 89, "link": "http://stackoverflow.com/users/1150329/tejas-patil", "user_type": "registered", "reputation": 4793}, "body": "<p>You need to add the hadoop logs for the Parse job. The stack trace attached is not showing that info. After u did that code change, did parsing happen successfully ?</p>\n", "creation_date": 1346333973, "is_accepted": false, "score": 0, "last_activity_date": 1346333973, "answer_id": 12198220}], "question_id": 12138288, "tags": ["apache", "web-crawler", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/12138288/nutch-2-0-fetch-page-repeatedly-when-a-job-failed", "last_activity_date": 1347881385, "owner": {"user_id": 220877, "answer_count": 26, "creation_date": 1259509700, "accept_rate": 45, "view_count": 78, "reputation": 1906}, "body": "<p>I use mysql as storage backend with nutch.</p>\n\n<p>Job failed when crawling some sites. Got the following exception and exit nutch when reaching this page: <a href=\"http://www.appchina.com/users.html\" rel=\"nofollow\">http://www.appchina.com/users.html</a></p>\n\n<pre><code>Exception in thread \"main\" java.lang.RuntimeException: job failed: name=parse, jobid=job_local_0004\n    at org.apache.nutch.util.NutchJob.waitForCompletion(NutchJob.java:47)\n    at org.apache.nutch.parse.ParserJob.run(ParserJob.java:249)\n    at org.apache.nutch.crawl.Crawler.runTool(Crawler.java:68)\n    at org.apache.nutch.crawl.Crawler.run(Crawler.java:171)\n    at org.apache.nutch.crawl.Crawler.run(Crawler.java:250)\n    at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n    at org.apache.nutch.crawl.Crawler.main(Crawler.java:257)\n</code></pre>\n\n<p>So I modify the ./src/java/org/apache/nutch/util/NutchJob.java\nchange the\n          if (getConfiguration().getBoolean(\"fail.on.job.failure\", true)) {\nto\n          if (getConfiguration().getBoolean(\"fail.on.job.failure\", false)) {</p>\n\n<p>After recompiling, I won't get any exception, but unlimited restart crawling.</p>\n\n<pre><code>FetcherJob : timelimit set for : -1\nFetcherJob: threads: 30\nFetcherJob: parsing: false\nFetcherJob: resuming: false\nUsing queue mode : byHost\nFetcher: threads: 30\nFetcher: throughput threshold: -1\nFetcher: throughput threshold sequence: 5\nQueueFeeder finished: total 2 records. Hit by time limit :0\nfetching http://www.appchina.com/\nfetching http://www.appchina.com/users.html\n-finishing thread FetcherThread0, activeThreads=29\n-finishing thread FetcherThread29, activeThreads=28\n...\n0/0 spinwaiting/active, 2 pages, 0 errors, 0.4 0.4 pages/s, 137 137 kb/s, 0 URLs in 0 queues\n-activeThreads=0\nParserJob: resuming:    false\nParserJob: forced reparse:  false\nParserJob: parsing all\nParsing http://www.appchina.com/\nParsing http://www.appchina.com/users.html\n</code></pre>\n\n<p>UPDATE\nerror in hadoop.log</p>\n\n<pre><code>2012-09-17 18:48:51,257 WARN  mapred.LocalJobRunner - job_local_0004\njava.io.IOException: java.sql.BatchUpdateException: Incorrect string value: '\\xE7\\x94\\xA8\\xE6\\x88\\xB7...' for column 'text' at row 1\n        at org.apache.gora.sql.store.SqlStore.flush(SqlStore.java:340)\n        at org.apache.gora.sql.store.SqlStore.close(SqlStore.java:185)\n        at org.apache.gora.mapreduce.GoraRecordWriter.close(GoraRecordWriter.java:55)\n        at org.apache.hadoop.mapred.MapTask$NewDirectOutputCollector.close(MapTask.java:651)\n        at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:766)\n        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:370)\n        at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:212)\nCaused by: java.sql.BatchUpdateException: Incorrect string value: '\\xE7\\x94\\xA8\\xE6\\x88\\xB7...' for column 'text' at row 1\n        at com.mysql.jdbc.PreparedStatement.executeBatchSerially(PreparedStatement.java:2028)\n        at com.mysql.jdbc.PreparedStatement.executeBatch(PreparedStatement.java:1451)\n        at org.apache.gora.sql.store.SqlStore.flush(SqlStore.java:328)\n        ... 6 more\nCaused by: java.sql.SQLException: Incorrect string value: '\\xE7\\x94\\xA8\\xE6\\x88\\xB7...' for column 'text' at row 1\n        at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:1073)\n        at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3609)\n        at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3541)\n        at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2002)\n        at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2163)\n        at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2624)\n        at com.mysql.jdbc.PreparedStatement.executeInternal(PreparedStatement.java:2127)\n        at com.mysql.jdbc.PreparedStatement.executeUpdate(PreparedStatement.java:2427)\n        at com.mysql.jdbc.PreparedStatement.executeBatchSerially(PreparedStatement.java:1980)\n        ... 8 more\n</code></pre>\n\n<p>UPDATE again</p>\n\n<p>I've drop the table gora created and create a similar table with a VARCHAR(128) id and utf8mb4 DEFAULT CHARSET. It works now. Why?</p>\n\n<p>Anyone help?</p>\n", "creation_date": 1346054796, "score": 1},
{"title": "Error in Nutch NoClassDefFoundError", "view_count": 140, "is_answered": true, "answers": [{"question_id": 12382355, "owner": {"user_id": 1094597, "accept_rate": 94, "link": "http://stackoverflow.com/users/1094597/nambari", "user_type": "registered", "reputation": 50171}, "body": "<pre><code>java.lang.NoClassDefFoundError: 3/logs\n</code></pre>\n\n<p>I don't think <code>3/logs</code> is valid class name in java. It seems you have typo some where while running program. Make sure no typos.</p>\n", "creation_date": 1347431590, "is_accepted": false, "score": 1, "last_activity_date": 1347431590, "answer_id": 12382393}], "question_id": 12382355, "tags": ["java", "tomcat", "nutch", "web-crawler"], "answer_count": 1, "link": "http://stackoverflow.com/questions/12382355/error-in-nutch-noclassdeffounderror", "last_activity_date": 1347431590, "owner": {"user_id": 1217934, "answer_count": 1, "creation_date": 1329560830, "accept_rate": 47, "view_count": 172, "reputation": 633}, "body": "<p>I am study nutch , and I am getting this error.</p>\n\n<p>I am not really sure how to fix this problem</p>\n\n<p>does anyone know the way to fix this program ?</p>\n\n<p>I am running nutch on the OS X mountain line..</p>\n\n<pre><code>apache-nutch-1.5.1 3  bin/nutch admin db -create\n\nbin/nutch: line 104: [: -nutch-1.5.1: binary operator expected\n\nException in thread \"main\" java.lang.NoClassDefFoundError: 3/logs\nCaused by: java.lang.ClassNotFoundException: 3.logs\nat java.net.URLClassLoader$1.run(URLClassLoader.java:202)\nat java.security.AccessController.doPrivileged(Native Method)\nat java.lang.ClassLoader.loadClass(ClassLoader.java:306)\nat sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301)\nat java.lang.ClassLoader.loadClass(ClassLoader.java:247)\n</code></pre>\n", "creation_date": 1347431415, "score": 0},
{"title": "Hit rate limitation in nutch", "view_count": 249, "owner": {"user_id": 234901, "answer_count": 411, "creation_date": 1261179837, "accept_rate": 97, "view_count": 1229, "location": "London, United Kingdom", "reputation": 16396}, "is_answered": true, "answers": [{"question_id": 12352338, "owner": {"user_id": 1526859, "accept_rate": 82, "link": "http://stackoverflow.com/users/1526859/alexander-chepurnoy", "user_type": "registered", "reputation": 474}, "body": "<p>Yes, You can limit rate by changing fetcher.server.delay, fetcher.server.min.delay, fetcher.threads.per.queue, fetcher.queue.mode settings values in nutch-default.xml file.</p>\n", "creation_date": 1347426544, "is_accepted": true, "score": 2, "last_activity_date": 1347426544, "answer_id": 12381562}], "question_id": 12352338, "tags": ["web-crawler", "nutch", "robots.txt"], "answer_count": 1, "link": "http://stackoverflow.com/questions/12352338/hit-rate-limitation-in-nutch", "last_activity_date": 1347426544, "accepted_answer_id": 12381562, "body": "<p>Is it possible to limit the hit rate/IP address in nutch? In other words, can I configure nutch so that it will only hit an IP x number of times per hour, etc.?</p>\n", "creation_date": 1347282184, "score": 0},
{"title": "How does Google Enterprise Search compare with Lucene/Solr/Nutch in search relevance for searching private documents?", "view_count": 816, "is_answered": true, "answers": [{"question_id": 12309792, "owner": {"user_id": 158763, "link": "http://stackoverflow.com/users/158763/eric-pugh", "user_type": "registered", "reputation": 1333}, "body": "<p>If Google Enterprise Search is the Google Search Appliance: <a href=\"http://www.google.com/enterprise/search/solutions_productivity.html\" rel=\"nofollow\">http://www.google.com/enterprise/search/solutions_productivity.html</a> then I believe the answer is no in terms of tapping all the combined knowledge of the world that Google.com represents.  </p>\n\n<p>Google Search Appliance isn't Google.com, with all of the search expertise, custom algorithms, and magic tuning that those services have.  It simply crawls your data, and them make it's matches based on what content on your intranet it finds and indexes.   </p>\n\n<p>It is much simpler to setup, however we've talked to many folks who end up migrating away when:</p>\n\n<ol>\n<li>the number of documents indexed grows to the point that GSA becomes very expensive</li>\n<li>The desire for more control over the search results overpowers the admitted ease of setup that GSA has!   GSA is a total black box when it comes to relevancy.</li>\n</ol>\n", "creation_date": 1347309265, "is_accepted": false, "score": 2, "last_activity_date": 1347309265, "answer_id": 12359029}], "question_id": 12309792, "tags": ["search", "solr", "lucene", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/12309792/how-does-google-enterprise-search-compare-with-lucene-solr-nutch-in-search-relev", "last_activity_date": 1347309265, "owner": {"user_type": "does_not_exist"}, "body": "<p>Does GES have an edge because it has access to vast amounts of data because of statistical analysis? </p>\n", "creation_date": 1346973836, "score": 1},
{"title": "Updating Solr Field Value", "view_count": 126, "is_answered": true, "answers": [{"question_id": 12351979, "owner": {"user_id": 1067466, "accept_rate": 82, "link": "http://stackoverflow.com/users/1067466/aravind-udayashankara", "user_type": "registered", "reputation": 4198}, "body": "<p>When you want to change a single field of a document you will have to reindex the whole document, as solr does not support updating of a field only. </p>\n", "creation_date": 1347281713, "is_accepted": false, "score": 0, "last_activity_date": 1347281713, "answer_id": 12352189}, {"last_edit_date": 1347282217, "owner": {"user_id": 950178, "link": "http://stackoverflow.com/users/950178/jayendra", "user_type": "registered", "reputation": 34896}, "body": "<p>Nope.<br>\nYou need to index the document again with all the fields.<br>\nSolr will delete and insert the document again.</p>\n\n<p>There is <a href=\"http://www.searchworkings.org/movies/-/content/training/382052\" rel=\"nofollow\">nice talk</a> about it you may want to hear.</p>\n", "question_id": 12351979, "creation_date": 1347281741, "is_accepted": false, "score": 1, "last_activity_date": 1347282217, "answer_id": 12352195}, {"question_id": 12351979, "owner": {"user_id": 167980, "accept_rate": 100, "link": "http://stackoverflow.com/users/167980/paige-cook", "user_type": "registered", "reputation": 18612}, "body": "<p>This functionality is available in the Solr version 4.0. That version is still in Beta, but will most likely be released before the end of the year. Please see the post - <a href=\"http://solr.pl/en/2012/07/09/solr-4-0-partial-documents-update/\" rel=\"nofollow\">Solr 4.0: Partial documents update</a> for more details on how this works.</p>\n", "creation_date": 1347284210, "is_accepted": false, "score": 1, "last_activity_date": 1347284210, "answer_id": 12352900}], "question_id": 12351979, "tags": ["solr", "lucene", "nutch"], "answer_count": 3, "link": "http://stackoverflow.com/questions/12351979/updating-solr-field-value", "last_activity_date": 1347284210, "owner": {"user_id": 1493254, "view_count": 5, "answer_count": 0, "creation_date": 1341073737, "reputation": 1}, "body": "<p>is there any possibility to update a value of a Solr-Field without reindexing the whole document?</p>\n", "creation_date": 1347281026, "score": 0},
{"title": "How to crawl English site and avoid crawling other languages?", "view_count": 447, "owner": {"user_id": 764809, "answer_count": 12, "creation_date": 1306070178, "accept_rate": 62, "view_count": 129, "location": "Tehran Iran", "reputation": 890}, "is_answered": true, "answers": [{"question_id": 12275779, "owner": {"user_id": 1565171, "link": "http://stackoverflow.com/users/1565171/arutaku", "user_type": "registered", "reputation": 3780}, "body": "<p>If you have a quick look to the HTTP Request parameters (http://en.wikipedia.org/wiki/List_of_HTTP_header_fields) you can ask for the content language and you will get an answer like this: \"Content-Language: en\".</p>\n\n<p>You do not need to do a GET request (and download the whole page), you could ask for this parameter in a HEAD request (in order to download only headers).</p>\n\n<p>About \"For example if two or three pages of a site were fetched and they weren't English nutch should leave the site and abandon those pages and all urls of them.\"\nA site could be multi-language. So you can get the 3 first pages in spanish (or whatever) and you will leave the site, although there are some pages in English.</p>\n", "creation_date": 1346828314, "is_accepted": true, "score": 1, "last_activity_date": 1346828314, "answer_id": 12276068}], "question_id": 12275779, "tags": ["nutch", "language-detection"], "answer_count": 1, "link": "http://stackoverflow.com/questions/12275779/how-to-crawl-english-site-and-avoid-crawling-other-languages", "last_activity_date": 1346828314, "accepted_answer_id": 12276068, "body": "<p>Hi I need to crawl only sites that their language is English. I know nutch can detect the langauge of sites by plugins like language detector But I need to prevent nutch from crawling the none English site. Although I know we need to crawl a page to understand the language of that  I want to leave the site at the first chance we could detect the language. Could you please tell me if its possible? For example if two or three pages of a site were fetched and they weren't English nutch should leave the site and abandon those pages and all urls of them. Thanks for any help.</p>\n", "creation_date": 1346827202, "score": 0},
{"title": "Nutch updatedb killed and skipped batch ids", "view_count": 724, "is_answered": true, "answers": [{"question_id": 12126757, "owner": {"user_id": 1150329, "accept_rate": 89, "link": "http://stackoverflow.com/users/1150329/tejas-patil", "user_type": "registered", "reputation": 4793}, "body": "<blockquote>\n  <p>some of the urls that I see are ones that arent in the seed.txt</p>\n</blockquote>\n\n<p>i think that this is happening due to url normalization. Nutch does this url normalization due to which the original url is changed or converted to a more standard format.</p>\n\n<p><strong>for #1:</strong> u injected and then executed generate-fetch phases ..right ? those 3 phases in yr question are required for parsing of crawled data, updating the db with newly discovered pages and to index them respectively.</p>\n\n<p><strong>for #2:</strong> sorry but i didnt get yr question.</p>\n\n<p><strong>for #3:</strong> no. topN set to 5 means nutch will select top 5 urls from the while bunch of urls elligible for fething. it will consider only these selected high scored urls for fetching.</p>\n\n<p><strong>for #4:</strong> that is a single command which invokes all nutch phases automatically. So u wont have to manually execute separate command for each phase. just have single command and it will do all stuff.</p>\n\n<p><strong>for #5:</strong> there will be some exception logged in the hadoop logs. provide the stack trace and error message so that i comment on it. without that i cant think of anything.</p>\n", "creation_date": 1346354430, "is_accepted": false, "score": 2, "last_activity_date": 1346354430, "answer_id": 12203970}], "question_id": 12126757, "tags": ["solr", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/12126757/nutch-updatedb-killed-and-skipped-batch-ids", "last_activity_date": 1346354430, "owner": {"user_id": 1131047, "answer_count": 5, "creation_date": 1325720014, "accept_rate": 83, "view_count": 26, "reputation": 102}, "body": "<p>I am using nutch 2.0 and solr 4.0 and am having minimal success I have 3 urls and my regex-urlfilter.xml is set to allow everything.\nI ran this script</p>\n\n<pre><code>#!/bin/bash\n\n# Nutch crawl\n\nexport NUTCH_HOME=~/java/workspace/Nutch2.0/runtime/local\n\n# depth in the web exploration\nn=1\n# number of selected urls for fetching\nmaxUrls=50000\n# solr server\nsolrUrl=http://localhost:8983\n\nfor (( i = 1 ; i &lt;= $n ; i++ ))\ndo\n\nlog=$NUTCH_HOME/logs/log                                                                                                                                                           \n\n# Generate\n$NUTCH_HOME/bin/nutch generate -topN $maxUrls &gt; $log\n\nbatchId=`sed -n 's|.*batch id: \\(.*\\)|\\1|p' &lt; $log`\n\n# rename log file by appending the batch id\nlog2=$log$batchId\nmv $log $log2\nlog=$log2\n\n# Fetch\n$NUTCH_HOME/bin/nutch fetch $batchId &gt;&gt; $log\n\n# Parse\n$NUTCH_HOME/bin/nutch parse $batchId &gt;&gt; $log\n\n# Update\n$NUTCH_HOME/bin/nutch updatedb &gt;&gt; $log\n\n# Index\n$NUTCH_HOME/bin/nutch solrindex $solrUrl $batchId &gt;&gt; $log\n\ndone\n----------------------------\n</code></pre>\n\n<p>Of course I bin/nutch inject urls before i run the script, but when I look at the logs, I see Skipping : different batch id and some of the urls that I see are ones that arent in the seed.txt and I want to include them\ninto solr, but they aren't added.\nI have 3 urls in my seed.txt</p>\n\n<p>After I ran this script I had tried\n<code>bin/nutch parse -force -all</code>\n<code>bin/nutch updatedb</code>\n<code>bin/nutch solrindex http://127.0.0.1:8983/solr/sites -reindex</code></p>\n\n<p>My questions are as follows.\n1. The last three commands why were they necessary?\n2. How do I get all of the urls during the parse job, even with the -force -all i still get different batch id skipping\n3. The script above, if i set generate -topN to 5. Does this mean if a site has a link to another site to another site to another site to another site to another site. That it will be included in the fetch/parse cycle?\n4. What about this command, why is this even mentioned :\n<code>bin/nutch crawl urls -solr http://127.0.0.1:8983/solr/sites -depth 3 -topN 10000 -threads 3.</code>\n5. When i run bin/nutch updateb it takes 1-2 mineuts then it echos Killed. This concerns me. Please Help.</p>\n\n<p>And yes, I have read a lot of pages on nutch and solr and I have been trying to figure this out for weeks now.</p>\n", "creation_date": 1345940887, "score": 1},
{"title": "automatic recrawl sites in nutch 1.4?", "view_count": 210, "is_answered": false, "answers": [{"question_id": 12086679, "owner": {"user_id": 1150329, "accept_rate": 89, "link": "http://stackoverflow.com/users/1150329/tejas-patil", "user_type": "registered", "reputation": 4793}, "body": "<p>Write a shell script. Below structure can be used:</p>\n\n<ol>\n<li>while loop</li>\n<li>......command which u use to invoke nutch once</li>\n<li>......sleep for few hours</li>\n<li>end loop</li>\n</ol>\n\n<p>Run this script using nohup or screen session. It will work great !!</p>\n", "creation_date": 1346334209, "is_accepted": false, "score": 0, "last_activity_date": 1346334209, "answer_id": 12198297}], "question_id": 12086679, "tags": ["nutch", "web-crawler"], "answer_count": 1, "link": "http://stackoverflow.com/questions/12086679/automatic-recrawl-sites-in-nutch-1-4", "last_activity_date": 1346334209, "owner": {"user_id": 1618925, "view_count": 2, "answer_count": 0, "creation_date": 1345705150, "reputation": 1}, "body": "<p>I want to recrawl my sites 3 times a day. I know I should write a script for this but i don't know how? and i don't know how run the script ?\ncan someone explain this step by step\nthanks</p>\n", "creation_date": 1345706606, "score": 0},
{"title": "Nutch crawl fails when run as a background process on linux", "view_count": 342, "is_answered": true, "answers": [{"question_id": 12181249, "owner": {"user_id": 1150329, "accept_rate": 89, "link": "http://stackoverflow.com/users/1150329/tejas-patil", "user_type": "registered", "reputation": 4793}, "body": "<p>The hung threads message is logged by Fetcher class when some requests seem to hang, despite all intentions. \nIn F<a href=\"http://www.docjar.com/html/api/org/apache/nutch/fetcher/Fetcher.java.html\" rel=\"nofollow\">etcher.java</a>, lines 926-930 ::</p>\n\n<pre><code>  if ((System.currentTimeMillis() - lastRequestStart.get()) &gt; timeout) {\n    if (LOG.isWarnEnabled()) {\n      LOG.warn(\"Aborting with \"+activeThreads+\" hung threads.\");\n    }\n    return;\n  }\n</code></pre>\n\n<p>The timeout for requests is defined by mapred.task.timeout and default value is 10 mins. You might increase it.. not sure if it will be a 100% clean fix.</p>\n\n<p>When I had observed this phenomenon, I added loggers in the code to find for which url the request hung more than 10 mins and concluded that for large files this issue was seen that too when the server was taking more time for data transfer.</p>\n", "creation_date": 1346333821, "is_accepted": false, "score": 1, "last_activity_date": 1346333821, "answer_id": 12198173}], "question_id": 12181249, "tags": ["linux", "ubuntu", "ssh", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/12181249/nutch-crawl-fails-when-run-as-a-background-process-on-linux", "last_activity_date": 1346333821, "owner": {"user_id": 937918, "answer_count": 15, "creation_date": 1315635668, "accept_rate": 23, "view_count": 44, "location": "Bangalore", "reputation": 180}, "body": "<p>When I run the Nutch crawl as a background process on Ubuntu in local mode, the Fetcher aborts with hung threads. The message is something like: </p>\n\n<p>WARN  fetcher.Fetcher - Aborting with \"X\" hung threads.</p>\n\n<p>I start off the script using nohup and &amp; as I want to log off from the session and have the crawler still run on the server. Else, when the crawl finishes at a certain depth and when the crawldb is being updated, the SSH session times out. I've tried configuring \"keep alive\" messages without much help. The command is something like:</p>\n\n<pre><code>nohup ./bin/nutch crawl ....... &amp;\n</code></pre>\n\n<p>Has anybody experienced this before? It seems to happen only when I use nohup or &amp;. </p>\n", "creation_date": 1346253526, "score": 0},
{"title": "Nutch error in Eclipse", "view_count": 268, "owner": {"age": 26, "answer_count": 180, "creation_date": 1331977994, "user_id": 1275577, "accept_rate": 100, "view_count": 715, "location": "\u0130zmir", "reputation": 3787}, "is_answered": true, "answers": [{"last_edit_date": 1334219145, "owner": {"user_id": 1243989, "link": "http://stackoverflow.com/users/1243989/andy", "user_type": "registered", "reputation": 1}, "body": "<p>I ran into the same problem. Here are two ways that might help:</p>\n\n<ul>\n<li>Modify conf/log4j.properties file to report DEBUG messages;</li>\n<li>read the hadoop.log file which is usually located in $NUTCH_HOME or $NUTCH_HOME/logs.</li>\n</ul>\n\n<p>By examining these messages, you should be able to spot the problem.</p>\n\n<p>Here is a tutorial on Running Nutch in Eclipse which also talks about several error handling.</p>\n", "question_id": 9956757, "creation_date": 1334218742, "is_accepted": false, "score": 0, "last_activity_date": 1334219145, "answer_id": 10119892}, {"question_id": 9956757, "owner": {"user_id": 1275577, "accept_rate": 100, "link": "http://stackoverflow.com/users/1275577/%c4%b0smet-alkan", "user_type": "registered", "reputation": 3787}, "body": "<p>I found 3 jars and added them to the project as external jars and it worked. Those jars are : <strong>cyberneko.jar</strong>, <strong>rome-0.9.jar</strong> and <strong>tagsoup-1.2.jar</strong> and you can find all by a simple google search. </p>\n", "creation_date": 1334992832, "is_accepted": true, "score": 0, "last_activity_date": 1334992832, "answer_id": 10257094}, {"last_edit_date": 1346157170, "owner": {"user_id": 1630240, "link": "http://stackoverflow.com/users/1630240/vinu-joseph", "user_type": "unregistered", "reputation": 11}, "body": "<p>Add the following to <code>ivy.xml</code>:</p>\n\n<pre><code>&lt;dependency org=\"rome\" name=\"rome\" rev=\"0.9\" /&gt;\n&lt;dependency org=\"net.sourceforge.nekohtml\" name=\"nekohtml\" rev=\"1.9.13\" /&gt;\n&lt;dependency org=\"org.ccil.cowan.tagsoup\" name=\"tagsoup\" rev=\"1.2.1\" /&gt;\n</code></pre>\n", "question_id": 9956757, "creation_date": 1346156272, "is_accepted": false, "score": 1, "last_activity_date": 1346157170, "answer_id": 12159064}], "question_id": 9956757, "tags": ["eclipse", "mongodb", "nutch"], "answer_count": 3, "link": "http://stackoverflow.com/questions/9956757/nutch-error-in-eclipse", "last_activity_date": 1346157170, "accepted_answer_id": 10257094, "body": "<p>I'm trying to run <strong>Apache Nutch</strong> from <strong>Eclipse</strong>. I followed the instructions at <a href=\"http://wiki.apache.org/nutch/RunNutchInEclipse\" rel=\"nofollow\">http://wiki.apache.org/nutch/RunNutchInEclipse</a>. However, sources of \"parse-html\" (both java and test) has errors. I run it anyway, it reads and fetches URL's from the <strong>seed.txt</strong> and returns this error:</p>\n\n<pre><code>Fetcher: finished at 2012-03-31 17:21:56, elapsed: 00:00:07\nParseSegment: starting at 2012-03-31 17:21:56\nParseSegment: segment: crawl/segments/20120331172142\nException in thread \"main\" java.io.IOException: Job failed!\n</code></pre>\n\n<p>I would like to point out that my goal is to get indexes from <strong>Nutch</strong> and store them in <strong>MongoDB</strong>.</p>\n", "creation_date": 1333204264, "score": 0},
{"title": "Nutch: Crawling every URL in a certain depth", "view_count": 2155, "owner": {"user_id": 1482243, "answer_count": 4, "creation_date": 1340702748, "accept_rate": 86, "view_count": 11, "reputation": 66}, "is_answered": true, "answers": [{"question_id": 11500387, "owner": {"user_id": 1204751, "accept_rate": 76, "link": "http://stackoverflow.com/users/1204751/crs", "user_type": "registered", "reputation": 283}, "body": "<p>topN sets the number of url to be fetched in every depth. In your first example depth is 3. Depth1 is seed url. And in depth2 and depth3, 5(topN value) urls will be fetched. 5*2 (depth2 and depth3) + 1 (seed url i.e depth1) = 11. TO fetch more urls you can increase topN. If you do not want to restrict then you can skip topN argument.</p>\n", "creation_date": 1342454947, "is_accepted": true, "score": 5, "last_activity_date": 1342454947, "answer_id": 11508219}], "question_id": 11500387, "tags": ["nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/11500387/nutch-crawling-every-url-in-a-certain-depth", "last_activity_date": 1346070137, "accepted_answer_id": 11508219, "body": "<p>My problem is to crawl every page and every document starting from a certain list of seed.</p>\n\n<p>I have installed nutch and make it run with the following commmand:</p>\n\n<pre><code>bin/nutch crawl urls -solr http://localhost:8983/solr/ -depth 3 -topN 5\n</code></pre>\n\n<p>I expected the nutch process to crawl something like 100 url, but it says it found just 11 document. So i tried to run nutch with this command:</p>\n\n<pre><code>bin/nutch crawl urls -solr http://localhost:8983/solr/ -depth 3 -topN 4\n</code></pre>\n\n<p>and it found 23 document.</p>\n\n<p>I'm running the process starting form the test seed <a href=\"http://nutch.apache.org\" rel=\"nofollow\">http://nutch.apache.org</a></p>\n\n<p>Why nutch has this behavior? How can I set nutch to crawl every url starting from my seeds in a certain depth?</p>\n", "creation_date": 1342426339, "score": 2},
{"title": "fetch specific title in every page with nutch and solr", "view_count": 361, "is_answered": false, "answers": [{"question_id": 12127890, "owner": {"user_id": 950178, "link": "http://stackoverflow.com/users/950178/jayendra", "user_type": "registered", "reputation": 34896}, "body": "<p>Check <a href=\"https://issues.apache.org/jira/browse/NUTCH-978\" rel=\"nofollow\">Nutch Plugin</a> which should allow you to extarct an element from a web page.</p>\n", "creation_date": 1345981571, "is_accepted": false, "score": 0, "last_activity_date": 1345981571, "answer_id": 12129848}], "question_id": 12127890, "tags": ["apache", "solr", "lucene", "nutch", "dismax"], "answer_count": 1, "link": "http://stackoverflow.com/questions/12127890/fetch-specific-title-in-every-page-with-nutch-and-solr", "last_activity_date": 1345981571, "owner": {"age": 25, "answer_count": 2, "creation_date": 1342958925, "user_id": 1543910, "accept_rate": 62, "view_count": 22, "location": "Iran", "reputation": 72}, "body": "<p>I have solr and nutch installed and my web page structure is that in every page the title is the same; e.g. Bank Something; but in every page there is a tag with an ID of TITLE, something like:</p>\n\n<pre><code>&lt;div ID=\"TITLE\"&gt;&lt;h1&gt;my page specific title&lt;/h1&gt;&lt;/div&gt;\n</code></pre>\n\n<p>I want to add another field to solr like second Title that fetch my page specific title and search words in it.(indeed now my page specific title is in content field and i want to have this in other field)</p>\n\n<p>How can I do this?!</p>\n", "creation_date": 1345959836, "score": 0},
{"title": "Change title of solr search like google", "view_count": 144, "is_answered": false, "answers": [{"question_id": 12120512, "owner": {"user_id": 650358, "accept_rate": 100, "link": "http://stackoverflow.com/users/650358/evan", "user_type": "registered", "reputation": 395}, "body": "<p>Try just adding a title tag.</p>\n\n<p><code>&lt;title&gt;my page specific title&lt;/title&gt;</code></p>\n\n<p>Or if you need to set it after the page has loaded you can use jquery</p>\n\n<pre><code>   &lt;script type=\"text/javascript\"&gt;\n      $(document).ready(function() {\n\n        document.title = \"Bank Something - \" + $(\".TITLE &gt; h1\").text();\n\n      });\n    &lt;/script&gt;\n</code></pre>\n", "creation_date": 1345922109, "is_accepted": false, "score": 0, "last_activity_date": 1345922109, "answer_id": 12124886}], "question_id": 12120512, "tags": ["apache", "solr", "lucene", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/12120512/change-title-of-solr-search-like-google", "last_activity_date": 1345959577, "owner": {"age": 25, "answer_count": 2, "creation_date": 1342958925, "user_id": 1543910, "accept_rate": 62, "view_count": 22, "location": "Iran", "reputation": 72}, "body": "<p>I have solr and nutch installed and my web page structure is that in every page the title is the same; e.g. Bank Something; but in every page there is a   tag with an ID of TITLE, something like:</p>\n\n<pre><code>&lt;div ID=\"TITLE\"&gt;&lt;h1&gt;my page specific title&lt;/h1&gt;&lt;/div&gt;\n</code></pre>\n\n<p>I want to add another field to solr like second Title that fetch my page specific title and search words in it.(indeed now my page specific title is in content field and i want to have this in other field)</p>\n\n<p>How can I do this?!</p>\n", "creation_date": 1345884164, "score": 0},
{"title": "Ajax-Solr does not query or find the Solr Server", "view_count": 174, "is_answered": false, "answers": [{"question_id": 12067809, "owner": {"user_id": 1617422, "link": "http://stackoverflow.com/users/1617422/barani", "user_type": "unregistered", "reputation": 1}, "body": "<p>Are you able to see the search query being issued by UI in SOLR logs? Try using firebug to debug the javascript..</p>\n", "creation_date": 1345648883, "is_accepted": false, "score": 0, "last_activity_date": 1345648883, "answer_id": 12076379}, {"question_id": 12067809, "owner": {"user_id": 1619892, "link": "http://stackoverflow.com/users/1619892/patrick", "user_type": "unregistered", "reputation": 1}, "body": "<p>Thanks for the answer. I checked everything with firebug and found the Problem. There was just a missing field declaration in the reuters.js</p>\n\n<p>Now the search Server works great.</p>\n", "creation_date": 1345728438, "is_accepted": false, "score": 0, "last_activity_date": 1345728438, "answer_id": 12092679}], "question_id": 12067809, "tags": ["ajax", "solr", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/12067809/ajax-solr-does-not-query-or-find-the-solr-server", "last_activity_date": 1345728438, "owner": {"user_id": 1609187, "view_count": 13, "answer_count": 11, "creation_date": 1345316445, "reputation": 136}, "body": "<p>I set up a little search server with Apache Nutch, Solr and Ajax-Solr as a Frontend. If i am using the solr server out of the ajax-solr tutorial everything is working fine but if i am going to use my own solr server (runns on the same computer) my frontend seems to search the server but didn't find it. \nI searched with google and here in the forum but i did not find something that's working.\nHas anyone an idea what went wrong?\n(Solr server is starting without any exceptions)</p>\n\n<p>Thanks</p>\n", "creation_date": 1345618719, "score": 1},
{"title": "apache nutch don&#39;t crawl website", "view_count": 1822, "owner": {"age": 25, "answer_count": 2, "creation_date": 1342958925, "user_id": 1543910, "accept_rate": 62, "view_count": 22, "location": "Iran", "reputation": 72}, "is_answered": true, "answers": [{"question_id": 11842913, "owner": {"user_id": 1570968, "accept_rate": 94, "link": "http://stackoverflow.com/users/1570968/prilia", "user_type": "registered", "reputation": 312}, "body": "<p>You can set the property \"Protocol.CHECK_ROBOTS\" to false in nutch-site.xml to ignore robots.txt.</p>\n", "creation_date": 1344331876, "is_accepted": false, "score": 2, "last_activity_date": 1344331876, "answer_id": 11843030}, {"last_edit_date": 1345357206, "owner": {"user_id": 1150329, "accept_rate": 89, "link": "http://stackoverflow.com/users/1150329/tejas-patil", "user_type": "registered", "reputation": 4793}, "body": "<p>In nutch-site.xml, set protocol.plugin.check.robots to false </p>\n\n<p>OR</p>\n\n<p>You can comment out the code where the robots check is done.\nIn Fetcher.java, lines 605-614 are doing the check. Comment that entire block</p>\n\n<pre><code>      if (!rules.isAllowed(fit.u)) {\n        // unblock\n        fetchQueues.finishFetchItem(fit, true);\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Denied by robots.txt: \" + fit.url);\n        }\n        output(fit.url, fit.datum, null, ProtocolStatus.STATUS_ROBOTS_DENIED, CrawlDatum.STATUS_FETCH_GONE);\n        reporter.incrCounter(\"FetcherStatus\", \"robots_denied\", 1);\n        continue;\n      }\n</code></pre>\n", "question_id": 11842913, "creation_date": 1345356871, "is_accepted": true, "score": 1, "last_activity_date": 1345357206, "answer_id": 12024613}], "question_id": 11842913, "tags": ["solr", "web-crawler", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/11842913/apache-nutch-dont-crawl-website", "last_activity_date": 1345357206, "accepted_answer_id": 12024613, "body": "<p>I have installed the apache nutch for web crawling. I want to crawl a website that has the following <code>robots.txt</code>:</p>\n\n<pre><code>User-Agent: *\nDisallow: /\n</code></pre>\n\n<p>Is there any way to crawl this website with apache nutch?</p>\n", "creation_date": 1344331524, "score": 1},
{"title": "Nutch 2 parse and outlinks", "view_count": 187, "is_answered": false, "answers": [{"question_id": 11932606, "owner": {"user_id": 984783, "accept_rate": 62, "link": "http://stackoverflow.com/users/984783/hugo", "user_type": "registered", "reputation": 65}, "body": "<p>The Webpage object is created from the information in nutch database, in my case hsql.</p>\n\n<p>The Webpage field outlinks(and some others) is filled after the parse process (after method getParse returns).</p>\n", "creation_date": 1344963594, "is_accepted": false, "score": 0, "last_activity_date": 1344963594, "answer_id": 11957243}], "question_id": 11932606, "tags": ["nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/11932606/nutch-2-parse-and-outlinks", "last_activity_date": 1344963594, "owner": {"user_id": 984783, "answer_count": 7, "creation_date": 1318025374, "accept_rate": 62, "view_count": 20, "reputation": 65}, "body": "<p>I've noticed that parse plugins like tika extract the outlinks from the content, but the object WebPage passed in method getParse/2 already have 2 arrays containing outlinks and inlinks.</p>\n\n<p>Whats the difference between the extraction in getParse and after fetch.</p>\n\n<p>Thanks.</p>\n", "creation_date": 1344854101, "score": 0},
{"title": "Nutch 2 hsql configuration", "view_count": 205, "is_answered": false, "question_id": 11931309, "tags": ["hsqldb", "nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/11931309/nutch-2-hsql-configuration", "last_activity_date": 1344849389, "owner": {"user_id": 984783, "answer_count": 7, "creation_date": 1318025374, "accept_rate": 62, "view_count": 20, "reputation": 65}, "body": "<p>How can i configure data mapping in this database using nutch. Probably should be some hsql-mapping.xml but the compiled nutch don't have this file. \nI am asking this because sometimes i get a sql error due to data truncation, so i need to alter the datalength of  the fields.</p>\n\n<p>Thanks</p>\n", "creation_date": 1344849389, "score": 1},
{"title": "Nutch Parsing plugin and redirects", "view_count": 183, "owner": {"user_id": 984783, "answer_count": 7, "creation_date": 1318025374, "accept_rate": 62, "view_count": 20, "reputation": 65}, "is_answered": true, "answers": [{"question_id": 11864240, "owner": {"user_id": 984783, "accept_rate": 62, "link": "http://stackoverflow.com/users/984783/hugo", "user_type": "registered", "reputation": 65}, "body": "<p>I've implemented the Protocol extension point and now i can save on database the redirects and loadtimes.</p>\n", "creation_date": 1344506940, "is_accepted": true, "score": 1, "last_activity_date": 1344506940, "answer_id": 11881060}], "question_id": 11864240, "tags": ["nutch", "web-crawler"], "answer_count": 1, "link": "http://stackoverflow.com/questions/11864240/nutch-parsing-plugin-and-redirects", "last_activity_date": 1344506940, "accepted_answer_id": 11881060, "body": "<p>I am using nutch 2.0, i've created a plugin for parsing html that implements Parser and works just fine.</p>\n\n<p>The problem is that i need to \"parse\" also pages that generate redirects (301,300), for getting the url and the http code.My  plugin ignores the redirected pages.</p>\n\n<p>Any ideas how i can obtain this information, maybe with other extension point?</p>\n", "creation_date": 1344427881, "score": 0},
{"title": "Nutch - does not crawl, says &quot;Stopping at depth=1 - no more URLs to fetch&quot;", "view_count": 1208, "is_answered": false, "answers": [{"question_id": 11710492, "owner": {"user_id": 1526859, "accept_rate": 82, "link": "http://stackoverflow.com/users/1526859/alexander-chepurnoy", "user_type": "registered", "reputation": 474}, "body": "<p>A site could blocks crawling via robots.txt /  or meta(name=\"robots\" content=\"noindex\") tag. Please check. </p>\n\n<p>PS. your log isn't clear:\n1. java.io.FileNotFoundException: /opt/apache-nutch-1.4-bin/runtime/local/logs/hadoop.log (No such file or directory)<br>\n2. solrUrl is not set, indexing will be skipped...</p>\n", "creation_date": 1344079599, "is_accepted": false, "score": 0, "last_activity_date": 1344079599, "answer_id": 11808268}], "question_id": 11710492, "tags": ["nutch", "web-crawler"], "answer_count": 1, "link": "http://stackoverflow.com/questions/11710492/nutch-does-not-crawl-says-stopping-at-depth-1-no-more-urls-to-fetch", "last_activity_date": 1344079599, "owner": {"age": 40, "answer_count": 234, "creation_date": 1308032052, "user_id": 797086, "view_count": 195, "location": "India", "reputation": 4657}, "body": "<p>It's been long since I've been trying to crawl using Nutch but it just doesn't seem to run. I'm trying to build a SOLR search for a website and using Nutch for crawling and indexing in Solr.</p>\n\n<p>There have been some permission problems originally but they have been fixed now. The URL I'm trying to crawl is <code>http://172.30.162.202:10200/</code>, which is not publicly accessible. It is an internal URL that can be reached from the Solr server. I tried browsing it using Lynx.</p>\n\n<p>Given below is the output from the Nutch command:</p>\n\n<pre><code>[abgu01@app01 local]$ ./bin/nutch crawl /home/abgu01/urls/url1.txt -dir /home/abgu01/crawl -depth 5 -topN 100\nlog4j:ERROR setFile(null,true) call failed.\njava.io.FileNotFoundException: /opt/apache-nutch-1.4-bin/runtime/local/logs/hadoop.log (No such file or directory)\n        at java.io.FileOutputStream.open(Native Method)\n        at java.io.FileOutputStream.&lt;init&gt;(FileOutputStream.java:212)\n        at java.io.FileOutputStream.&lt;init&gt;(FileOutputStream.java:136)\n        at org.apache.log4j.FileAppender.setFile(FileAppender.java:290)\n        at org.apache.log4j.FileAppender.activateOptions(FileAppender.java:164)\n        at org.apache.log4j.DailyRollingFileAppender.activateOptions(DailyRollingFileAppender.java:216)\n        at org.apache.log4j.config.PropertySetter.activate(PropertySetter.java:257)\n        at org.apache.log4j.config.PropertySetter.setProperties(PropertySetter.java:133)\n        at org.apache.log4j.config.PropertySetter.setProperties(PropertySetter.java:97)\n        at org.apache.log4j.PropertyConfigurator.parseAppender(PropertyConfigurator.java:689)\n        at org.apache.log4j.PropertyConfigurator.parseCategory(PropertyConfigurator.java:647)\n        at org.apache.log4j.PropertyConfigurator.configureRootCategory(PropertyConfigurator.java:544)\n        at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:440)\n        at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:476)\n        at org.apache.log4j.helpers.OptionConverter.selectAndConfigure(OptionConverter.java:471)\n        at org.apache.log4j.LogManager.&lt;clinit&gt;(LogManager.java:125)\n        at org.slf4j.impl.Log4jLoggerFactory.getLogger(Log4jLoggerFactory.java:73)\n        at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:242)\n        at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:254)\n        at org.apache.nutch.crawl.Crawl.&lt;clinit&gt;(Crawl.java:43)\nlog4j:ERROR Either File or DatePattern options are not set for appender [DRFA].\nsolrUrl is not set, indexing will be skipped...\ncrawl started in: /home/abgu01/crawl\nrootUrlDir = /home/abgu01/urls/url1.txt\nthreads = 10\ndepth = 5\nsolrUrl=null\ntopN = 100\nInjector: starting at 2012-07-27 15:47:00\nInjector: crawlDb: /home/abgu01/crawl/crawldb\nInjector: urlDir: /home/abgu01/urls/url1.txt\nInjector: Converting injected urls to crawl db entries.\nInjector: Merging injected urls into crawl db.\nInjector: finished at 2012-07-27 15:47:03, elapsed: 00:00:02\nGenerator: starting at 2012-07-27 15:47:03\nGenerator: Selecting best-scoring urls due for fetch.\nGenerator: filtering: true\nGenerator: normalizing: true\nGenerator: topN: 100\nGenerator: jobtracker is 'local', generating exactly one partition.\nGenerator: Partitioning selected urls for politeness.\nGenerator: segment: /home/abgu01/crawl/segments/20120727154705\nGenerator: finished at 2012-07-27 15:47:06, elapsed: 00:00:03\nFetcher: starting at 2012-07-27 15:47:06\nFetcher: segment: /home/abgu01/crawl/segments/20120727154705\nUsing queue mode : byHost\nFetcher: threads: 10\nFetcher: time-out divisor: 2\nQueueFeeder finished: total 1 records + hit by time limit :0\nUsing queue mode : byHost\nUsing queue mode : byHost\nfetching http://172.30.162.202:10200/\n-finishing thread FetcherThread, activeThreads=1\nUsing queue mode : byHost\n-finishing thread FetcherThread, activeThreads=1\nUsing queue mode : byHost\n-finishing thread FetcherThread, activeThreads=1\nUsing queue mode : byHost\n-finishing thread FetcherThread, activeThreads=1\nUsing queue mode : byHost\n-finishing thread FetcherThread, activeThreads=1\nUsing queue mode : byHost\n-finishing thread FetcherThread, activeThreads=1\nUsing queue mode : byHost\n-finishing thread FetcherThread, activeThreads=1\nUsing queue mode : byHost\n-finishing thread FetcherThread, activeThreads=1\nUsing queue mode : byHost\nFetcher: throughput threshold: -1\n-finishing thread FetcherThread, activeThreads=1\nFetcher: throughput threshold retries: 5\n-finishing thread FetcherThread, activeThreads=0\n-activeThreads=0, spinWaiting=0, fetchQueues.totalSize=0\n-activeThreads=0\nFetcher: finished at 2012-07-27 15:47:08, elapsed: 00:00:02\nParseSegment: starting at 2012-07-27 15:47:08\nParseSegment: segment: /home/abgu01/crawl/segments/20120727154705\nParseSegment: finished at 2012-07-27 15:47:09, elapsed: 00:00:01\nCrawlDb update: starting at 2012-07-27 15:47:09\nCrawlDb update: db: /home/abgu01/crawl/crawldb\nCrawlDb update: segments: [/home/abgu01/crawl/segments/20120727154705]\nCrawlDb update: additions allowed: true\nCrawlDb update: URL normalizing: true\nCrawlDb update: URL filtering: true\nCrawlDb update: 404 purging: false\nCrawlDb update: Merging segment data into db.\nCrawlDb update: finished at 2012-07-27 15:47:10, elapsed: 00:00:01\nGenerator: starting at 2012-07-27 15:47:10\nGenerator: Selecting best-scoring urls due for fetch.\nGenerator: filtering: true\nGenerator: normalizing: true\nGenerator: topN: 100\nGenerator: jobtracker is 'local', generating exactly one partition.\nGenerator: 0 records selected for fetching, exiting ...\nStopping at depth=1 - no more URLs to fetch.\nLinkDb: starting at 2012-07-27 15:47:11\nLinkDb: linkdb: /home/abgu01/crawl/linkdb\nLinkDb: URL normalize: true\nLinkDb: URL filter: true\nLinkDb: adding segment: file:/home/abgu01/crawl/segments/20120727154705\nException in thread \"main\" java.io.IOException: Job failed!\n        at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:1252)\n        at org.apache.nutch.crawl.LinkDb.invert(LinkDb.java:175)\n        at org.apache.nutch.crawl.LinkDb.invert(LinkDb.java:149)\n        at org.apache.nutch.crawl.Crawl.run(Crawl.java:143)\n        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n        at org.apache.nutch.crawl.Crawl.main(Crawl.java:55)\n</code></pre>\n\n<p>Can anyone please suggest what could be the reason for crawl not running? It always ends by saying \"Stopping at depth=1 - no more URLs to fetch\" irrespective of the value of <code>depth</code> or <code>topN</code> parameters. And I think the reason for it is (looking at the output above) that Fetcher isn't able to fetch any content from the URL.</p>\n\n<p>Any inputs are appreciated!</p>\n", "creation_date": 1343576098, "score": 0},
{"title": "Nutch: how to exclude non-English pages?", "view_count": 116, "is_answered": true, "answers": [{"last_edit_date": 1343983771, "owner": {"user_id": 1150329, "accept_rate": 89, "link": "http://stackoverflow.com/users/1150329/tejas-patil", "user_type": "registered", "reputation": 4793}, "body": "<p>Do you have any url pattern that is followed by all English pages?\neg. Wikipedia has \"<code>en</code>\" in the url for english like </p>\n\n<blockquote>\n  <p><a href=\"http://en.wikipedia.org/wiki/Category:Wikipedia_books\" rel=\"nofollow\">http://en.wikipedia.org/wiki/Category:Wikipedia_books</a></p>\n</blockquote>\n\n<p>. For other language, it does NOT has <code>en</code> in the url like </p>\n\n<blockquote>\n  <p><a href=\"http://gl.wikipedia.org/wiki/Categor%C3%ADa:Wikipedia:Libros\" rel=\"nofollow\">http://gl.wikipedia.org/wiki/Categor%C3%ADa:Wikipedia:Libros</a></p>\n</blockquote>\n\n<p>If you can define that, then it will be easily done by adding regex rule in the $NUTCH_CONF_DIR/regex-urlfilter.txt file so include pages which have the pattern for english and discard the other ones.</p>\n", "question_id": 10277612, "creation_date": 1335194549, "is_accepted": false, "score": 1, "last_activity_date": 1343983771, "answer_id": 10283421}], "question_id": 10277612, "tags": ["indexing", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/10277612/nutch-how-to-exclude-non-english-pages", "last_activity_date": 1343983771, "owner": {"user_id": 322034, "answer_count": 17, "creation_date": 1271835632, "accept_rate": 68, "view_count": 174, "reputation": 1720}, "body": "<p>I was wondering if there is a simple mechanism to exclude pages that are non-English from the indexing process? For example, the dmoz seed urls list contains wikipedia pages in many languages and they are the prime candidates for exclusion.</p>\n\n<p>Any tips?</p>\n", "creation_date": 1335171887, "score": 1},
{"title": "Map static field between nutch and solr", "view_count": 560, "is_answered": true, "answers": [{"question_id": 9078643, "owner": {"user_id": 1389007, "link": "http://stackoverflow.com/users/1389007/andreas", "user_type": "registered", "reputation": 11}, "body": "<p>It looks like the entry in nutch-default.xml is wrong.\nAccording to the plugin source \"index.static\" instead of \"index-static\" is the right name for the property.</p>\n\n<pre><code>String fieldsString = conf.get(\"index.static\", null);\n</code></pre>\n\n<p>After using that in my nutch-site.xml I was able to send multiple fields to my solr server.</p>\n\n<p>Also make sure that the plugin is added to list of included plugins in the \"plugin.includes\" property.</p>\n", "creation_date": 1336726767, "is_accepted": false, "score": 1, "last_activity_date": 1336726767, "answer_id": 10548430}], "question_id": 9078643, "tags": ["static", "solr", "field", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/9078643/map-static-field-between-nutch-and-solr", "last_activity_date": 1343916873, "owner": {"user_id": 715860, "answer_count": 1, "creation_date": 1295599530, "accept_rate": 47, "view_count": 47, "reputation": 145}, "body": "<p>I use nutch 1.4 and I would like to map static field to Solr.</p>\n\n<p>I know there is the index-static plugin. I configured it in nutch-site.xml like this :</p>\n\n<pre><code>    &lt;property&gt;\n        &lt;name&gt;index-static&lt;/name&gt;\n        &lt;value&gt;field:value&lt;/value&gt;\n    &lt;/property&gt;\n</code></pre>\n\n<p>However, the value is not sent to Solr.</p>\n\n<p>Does anyone have a solution ?</p>\n", "creation_date": 1328008858, "score": 0},
{"title": "What jars from Nutch do i need to write my own Crawl.java", "view_count": 773, "owner": {"age": 33, "answer_count": 17, "creation_date": 1254641796, "user_id": 183871, "accept_rate": 94, "view_count": 234, "location": "Hyderabad, India", "reputation": 1282}, "is_answered": true, "answers": [{"last_edit_date": 1279827799, "owner": {"user_id": 183100, "link": "http://stackoverflow.com/users/183100/pascal-dimassimo", "user_type": "registered", "reputation": 5766}, "body": "<p>One simple way is to package your code in a jar. Be sure to include a main in one of the class that starts your crawling. Drop that jar file in the lib folder of your Nutch installation. You can now start your crawling with a command like (assuming that your PATH is correctly set to find the <strong>nutch</strong> command):</p>\n\n<pre><code>nutch com.xyz.YourCrawlerMain\n</code></pre>\n\n<p>where \"com.xyz.YourCrawlerMain\" represents your main class to launch your crawling.</p>\n\n<p>This will launch your crawler with the Nutch classpath correctly set.</p>\n\n<p>For the configuration files, just update them directly in the conf folder of your Nutch installation.</p>\n\n<p><strong>UPDATE</strong></p>\n\n<p>I'm working on something similar and I am able to make nutch work from my app with these settings: set your classpath to include the Nutch folder (so it can find the plugins), the Nutch/conf folder and include all jars from Nutch/lib + nutch.jar from the nutch folder.</p>\n\n<p>But beware if your app is running in a web container. I had to mess with the classpath to make it works... </p>\n", "question_id": 3310050, "creation_date": 1279810945, "is_accepted": true, "score": 1, "last_activity_date": 1279827799, "answer_id": 3310314}], "question_id": 3310050, "tags": ["java", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/3310050/what-jars-from-nutch-do-i-need-to-write-my-own-crawl-java", "last_activity_date": 1343758208, "accepted_answer_id": 3310314, "body": "<p>I am trying to write my own version of Crawl.java from Nutch where I'd do a little different stuff. I don't want to work with Nutch source code. I just want to cleanly import a few jars and get going with my application. How should i provide conf/crawl-urlfilter.txt and other required conf files?</p>\n\n<p>Could someone help me here? \nThanks</p>\n", "creation_date": 1279809621, "score": 1},
{"title": "error when using solr and Integrating nutch and solr(HTTP ERROR 500)", "view_count": 588, "is_answered": true, "answers": [{"question_id": 11633285, "owner": {"user_id": 1526859, "accept_rate": 82, "link": "http://stackoverflow.com/users/1526859/alexander-chepurnoy", "user_type": "registered", "reputation": 474}, "body": "<p>Check,please, whether are Nutch 1.5.1 and Solr 3.6.1 compatible (are they having same versions of lucene-core and solr-solrj jars). I got some problems with incompatible versions, but not with 1.5/3.6 .</p>\n", "creation_date": 1343141672, "is_accepted": false, "score": 0, "last_activity_date": 1343141672, "answer_id": 11633560}, {"question_id": 11633285, "owner": {"user_id": 303106, "accept_rate": 75, "link": "http://stackoverflow.com/users/303106/ravish-bhagdev", "user_type": "registered", "reputation": 488}, "body": "<p>This is just bit of red herring.  The line that specifies version number something like:</p>\n\n<pre><code>&lt;schema name=\"nutch\" version=\"1.5.1\"&gt;\n</code></pre>\n\n<p>is causing it because the value of version is being parsed as float.  remove the extra dot.  Change it to 1.5 or 1.51 to make it valid float and restart your solr instance.  The exception should disappear.</p>\n", "creation_date": 1343727487, "is_accepted": false, "score": 1, "last_activity_date": 1343727487, "answer_id": 11737203}], "question_id": 11633285, "tags": ["solr", "integration", "web-crawler", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/11633285/error-when-using-solr-and-integrating-nutch-and-solrhttp-error-500", "last_activity_date": 1343727487, "owner": {"user_id": 1065130, "answer_count": 0, "creation_date": 1322204442, "accept_rate": 60, "view_count": 32, "reputation": 126}, "body": "<p>I have Linux Ubuntu 12.04 installed and I'm trying to install nutch 1.5.1 and solr 3.6.1 and integrate theme together to crawl seed urls.<br>\nI'm using <a href=\"http://wiki.apache.org/nutch/NutchTutorial\" rel=\"nofollow\">This</a> tutorial to get this work.<br>\nI followed the steps before 3.2 and skipped to <a href=\"http://wiki.apache.org/nutch/NutchTutorial#A4._Setup_Solr_for_search\" rel=\"nofollow\">step 4</a> and I can access to<br>\nlocalhost:8983/solr/admin/  </p>\n\n<p>without error.<br>\nbut when going to step 6 and copying schema.xml from conf folder of nutch to example/solr/conf folder of solr\nsolr/admin page occurs a java error,below:</p>\n\n<p><img src=\"http://i.stack.imgur.com/dVUMy.png\" alt=\"first part of page\">\n<img src=\"http://i.stack.imgur.com/vXjCV.png\" alt=\"second part\"></p>\n\n<p>How can I handle that?</p>\n\n<p>one more thing to ask....\nI have another <a href=\"http://www.scribd.com/doc/61813114/Running-Nutch-Solr\" rel=\"nofollow\">tutorial</a> for this that looks good but in first step it mentions that add some code to nutch-site.xml file in /conf/ and /runtime/local/conf/ folder </p>\n\n<p>but in nutch folder there is no runtime folder.In step 4 this folder mentioned too.\nany suggestion?  </p>\n\n<p>thanks in advance</p>\n", "creation_date": 1343140815, "score": 0},
{"title": "nutch and solr for multiple Domain", "view_count": 319, "owner": {"age": 31, "answer_count": 25, "creation_date": 1251870843, "user_id": 167033, "accept_rate": 68, "view_count": 133, "location": "India", "reputation": 806}, "is_answered": true, "answers": [{"question_id": 11670461, "owner": {"user_id": 565296, "link": "http://stackoverflow.com/users/565296/umar", "user_type": "registered", "reputation": 2041}, "body": "<p>This should be possible right out of the box. When you index to solr using nutch schema it has a field called site that stores the domain. On the search interface(that you will build) when you select a domain (aka site) you just have to pass a filter query like \"site:domain\" so that the results are restricted to the domain searched.</p>\n\n<p>NOTE: If you want to restrict crawls to the injected domains only make sure you set the external links property in nutch to false.</p>\n\n<p>Hope that answers your question.</p>\n", "creation_date": 1343576638, "is_accepted": true, "score": 1, "last_activity_date": 1343576638, "answer_id": 11710539}], "question_id": 11670461, "tags": ["solr", "lucene", "search-engine", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/11670461/nutch-and-solr-for-multiple-domain", "last_activity_date": 1343576638, "accepted_answer_id": 11710539, "body": "<p>I want to create custom search engine for multiple domain.<br>\nHow can I use solr with nutch to create a custom search for 500+ domains, while searching each domain should be able to show its own data.</p>\n\n<p>e.g.<br>\n example.com exapmle2.com example3.com and so on, When ever user searches on example.com he should get data which belongs to example.com same for example2.com and so on  </p>\n\n<p>these website may be blog post, e-commerce site, classified site or hotel reservation site. </p>\n\n<p>any suggestion would be appreciated.  </p>\n", "creation_date": 1343309596, "score": 1},
{"title": "what is going on inside of Nutch 2?", "view_count": 1076, "owner": {"user_id": 1065130, "answer_count": 0, "creation_date": 1322204442, "accept_rate": 60, "view_count": 32, "reputation": 126}, "is_answered": true, "answers": [{"last_edit_date": 1343458327, "owner": {"user_id": 1470293, "link": "http://stackoverflow.com/users/1470293/xantix", "user_type": "registered", "reputation": 2610}, "body": "<h1>Short Answer</h1>\n\n<p>In short, they have developed a webcrawler designed to very efficiently crawl the web from a many computer environment (but which can also be run on a single computer). </p>\n\n<p>You can start crawling the web without actually needing to know how they implemented it. </p>\n\n<p>The page you reference describes how it is implemented. </p>\n\n<h1>Technology behind it</h1>\n\n<p>They make use of Hadoop which is an open source java project which is designed along the same lines of MapReduce. MapReduce is the technology Google uses to crawl and organize the web.</p>\n\n<p>I've attended a few lectures on MapReduce/Hadoop, and unfortunately, I don't know if anyone at this time can explain it in a complete and easy-to-understand way (they're kind of opposites).</p>\n\n<p>Take a look at the wikipedia page for <a href=\"http://en.wikipedia.org/wiki/MapReduce\">MapReduce</a>.</p>\n\n<p>The basic idea is to send a job to the Master Node, the Master breaks the work up into pieces and sends it (maps it) to the various Worker Nodes (other computers or threads) which perform their assigned sub-task, and then sends the sub-result back to Master.</p>\n\n<p>Once the Master Node gets all the sub-results (or some of the sub-results) it starts to combine them (reduce them) into the final answer.</p>\n\n<p>All of these tasks are done at the same time, and each computer is given the right amount  of work to keep it occupied the whole time.</p>\n\n<h1>How to Crawl</h1>\n\n<p>Consists of 4 jobs:</p>\n\n<ol>\n<li>Generate</li>\n<li>Fetch</li>\n<li>Parse</li>\n<li>Update Database</li>\n</ol>\n\n<h2>*Generate</h2>\n\n<p>Start with a list of webpages containing the pages you want to start crawling from: The \"Webtable\".</p>\n\n<p>The Master node sends all of the pages in that list to its slaves (but if two pages have the same domain they are sent to the same slave).</p>\n\n<p>The Slave takes its assigned webpage(s) and:</p>\n\n<ol>\n<li>Has this already been generated? If so, skip it.</li>\n<li>Normalize the URL since \"http://www.google.com/\" and \"http://www.google.com/../\" is actually the same webpage.</li>\n<li>return an initial score along with the webpage back to the Master.</li>\n</ol>\n\n<p>(the Master partitions the webpages when it sends it to its slaves so that they all finish at the same time)</p>\n\n<p>The Master now chooses the topN (maybe the user just wanted to start with 10 initial pages), and marks them as chosen in the webtable.</p>\n\n<h2>*Fetch</h2>\n\n<p>Master looks at each URL in the webtable, maps the ones which were marked onto slaves to process them.</p>\n\n<p>Slaves fetch each URL from the Internet as fast as the internet connection will let them, they have a queue for each domain.</p>\n\n<p>They return the URL along with the HTML text of the webpage to the Master.</p>\n\n<h2>*Parse</h2>\n\n<p>Master looks at each webpage in the webtable, if it is marked as fetched, it sends it to its slaves to parse it. </p>\n\n<p>The slave first checks to see if it was already parsed by a different slave, if so skips it.</p>\n\n<p>Otherwise, it parses the webpage and saves the result to webtable.</p>\n\n<h2>*Update Database</h2>\n\n<p>Master looks at each webpage in the webtable, sends the parsed rows to its slaves.</p>\n\n<p>The slaves receive these Parsed URLs, calculate a score for them based on the number of links away from those pages (and the text near those links), and sends the Urls and scores back to the Master (which is sorted by score when it gets back to the Master because of the Partitioning).</p>\n\n<p>The master calculates and updates the webpage scores based on the number of links to those pages from other ones.</p>\n\n<p>The master stores this all to the database.</p>\n\n<h1>Repeat</h1>\n\n<p>When the pages were parsed, the links out of those webpages were added into the webtable. You can now repeat this process on just pages you haven't looked at yet to keep expanding your visited pages. Eventually you will reach most of the Internet after enough iterations of the four above steps. </p>\n\n<h1>Conclusion</h1>\n\n<p>MapReduce is a cool system. </p>\n\n<p>A lot of effort has been applied to make it as efficient as possible. </p>\n\n<p>They can handle computers breaking down in the middle of the job and reassigning the work to other slaves. They can handle some slaves being faster than others. </p>\n\n<p>The Master may decide to do the slaves' tasks on its own machine instead of sending it out to a slave if it will be more efficient. The communication network is incredibly advanced.</p>\n\n<p>MapReduce lets you write simple code:</p>\n\n<p>Define a Mapper, an optional Partitioner, and a Reducer.</p>\n\n<p>Then let MapReduce figure out how best to do that with all the computer resources it has access to, even if it is a single computer with a slow internet connection, or a kila-cluster. (maybe even Mega-clusters).</p>\n", "question_id": 11696422, "creation_date": 1343456695, "is_accepted": true, "score": 16, "last_activity_date": 1343458327, "answer_id": 11698848}], "question_id": 11696422, "tags": ["algorithm", "analysis", "nutch", "infrastructure"], "answer_count": 1, "link": "http://stackoverflow.com/questions/11696422/what-is-going-on-inside-of-nutch-2", "last_activity_date": 1343458327, "accepted_answer_id": 11698848, "body": "<p>I eager to know (and have to know) about the nutch and its algorithms (because it relates to my project) that it uses to fetch,classify,...(generally Crawling).<br>\nI read <a href=\"http://wiki.apache.org/nutch/Nutch2Crawling\" rel=\"nofollow\">this</a> material but its a little hard to understand.<br>\nIs there anyone who can explain this to me in a complete and easy-to-understand way?<br>\nthanks in advance.</p>\n", "creation_date": 1343427739, "score": 4},
{"title": "Difference between Nutch crawl giving depth=&#39;N&#39; and crawling in loop N times with depth=&#39;1&#39;", "view_count": 649, "is_answered": false, "answers": [{"question_id": 11447485, "owner": {"user_id": 1530118, "link": "http://stackoverflow.com/users/1530118/cloksmith", "user_type": "registered", "reputation": 26}, "body": "<p>I have found that the behavior of the Nutch fetching changes when running stand-alone (straight to hard disk) and integrated with a Hadoop cluster.  The Generator score filtering appears to be much higher with a Hadoop cluster, so the \"-topN\" setting needs to be adequitely high.</p>\n\n<p>I would suggest running your crawl with a high (at least 1000) \"-topN\" and not the <a href=\"http://wiki.apache.org/nutch/nutch-0.8-dev/bin/nutch_crawl\" rel=\"nofollow\">default value of 5</a>.</p>\n\n<p>This is similar to my response <a href=\"http://stackoverflow.com/questions/4479846/empty-nutch-crawl-list/11660294#11660294\">here</a>.</p>\n\n<p>After doing this, I found that my Nutch crawls on stand-alone and HDFS started to line up better.</p>\n", "creation_date": 1343435931, "is_accepted": false, "score": 0, "last_activity_date": 1343435931, "answer_id": 11697251}], "question_id": 11447485, "tags": ["hadoop", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/11447485/difference-between-nutch-crawl-giving-depth-n-and-crawling-in-loop-n-times-wit", "last_activity_date": 1343435931, "owner": {"user_id": 1396019, "answer_count": 1, "creation_date": 1337081617, "view_count": 3, "location": "India", "reputation": 16}, "body": "<p>Background of my problem: I am running Nutch1.4 on Hadoop0.20.203. There are series of MapReduce jobs that i am performing on Nutch segments to get final output. But waiting for whole crawl to happen before running mapreduce causes solution to run for longer time. I am now triggering MapReduce jobs on segments as soon as they are dumped. I am running crawl in a loop('N=depth' times ) by giving depth=1.I  am getting some urls getting lost when i crawl with depth 1 in a loop N times vs crawl giving depth N.</p>\n\n<p>Please find below pseudo code:</p>\n\n<p><strong>Case 1</strong>: Nutch crawl on Hadoop giving depth=3. </p>\n\n<p>// Create the list object to store arguments which we are going to pass to NUTCH</p>\n\n<p>List nutchArgsList = new ArrayList();</p>\n\n<p>nutchArgsList.add(\"-depth\");</p>\n\n<p>nutchArgsList.add(Integer.toString(3));</p>\n\n<p>&lt;...other nutch args...></p>\n\n<p>ToolRunner.run(nutchConf, new Crawl(), nutchArgsList.toArray(new String[nutchArgsList.size()]));</p>\n\n<p><strong>Case 2</strong>: Crawling in loop 3 times with depth='1'</p>\n\n<p>for(int depthRun=0;depthRun&lt; 3;depthRun++)\n{</p>\n\n<p>// Create the list object to store arguments which we are going to pass to NUTCH</p>\n\n<p>List nutchArgsList = new ArrayList();</p>\n\n<p>nutchArgsList.add(\"-depth\");</p>\n\n<p>nutchArgsList.add(Integer.toString(1)); //<strong>NOTE</strong> i have given depth as 1 here</p>\n\n<p>&lt;...other nutch args...></p>\n\n<p>ToolRunner.run(nutchConf, new Crawl(), nutchArgsList.toArray(new String[nutchArgsList.size()]));</p>\n\n<p>}</p>\n\n<p>I am getting some urls getting lost(db unfetched) when i crawling in loop as many times as depth. </p>\n\n<p>I have tried this on standalone Nutch where i run with depth 3 vs running 3 times over same urls with depth 1. I have compared the crawldb and urls difference is only 12. But when i do the same on Hadoop using toolrunner i am getting 1000 urls as db_unfetched. </p>\n\n<p>As far i understood till now,Nutch triggers crawl in a loop as many times as depth value. Please suggest. </p>\n\n<p>Also please let me know why difference is huge when i do this on Hadoop using toolrunner vs doing the same on standalone Nutch.  </p>\n\n<p>Thanks in advandce.</p>\n", "creation_date": 1342080237, "score": 0},
{"title": "Empty Nutch crawl list", "view_count": 531, "owner": {"age": 31, "answer_count": 1714, "creation_date": 1283310779, "user_id": 436560, "accept_rate": 86, "view_count": 2755, "location": "Arad, Romania", "reputation": 16749}, "is_answered": true, "answers": [{"question_id": 4479846, "owner": {"user_id": 271458, "link": "http://stackoverflow.com/users/271458/slick86", "user_type": "registered", "reputation": 1772}, "body": "<p>Its most likely your regex-urlfilter.xml. Try using this and see if it fixes the problem</p>\n\n<p>-^(file|ftp|mailto):</p>\n\n<p>-.(gif|GIF|jpg|JPG|png|PNG|ico|js|ICO|doc|mp3|MP3|DOC|css|rss|sit|eps|wmf|zip|ppt|mpg|xls|gz|rpm|tgz|mov|MOV|exe|jpeg|JPEG|bmp|BMP)$</p>\n\n<p>-.*(/[^/]+)/[^/]+\\1/[^/]+\\1/</p>\n\n<p>+.</p>\n", "creation_date": 1296802196, "is_accepted": false, "score": 0, "last_activity_date": 1296802196, "answer_id": 4895319}, {"question_id": 4479846, "owner": {"user_id": 1530118, "link": "http://stackoverflow.com/users/1530118/cloksmith", "user_type": "registered", "reputation": 26}, "body": "<p>I recently ran into this issue and found that most responses concerned the (regex|crawl)-urlfiters.txt.  Another thing to check is your '-topN' settings. This needs to be large enough for the generator to pass all filters.</p>\n\n<p>I hope this helps.</p>\n", "creation_date": 1343259535, "is_accepted": true, "score": 1, "last_activity_date": 1343259535, "answer_id": 11660294}], "question_id": 4479846, "tags": ["java", "eclipse", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/4479846/empty-nutch-crawl-list", "last_activity_date": 1343259535, "accepted_answer_id": 11660294, "body": "<p>I'm trying to run a crawl using Nutch in Eclipse.</p>\n\n<p>I'm using a file called urls, and it contains</p>\n\n<p><a href=\"http://www.google.com/\" rel=\"nofollow\">http://www.google.com/</a></p>\n\n<p>However, when I run the project, the Generator class tells me that:</p>\n\n<p>\"0 records selected for fetching, exiting\"</p>\n\n<p>How can I solve this issue?</p>\n\n<p>I've followed these documentations:</p>\n\n<p><a href=\"http://wiki.apache.org/nutch/RunNutchInEclipse1.0\" rel=\"nofollow\">http://wiki.apache.org/nutch/RunNutchInEclipse1.0</a></p>\n\n<p><a href=\"http://wiki.apache.org/nutch/NutchTutorial\" rel=\"nofollow\">http://wiki.apache.org/nutch/NutchTutorial</a></p>\n\n<p>Any help would be greatly appreciated.</p>\n", "creation_date": 1292701888, "score": 0},
{"title": "Can Nutch crawl video sites?", "view_count": 505, "is_answered": false, "answers": [{"question_id": 11576476, "owner": {"user_id": 1526859, "accept_rate": 82, "link": "http://stackoverflow.com/users/1526859/alexander-chepurnoy", "user_type": "registered", "reputation": 474}, "body": "<p>Yup, it's possible with custom plugin for parsing and indexing filter. And also, it's unknown whether are any videos on site before crawl. So you need to enter domains list for crawling manually or implement some algorithm to generate seed domains, for effective crawling. </p>\n", "creation_date": 1343035095, "is_accepted": false, "score": 0, "last_activity_date": 1343035095, "answer_id": 11609616}], "question_id": 11576476, "tags": ["video", "indexing", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/11576476/can-nutch-crawl-video-sites", "last_activity_date": 1343035095, "owner": {"user_id": 1500772, "view_count": 15, "answer_count": 0, "creation_date": 1341385454, "reputation": 1}, "body": "<p>Is it possible to use Nutch to crawl sites with only video files?\nAppreciate any insight into this.</p>\n", "creation_date": 1342775996, "score": 0},
{"title": "Is it possible to have a static index field for Liferay using solr-web plugin?", "view_count": 249, "is_answered": false, "answers": [{"question_id": 11573256, "owner": {"user_id": 950178, "link": "http://stackoverflow.com/users/950178/jayendra", "user_type": "registered", "reputation": 34896}, "body": "<p>Not sure for Liferay configuration, however you can add a default value in the <code>schema.xml</code> which will be applied to documents.</p>\n\n<pre><code>&lt;field name=\"source\" type=\"string\" indexed=\"true\" stored=\"true\" default=\"Nutch\" /&gt;\n</code></pre>\n", "creation_date": 1342782390, "is_accepted": false, "score": 0, "last_activity_date": 1342782390, "answer_id": 11578080}], "question_id": 11573256, "tags": ["solr", "indexing", "liferay", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/11573256/is-it-possible-to-have-a-static-index-field-for-liferay-using-solr-web-plugin", "last_activity_date": 1342782390, "owner": {"user_id": 1500772, "view_count": 15, "answer_count": 0, "creation_date": 1341385454, "reputation": 1}, "body": "<p>Can anyone tell me if I can associate a static index field for Liferay using the solr-web.plugin? Is there a way to define a static index in solr?</p>\n\n<p>I need something similar to the following configuration in Nutch</p>\n\n<pre><code>&lt;property&gt; \n    &lt;name&gt;index.static&lt;/name&gt; \n    &lt;value&gt;source:nutch&lt;/value&gt; \n&lt;/property&gt; \n</code></pre>\n\n<p>This will add the field \"source\" as an index and its value as \"nutch\" to all documents in Nutch. Anything similar to this for Liferay + Solr?</p>\n", "creation_date": 1342761185, "score": 0},
{"title": "Best web crawlers for commercial purpose?", "view_count": 602, "is_answered": false, "answers": [{"question_id": 11136637, "owner": {"user_id": 1473598, "accept_rate": 78, "link": "http://stackoverflow.com/users/1473598/oak", "user_type": "registered", "reputation": 346}, "body": "<p>I've been working with nutch for awhile now and it seems like it would fit this criteria pretty well. Plugin systems allow you to crawl new materials and easy deployment. One thing I had difficulty with was getting it to use multiple proxies but overall it is very customizable.  </p>\n", "creation_date": 1341961541, "is_accepted": false, "score": 0, "last_activity_date": 1341961541, "answer_id": 11423295}], "question_id": 11136637, "tags": ["web-crawler", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/11136637/best-web-crawlers-for-commercial-purpose", "last_activity_date": 1342039693, "owner": {"user_id": 1457252, "answer_count": 4, "creation_date": 1339706144, "accept_rate": 51, "view_count": 128, "reputation": 520}, "body": "<p>I am developing a system that crawls tens of millions of webpages, which will go on live.\nI would rather not develop a crawler from scratch.</p>\n\n<p>Which open-source web crawlers fit the following criteria:</p>\n\n<ul>\n<li>can be customized</li>\n<li>highly scalable </li>\n<li>crawls ajax websites easily</li>\n<li>crawls intelligently</li>\n<li>obeys politeness</li>\n</ul>\n\n<p>In case I have missed any, please evaluate other criteria that you think are important.</p>\n\n<p>I have a list of the following open-source crawlers. Do they posses the features mentioned above?</p>\n\n<ol>\n<li>Scrapy</li>\n<li>Mechanize</li>\n<li>Nutch</li>\n<li>Heritrix</li>\n<li>flax</li>\n<li>httrack</li>\n<li>Spidher</li>\n<li>Searcharoo </li>\n</ol>\n", "creation_date": 1340276107, "score": 0},
{"title": "Nutch and Solr in Eclipse in WINDOWS", "view_count": 424, "owner": {"user_id": 462941, "answer_count": 0, "creation_date": 1285857439, "accept_rate": 100, "view_count": 21, "reputation": 48}, "is_answered": true, "answers": [{"question_id": 11433495, "owner": {"user_id": 1204751, "accept_rate": 76, "link": "http://stackoverflow.com/users/1204751/crs", "user_type": "registered", "reputation": 283}, "body": "<p>You have to follow RunNutchInEclipse guide. There are no other guides as far as I know. Even I tried initially to run on windows and I gave up later and started using on Ubuntu/Mac. There will be many issues which you will see even if you follow the guide. There is some problem with hadoop on windows. The hadoop version which comes with Nutch 1.3+ does not work on windows. So we need to change hadoop version to some older version. Or we should use Nutch 1.2. I remember seeing a tutorial in youtube. See if that helps. But that said it is definitely easier to use Nutch on Unix flavors.</p>\n", "creation_date": 1342026846, "is_accepted": true, "score": 0, "last_activity_date": 1342026846, "answer_id": 11437896}], "question_id": 11433495, "tags": ["eclipse", "solr", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/11433495/nutch-and-solr-in-eclipse-in-windows", "last_activity_date": 1342026846, "accepted_answer_id": 11437896, "body": "<p>I want to use nutch and solr integrated in eclipse and my os is windows. I want to use binary files and dependencies with maven then develop code in eclipse. Is there any <em>clear</em> tutorial for doing this?</p>\n\n<p>I have tried RunNutchInEclipse guide but it is so nasty! Any useful links or advices are welcome.</p>\n\n<p>Thnx.</p>\n", "creation_date": 1342012444, "score": 0},
{"title": "Where can i find nutch.war file?", "view_count": 1008, "owner": {"user_id": 1162512, "answer_count": 27, "creation_date": 1327163131, "accept_rate": 58, "view_count": 772, "reputation": 2192}, "is_answered": true, "answers": [{"last_edit_date": 1341929884, "owner": {"user_id": 3333, "accept_rate": 83, "link": "http://stackoverflow.com/users/3333/paul-tomblin", "user_type": "registered", "reputation": 113417}, "body": "<p>I build nutch from the command line, and it doesn't build a war file, it builds a jar file called nutch-n.n.job</p>\n\n<p>Oh, on further investigation (ie. 10 seconds with Google) I discovered that you build a war file with the command \"ant war\".  It's a pity you didn't think to type \"nutch installation\" into Google instead of repeatedly complaining how nobody answered your question.</p>\n\n<p><a href=\"http://www.nutchinstall.blogspot.ca/\" rel=\"nofollow\">http://www.nutchinstall.blogspot.ca/</a>\nhttp://wiki.apache.org/nutch/NutchTutorial/</p>\n", "question_id": 11388453, "creation_date": 1341891418, "is_accepted": true, "score": 2, "last_activity_date": 1341929884, "answer_id": 11406247}], "question_id": 11388453, "tags": ["eclipse", "solr", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/11388453/where-can-i-find-nutch-war-file", "last_activity_date": 1341929884, "accepted_answer_id": 11406247, "body": "<p>Q-1) I tried building 'build.xml' from nutch folder by running it in eclipse ant, but to my bad luck,i couldnt find nutch.war anywhere in my workplace.<br>\nIt aint available inside the unzipped nutch-bin also.<br>\nWhere can i find nutch.war file??\n<br><br></p>\n\n<p>Q-2) I read somewhere that its better to crawl from Nutch and search via Solr. So, I integrated Solr within Nutch. Would that mean I would see a new replacement searching UI for Solr instead of Nutch searching UI or would it be same as Nutch UI ??\n<br> I mean this Nutch UI [1]: <a href=\"http://www.google.co.in/imgres?q=nutch&amp;um=1&amp;hl=en&amp;sa=N&amp;biw=1366&amp;bih=667&amp;tbm=isch&amp;tbnid=29yBPvjJbGXPTM:&amp;imgrefurl=http://www.java.net/pub/a/today/2006/02/16/introduction-to-nutch-2.html&amp;docid=69xYu60x353AyM&amp;imgurl=http://today.java.net/images/2006/02/nutch-figure-2-1.gif&amp;w=500&amp;h=325&amp;ei=B0D6T8bgA5GrrAeO_Z3eBg&amp;zoom=1&amp;iact=hc&amp;vpx=687&amp;vpy=192&amp;dur=2329&amp;hovh=181&amp;hovw=279&amp;tx=149&amp;ty=100&amp;sig=101777148678843861437&amp;page=1&amp;tbnh=121&amp;tbnw=186&amp;start=0&amp;ndsp=18&amp;ved=1t:429,r:3,s:0,i:96\" rel=\"nofollow\">http://www.google.co.in/imgres?q=nutch&amp;um=1&amp;hl=en&amp;sa=N&amp;biw=1366&amp;bih=667&amp;tbm=isch&amp;tbnid=29yBPvjJbGXPTM:&amp;imgrefurl=http://www.java.net/pub/a/today/2006/02/16/introduction-to-nutch-2.html&amp;docid=69xYu60x353AyM&amp;imgurl=http://today.java.net/images/2006/02/nutch-figure-2-1.gif&amp;w=500&amp;h=325&amp;ei=B0D6T8bgA5GrrAeO_Z3eBg&amp;zoom=1&amp;iact=hc&amp;vpx=687&amp;vpy=192&amp;dur=2329&amp;hovh=181&amp;hovw=279&amp;tx=149&amp;ty=100&amp;sig=101777148678843861437&amp;page=1&amp;tbnh=121&amp;tbnw=186&amp;start=0&amp;ndsp=18&amp;ved=1t:429,r:3,s:0,i:96</a></p>\n", "creation_date": 1341800967, "score": 0},
{"title": "Information on Nutch , Hadoop , Solr, MapReduce and Mahout", "view_count": 1041, "owner": {"user_id": 1162512, "answer_count": 27, "creation_date": 1327163131, "accept_rate": 58, "view_count": 772, "reputation": 2192}, "is_answered": true, "answers": [{"last_edit_date": 1341356690, "owner": {"user_id": 432745, "link": "http://stackoverflow.com/users/432745/pyfunc", "user_type": "registered", "reputation": 41629}, "body": "<p>1) Since,I'll be running these open source software on only 1 machine,ie, my laptop on localhost... How would Hadoop be beneficial in my case as it forms clusters? </p>\n\n<blockquote>\n  <p>Hadoop was created to process large scale data. Hadoop is a\n  distributed application. It is not going to provide you benefits on a\n  single machine. </p>\n</blockquote>\n\n<p>How would clusters be formed on only 1 machine??</p>\n\n<blockquote>\n  <p>Install Hadoop in pseudo cluster mode</p>\n</blockquote>\n\n<p>What would be the importance of MapReduce in my case?</p>\n\n<blockquote>\n  <p>Again, if you want to process pages fetched by a crawler on the scale of 1000s of gigabyte. Map-Reduce is useful in processing such large data</p>\n</blockquote>\n\n<p>How would MAHOUT,CASSANDRA and HBASE effect my engine???</p>\n\n<blockquote>\n  <p>They are different tools for different needs.  </p>\n  \n  <p>Mahout is machine\n  learning algorithms adapted for running as map-reduce tasks on Hadoop\n  or local files. Do you want to learn languages like Google Translate,\n  you can use it.</p>\n  \n  <p>HBase is a no-sql database that provides more real time data\n  processing over ad hoc analysis for which map-reduce is more useful.</p>\n</blockquote>\n\n<p>I would suggest that you go back to your problem statement, design with as little tools as required and when you hit the notes, you will understand when some of these tools could be useful. </p>\n", "question_id": 11319941, "creation_date": 1341355117, "is_accepted": true, "score": 5, "last_activity_date": 1341356690, "answer_id": 11320198}], "question_id": 11319941, "tags": ["solr", "hadoop", "mapreduce", "nutch", "mahout"], "answer_count": 1, "link": "http://stackoverflow.com/questions/11319941/information-on-nutch-hadoop-solr-mapreduce-and-mahout", "last_activity_date": 1341356690, "accepted_answer_id": 11320198, "body": "<p>PS: Correct me if I am wrong in any line</p>\n\n<p>I am building a search engine with Nutch and Solr.<br>\nI know by using Solr, I can enhance the efficiency of Searching- let Nutch do the crawling alone of the <b>entire</b> web.<br>\nI also know that Hadoop is used to handle petabytes of data by forming clusters and MapReduce.<br></p>\n\n<p>Now , What i want to know is that <br>\n1) Since,I'll be running these open source softwares on only 1 machine,ie, my laptop on localhost... How would Hadoop be beneficial in my case as it forms clusters? How would clusters be formed on only 1 machine??<br><br>\n2) What would be the importance of MapReduce in my case?<br><br>\n3) How would MAHOUT,CASSANDRA and HBASE effect my engine???<br><br>\nAny help on this aspect is very much appreciated.Apologize me if I asked a noob question!!<br>\nThanks<br>\nRegards</p>\n", "creation_date": 1341353589, "score": 0},
{"title": "Effect of depth, topn in nutch crawl", "view_count": 1353, "owner": {"user_id": 995776, "answer_count": 1, "creation_date": 1318609384, "accept_rate": 56, "view_count": 23, "reputation": 192}, "is_answered": true, "answers": [{"question_id": 11304550, "owner": {"user_id": 1204751, "accept_rate": 76, "link": "http://stackoverflow.com/users/1204751/crs", "user_type": "registered", "reputation": 283}, "body": "<p>depth is number of hops from root and topn is maximum link to be fetched in each level. So AFAIK by increasing depth it will definitely increase time taken to crawl. Changing depth from 100 to 1000 should increase the crawling time very much.</p>\n", "creation_date": 1341331402, "is_accepted": true, "score": 1, "last_activity_date": 1341331402, "answer_id": 11314851}], "question_id": 11304550, "tags": ["nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/11304550/effect-of-depth-topn-in-nutch-crawl", "last_activity_date": 1341331402, "accepted_answer_id": 11314851, "body": "<p>I have always wondered what is the effect of depth and topn for a nutch crawl? For example, let's assume a depth of 100 and topn of 10000 ensures a full crawl, would changing the depth to 1000 affect the time taken for the crawl? So to crawl a unfamiliar website, is it ok to give a arbitrarily large depth and topn?</p>\n\n<p>Thanks for the help,</p>\n\n<p>Ananth.</p>\n", "creation_date": 1341290229, "score": 2},
{"title": "Protocol used by nutch", "view_count": 51, "is_answered": false, "answers": [{"question_id": 10892893, "owner": {"user_id": 12016, "accept_rate": 77, "link": "http://stackoverflow.com/users/12016/mana", "user_type": "registered", "reputation": 2720}, "body": "<p>Nutch is a web crawler, so I guess it is using HTTP protocol. Most likely <strong>HTTP GET</strong> to fetch pages.</p>\n\n<p>If you need more information (e. g. user agend of nutch) consider setting up a apache web server on your machine and crawl some test pages. Then have a look at the apache access log.</p>\n", "creation_date": 1341242897, "is_accepted": false, "score": 0, "last_activity_date": 1341242897, "answer_id": 11296665}], "question_id": 10892893, "tags": ["protocols", "nutch", "charles"], "answer_count": 1, "link": "http://stackoverflow.com/questions/10892893/protocol-used-by-nutch", "last_activity_date": 1341242897, "owner": {"age": 25, "answer_count": 223, "creation_date": 1309099044, "user_id": 816213, "accept_rate": 90, "view_count": 869, "location": "Noida, India", "reputation": 10961}, "body": "<p>Can somebody please tell me which protocol is used by nutch for fetching pages.\nI wanted to check what kind of request does nutch makes ?</p>\n\n<p>I used charles proxy to see the request information but sadly nothing obtained there.\nAm i missing something about charles proxy or about nutch ??</p>\n\n<p>I have also tried wireshark but there cam too many packets and I could not identify which one was of nutch ?</p>\n\n<p>Please help..</p>\n", "creation_date": 1338879132, "score": 0},
{"title": "Different pages to different Nutch cores (within the same domain)", "view_count": 84, "is_answered": false, "answers": [{"question_id": 7727027, "owner": {"user_id": 12016, "accept_rate": 77, "link": "http://stackoverflow.com/users/12016/mana", "user_type": "registered", "reputation": 2720}, "body": "<p>Nutch only works with a single index afaik. Either a page gets crawled and indexed -- or it doesn't. \nYou may use <strong>Regex URL Filters</strong> to prevent some pages from being crawled.</p>\n\n<p>The pages you promoted are unfortunately quite identical. The headers are identical, except the title tag. You can't get any information from the URL either.</p>\n\n<p>Assuming there is a typo in the headline of your question and you ment to add different pages to different Solr cores, you could do the following:</p>\n\n<ul>\n<li>Add all pages to both solr cores</li>\n<li><p>Execute a delete query for the french core where you remove everything not matching a certain criteria:</p>\n\n<p>curl <strong>$FRENCH_SERVER</strong>/update -H \"Content-Type: text/xml\" --data-binary '<strong>NOT title:French</strong>' 2&amp;>1\ncurl <strong>$JAPANESE_SERVER</strong>/update -H \"Content-Type: text/xml\" --data-binary '<strong>NOT title:Japan</strong>' 2&amp;>1</p></li>\n</ul>\n\n<p>(these commands are not tested, do this on your own risk :).</p>\n", "creation_date": 1341232229, "is_accepted": false, "score": 0, "last_activity_date": 1341232229, "answer_id": 11293744}], "question_id": 7727027, "tags": ["solr", "search-engine", "nutch", "lucene"], "answer_count": 1, "link": "http://stackoverflow.com/questions/7727027/different-pages-to-different-nutch-cores-within-the-same-domain", "last_activity_date": 1341232229, "owner": {"user_id": 989623, "view_count": 13, "answer_count": 0, "creation_date": 1318340384, "reputation": 16}, "body": "<p>How can I instruct Nutch to treat page#1 as belonging to a core and page#2 as belonging to a different core (both pages from the same domain)?</p>\n\n<p>Practical situation: let's say Nutch is crawling and indexing www.businessweek.com; let's also say that I have one core called \"Japan\" and another core called \"France\".</p>\n\n<p>I want the page <a href=\"http://www.businessweek.com/magazine/content/05_51/b3964049.htm\" rel=\"nofollow\">http://www.businessweek.com/magazine/content/05_51/b3964049.htm</a> to be indexed only for the France core, since it's relevant for France but irrelevant for Japan.</p>\n\n<p>Consequently, I want the page <a href=\"http://www.businessweek.com/magazine/content/11_27/b4235016555525.htm\" rel=\"nofollow\">http://www.businessweek.com/magazine/content/11_27/b4235016555525.htm</a> to be indexed only for the Japan core, but not for France.</p>\n\n<p>Assuming we already know how to identify that a certain page belongs to a specific tag... how can Nutch be instructed about that?</p>\n", "creation_date": 1318340560, "score": 3},
{"title": "Nutch Exception: &#39;&#39;..cannot be resolved to a type&quot;", "view_count": 711, "owner": {"user_id": 1162512, "answer_count": 27, "creation_date": 1327163131, "accept_rate": 58, "view_count": 772, "reputation": 2192}, "is_answered": true, "answers": [{"question_id": 11275729, "owner": {"user_id": 1204751, "accept_rate": 76, "link": "http://stackoverflow.com/users/1204751/crs", "user_type": "registered", "reputation": 283}, "body": "<p>Do you want it to run from eclipse? AFAIK only if you want to modify Nutch code it makes sense to run from eclipse and do all set up. In case if you want to run from eclipse for some reason, 3rd point means: You need to add those path. i.e within plugin folder there are many other folder. You need to manually expand each folder and add src/test and src/java. Ex: there is folder called creative commons. In that there src and within that there is java and test folder. You need to select both. Similarly do for all directories within plugin folder.</p>\n", "creation_date": 1341190991, "is_accepted": true, "score": 2, "last_activity_date": 1341190991, "answer_id": 11286846}], "question_id": 11275729, "tags": ["eclipse", "exception", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/11275729/nutch-exception-cannot-be-resolved-to-a-type", "last_activity_date": 1341190991, "accepted_answer_id": 11286846, "body": "<p>I followed the steps using this site : <a href=\"http://wiki.apache.org/nutch/RunNutchInEclipse\" rel=\"nofollow\">http://wiki.apache.org/nutch/RunNutchInEclipse</a>  </p>\n\n<p>I encountered a problem while running which that says</p>\n\n<pre><code>Exception in thread \"main\" java.lang.Error: Unresolved compilation problems: \nString cannot be resolved to a type\nException cannot be resolved to a type\nSystem cannot be resolved\n\nat org.apache.nutch.crawl.Crawl.main(Crawl.java:53)\n</code></pre>\n\n<p>The build was successful.</p>\n\n<p>I am using nutch 1.4.</p>\n\n<p>Moreover, I would like to tell that i didnt understood the point 3 of section <b>\"Establish the Eclipse environment for Nutch\"</b> and skipped it. I guess the problem lies there only.\nCan you please help me as the more I try to resolve it, the more i get frustrated.Trying it from past 2 days.\nIt's a humble request please help.</p>\n", "creation_date": 1341073770, "score": 0},
{"title": "not able run nutch in netbeans?", "view_count": 384, "is_answered": false, "question_id": 11214802, "tags": ["java", "jsp", "tomcat", "servlets", "nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/11214802/not-able-run-nutch-in-netbeans", "last_activity_date": 1340749615, "owner": {"user_id": 1457252, "answer_count": 4, "creation_date": 1339706144, "accept_rate": 51, "view_count": 128, "reputation": 520}, "body": "<p>I dont know which path to give under \"Web page\" and \"WEB-INF\" folder..</p>\n\n<p>I tried to give the path \"tomcat/webapps/roots/WEB-INF\" under WEB-INF </p>\n\n<p>Under source i gave the path to the \"src\" folder which was kept in nutch1.4 source folder...</p>\n\n<p>and to lib, the path given is \"lib\" folder under nutch1.4 bin folder....</p>\n\n<p>the build was successful but when i run it....it displays the following exception!!!</p>\n\n<pre><code>javax.servlet.ServletException: java.lang.NoSuchMethodError: org.eclipse.jdt.internal.compiler.CompilationResult.getProblems()   [Lorg/eclipse/jdt/core/compiler/IProblem;\n\n org.apache.jasper.servlet.JspServlet.service(JspServlet.java:272)\njavax.servlet.http.HttpServlet.service(HttpServlet.java:717)\norg.netbeans.modules.web.monitor.server.MonitorFilter.doFilter(MonitorFilter.java:390)\n</code></pre>\n\n<p>where the root cause is </p>\n\n<pre><code>java.lang.NoSuchMethodError: org.eclipse.jdt.internal.compiler.CompilationResult.getProblems()[Lorg/eclipse/jdt/core/compiler/IProblem;\norg.apache.jasper.compiler.JDTCompiler$2.acceptResult(JDTCompiler.java:341)\norg.eclipse.jdt.internal.compiler.Compiler.compile(Compiler.java:398)\norg.apache.jasper.compiler.JDTCompiler.generateClass(JDTCompiler.java:399)\norg.apache.jasper.compiler.Compiler.compile(Compiler.java:288)\norg.apache.jasper.compiler.Compiler.compile(Compiler.java:267)\norg.apache.jasper.compiler.Compiler.compile(Compiler.java:255)\norg.apache.jasper.JspCompilationContext.compile(JspCompilationContext.java:563)\norg.apache.jasper.servlet.JspServletWrapper.service(JspServletWrapper.java:293)\norg.apache.jasper.servlet.JspServlet.serviceJspFile(JspServlet.java:314)\norg.apache.jasper.servlet.JspServlet.service(JspServlet.java:264)\njavax.servlet.http.HttpServlet.service(HttpServlet.java:717)\n    org.netbeans.modules.web.monitor.server.MonitorFilter.doFilter(MonitorFilter.java:390)\n</code></pre>\n", "creation_date": 1340739661, "score": 0},
{"title": "Crawling using Nutch...Shows an IOException", "view_count": 2421, "is_answered": false, "answers": [{"question_id": 11164940, "owner": {"user_id": 758280, "accept_rate": 100, "link": "http://stackoverflow.com/users/758280/jeffrey", "user_type": "registered", "reputation": 30715}, "body": "<p>I ran into this problem a couple days ago as well. The newer versions of Hadoop have trouble when it comes to interacting with Windows. You can either switch over to a *nix platform (which you should probably do, almost all of the support for Nutch is aimed at *nix users) or downgrade your version Nutch. The newest version of Nutch which I found to work on Windows Server 2008 was <a href=\"http://archive.apache.org/dist/nutch/\" rel=\"nofollow\">1.2</a>.</p>\n", "creation_date": 1340405228, "is_accepted": false, "score": 0, "last_activity_date": 1340405228, "answer_id": 11165107}], "question_id": 11164940, "tags": ["java", "open-source", "web-crawler", "nutch", "ioexception"], "answer_count": 1, "link": "http://stackoverflow.com/questions/11164940/crawling-using-nutch-shows-an-ioexception", "last_activity_date": 1340486407, "owner": {"user_id": 1457252, "answer_count": 4, "creation_date": 1339706144, "accept_rate": 51, "view_count": 128, "reputation": 520}, "body": "<p>I've started using Nutch and everything was fine until I encountered an <code>IOException</code> exception,</p>\n\n<pre><code>$ ./nutch crawl urls -dir myCrawl -depth 2 -topN 4\ncygpath: can't convert empty path\nsolrUrl is not set, indexing will be skipped...\ncrawl started in: myCrawl\nrootUrlDir = urls\nthreads = 10\ndepth = 2\nsolrUrl=null\ntopN = 4\nInjector: starting at 2012-06-23 03:37:51\nInjector: crawlDb: myCrawl/crawldb\nInjector: urlDir: urls\nInjector: Converting injected urls to crawl db entries.\nException in thread \"main\" java.io.IOException: Failed to set permissions of path: \\tmp\\hadoop-Rahul\\mapred\\staging\\Rahul255889423\\.staging to 0700\n    at org.apache.hadoop.fs.FileUtil.checkReturnValue(FileUtil.java:682)\n    at org.apache.hadoop.fs.FileUtil.setPermission(FileUtil.java:655)\n    at      org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:509)\n    at org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:344)\n    at org.apache.hadoop.fs.FilterFileSystem.mkdirs(FilterFileSystem.java:189)\n    at org.apache.hadoop.mapreduce.JobSubmissionFiles.getStagingDir(JobSubmissionFiles.java:116)\n    at org.apache.hadoop.mapred.JobClient$2.run(JobClient.java:856)\n    at org.apache.hadoop.mapred.JobClient$2.run(JobClient.java:850)\n    at java.security.AccessController.doPrivileged(Native Method)\n    at javax.security.auth.Subject.doAs(Subject.java:415)\n    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1083)\n    at org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:850)\n    at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:824)\n    at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:1261)\n    at org.apache.nutch.crawl.Injector.inject(Injector.java:217)\n    at org.apache.nutch.crawl.Crawl.run(Crawl.java:127)\n    at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n    at org.apache.nutch.crawl.Crawl.main(Crawl.java:55)\n</code></pre>\n\n<hr>\n\n<p>@jeffery ---    i downgraded my nutch version n encountered a new problem,which is out of my scope to understand....\nPlzz help....</p>\n\n<pre><code>$ ./nutch crawl urls -dir myCrawl -depth 4 -topN 5\ncygpath: can't convert empty path\nsolrUrl is not set, indexing will be skipped...\ncrawl started in: myCrawl\nroot UrlDir = urls\nthreads = 10\ndepth = 4\nsolrUrl=null\ntopN = 5\nInjector: starting at 2012-06-23 22:30:28\nInjector: crawlDb: myCrawl/crawldb\nInjector: urlDir: urls\nInjector: Converting injected urls to crawl db entries.\nException in thread \"main\" java.io.IOException: Job failed!\n    at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:1252)\n    at org.apache.nutch.crawl.Injector.inject(Injector.java:217)\n    at org.apache.nutch.crawl.Crawl.run(Crawl.java:127)\n    at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n    at org.apache.nutch.crawl.Crawl.main(Crawl.java:55)\n</code></pre>\n\n<p>What's the problem this tym???</p>\n", "creation_date": 1340404030, "score": 6},
{"title": "IOExeption when crawling with nutch", "view_count": 1462, "is_answered": false, "answers": [{"question_id": 9300371, "owner": {"user_id": 28760, "accept_rate": 95, "link": "http://stackoverflow.com/users/28760/lirik", "user_type": "registered", "reputation": 23398}, "body": "<p>I think that you might have this problem: <a href=\"http://wiki.apache.org/nutch/NutchGotchas\" rel=\"nofollow\">http://wiki.apache.org/nutch/NutchGotchas</a></p>\n\n<p>The answer provided there states:</p>\n\n<blockquote>\n  <p>The answer we find addressed the situation is that you're most likely out of disk space in /tmp. Consider using another location, or possibly another partition for hadoop.tmp.dir (which can be set in nutch-site.xml) with plenty of room for large transient files or using a Hadoop cluster. </p>\n</blockquote>\n", "creation_date": 1329367098, "is_accepted": false, "score": 0, "last_activity_date": 1329367098, "answer_id": 9305628}], "question_id": 9300371, "tags": ["web-crawler", "nutch", "ioexception"], "answer_count": 1, "link": "http://stackoverflow.com/questions/9300371/ioexeption-when-crawling-with-nutch", "last_activity_date": 1340480594, "owner": {"user_id": 924923, "answer_count": 1, "creation_date": 1314953369, "accept_rate": 12, "view_count": 58, "reputation": 79}, "body": "<p>After one day crawling with nutch(1.4) ... at last i got the bad bad below exception:</p>\n\n<pre><code>.\n.\n.\n\n-finishing thread FetcherThread, activeThreads=0\n-activeThreads=0, spinWaiting=0, fetchQueues.totalSize=0\n-activeThreads=0\nFetcher: java.io.IOException: Job failed!\n    at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:1252)\n    at org.apache.nutch.fetcher.Fetcher.fetch(Fetcher.java:1204)\n    at org.apache.nutch.fetcher.Fetcher.run(Fetcher.java:1240)\n    at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n    at org.apache.nutch.fetcher.Fetcher.main(Fetcher.java:1213)\n.\n.\n</code></pre>\n\n<p>.</p>\n\n<p>i have 20 news site and input argument of nutch is : depth 3 and topN -1\ni have enough space in root directory of my linux and about 4GB of ram\nhow can i solve this issue?\nthanks.</p>\n", "creation_date": 1329335801, "score": 0},
{"title": "I&#39;m following the Nutch tutorial, and getting a &quot;No URLs to fetch&quot; error", "view_count": 257, "is_answered": false, "answers": [{"question_id": 11143103, "owner": {"user_id": 1204751, "accept_rate": 76, "link": "http://stackoverflow.com/users/1204751/crs", "user_type": "registered", "reputation": 283}, "body": "<p>Configuration looks fine to me. You have made these changes in runtime/local folder right?\nseed.txt will be in NUTCH_HOME/runtime/local/urls folder and \nregex-urlfilter.txt and nutch-site.xml will be in NUTCH_HOME/runtime/local/conf folder</p>\n\n<p>NUTCH_HOME is installation directory</p>\n", "creation_date": 1340428356, "is_accepted": false, "score": 0, "last_activity_date": 1340428356, "answer_id": 11167058}], "question_id": 11143103, "tags": ["solr", "lucene", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/11143103/im-following-the-nutch-tutorial-and-getting-a-no-urls-to-fetch-error", "last_activity_date": 1340428356, "owner": {"user_id": 847284, "view_count": 1, "answer_count": 1, "creation_date": 1310769500, "reputation": 16}, "body": "<p>Following the Apache Nutch tutorial here: </p>\n\n<p>As indicated in the tutorial, I've set the last line of my regex-urlfilter.txt to:</p>\n\n<pre><code>+^http://([a-z0-9]*\\.)*nutch.apache.org/\n</code></pre>\n\n<p>My nutch-site.xml file contains only the lines</p>\n\n<pre><code>&lt;property&gt;\n &lt;name&gt;http.agent.name&lt;/name&gt;\n &lt;value&gt;My Nutch Spider&lt;/value&gt;\n&lt;/property&gt;\n</code></pre>\n\n<p>And my seed.txt file is:</p>\n\n<pre><code>http://nutch.apache.org/\n</code></pre>\n\n<p>However, when I crawl with</p>\n\n<pre><code>bin/nutch crawl urls -dir crawl -depth 3 -topN 5\n</code></pre>\n\n<p>I get a \"No URLs to fetch\" error. Anyone know why?</p>\n", "creation_date": 1340297972, "score": 0},
{"title": "Handling multiple connections to the host simultaneously", "view_count": 201, "owner": {"user_id": 295444, "view_count": 2, "answer_count": 0, "creation_date": 1268809921, "reputation": 8}, "is_answered": true, "answers": [{"question_id": 2460433, "owner": {"user_id": 279623, "accept_rate": 81, "link": "http://stackoverflow.com/users/279623/tomislav-nakic-alfirevic", "user_type": "registered", "reputation": 7360}, "body": "<p>From nutch-default.xml:</p>\n\n<pre><code>&lt;property&gt;\n  &lt;name&gt;fetcher.threads.fetch&lt;/name&gt;\n  &lt;value&gt;10&lt;/value&gt;\n  &lt;description&gt;The number of FetcherThreads the fetcher should use.\n    This is also determines the maximum number of requests that are \n    made at once (each FetcherThread handles one connection).&lt;/description&gt;\n&lt;/property&gt;\n\n&lt;property&gt;\n  &lt;name&gt;fetcher.threads.per.host&lt;/name&gt;\n  &lt;value&gt;1&lt;/value&gt;\n  &lt;description&gt;This number is the maximum number of threads that\n    should be allowed to access a host at one time.&lt;/description&gt;\n&lt;/property&gt;\n</code></pre>\n\n<p>As noted above, the number of connections is at most equal to the number of threads. The first property controls the overall number of connections and the second the number of connections per host - this is the one you need to set.</p>\n", "creation_date": 1268816210, "is_accepted": true, "score": 0, "last_activity_date": 1268816210, "answer_id": 2460834}], "question_id": 2460433, "tags": ["hadoop", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/2460433/handling-multiple-connections-to-the-host-simultaneously", "last_activity_date": 1340279234, "accepted_answer_id": 2460834, "body": "<p>How can I handle a number of connections to the host at the same time?</p>\n", "creation_date": 1268809921, "score": 1},
{"title": "How to show the result of solr in jsp page instead of xml format?", "view_count": 951, "is_answered": true, "answers": [{"question_id": 11030225, "owner": {"user_id": 1354980, "accept_rate": 100, "link": "http://stackoverflow.com/users/1354980/ryacii", "user_type": "registered", "reputation": 467}, "body": "<p>Basically you need to parse xml on your java code. <br>\nBefore doing this, you need to find ways to call solr url in your code.\nThere are several ways of doing this, the basic one is by using <code>org.apache.http.client.HttpClient</code> to execute post method. <br>\nYou can then parse the returned data, xml format data and load it on your jsp pages.\n<br><br>\nPlease refer to the following tutorials: <br>\n1. <a href=\"http://hc.apache.org/httpclient-3.x/tutorial.html\" rel=\"nofollow\">Overview on how to use HttpClient</a> <br>\n2. <a href=\"http://www.java-samples.com/showtutorial.php?tutorialid=152\" rel=\"nofollow\">Parsing XML using Java</a></p>\n", "creation_date": 1339667002, "is_accepted": false, "score": 2, "last_activity_date": 1339667002, "answer_id": 11030488}], "question_id": 11030225, "tags": ["java", "solr", "nutch", "requesthandler", "websolr"], "answer_count": 1, "link": "http://stackoverflow.com/questions/11030225/how-to-show-the-result-of-solr-in-jsp-page-instead-of-xml-format", "last_activity_date": 1339667341, "owner": {"user_id": 1350726, "answer_count": 0, "creation_date": 1335165839, "accept_rate": 0, "view_count": 13, "reputation": 11}, "body": "<p>i am new to solr.</p>\n\n<p>i want the result of solr output into jsp format or page</p>\n\n<p>by default it shows result in xml format.</p>\n\n<p>please help me.</p>\n", "creation_date": 1339666030, "score": 1},
{"title": "Nutch topN selection criterion", "view_count": 169, "is_answered": true, "answers": [{"question_id": 10909331, "owner": {"user_id": 1204751, "accept_rate": 76, "link": "http://stackoverflow.com/users/1204751/crs", "user_type": "registered", "reputation": 283}, "body": "<p>Yes. TopN considers page score.</p>\n", "creation_date": 1338997637, "is_accepted": false, "score": 1, "last_activity_date": 1338997637, "answer_id": 10917642}], "question_id": 10909331, "tags": ["web-crawler", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/10909331/nutch-topn-selection-criterion", "last_activity_date": 1338997637, "owner": {"user_id": 937918, "answer_count": 15, "creation_date": 1315635668, "accept_rate": 23, "view_count": 44, "location": "Bangalore", "reputation": 180}, "body": "<p>Does the topN threshold consider page score for the selection. If it's set to say 10, does Nutch queue up the 10 top scoring URLs on a page? Does this work through the webgraph or is it just the first 10 that it comes across on a page?</p>\n", "creation_date": 1338965007, "score": 0},
{"title": "Hadoop MapFile: how to transfer the writable value to its original class?", "view_count": 108, "owner": {"age": 29, "answer_count": 2, "creation_date": 1337846819, "user_id": 1414436, "accept_rate": 80, "view_count": 12, "location": "Beijing, China", "reputation": 23}, "is_answered": true, "answers": [{"question_id": 10830424, "owner": {"user_id": 1279787, "accept_rate": 75, "link": "http://stackoverflow.com/users/1279787/chris-white", "user_type": "registered", "reputation": 23571}, "body": "<p>If the runtime type of <code>value</code> is <a href=\"http://nutch.apache.org/apidocs-1.4/org/apache/nutch/protocol/Content.html\" rel=\"nofollow\"><code>org.apache.nutch.protocol.Content</code></a> then yes, you can cast to that type and then call the <code>getContent()</code> method</p>\n\n<pre><code>Content content = (Content) value;\ncontent.getContent();\n</code></pre>\n", "creation_date": 1338517929, "is_accepted": true, "score": 0, "last_activity_date": 1338517929, "answer_id": 10843697}], "question_id": 10830424, "tags": ["hadoop", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/10830424/hadoop-mapfile-how-to-transfer-the-writable-value-to-its-original-class", "last_activity_date": 1338517929, "accepted_answer_id": 10843697, "body": "<p>There is nutch data in Hadoop, and I get the value by the method get(key,val), but how can I transfer the writable data to its original class. My code is paste bellow:</p>\n\n<pre><code>Text key = new Text();\nkey.set(url);\nWritable value = null;\nvalue = reader.get(key, value);\n</code></pre>\n\n<p>the value is like this:</p>\n\n<pre><code>Version: -1\nurl:http://www.google.com\nbase: http://www.google.com\ncontentType: application/xhtml+xml\nmetadata:***\nContent:\n&lt;!DOCTYPE html****\n</code></pre>\n\n<p>And how can I get the get the Content only or contentType?\nCan I transfer the value to its original class and use getContent() method?</p>\n", "creation_date": 1338454878, "score": 0},
{"title": "Solr admin shows nothing (nutch)", "view_count": 407, "is_answered": false, "answers": [{"question_id": 10813792, "owner": {"user_id": 167980, "accept_rate": 100, "link": "http://stackoverflow.com/users/167980/paige-cook", "user_type": "registered", "reputation": 18612}, "body": "<p>Please check the fieldType for your content field in your schema.xml file. If it is set to <code>string</code> then that would explain why you are not getting any query results for specific text values. You should use a fieldType of <code>text_general</code> or something similar to get better search results as that fieldType will tokenize, filter and stem the values that are indexed. Please reference <a href=\"http://wiki.apache.org/solr/AnalyzersTokenizersTokenFilters\" rel=\"nofollow\">Analyzers, Tokenizers, and Token Filters</a> on the <a href=\"http://wiki.apache.org/solr/\" rel=\"nofollow\">Solr Wiki</a> for more information.</p>\n", "creation_date": 1338377574, "is_accepted": false, "score": 0, "last_activity_date": 1338377574, "answer_id": 10815456}], "question_id": 10813792, "tags": ["solr", "indexing", "nutch", "luke"], "answer_count": 1, "link": "http://stackoverflow.com/questions/10813792/solr-admin-shows-nothing-nutch", "last_activity_date": 1338377574, "owner": {"user_id": 1315894, "answer_count": 2, "creation_date": 1333646440, "accept_rate": 0, "view_count": 19, "reputation": 211}, "body": "<p>I've successfully created a Solr index crawling a few pages using nutch. Querying the index using Luke I get the expected results (default field 'content'). However, when I try using the solr/admin interface I get everything using q=<em>:</em> as expected:</p>\n\n<pre><code>&lt;response&gt;\n&lt;lst name=\"responseHeader\"&gt;\n  &lt;int name=\"status\"&gt;0&lt;/int&gt;\n  &lt;int name=\"QTime\"&gt;0&lt;/int&gt;\n  &lt;lst name=\"params\"&gt;\n    &lt;str name=\"indent\"&gt;on&lt;/str&gt;\n    &lt;str name=\"start\"&gt;0&lt;/str&gt;\n    &lt;str name=\"q\"&gt;*:*&lt;/str&gt;\n    &lt;str name=\"version\"&gt;2.2&lt;/str&gt;\n    &lt;str name=\"rows\"&gt;10&lt;/str&gt;\n  &lt;/lst&gt;\n&lt;/lst&gt;\n&lt;result name=\"response\" numFound=\"11\" start=\"0\"&gt;\n  &lt;doc&gt;\n    &lt;float name=\"boost\"&gt;1.0&lt;/float&gt;\n    &lt;str name=\"content\"&gt;\n      'a lot of text...'\n    &lt;/str&gt;\n    &lt;str name=\"digest\"&gt;f73dc90d5ab992f62ba3980de2312dfe&lt;/str&gt;\n    &lt;str name=\"id\"&gt;http://thenet.net/&lt;/str&gt;\n    &lt;str name=\"segment\"&gt;20120529084510&lt;/str&gt;\n    &lt;str name=\"title\"&gt;1 &lt; 2 &lt; 3&lt;/str&gt;\n    &lt;date name=\"tstamp\"&gt;2012-05-29T06:45:12.872Z&lt;/date&gt;\n    &lt;str name=\"url\"&gt;http://theurl.net&lt;/str&gt;\n  &lt;/doc&gt;\n</code></pre>\n\n<p>but absolutely nothing when querying for specific strings:</p>\n\n<pre><code>&lt;response&gt;\n  &lt;lst name=\"responseHeader\"&gt;\n    &lt;int name=\"status\"&gt;0&lt;/int&gt;\n    &lt;int name=\"QTime\"&gt;0&lt;/int&gt;\n    &lt;lst name=\"params\"&gt;\n      &lt;str name=\"indent\"&gt;on&lt;/str&gt;\n      &lt;str name=\"start\"&gt;0&lt;/str&gt;\n      &lt;str name=\"q\"&gt;java&lt;/str&gt;\n      &lt;str name=\"version\"&gt;2.2&lt;/str&gt;\n      &lt;str name=\"rows\"&gt;10&lt;/str&gt;\n    &lt;/lst&gt;\n  &lt;/lst&gt;\n  &lt;result name=\"response\" numFound=\"0\" start=\"0\"/&gt;\n&lt;/response&gt;\n</code></pre>\n\n<p>At the Solr admin interface it says:</p>\n\n<pre><code>mro:8983\ncwd=/$PATH_TO_SOLR_AND_NUTH_DIRS/solr/example SolrHome=solr/./ \nHTTP caching is OFF\n</code></pre>\n\n<p>In schema.xml 'content' is default search field.</p>\n\n<p>Any help will be much appreciated!</p>\n", "creation_date": 1338371185, "score": 0},
{"title": "Is it possible to direct output of map-reduce to multiple Map files?", "view_count": 339, "owner": {"user_id": 1150329, "answer_count": 207, "creation_date": 1326629590, "accept_rate": 89, "view_count": 969, "location": "California", "reputation": 4793}, "is_answered": true, "answers": [{"last_edit_date": 1338373430, "owner": {"user_id": 1279787, "accept_rate": 75, "link": "http://stackoverflow.com/users/1279787/chris-white", "user_type": "registered", "reputation": 23571}, "body": "<p>Look instead at <a href=\"http://hadoop.apache.org/common/docs/current/api/org/apache/hadoop/mapred/lib/MultipleOutputs.html\" rel=\"nofollow\">MultipleOutputs</a>, you'll have to write a custom reducer to call the MultipleOutputs.getCollector() method for each type, there's example usage in the javadocs.</p>\n\n<p><strong>In your job configuration:</strong></p>\n\n<pre><code> MultipleOutputs.addMultiNamedOutput(conf, \"map\",\n   org.apache.hadoop.mapred.MapFileOutputFormat.class,\n   LongWritable.class, Text.class);\n</code></pre>\n", "question_id": 10701861, "creation_date": 1337726933, "is_accepted": true, "score": 1, "last_activity_date": 1338373430, "answer_id": 10711209}], "question_id": 10701861, "tags": ["hadoop", "mapreduce", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/10701861/is-it-possible-to-direct-output-of-map-reduce-to-multiple-map-files", "last_activity_date": 1338373430, "accepted_answer_id": 10711209, "body": "<p><strong>Here is the use case:</strong></p>\n\n<p>I have a nutch crawldb (its a hadoop map file) containing data about urls which includes its status as visited and not-visited. I want to split it into 2 crawldb (map files) based on the status of the urls. </p>\n\n<p>Till now i tried using <a href=\"http://hadoop.apache.org/common/docs/current/api/org/apache/hadoop/mapred/lib/MultipleOutputFormat.html\" rel=\"nofollow\">MultipleOutputFormat</a> but I read that it will work for sequence files or text files and NOT map files.</p>\n\n<p>(FYI: i am using hadoop v20.2)</p>\n", "creation_date": 1337688922, "score": 0},
{"title": "Run nutch in OSGI environment", "view_count": 103, "is_answered": true, "answers": [{"question_id": 10810022, "owner": {"user_id": 1165637, "accept_rate": 96, "link": "http://stackoverflow.com/users/1165637/oleksi", "user_type": "registered", "reputation": 10490}, "body": "<p>You need to look for a bundle that implements Nutch. A quick google turned up <a href=\"http://wiki.apache.org/nutch/NutchOSGi\" rel=\"nofollow\">this</a>. If this particular bundle doesn't meet your needs, you can just embed the Nutch dependency into one of your own bundles, using the Embed-Dependency field in your bundle's manifest.</p>\n", "creation_date": 1338352649, "is_accepted": false, "score": 3, "last_activity_date": 1338352649, "answer_id": 10810047}, {"last_edit_date": 1338364469, "owner": {"user_id": 1104727, "link": "http://stackoverflow.com/users/1104727/holly-cummins", "user_type": "registered", "reputation": 5087}, "body": "<p>A bundle which already implements Nutch, as the other answer suggests, is your best bet. More generally, you can easily convert an existing jar into an OSGi bundle (without having the source code) using the <a href=\"http://www.aqute.biz/Bnd/Wrapping\" rel=\"nofollow\">wrap</a> function of the <a href=\"http://www.aqute.biz/Code/Bnd\" rel=\"nofollow\">bnd tool</a>. If you do have the source code and want to recompile, <a href=\"http://felix.apache.org/site/apache-felix-maven-bundle-plugin-bnd.html\" rel=\"nofollow\">maven's bundle plugin</a> and the bundle packaging will generate bundles.</p>\n\n<p>In the interests of modularity, which is one of OSGi's main drivers, I'd suggest using a separate Nutch bundle rather than embedding the dependency.</p>\n", "question_id": 10810022, "creation_date": 1338360026, "is_accepted": false, "score": 1, "last_activity_date": 1338364469, "answer_id": 10811164}], "question_id": 10810022, "tags": ["osgi", "bundle", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/10810022/run-nutch-in-osgi-environment", "last_activity_date": 1338364469, "owner": {"age": 25, "answer_count": 223, "creation_date": 1309099044, "user_id": 816213, "accept_rate": 90, "view_count": 869, "location": "Noida, India", "reputation": 10961}, "body": "<p>I am new to OSGI framwork So this question might seem silly.</p>\n\n<p>Can we run Apache Nutch 1.4 in OSGI framework. I want to create an OSGI bundle of nutch. I am using eclipse indigo to compile the Nutch source code. So i think there should be some kind of plugin which can create OSGI bundle instead of jar after the compilation. I just need the OSGI bundle of Nutch.</p>\n\n<p>I don't even know if it is possible.</p>\n", "creation_date": 1338352373, "score": 1},
{"title": "Nutch: Data read and adding metadata", "view_count": 1902, "owner": {"user_id": 1204751, "answer_count": 10, "creation_date": 1329026289, "accept_rate": 76, "view_count": 49, "reputation": 283}, "is_answered": true, "answers": [{"question_id": 10772031, "owner": {"user_id": 1204751, "accept_rate": 76, "link": "http://stackoverflow.com/users/1204751/crs", "user_type": "registered", "reputation": 283}, "body": "<p>Useful commands.</p>\n\n<p>Begin crawl</p>\n\n<pre><code>bin/nutch crawl urls -dir crawl -depth 3 -topN 5\n</code></pre>\n\n<p>Get statistics of crawled URL's</p>\n\n<pre><code>bin/nutch readdb crawl/crawldb -stats\n</code></pre>\n\n<p>Read segment (gets all the data from web pages)</p>\n\n<pre><code>bin/nutch readseg -dump crawl/segments/* segmentAllContent\n</code></pre>\n\n<p>Read segment (gets only the text field)</p>\n\n<pre><code>bin/nutch readseg -dump crawl/segments/* segmentTextContent -nocontent -nofetch -nogenerate -     noparse -noparsedata\n</code></pre>\n\n<p>Get all list of known links to each URL, including both the source URL and anchor text of the link.</p>\n\n<pre><code>bin/nutch readlinkdb crawl/linkdb/ -dump linkContent\n</code></pre>\n\n<p>Get all URL's crawled. Also gives other information like whether it was fetched, fetched time, modified time etc.</p>\n\n<pre><code>bin/nutch readdb crawl/crawldb/ -dump crawlContent\n</code></pre>\n\n<p>For the second part. i.e to add new field I am planning to use index-extra plugin or to write custom plugin. </p>\n\n<p>Refer:</p>\n\n<p><a href=\"https://issues.apache.org/jira/browse/NUTCH-422\" rel=\"nofollow\">this</a> and <a href=\"http://wiki.apache.org/nutch/WritingPluginExample\" rel=\"nofollow\">this</a></p>\n", "creation_date": 1338274054, "is_accepted": true, "score": 3, "last_activity_date": 1338274054, "answer_id": 10794335}], "question_id": 10772031, "tags": ["solr", "lucene", "web-crawler", "semantic-web", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/10772031/nutch-data-read-and-adding-metadata", "last_activity_date": 1338274054, "accepted_answer_id": 10794335, "body": "<p>I recently started looking apache nutch. I could do setup and able to crawl web pages of my interest with nutch. I am not quite understanding on how to read this data. I basically want to associate data of each page with some metadata(some random data for now) and store them locally which will be later used for searching(semantic). Do I need to use solr or lucene for the same? I am new to all of these. As far I know Nutch is used to crawl web pages. Can it do some additional features like adding metadata to the crawled data?</p>\n", "creation_date": 1338098949, "score": 5},
{"title": "nutch hadoop only one slave is crawling", "view_count": 200, "is_answered": false, "answers": [{"question_id": 10730903, "owner": {"user_id": 261079, "link": "http://stackoverflow.com/users/261079/david-gruzman", "user_type": "registered", "reputation": 6644}, "body": "<p>As part of any Hadoop MR job design there is a decision how to split the work between mappers.\nIn Your case nutch splits the fetching process by sites, and as a result only one mapper is used to fetch the data. If you hade more sites, it would split the load. <br>\nHere is a good description of the process: How does Nutch work with Hadoop cluster?</p>\n", "creation_date": 1337834828, "is_accepted": false, "score": 0, "last_activity_date": 1337834828, "answer_id": 10731305}], "question_id": 10730903, "tags": ["hadoop", "fetch", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/10730903/nutch-hadoop-only-one-slave-is-crawling", "last_activity_date": 1337834828, "owner": {"user_id": 1138315, "view_count": 20, "answer_count": 0, "creation_date": 1326099780, "reputation": 23}, "body": "<p>I have a 3-slaves hadoop cluster and I am performing a crawl on a single website. However, only 1 slave is performing fetching (though the other slaves are still alive). Is this normal behavior if only 1 domain is crawled? Is there any way to force the other slaves to fetch?</p>\n\n<p>Thanks.</p>\n", "creation_date": 1337831548, "score": 0},
{"title": "Update Nutch to fetch parent of every URL fetched", "view_count": 351, "is_answered": true, "answers": [{"question_id": 10699639, "owner": {"user_id": 1150329, "accept_rate": 89, "link": "http://stackoverflow.com/users/1150329/tejas-patil", "user_type": "registered", "reputation": 4793}, "body": "<p>I think the information can be obtained by generating linkdb.</p>\n\n<p><strong>The link database, or linkdb</strong>: This contains the list of known links to each URL, including both the source URL and anchor text of the link. It maintains an inverted link map, listing incoming links for each url. </p>\n\n<pre><code>bin/nutch invertlinks crawldb/linkdb -dir crawldb/segments\n</code></pre>\n\n<p>In parsing phase, nutch generates outlinks out of the crawled content and later the newly discovered urls are stored to crawldb in update phase. The new urls are fetch in next cycle / round of nutch crawl.</p>\n", "creation_date": 1337830502, "is_accepted": false, "score": 2, "last_activity_date": 1337830502, "answer_id": 10730800}], "question_id": 10699639, "tags": ["java", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/10699639/update-nutch-to-fetch-parent-of-every-url-fetched", "last_activity_date": 1337830502, "owner": {"age": 25, "answer_count": 223, "creation_date": 1309099044, "user_id": 816213, "accept_rate": 90, "view_count": 869, "location": "Noida, India", "reputation": 10961}, "body": "<p>As I run Apache Nutch 1.4 crawler, I want to store some additional information. I want to store the parent of every URL.</p>\n\n<p>For example, I want to crawl a page a.html that has 2 anchor links to b.html and c.html So when I crawl a.html, I should get something like this :-</p>\n\n<pre><code>a.html null\nb.html a.html\nc.html a.html\n</code></pre>\n\n<p>I want to store something like this.\nI have read how nutch works and have run nutch in eclipse too. I also read fetcher.java and logged where it fetched content. But I got no success in knowing where Nutch fetches the child URLs of a given page. I think this step takes place after parsing step.</p>\n", "creation_date": 1337680299, "score": 1},
{"title": "Nutch Crawling Using Regex", "view_count": 1112, "is_answered": false, "answers": [{"question_id": 10652059, "owner": {"user_id": 335362, "accept_rate": 42, "link": "http://stackoverflow.com/users/335362/sam", "user_type": "registered", "reputation": 461}, "body": "<p>I don't know anything about nutch, but I can help with the regex. As per my comment above, right now it's now clear to me exactly what you want to match.</p>\n\n<p>From the examples you have so far:</p>\n\n<pre><code>\".*\\.html\" will match anything that ends \".html\"\n</code></pre>\n", "creation_date": 1337343108, "is_accepted": false, "score": 0, "last_activity_date": 1337343108, "answer_id": 10652411}, {"question_id": 10652059, "owner": {"user_id": 1150329, "accept_rate": 89, "link": "http://stackoverflow.com/users/1150329/tejas-patil", "user_type": "registered", "reputation": 4793}, "body": "<p>You want <a href=\"http://mywebsite.com/a/b/\" rel=\"nofollow\">http://mywebsite.com/a/b/</a> or <a href=\"http://mywebsite.com/a/\" rel=\"nofollow\">http://mywebsite.com/a/</a> to get crawled and store only Type A urls ie. <a href=\"http://mywebsite.com/page.html\" rel=\"nofollow\">http://mywebsite.com/page.html</a> ?</p>\n\n<p>And its not clear what you mean by storing : is it about segments or crawldb ?\nNote that if you crawl those pages to discover type A urls, the content WILL get stored in segments. You cant escape that.</p>\n\n<p>My suggestion:\nCrawl entire thing. Later on, remove the unwanted things by setting the regex urlfilter file and running <a href=\"http://wiki.apache.org/nutch/bin/nutch_updatedb\" rel=\"nofollow\">updatedb</a> and <a href=\"http://wiki.apache.org/nutch/IntranetRecrawl\" rel=\"nofollow\">updatesegs</a> commands.</p>\n", "creation_date": 1337447898, "is_accepted": false, "score": -1, "last_activity_date": 1337447898, "answer_id": 10667335}], "question_id": 10652059, "tags": ["regex", "web-crawler", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/10652059/nutch-crawling-using-regex", "last_activity_date": 1337577918, "owner": {"user_id": 1280148, "answer_count": 0, "creation_date": 1332224230, "accept_rate": 25, "view_count": 4, "reputation": 23}, "body": "<p>I wanted to crawl a website but save only those web pages ,\nwhich are like :</p>\n\n<p>Type A : <a href=\"http://mywebsite.com/page.html\" rel=\"nofollow\">http://mywebsite.com/page.html</a></p>\n\n<p>any other links like. : <a href=\"http://mywebsite.com/a/b/\" rel=\"nofollow\">http://mywebsite.com/a/b/</a> or <a href=\"http://mywebsite.com/a/\" rel=\"nofollow\">http://mywebsite.com/a/</a>\nor any thing like this should be in queue to be crwaled , but they should be just used to get more url likes \"type A\" but not stored .</p>\n\n<p>How to use regular expression in regex-urlfilter.txt in nutch .</p>\n\n<p>or in RegexUrlNormalizer.xml</p>\n\n<p>\" I think that crawling on whole would be better option . But I also want that  while crawling . If a URL has anchor links like Type A : <a href=\"http://mywebsite.com/page.html\" rel=\"nofollow\">http://mywebsite.com/page.html</a>  . I want to store that link A was obtained through this webpage. So after I crawl , I can remove pages and obtain only Type A pages and I have that, this type A webpage was obtained from what particular webpage .Can I configure nutch for doing so ? , If Yes , Some hints . Do I need to modify source code. Write my own plugin in nutch . ?\"</p>\n", "creation_date": 1337341595, "score": 0},
{"title": "Java Lucene integration with .Net", "view_count": 1843, "owner": {"age": 38, "answer_count": 29, "creation_date": 1217828332, "user_id": 253, "accept_rate": 85, "view_count": 403, "location": "London, United Kingdom", "reputation": 1832}, "is_answered": true, "answers": [{"question_id": 204519, "owner": {"user_id": 21239, "accept_rate": 81, "link": "http://stackoverflow.com/users/21239/mauricio-scheffer", "user_type": "registered", "reputation": 81026}, "body": "<p>Instead of using Lucene, you could use <a href=\"http://lucene.apache.org/solr/\" rel=\"nofollow\">Solr</a> to index with nutch (see <a href=\"http://blog.foofactory.fi/2007/02/online-indexing-integrating-nutch-with.html\" rel=\"nofollow\">here</a>), then you can connect very easily to Solr using one of the two libraries available: <a href=\"http://www.codeplex.com/solrsharp\" rel=\"nofollow\">SolrSharp</a> and <a href=\"http://code.google.com/p/solrnet/\" rel=\"nofollow\">SolrNet</a>.</p>\n", "creation_date": 1224075292, "is_accepted": true, "score": 3, "last_activity_date": 1224075292, "answer_id": 204589}, {"question_id": 204519, "owner": {"user_id": 459, "accept_rate": 89, "link": "http://stackoverflow.com/users/459/dlamblin", "user_type": "registered", "reputation": 21007}, "body": "<p>Instead of using Solr, I wrote a java based indexer that runs in a cron job, and a java based web service for querying. I actually didn't index pages so much as different types of data that the .net site uses to build the pages. So there's actually 4 different indexes each with a different document structure that can all be queried in about the same way (say: users, posts, messages, photos).</p>\n\n<p>By defining an XSD for the web service responses I was able to both generate classes in .net and java to store a representation of the documents.  The web service basically runs the query on the right index and fills out the response xml from the hits. The .net client parses that back into objects. There's also a json interface for any client side JavaScript.</p>\n", "creation_date": 1224081689, "is_accepted": false, "score": 0, "last_activity_date": 1224081689, "answer_id": 204980}, {"question_id": 204519, "owner": {"user_id": 31015, "accept_rate": 20, "link": "http://stackoverflow.com/users/31015/winston-fassett", "user_type": "registered", "reputation": 2238}, "body": "<p>In case it wasn't totally clear from the other answers, Lucene.NET and Lucene (Java) use the same index format, so you should be able continue to use your existing (Java-based) mechanisms for <strong>indexing</strong>, and then use Lucene.NET inside your .NET web application to <strong>query</strong> the index.</p>\n\n<p>From <a href=\"http://incubator.apache.org/lucene.net/\">the Lucene.NET incubator site</a>:</p>\n\n<blockquote>\n  <p>In addition to the APIs and classes\n  port to C#, the algorithm of Java\n  Lucene is ported to C# Lucene. This\n  means an index created with Java\n  Lucene is <strong>back-and-forth compatible</strong>\n  with the C# Lucene; both at reading,\n  writing and updating. In fact <strong>a Lucene\n  index can be concurrently searched and\n  updated using Java Lucene and C#\n  Lucene processes</strong></p>\n</blockquote>\n", "creation_date": 1225135534, "is_accepted": false, "score": 5, "last_activity_date": 1225135534, "answer_id": 241124}, {"last_edit_date": 1235508136, "owner": {"user_id": 37379, "accept_rate": 89, "link": "http://stackoverflow.com/users/37379/sam", "user_type": "registered", "reputation": 6017}, "body": "<p>I'm also working on this.  </p>\n\n<p><a href=\"http://today.java.net/pub/a/today/2006/02/16/introduction-to-nutch-2.html\" rel=\"nofollow\">http://today.java.net/pub/a/today/2006/02/16/introduction-to-nutch-2.html</a></p>\n\n<p>It seems you can submit your query to nutch and get the rss results back.</p>\n\n<p>edit:</p>\n\n<p>Got this working today in a windows form as a proof of concept.  Two textboxes(searchurl and query), one for the server url and one for the query.  One datagrid view.  </p>\n\n<pre><code>private void Form1_Load(object sender, EventArgs e)\n        {\n            searchurl.Text = \"http://localhost:8080/opensearch?query=\";\n\n\n    }\n\n    private void search_Click(object sender, EventArgs e)\n    {\n        string uri;\n\n        uri = searchurl.Text.ToString() + query.Text.ToString();\n        Console.WriteLine(uri);\n\n        XmlDocument myXMLDocument = new XmlDocument();\n\n        myXMLDocument.Load(uri);\n\n        DataSet ds = new DataSet();\n\n        ds.ReadXml(new XmlNodeReader(myXMLDocument));\n\n        SearchResultsGridView1.DataSource = ds;\n        SearchResultsGridView1.DataMember = \"item\";\n\n    }\n</code></pre>\n", "question_id": 204519, "creation_date": 1235493567, "is_accepted": false, "score": 1, "last_activity_date": 1235508136, "answer_id": 582489}, {"question_id": 204519, "owner": {"user_id": 56524, "accept_rate": 45, "link": "http://stackoverflow.com/users/56524/mp", "user_type": "registered", "reputation": 10378}, "body": "<p>Why not switch from java lucene to the dot net version. Sure it's an investment but it's mostly a class substitution exercise. The last thing you need is more layers that add no value other than just being glue. Less glue and more stuff is what you should aim for...</p>\n", "creation_date": 1242628733, "is_accepted": false, "score": 0, "last_activity_date": 1242628733, "answer_id": 876477}, {"question_id": 204519, "owner": {"user_id": 1344070, "accept_rate": 100, "link": "http://stackoverflow.com/users/1344070/keisar", "user_type": "registered", "reputation": 2598}, "body": "<p>Got here by searching for a comparison between SolrNet and SolrSharp, just thought I'd leave here my impressions.</p>\n\n<p>It seems like SolarSharp is a dead project (wasn't updated for a long time) so the only option is SolarNet.</p>\n\n<p>I hope this will help someone, I would have left a comment to the accepted answer but I don't have enough reputation yet :)</p>\n", "creation_date": 1336837717, "is_accepted": false, "score": 1, "last_activity_date": 1336837717, "answer_id": 10565159}], "question_id": 204519, "tags": [".net", "lucene", "solr", "nutch"], "answer_count": 6, "link": "http://stackoverflow.com/questions/204519/java-lucene-integration-with-net", "last_activity_date": 1337266284, "accepted_answer_id": 204589, "body": "<p>I've got nutch and lucene setup to crawl and index some sites and I'd like to use a .net website instead of the JSP site that comes with nutch.</p>\n\n<p>Can anyone recommend some solutions?</p>\n\n<p>I've seen solutions where there was an app running on the index server which the .Net site used remoting to connect to.</p>\n\n<p>Speed is a consideration obviously so can this still perform well?</p>\n\n<p><strong>Edit:</strong> could NHibernate.Search work for this?</p>\n\n<p><strong>Edit:</strong> We ended up going with Solr index servers being used by our ASP.net site with the <a href=\"http://code.google.com/p/solrnet/\" rel=\"nofollow\">solrnet</a> library.</p>\n", "creation_date": 1224073632, "score": 3},
{"title": "Solr: undefined field text", "view_count": 1449, "owner": {"age": 36, "answer_count": 133, "creation_date": 1275503139, "user_id": 356759, "accept_rate": 58, "view_count": 986, "location": "Munich, Germany", "reputation": 4310}, "is_answered": true, "answers": [{"question_id": 10501197, "owner": {"user_id": 356759, "accept_rate": 58, "link": "http://stackoverflow.com/users/356759/nottinhill", "user_type": "registered", "reputation": 4310}, "body": "<p>Turns out, I used the wrong search syntax. You have to use </p>\n\n<pre><code>*:*\n</code></pre>\n\n<p>or </p>\n\n<pre><code>xmlfield:searchtext \n</code></pre>\n\n<p>It's also a good idea to get familiar with the schema.xml document.</p>\n", "creation_date": 1337224664, "is_accepted": true, "score": 1, "last_activity_date": 1337224664, "answer_id": 10629383}], "question_id": 10501197, "tags": ["apache", "solr", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/10501197/solr-undefined-field-text", "last_activity_date": 1337224664, "accepted_answer_id": 10629383, "body": "<p>When running latest (05/2012) Solr based n Nutch, folowing the official tutorial by Apache, I get:</p>\n\n<pre><code>HTTP ERROR 400\n\nProblem accessing /solr/select. Reason:\n\n    undefined field text\n</code></pre>\n", "creation_date": 1336489272, "score": 0},
{"title": "Nutch doesn&#39;t get UTF-8 characters", "view_count": 876, "owner": {"age": 26, "answer_count": 180, "creation_date": 1331977994, "user_id": 1275577, "accept_rate": 100, "view_count": 715, "location": "\u0130zmir", "reputation": 3787}, "is_answered": true, "answers": [{"question_id": 10154532, "owner": {"user_id": 1150329, "accept_rate": 89, "link": "http://stackoverflow.com/users/1150329/tejas-patil", "user_type": "registered", "reputation": 4793}, "body": "<p>There are 3 possibilities that I can think of:</p>\n\n<ol>\n<li>Nutch works fine and your code writes things correctly to files but your environment (terminal/editor) is not displaying the characters properly on output console.</li>\n<li>Your code for writing out the content (crawled by nutch) is not taking care of UTF-8 encoding.</li>\n<li>Nutch not handling UTF-8 encoding correctly. </li>\n</ol>\n\n<p>I had crawled pages, which had Chinese characters in it, with Nucth and I was able to see some garbage characters in the <code>readseg</code> output (this was with nutch 1.0). Later after I installed some language plugins and tweaked the settings in the terminal, I could see the characters. So, I think that #3 is not likely and you must focus on #1 and #2.</p>\n", "creation_date": 1334489637, "is_accepted": true, "score": 1, "last_activity_date": 1334489637, "answer_id": 10161670}, {"question_id": 10154532, "owner": {"user_id": 1393080, "link": "http://stackoverflow.com/users/1393080/user1393080", "user_type": "registered", "reputation": 1}, "body": "<p>I'm thinking that i've solved encoding problem. See the code below:</p>\n\n<pre><code>co = true;\nfe = true;\nge = true;\npa = true;\npd = true;\npt = true;\nSegmentReader segmentReader2 = new SegmentReader(crwlNutchCommon.nutch_conf, co, fe, ge, pa, pd, pt);\nHashMap&lt;String, List&lt;Writable&gt;&gt; hm = new HashMap&lt;String, List&lt;Writable&gt;&gt;();\nsegmentReader2.get(path, new Text(\"some_url\"), \n</code></pre>\n\n<p>new OutputStreamWriter(new FileOutputStream(\"somefile1\"), \"UTF-8\"),hm);</p>\n\n<p>The file somefile1 will probably have wrong encoding, but go further:</p>\n\n<pre><code>FileOutputStream fos; \nDataOutputStream dos;\nFile file= new File(\"somefile2\");\nfos = new FileOutputStream(file);\ndos=new DataOutputStream(fos);\nhm.get(\"co\").get(0).write(dos); // look\n</code></pre>\n\n<p>And that's working! somefile2 will be \"raw\", without any encoding modification, but with some extra data in the beginning and after the ending - I think it will be possible to parse them analyzing \"Content.java\" source file.</p>\n", "creation_date": 1336975617, "is_accepted": false, "score": 0, "last_activity_date": 1336975617, "answer_id": 10578174}, {"question_id": 10154532, "owner": {"user_id": 1393080, "link": "http://stackoverflow.com/users/1393080/user1393080", "user_type": "registered", "reputation": 1}, "body": "<p>Following you I decided to modify Fetcher class and added support to save content directly to mysql database. It works much better and faster.</p>\n", "creation_date": 1337078480, "is_accepted": false, "score": 0, "last_activity_date": 1337078480, "answer_id": 10599019}], "question_id": 10154532, "tags": ["eclipse", "ubuntu", "character-encoding", "nutch"], "answer_count": 3, "link": "http://stackoverflow.com/questions/10154532/nutch-doesnt-get-utf-8-characters", "last_activity_date": 1337078480, "accepted_answer_id": 10161670, "body": "<p>I crawl pages with <strong>Nutch</strong> and before indexing, I save the contents into separate files in the <strong>Fetcher</strong> class, so I don't use <strong>-readseg</strong> to get them back from indexed files. However, special characters such as \"\u00fc\" and \"\u00e7\" are saved as \"?\". </p>\n\n<p>I did everything recommended in <a href=\"http://wiki.apache.org/tomcat/FAQ/CharacterEncoding\" rel=\"nofollow\">Nutch Wiki page</a>. Edited the <strong></strong> tag's encoding attribute to UTF-8, it still doesn't work. I ran into some recommendations about making language changes in system files. I work in <strong>Ubuntu 11.10</strong>.</p>\n", "creation_date": 1334415391, "score": 0},
{"title": "How to configure nutch 1.4 with hadoop?", "view_count": 752, "is_answered": false, "answers": [{"question_id": 10546393, "owner": {"user_id": 1150329, "accept_rate": 89, "link": "http://stackoverflow.com/users/1150329/tejas-patil", "user_type": "registered", "reputation": 4793}, "body": "<p>The error which u hav pasted comes if NUTCH_CONF_DIR and NUTCH_HOME are not exported or pointed to wrong location. </p>\n\n<p>See this <a href=\"http://www.rui-yang.com/develop/build-nutch-1-4-cluster-with-hadoop/\" rel=\"nofollow\">http://www.rui-yang.com/develop/build-nutch-1-4-cluster-with-hadoop/</a> for detailed steps for using nutch 1.4 with hadoop 0.20</p>\n", "creation_date": 1336729677, "is_accepted": false, "score": 0, "last_activity_date": 1336729677, "answer_id": 10549079}], "question_id": 10546393, "tags": ["java", "hadoop", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/10546393/how-to-configure-nutch-1-4-with-hadoop", "last_activity_date": 1336729677, "owner": {"user_id": 1350726, "answer_count": 0, "creation_date": 1335165839, "accept_rate": 0, "view_count": 13, "reputation": 11}, "body": "<p>When I am firing this command</p>\n\n<pre><code>./bin/hadoop dfs -put url/urls_test url/urls_test\n</code></pre>\n\n<p>URLs are getting up correctly.</p>\n\n<p>But below command is showing error</p>\n\n<pre><code>./bin/nutch crawl url/urls_test -dir cr_demo -depth 1\n</code></pre>\n\n<p>ERROR</p>\n\n<pre><code>java.lang.RuntimeException: Error in configuring object\n</code></pre>\n", "creation_date": 1336717313, "score": 0},
{"title": "Does nutch crawl over forms?", "view_count": 342, "is_answered": true, "answers": [{"question_id": 10536926, "owner": {"user_id": 1150329, "accept_rate": 89, "link": "http://stackoverflow.com/users/1150329/tejas-patil", "user_type": "registered", "reputation": 4793}, "body": "<p>Nutch gets the html source of the desired page via HTTP request. Now the html source of the page can contain drop-down list coded inside it. If that is coded using complex scripting like dojo / ajax then it wont be able to INTERPRET it as a browser would do. If the outlinks of the drop-down list are seen right away in the html source, then nutch will get those pages crawled. Apart from normal textual content, Nutch also does parsing for Java script portions of the html page.</p>\n\n<p>Now for verifying this, open the page in bowser / wget it. View the page source in text editor like notepad / vi. Can you see the outlinks to drop-down box there ? if yes, then nutch will crawl those outlinks else not. </p>\n", "creation_date": 1336706193, "is_accepted": false, "score": 1, "last_activity_date": 1336706193, "answer_id": 10544965}], "question_id": 10536926, "tags": ["solr", "lucene", "web-crawler", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/10536926/does-nutch-crawl-over-forms", "last_activity_date": 1336706193, "owner": {"user_id": 1259550, "answer_count": 2, "creation_date": 1331304557, "accept_rate": 71, "view_count": 21, "reputation": 69}, "body": "<p>I was wondering if nutch 1.4 crawls over forms right out of the box. For example if there is a drop-down list, would it try to get all possible pages combined from the items in the drop-down list ??</p>\n\n<p>Thanks</p>\n", "creation_date": 1336662423, "score": 1},
{"title": "How to configure nutch 1.4?", "view_count": 1716, "is_answered": true, "answers": [{"question_id": 10515521, "owner": {"user_id": 1150329, "accept_rate": 89, "link": "http://stackoverflow.com/users/1150329/tejas-patil", "user_type": "registered", "reputation": 4793}, "body": "<p>See here <a href=\"http://www.params.me/2011/07/apache-nutch-13-setup.html\" rel=\"nofollow\">http://www.params.me/2011/07/apache-nutch-13-setup.html</a></p>\n\n<p>These steps worked for me. Let me know if you face any issues with this.</p>\n", "creation_date": 1336705466, "is_accepted": false, "score": 2, "last_activity_date": 1336705466, "answer_id": 10544878}], "question_id": 10515521, "tags": ["java", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/10515521/how-to-configure-nutch-1-4", "last_activity_date": 1336705466, "owner": {"user_id": 1350726, "answer_count": 0, "creation_date": 1335165839, "accept_rate": 0, "view_count": 13, "reputation": 11}, "body": "<p>I am using this command to crawl some URLs:</p>\n\n<pre><code>./bin/nutch crawl urls/urls -dir crawl -depth 2 \n</code></pre>\n\n<p>It is showing the following error:</p>\n\n<pre><code>Could not find or load main class org.apache.nutch.crawl.Crawl\n</code></pre>\n\n<p>How can I fix this?</p>\n", "creation_date": 1336564302, "score": 1},
{"title": "Using Nutch solrindex to index to multiple cores?", "view_count": 1120, "owner": {"user_id": 1154870, "answer_count": 37, "creation_date": 1326834146, "accept_rate": 56, "view_count": 218, "reputation": 2386}, "is_answered": true, "answers": [{"question_id": 10394893, "owner": {"user_id": 633239, "accept_rate": 90, "link": "http://stackoverflow.com/users/633239/javanna", "user_type": "registered", "reputation": 38114}, "body": "<p>I'm not aware of any core parameter. You should just include the name of the core in your <a href=\"http://wiki.apache.org/nutch/bin/nutch%20solrindex\">solr url</a> parameter like this: <code>http://localhost:8983/solr/core0</code>.</p>\n", "creation_date": 1335866621, "is_accepted": true, "score": 8, "last_activity_date": 1335866621, "answer_id": 10396188}, {"question_id": 10394893, "owner": {"user_id": 1259550, "accept_rate": 71, "link": "http://stackoverflow.com/users/1259550/breakdown1986", "user_type": "registered", "reputation": 69}, "body": "<p>In nutch 1.4, this is what I use to index to different cores:</p>\n\n<p>bin/nutch crawl urls/url1 -solr <a href=\"http://localhost:8983/solr/core1\" rel=\"nofollow\">http://localhost:8983/solr/core1</a> -depth 10 -topN 10000</p>\n", "creation_date": 1336662917, "is_accepted": false, "score": 0, "last_activity_date": 1336662917, "answer_id": 10537101}], "question_id": 10394893, "tags": ["solr", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/10394893/using-nutch-solrindex-to-index-to-multiple-cores", "last_activity_date": 1336662917, "accepted_answer_id": 10396188, "body": "<p>Is there parameter in the <code>bin/nutch solrindex</code> command to indicate which Solr core to index to? </p>\n", "creation_date": 1335857852, "score": 5},
{"title": "Setup Nutch 1.3 and Solr 3.1", "view_count": 1902, "is_answered": false, "answers": [{"question_id": 7881202, "owner": {"user_id": 950178, "link": "http://stackoverflow.com/users/950178/jayendra", "user_type": "registered", "reputation": 34896}, "body": "<p>Seems the content field definition is missing in the schema.xml.</p>\n\n<p>e.g.</p>\n\n<pre><code>&lt;field name=\"content\" type=\"text\" stored=\"false\" indexed=\"true\"/&gt;\n</code></pre>\n\n<p>The example schema.xml @ <a href=\"http://svn.apache.org/viewvc/nutch/branches/branch-1.3/conf/schema.xml?view=markup\" rel=\"nofollow\">http://svn.apache.org/viewvc/nutch/branches/branch-1.3/conf/schema.xml?view=markup</a> seems to have it. You may want to check the schema.xml you copied over.</p>\n", "creation_date": 1319651347, "is_accepted": false, "score": 0, "last_activity_date": 1319651347, "answer_id": 7906712}], "question_id": 7881202, "tags": ["java", "solr", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/7881202/setup-nutch-1-3-and-solr-3-1", "last_activity_date": 1336484861, "owner": {"user_id": 1011552, "view_count": 23, "answer_count": 17, "creation_date": 1319484903, "reputation": 190}, "body": "<p>I am trying to get nutch 1.3 and solr 3.1 working together. </p>\n\n<p>Note: I am using Windows and have Cygwin installed. </p>\n\n<p>I have nutch installed and did a basic crawl (running from runtime/local)</p>\n\n<blockquote>\n  <p>bin/nutch crawl urls -dir crawl -depth 3</p>\n</blockquote>\n\n<p>This seems to have worked based on teh logs (crawl.log)\n    ... \n    LinkDb: finished at 2011-10-24 14:22:47, elapsed: 00:00:02\n    crawl finished: crawl</p>\n\n<p>I have solr installed and verified install with localhost:8983/solr/admin</p>\n\n<p>I copied the nutch schema.xml file to the example\\solr\\conf folder</p>\n\n<p>When I run the following command</p>\n\n<blockquote>\n  <p>bin/nutch solrindex <a href=\"http://localhost:8983/solr\" rel=\"nofollow\">http://localhost:8983/solr</a> crawl/crawldb crawl/linkdb crawl/segments/*</p>\n</blockquote>\n\n<p>I get the following error (hadoop.log)</p>\n\n<pre><code>2011-10-24 15:39:26,467 WARN  mapred.LocalJobRunner - job_local_0001\norg.apache.solr.common.SolrException: ERROR:unknown field 'content'\n\nERROR:unknown field 'content'\nrequest: http://localhost:8983/solr/update?wt=javabin&amp;version=2\n...\norg.apache.nutch.indexer.IndexerOutputFormat$1.close(IndexerOutputFormat.java:48)\n    at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:474)\n    at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:411)\n    at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:216)\n2011-10-24 15:39:26,676 ERROR solr.SolrIndexer - java.io.IOException: Job failed!\n</code></pre>\n\n<p>What am I missing?</p>\n", "creation_date": 1319486197, "score": 2},
{"title": "How to retrieve a HTML page who was crawled using cache?", "view_count": 493, "is_answered": true, "answers": [{"question_id": 8105658, "owner": {"user_id": 1275577, "accept_rate": 100, "link": "http://stackoverflow.com/users/1275577/%c4%b0smet-alkan", "user_type": "registered", "reputation": 3787}, "body": "<p>You can retrieve raw HTML from Nutch index files by:</p>\n\n<pre><code>bin/nutch readseg -dump crawl-test/segments/.. dump -nogenerate \n-noparse -noparsedata -noparsetext\n</code></pre>\n\n<p>or in Java:</p>\n\n<pre><code>Configuration conf = NutchConfiguration.create(); \nSegmentReader reader = new SegmentReader(conf, true, false, false, \nfalse, false, false);  \nreader.get(); \n</code></pre>\n\n<p>Also this <a href=\"http://dynamicguy.com/2010/09/nutch-cheat-sheet/\" rel=\"nofollow\">link</a> would be very helpful too. I should say I didn't work with Solr but only with Nutch, hope this helps.</p>\n", "creation_date": 1336434338, "is_accepted": false, "score": 1, "last_activity_date": 1336434338, "answer_id": 10490644}], "question_id": 8105658, "tags": ["solr", "information-retrieval", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/8105658/how-to-retrieve-a-html-page-who-was-crawled-using-cache", "last_activity_date": 1336434338, "owner": {"user_id": 1043265, "view_count": 2, "answer_count": 0, "creation_date": 1321112341, "reputation": 11}, "body": "<p>I'm using nutch1.3 to crawl some webpage and solr to generate each index. Performing a simple search on <a href=\"http://localhost:8983/solr/\" rel=\"nofollow\">http://localhost:8983/solr/</a>, it returns a result with various docs, each doc has some fields, I think these are the key to solve my problem: </p>\n\n<blockquote>\n  <p>content: returns all the text crawled from the original html page, but unfortunately without the original html tags :(</p>\n  \n  <p>segment: returns the nutch segment where the data was extracted.</p>\n  \n  <p>digest: I think this field its the Solr's index for this page, because its unique and was generated by Solr.</p>\n</blockquote>\n\n<p>The <strong>main question</strong> is: How I can retrieve the original HTML page using that I already cached? I think there is a way to use a combination of 'segment' and 'digest' fields to retrieve the cached html page, but act I'm not lucky to discover how.</p>\n\n<p>ps1: I need this because I'm doing a offline search based on crawled html pages and need to show a 'cached view' like Google does.</p>\n\n<p>ps2: On nutch 1.2 I was able to do it, using 'nutch-1.2.war' its possible to search a term and  there is a button 'cached' who shows a page (http://localhost:8080/nutch-1.2/cached.jsp?idx=0&amp;id=5) with the cached page.</p>\n", "creation_date": 1321114365, "score": 2},
{"title": "What java classes are associated with these nutch commands?", "view_count": 332, "is_answered": false, "answers": [{"question_id": 4511016, "owner": {"user_id": 154146, "link": "http://stackoverflow.com/users/154146/brian", "user_type": "registered", "reputation": 1203}, "body": "<p>See: <a href=\"http://wiki.apache.org/nutch/CommandLineOptions\" rel=\"nofollow\">Nutch - Commandline Options</a></p>\n\n<p>Click on the respective command for each of the commands you've listed and the destination page will outline which class is being used:</p>\n\n<p>For example:</p>\n\n<ul>\n<li>nutch inject - <a href=\"http://wiki.apache.org/nutch/bin/nutch_inject\" rel=\"nofollow\">bin/nutch_inject</a></li>\n</ul>\n", "creation_date": 1304716805, "is_accepted": false, "score": 0, "last_activity_date": 1304716805, "answer_id": 5917130}, {"last_edit_date": 1336398714, "owner": {"user_id": 1275577, "accept_rate": 100, "link": "http://stackoverflow.com/users/1275577/%c4%b0smet-alkan", "user_type": "registered", "reputation": 3787}, "body": "<p>Follow this <a href=\"http://wiki.apache.org/nutch/RunNutchInEclipse\" rel=\"nofollow\">guide</a>. The class names are generally same with the command names. Look for the classes with <strong>main</strong> in them.</p>\n", "question_id": 4511016, "creation_date": 1336267833, "is_accepted": false, "score": 0, "last_activity_date": 1336398714, "answer_id": 10467387}], "question_id": 4511016, "tags": ["java", "solr", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/4511016/what-java-classes-are-associated-with-these-nutch-commands", "last_activity_date": 1336398714, "owner": {"user_id": 394616, "answer_count": 4, "creation_date": 1279366285, "accept_rate": 67, "view_count": 37, "reputation": 194}, "body": "<p>I have the following commands which I've batched together. It runs Nutch and sends the results to Solr. I have read that these match up to Java methods which I'd like to use to run programmatically.</p>\n\n<p>Which Java classes do these match up to?</p>\n\n<pre><code>bin/nutch inject crawl/crawldb urls(text file containing list of urls)\nbin/nutch generate crawl/crawldb crawl/segments\nexport SEGMENT=crawl/segments/`ls -tr crawl/segments|tail -1`\nbin/nutch fetch $SEGMENT -noParsing\nbin/nutch parse $SEGMENT\nbin/nutch updatedb crawl/crawldb $SEGMENT -filter -normalize\nbin/nutch invertlinks crawl/linkdb -dir crawl/segments\nbin/nutch solrindex http://localhost:8080/solr/ crawl/crawldb crawl/linkdb crawl/segments/*\n</code></pre>\n\n<p>Thanks</p>\n", "creation_date": 1293033671, "score": 0},
{"title": "How to configure Nutch to avoid crawling nonsense calendar webpage", "view_count": 126, "owner": {"user_id": 561629, "answer_count": 2, "creation_date": 1294080535, "accept_rate": 61, "view_count": 48, "reputation": 215}, "is_answered": true, "answers": [{"question_id": 10454829, "owner": {"user_id": 1150329, "accept_rate": 89, "link": "http://stackoverflow.com/users/1150329/tejas-patil", "user_type": "registered", "reputation": 4793}, "body": "<p>There is no other way apart from regex url filtering that can do this. You can keep adding new patterns to the regex file whenever you see an undesired page makes it through the crawled content.</p>\n", "creation_date": 1336267397, "is_accepted": true, "score": 1, "last_activity_date": 1336267397, "answer_id": 10467353}], "question_id": 10454829, "tags": ["calendar", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/10454829/how-to-configure-nutch-to-avoid-crawling-nonsense-calendar-webpage", "last_activity_date": 1336267397, "accepted_answer_id": 10467353, "body": "<p>I am using Nutch to index a website. I notice that Nutch has crawled some junk webpages, such as http://<strong><em>*</em>**<em>*</em>**</strong>/category/events/2015-11. This webpage is about the event occurring in 2015, 11. This is completely nonsense for me. I want to know is it possible for Nutch to intelligently skip such webpages. It may be argued that I can use Regex to avoid this. However, as the naming pattern of calendar webpages are not the same all the time, there is no way to write a perfect Regex for this. I know Heritrix (a Internet archive crawler) has such capabilities to avoid crawling nonsense calendar webpage. Does anyone solve this issue?</p>\n", "creation_date": 1336158881, "score": 0},
{"title": "Connecting MySQL to Apache nutch", "view_count": 1704, "is_answered": true, "answers": [{"question_id": 4673757, "owner": {"user_id": 465179, "accept_rate": 98, "link": "http://stackoverflow.com/users/465179/mat-banik", "user_type": "registered", "reputation": 8395}, "body": "<p>Get source from <a href=\"http://mirror.nyi.net/apache//nutch/apache-nutch-1.2-src.zip\" rel=\"nofollow\">http://mirror.nyi.net/apache//nutch/apache-nutch-1.2-src.zip</a></p>\n\n<p>Open <code>org.apache.nutch.crawl.Crawl</code> class in your editor.  </p>\n\n<p>Lookup variable <code>Path crawlDb = new Path(dir + \"/crawldb\");</code></p>\n\n<p>The variable will give a hint on where to replace the code in order to get your own <code>CustomMySQLCrawl</code> class.</p>\n\n<p>The persistence is happening during this call: <code>crawlDbTool.update(crawlDb, segs, true, true); // update crawldb</code> So there is where you should save it to the database. You might want to consider integrating hibernate at this point. </p>\n", "creation_date": 1294868045, "is_accepted": false, "score": 3, "last_activity_date": 1294868045, "answer_id": 4674297}, {"question_id": 4673757, "owner": {"user_id": 183100, "link": "http://stackoverflow.com/users/183100/pascal-dimassimo", "user_type": "registered", "reputation": 5766}, "body": "<p>I see 2 possibilities: either you take the content from the Lucene index created by Nutch at the end of the crawl job (I think it is removed in Nutch 2.0) OR take the data from the segment at each iteration. </p>\n\n<p>If what is put in the Lucene index is enough for you, it may be easier that way. But if you need more, each segment contain everything that was fetched by Nutch.</p>\n", "creation_date": 1295903606, "is_accepted": false, "score": 1, "last_activity_date": 1295903606, "answer_id": 4787233}, {"question_id": 4673757, "owner": {"user_id": 1275577, "accept_rate": 100, "link": "http://stackoverflow.com/users/1275577/%c4%b0smet-alkan", "user_type": "registered", "reputation": 3787}, "body": "<p>If you will use Nutch's binary executable, run -readseg command after crawling. It will give you a huge file which contains all the raw html and other info in it. You can parse and save the needed data to database after that.</p>\n\n<p>If you willing to run Nutch in Eclipse, you should add some code to the class Fetcher.</p>\n\n<pre><code>pstatus = output(fit.url, fit.datum, content, status, CrawlDatum.STATUS_FETCH_SUCCESS);\nupdateStatus(content.getContent().length);\n</code></pre>\n\n<p>Write a simple call and write to database code after these lines in Fetcher class. You can get the raw html by:</p>\n\n<pre><code>content.getContent();\n</code></pre>\n\n<p>This returns a byte array representation of the html file, convert it to String and save it to your database. You might suffer from character encoding: <a href=\"http://wiki.apache.org/nutch/GettingNutchRunningWithUtf8\" rel=\"nofollow\">Nutch with UTF-8</a> to configure Nutch. However, the problem generally occured by Eclipse's encoding. To overcome that, take the substring of the content which includes \"charset\" value and:</p>\n\n<pre><code>String yourContent = new String(content.getContent, encodingYouFound);\n</code></pre>\n\n<p>\"encoding\" here is a String, so it will be enough to retrieve it from the \"content\". If you can't, some sites might not have the charset attribute, use a general encoding such as UTF-8.</p>\n", "creation_date": 1336266933, "is_accepted": false, "score": 1, "last_activity_date": 1336266933, "answer_id": 10467314}], "question_id": 4673757, "tags": ["java", "mysql", "nutch"], "answer_count": 3, "link": "http://stackoverflow.com/questions/4673757/connecting-mysql-to-apache-nutch", "last_activity_date": 1336266933, "owner": {"user_id": 358193, "answer_count": 75, "creation_date": 1275635583, "accept_rate": 75, "view_count": 503, "location": "Mumbai, India", "reputation": 1952}, "body": "<p>I am using Apache Nutch first time. How can I store data into a MySQL database after crawling? I want to be able to easily use the data in other web applications.</p>\n\n<p>I found a <a href=\"http://stackoverflow.com/questions/3227259/nutch-mysql-integration\">question related</a>, but I don't clearly understand which part of the code id gona replace by MySQL connector. Please help with a short code example.</p>\n", "creation_date": 1294864583, "score": 3},
{"title": "Nutch 1.3: change User-Agent", "view_count": 548, "is_answered": false, "answers": [{"question_id": 6890462, "owner": {"user_id": 12016, "accept_rate": 77, "link": "http://stackoverflow.com/users/12016/mana", "user_type": "registered", "reputation": 2720}, "body": "<p>Since the user agent is manifested in the config file (nutch-site.xml) there is no possibility to change that for a certain domain.</p>\n\n<p>I suggest that you create an instance of nutch for each domain you want to crawl. Within each instance you set the url-filter, seed url and user agent matching the domain you want to crawl.</p>\n\n<p>This should allow you to execute each crawl with custom settings.</p>\n\n<p>cheers mana</p>\n", "creation_date": 1312443689, "is_accepted": false, "score": 0, "last_activity_date": 1312443689, "answer_id": 6937832}], "question_id": 6890462, "tags": ["user-agent", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/6890462/nutch-1-3-change-user-agent", "last_activity_date": 1336131534, "owner": {"user_id": 676042, "view_count": 0, "answer_count": 0, "creation_date": 1301020848, "reputation": 1}, "body": "<p>I need to change the user-agent string for each crawled domain. I use standard Nutch crawl utility code, it crawls one domain per time. It's being started in multithreading mode to crawl many domains. I need to pass to domain string [botname]+domainID to, but I'm unsure how to implement it? </p>\n", "creation_date": 1312127242, "score": 0},
{"title": "nutch-solr: Formatting date from web page metadata into correct Solr format", "view_count": 1054, "is_answered": true, "answers": [{"last_edit_date": 1336126909, "owner": {"user_id": 633239, "accept_rate": 90, "link": "http://stackoverflow.com/users/633239/javanna", "user_type": "registered", "reputation": 38114}, "body": "<p>Have a look at the <a href=\"http://lucene.apache.org/solr/api/org/apache/solr/schema/DateField.html\" rel=\"nofollow\"><code>DateField</code> docs</a>. It contains some examples of the correct format for date. This should be ok: <code>2011-12-05T00:00:00Z</code>. Don't forget to convert the date to UTC before sending it to Solr.</p>\n\n<p>But your problem is more how to tell nutch to index this field as date converting it to the right format. I had a look at the source code but I haven't found a way out of the box. I guess you should write some code, probably extend the plugin you're using, maybe you could even contribute it back to the project.</p>\n", "question_id": 10445095, "creation_date": 1336124038, "is_accepted": false, "score": 1, "last_activity_date": 1336126909, "answer_id": 10446293}], "question_id": 10445095, "tags": ["datetime", "solr", "date-format", "nutch", "date-formatting"], "answer_count": 1, "link": "http://stackoverflow.com/questions/10445095/nutch-solr-formatting-date-from-web-page-metadata-into-correct-solr-format", "last_activity_date": 1336126909, "owner": {"user_id": 1254739, "view_count": 11, "answer_count": 0, "creation_date": 1331125619, "reputation": 28}, "body": "<p>I am currently configuring nutch and solr to index web pages and their metadata.\nThere are metadata tags containing dates in the format yyyy-mm-dd, hence they miss the required time format extension to work as a solr.DateField or solr.TrieDateField.\nI would like to use date ranges on the date fields, does not work without having them in one of these formats, right?</p>\n\n<p>Currently my dates are included as text in the schema:</p>\n\n<pre><code>&lt;fieldType name=\"string\" class=\"solr.StrField\" sortMissingLast=\"true\" omitNorms=\"true\"/&gt;\n... \n&lt;field name=\"Date Modified\" type=\"string\" stored=\"true\" indexed=\"true\" multiValued=\"true\"/&gt;\n</code></pre>\n\n<p>Whenever I use this for the field definition instead...</p>\n\n<pre><code>&lt;fieldType name=\"date\" class=\"solr.TrieDateField\" omitNorms=\"true\" positionIncrementGap=\"0\"/&gt;\n&lt;field name=\"Date Modified\" type=\"date\" stored=\"true\" indexed=\"true\" multiValued=\"true\"/&gt;\n</code></pre>\n\n<p>...I receive this error message:</p>\n\n<pre><code>2012-05-02 23:45:58,370 WARN  mapred.LocalJobRunner - job_local_0029\norg.apache.solr.common.SolrException: ERROR: [doc=http://ec.gc.ca/] Error adding field     'Date Modified'='2011-12-05'\n\nERROR: [doc=http://ec.gc.ca/] Error adding field 'Date Modified'='2011-12-05'\n\nrequest: http://localhost:8983/solr/update?wt=javabin&amp;version=2\n    at     org.apache.solr.client.solrj.impl.CommonsHttpSolrServer.request(CommonsHttpSolrServer.java:430)\n    at     org.apache.solr.client.solrj.impl.CommonsHttpSolrServer.request(CommonsHttpSolrServer.java    :244)\n    at     org.apache.solr.client.solrj.request.AbstractUpdateRequest.process(AbstractUpdateRequest.j    ava:105)\n    at org.apache.solr.client.solrj.SolrServer.add(SolrServer.java:49)\n    at org.apache.nutch.indexer.solr.SolrWriter.close(SolrWriter.java:93)\n    at     org.apache.nutch.indexer.IndexerOutputFormat$1.close(IndexerOutputFormat.java:48)\n    at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:474)\n    at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:411)\n    at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:216)\n2012-05-02 23:45:58,966 ERROR solr.SolrIndexer - java.io.IOException: Job failed!\n</code></pre>\n\n<p>I browsed through different forums, but did not find a solution that worked. It might be I missed something about Data Import Handling.\nDo I need to change something in the nutch configuration to get the date fields in the right format?</p>\n\n<p>Thanks a lot!</p>\n\n<p>All the best</p>\n", "creation_date": 1336119135, "score": 2},
{"title": "Job failed when index with solr", "view_count": 492, "is_answered": false, "question_id": 10361312, "tags": ["solr", "hadoop", "nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/10361312/job-failed-when-index-with-solr", "last_activity_date": 1335595782, "owner": {"age": 27, "answer_count": 19, "creation_date": 1331644819, "user_id": 1266556, "accept_rate": 75, "view_count": 41, "location": "Shanghai, China", "reputation": 362}, "body": "<p>I have integrated nutch with hadoop and ran a crawl with it. But when I want to create solrindex for the data, I got Job Failed. This is my command:<br/></p>\n\n<pre><code>$bin/nutch solrindex http://namenode:8983/solr/ crawl/crawldb -linkdb crawl/linkdb crawl/segments/*\n</code></pre>\n\n<p><br>the directory 'crawl' stores the data and it located in HDFS. I just used the example within the solr release. And it works fine when ran the command in single node mode(not in distributed mode). Here is a fragment of output and forgive me for posting it in such a mussy way:</p>\n\n<pre><code>12/04/28 13:51:22 INFO mapred.JobClient:  map 100% reduce 23%\n12/04/28 13:51:30 INFO mapred.JobClient: Task Id : attempt_201204212112_0076_r_000000_0, Status : FAILED\njava.io.IOException\n    at org.apache.nutch.indexer.solr.SolrWriter.makeIOException(SolrWriter.java:103)\n    at org.apache.nutch.indexer.solr.SolrWriter.close(SolrWriter.java:98)\n    at org.apache.nutch.indexer.IndexerOutputFormat$1.close(IndexerOutputFormat.java:48)\n    at org.apache.hadoop.mapred.ReduceTask$OldTrackingRecordWriter.close(ReduceTask.java:466)\n    at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:530)\n    at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:420)\n    at org.apache.hadoop.mapred.Child$4.run(Child.java:255)\n    at java.security.AccessController.doPrivileged(Native Method)\n    at javax.security.auth.Subject.doAs(Unknown Source)\n    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1059)\n    at org.apache.hadoop.mapred.Child.main(Child.java:249)\nCaused by: org.apache.solr.client.solrj.SolrServerException: java.net.ConnectException: Connection refused\n    at org.apache.solr.client.solrj.impl.CommonsHttpSolrServer.request(CommonsHttpSolrServer.java:478)\n    at org.apache.solr.client.solrj.impl.CommonsHttpSolrServer.request(CommonsHttpSolrServer.java:244)\n    at org.apache.solr.client.solrj.request.AbstractUpdateRequest.process(AbstractUpdateRequest.java:105)\n    at org.apache.solr.client.solrj.SolrServer.add(SolrServer.java:49)\n    at org.apache.nutch.indexer.solr.SolrWriter.close(SolrWriter.java:93)\n    ... 9 more\nCaused by: java.net.ConnectException: Connection refused\n    at java.net.PlainSocketImpl.socketConnect(Native Method)\n    at java.net.PlainSocketImpl.doConnect(Unknown Source)\n    at java.net.PlainSocketImpl.connectToAddress(Unknown Source)\n    at java.net.PlainSocketImpl.connect(Unknown Source)\n    at java.net.SocksSocketImpl.connect(Unknown Source)\n    at java.net.Socket.connect(Unknown Source)\n    at java.net.Socket.connect(Unknown Source)\n    at java.net.Socket.&lt;init&gt;(Unknown Source)\n    at java.net.Socket.&lt;init&gt;(Unknown Source)\n    at org.apache.commons.httpclient.protocol.DefaultProtocolSocketFactory.createSocket(DefaultProtocolSocketFactory.java:79)\n    at org.apache.commons.httpclient.protocol.DefaultProtocolSocketFactory.createSocket(DefaultProtocolSocketFactory.java:121)\n    at org.apache.commons.httpclient.HttpConnection.open(HttpConnection.java:706)\n    at org.apache.commons.httpclient.HttpMethodDirector.executeWithRetry(HttpMethodDirector.java:386)\n    at org.apache.commons.httpclient.HttpMethodDirector.executeMethod(HttpMethodDirector.java:170)\n    at org.apache.commons.httpclient.HttpClient.executeMethod(HttpClient.java:396)\n    at org.apache.commons.httpclient.HttpClient.executeMethod(HttpClient.java:324)\n    at org.apache.solr.client.solrj.impl.CommonsHttpSolrServer.request(CommonsHttpSolrServer.java:422)\n    ... 13 more\n</code></pre>\n", "creation_date": 1335595782, "score": 0},
{"title": "Error message nutch eclipse", "view_count": 70, "is_answered": false, "answers": [{"question_id": 10311239, "owner": {"user_id": 27905, "link": "http://stackoverflow.com/users/27905/nitind", "user_type": "registered", "reputation": 11319}, "body": "<p>Your project needs to point to a valid JRE, which itself needs to be properly set up n the Installed JREs preference page.</p>\n", "creation_date": 1335450667, "is_accepted": false, "score": 0, "last_activity_date": 1335450667, "answer_id": 10335566}], "question_id": 10311239, "tags": ["eclipse", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/10311239/error-message-nutch-eclipse", "last_activity_date": 1335450667, "owner": {"age": 36, "answer_count": 0, "creation_date": 1334842743, "user_id": 1344145, "view_count": 6, "location": "Bulgaria", "reputation": 1}, "body": "<p>I am trying to build nutch 1.4 from the source code within eclipse env. Everything is ok but I receive an error message in class org.apache.nutch.util.domain.TopLevelDomain.java that says </p>\n\n<pre><code>The type java.lang.Object cannot be resolved. \nIt is indirectly referenced from required .class files\n</code></pre>\n\n<p>What could be the reason when Object is the main class and Libraries for JRE are included.</p>\n", "creation_date": 1335339118, "score": -1},
{"title": "how to make nutch crawler crawl", "view_count": 583, "is_answered": false, "answers": [{"question_id": 4291057, "owner": {"user_id": 484046, "accept_rate": 81, "link": "http://stackoverflow.com/users/484046/millebii", "user_type": "registered", "reputation": 976}, "body": "<p>Yes and no.</p>\n\n<p>crawl-urlfiler.txt act as a filter, so only urls on apache.org will ever be crawled in your example</p>\n\n<p>The url folder gives the 'seed' urls where to let the crawler start. \nSo if you want the crawler to stay in a set of sites, you will want to make sure they have a positive match with the filter... otherwise it will crawl the entire web. This may mean you have to put the list of sites in the filter</p>\n", "creation_date": 1291579855, "is_accepted": false, "score": 0, "last_activity_date": 1291579855, "answer_id": 4360882}], "question_id": 4291057, "tags": ["nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/4291057/how-to-make-nutch-crawler-crawl", "last_activity_date": 1335376053, "owner": {"user_id": 520072, "view_count": 5, "answer_count": 0, "creation_date": 1290685000, "reputation": 11}, "body": "<p>i have some doubt in nutch \nwhile i used the wiki i am asked to edit the <code>crawl-urlfilter.txt</code></p>\n\n<pre><code>+^http://([a-z0-9]*\\.)*apache.org/\n</code></pre>\n\n<p>and i am asked to create an url folder and an list of url...</p>\n\n<p>do i need to create all the links in <code>crawl-urlfilter.txt</code> and in the list of url ...</p>\n", "creation_date": 1290851930, "score": 1},
{"title": "How to read the content from Nutch index?", "view_count": 2247, "owner": {"age": 27, "answer_count": 19, "creation_date": 1331644819, "user_id": 1266556, "accept_rate": 75, "view_count": 41, "location": "Shanghai, China", "reputation": 362}, "is_answered": true, "answers": [{"question_id": 10274242, "owner": {"user_id": 1150329, "accept_rate": 89, "link": "http://stackoverflow.com/users/1150329/tejas-patil", "user_type": "registered", "reputation": 4793}, "body": "<p>Use <a href=\"http://code.google.com/p/luke/\" rel=\"nofollow\">luke tool</a> to browse the nutch indexes. The dump index option can create an xml file for entire index. If you have to do it via code, then you need to learn lucene.</p>\n\n<p>To read the crawled content, use the <a href=\"http://wiki.apache.org/nutch/nutch-0.8-dev/bin/nutch_segread\" rel=\"nofollow\">nutch segment reader</a>.</p>\n", "creation_date": 1335150660, "is_accepted": true, "score": 0, "last_activity_date": 1335150660, "answer_id": 10274371}, {"question_id": 10274242, "owner": {"user_id": 857457, "accept_rate": 40, "link": "http://stackoverflow.com/users/857457/john-rene", "user_type": "registered", "reputation": 13}, "body": "<p>@freedom Hi, you can use the provided read command like \"bin/nutch readseg xxx\"to read the content.\nAlso,I am from NanJing and doing research with Nutch these days.Hope to communicate with you.You can see my email in my profile Page.\nHope <a href=\"http://arifn.web.id/blog/2010/11/29/simple-crawling-with-nutch.html\" rel=\"nofollow\">this</a> will help you</p>\n", "creation_date": 1335258479, "is_accepted": false, "score": 0, "last_activity_date": 1335258479, "answer_id": 10294921}], "question_id": 10274242, "tags": ["lucene", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/10274242/how-to-read-the-content-from-nutch-index", "last_activity_date": 1335258479, "accepted_answer_id": 10274371, "body": "<p>I'm new for Nutch. Now I can crawl and index the web pages using Nutch, but I don't know how to read the index and extract data from it. Could anyone introduce to me some useful tools to read the index? I want to add a Chinese Language Analyzer and a IndexFilter plugin, so I want to read the index to validate my plugin. And also,I want to do some process with the data I crawled using Java. Thanks.</p>\n", "creation_date": 1335149222, "score": 3},
{"title": "Is Solr necessary to index crawled data for Nutch?", "view_count": 510, "owner": {"age": 27, "answer_count": 19, "creation_date": 1331644819, "user_id": 1266556, "accept_rate": 75, "view_count": 41, "location": "Shanghai, China", "reputation": 362}, "is_answered": true, "answers": [{"last_edit_date": 1335202058, "owner": {"user_id": 1150329, "accept_rate": 89, "link": "http://stackoverflow.com/users/1150329/tejas-patil", "user_type": "registered", "reputation": 4793}, "body": "<p>Solr uses lucene internally. <strong>Since 2005, nutch was designated as a subproject of Lucene.</strong> Historically, nutch used lucene indexes and was a full fledged search engine (this was until ver 1.0) . It had crawling capability and even support to index data and UI via browser to query the indexed data (similar to that like a google search). </p>\n\n<p>As the initial design was based around lucene (it was another apache project which earned lot of kudos at that period and still rocks), the <strong>nutch code was NOT changed or made generic so that other indexing frameworks could have been used</strong>. If you want to, then you need lots of efforts to put your indexing framework with it.</p>\n\n<p><strong>In recent versions</strong>, (nutch ver 1.3 and further), the Nutch dev team realized that its difficult to track the work involved in indexing due to changing needs and expertise required. <strong>It was better to delegate the responsibility of indexing to Solr</strong> (its a lucene based indexing framework). The Nutch developers focus only on the crawling part. So now nutch is not a full fledged search engine but its a full fledged web crawler. </p>\n\n<p>Hope this answers your query. You can browse <a href=\"http://nutch.apache.org/#Apache+Nutch+News\" rel=\"nofollow\">nutch news</a> for more info. </p>\n\n<p><strong>Latest happenings:</strong></p>\n\n<p>Recently there are efforts going on to create a generic library for crawlers (under commons). This project is <a href=\"http://code.google.com/p/crawler-commons/\" rel=\"nofollow\">commons-crawler</a> which will have all functions required for a web crawler and can be used for creating crawlers. Further nutch versions will be using this library as a dependency.</p>\n", "question_id": 10281681, "creation_date": 1335193880, "is_accepted": true, "score": 3, "last_activity_date": 1335202058, "answer_id": 10283219}], "question_id": 10281681, "tags": ["solr", "lucene", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/10281681/is-solr-necessary-to-index-crawled-data-for-nutch", "last_activity_date": 1335202058, "accepted_answer_id": 10283219, "body": "<p><br>I have learnt Nutch for few days. People here help me a lot.Thank all of you. Here is the Question: \n<br>I found that Nutch1.4 only contains one Indexer--solrindex. Is Solr the only way for Nutch to index the crawled data? If not, what are the other ways?\n<br>I'm also wondering why Nutch1.4 uses Solr to index the data. Why not do it itself? Doesn't it increase the coupling of these two projects?\n<br>Thanks.</p>\n", "creation_date": 1335188321, "score": 0},
{"title": "Difference between two solr indexes", "view_count": 763, "is_answered": true, "answers": [{"question_id": 10185494, "owner": {"user_id": 1150329, "accept_rate": 89, "link": "http://stackoverflow.com/users/1150329/tejas-patil", "user_type": "registered", "reputation": 4793}, "body": "<p>Answer from <a href=\"http://lucene.472066.n3.nabble.com/Difference-between-two-solr-indexes-tt3916328.html\" rel=\"nofollow\">here</a>:</p>\n\n<p>If there are only 100'000 documents dump all document ids and make diff. If you're using linux based system you can just use simple tools to do it. Something like that can be helpful</p>\n\n<pre><code>curl \"&lt;a href=\"http://your.hostA:port/solr/index/select?*:*&amp;fl=id&amp;wt=csv\"&gt;http://your.hostA:port/solr/index/select?*:*&amp;fl=id&amp;wt=csv\" &gt; /tmp/idsA\ncurl \"&lt;a href=\"http://your.hostB:port/solr/index/select?*:*&amp;fl=id&amp;wt=csv\"&gt;http://your.hostB:port/solr/index/select?*:*&amp;fl=id&amp;wt=csv\" &gt; /tmp/idsB\ndiff /tmp/idsA /tmp/idsB | grep \"&lt;\\|&gt;\" | awk '{print $2;}' | sed\n's/\\(.*\\)/&lt;id&gt;\\1&lt;\\/id&gt;/g' &gt; /tmp/ids_to_delete.xml\n</code></pre>\n\n<p>Now you have file. Now you can just add to that file \"<code>&lt;delete&gt;</code>\" and\n\"<code>&lt;/detele&gt;</code>\" and upload that file into solr using curl</p>\n\n<pre><code>curl -X POST -d @/tmp/ids_to_delete.xml \"&lt;a href=\"http://your.hostA:port\"&gt;http://your.hostA:port\n/solr/index/upadte\" \n</code></pre>\n", "creation_date": 1335194705, "is_accepted": false, "score": 1, "last_activity_date": 1335194705, "answer_id": 10283472}], "question_id": 10185494, "tags": ["solr", "lucene", "search-engine", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/10185494/difference-between-two-solr-indexes", "last_activity_date": 1335194705, "owner": {"user_id": 1147008, "answer_count": 0, "creation_date": 1326432403, "accept_rate": 50, "view_count": 20, "reputation": 28}, "body": "<p>I have two solr indexes , Index A contains 100000 docs and B contains 110000 docs, A is subset of B, i have to perform operation where \nA XOR B = result \nand delete result.</p>\n", "creation_date": 1334639849, "score": -1},
{"title": "How to configure Nutch and solr in ubuntu 10.10?", "view_count": 1202, "is_answered": false, "answers": [{"last_edit_date": 1335035510, "owner": {"user_id": 1150329, "accept_rate": 89, "link": "http://stackoverflow.com/users/1150329/tejas-patil", "user_type": "registered", "reputation": 4793}, "body": "<p><a href=\"http://www.params.me/2011/07/apache-nutch-13-setup.html\" rel=\"nofollow\">This</a> is for nutch installation and <a href=\"http://www.params.me/2011/07/nutch-13-and-solr-integration.html\" rel=\"nofollow\">this</a> is for integration with Solr.</p>\n\n<p>Regarding the parsers, nutch has its own set of parsers and you dont have to bother about parsing. Trigger the crawl command, its done automatically. Unless you want to parse things apart from those provided by nutch, it wont be an issue for you. If you want nutch to parse some .xyz files, then you nedd to write parser plugins for that and integrate with nutch.</p>\n", "question_id": 10254816, "creation_date": 1334984099, "is_accepted": false, "score": 0, "last_activity_date": 1335035510, "answer_id": 10256337}], "question_id": 10254816, "tags": ["solr", "search-engine", "web-crawler", "nutch", "ubuntu-10.10"], "answer_count": 1, "link": "http://stackoverflow.com/questions/10254816/how-to-configure-nutch-and-solr-in-ubuntu-10-10", "last_activity_date": 1335183142, "owner": {"age": 26, "answer_count": 0, "creation_date": 1334056673, "user_id": 1323904, "view_count": 16, "location": "New Delhi", "reputation": 7}, "body": "<p>I'm trying to build a search engine for my final year project. I have done lots of research on this topic in the last 2 months.\nAnd I found that I will need a crawler to crawl the Internet, a parser, and an indexer.</p>\n\n<p>I am trying to use Nutch as crawler and solr to index data crawled by Nutch. But I am stuck in the installation part of both of them. I am trying to install Nutch and solr in my system with the help of tutorials on the internet, but nothing worked for me.</p>\n\n<p><strong>I need  some kind of installation guide or a link where I can learn how to install and integrate Nutch and solr</strong>.</p>\n\n<p>Next I am stuck with the parser. I have no idea about this phase. I need help here on how to do the parsing of data before indexing.</p>\n\n<p>I don't want to build Google or something. All I need is certain items from certain websites to be searched.</p>\n\n<p>I have Java experience and I can work with it comfortably but I am not a professional like you guys, and please do tell me whether I am going in the right direction or not, and what I should do next.</p>\n\n<p>I am using Ubuntu 10.10, and I have Apache Tomcat 7.</p>\n", "creation_date": 1334964514, "score": -1},
{"title": "Find all the web pages in a domain and its subdomains", "view_count": 1793, "owner": {"age": 35, "answer_count": 2, "creation_date": 1271862384, "user_id": 322419, "accept_rate": 60, "view_count": 113, "reputation": 706}, "is_answered": true, "answers": [{"last_edit_date": 1335136776, "owner": {"user_id": 807103, "accept_rate": 97, "link": "http://stackoverflow.com/users/807103/sunnyrjuneja", "user_type": "registered", "reputation": 4378}, "body": "<p>If you're familiar with ruby, consider using anemone. Wonderful crawling framework. Here is sample code that works out of the box.  </p>\n\n<pre><code>require 'anemone'\n\nurls = []\n\nAnemone.crawl(site_url)\n  anemone.on_every_page do |page|\n    urls &lt;&lt; page.url\n  end\nend\n</code></pre>\n\n<p><a href=\"https://github.com/chriskite/anemone\" rel=\"nofollow\">https://github.com/chriskite/anemone</a></p>\n\n<p>Disclaimer: You need to use a patch from the issues to crawl subdomains and you might want to consider adding a maximum page count.</p>\n", "question_id": 10272920, "creation_date": 1335136138, "is_accepted": true, "score": 3, "last_activity_date": 1335136776, "answer_id": 10272966}, {"question_id": 10272920, "owner": {"user_id": 377270, "link": "http://stackoverflow.com/users/377270/sarnold", "user_type": "registered", "reputation": 77262}, "body": "<p>The easiest way to find all subdomains of a given domain is to ask the DNS administrators of the site in question to provide you with a <a href=\"http://en.wikipedia.org/wiki/DNS_zone_transfer\" rel=\"nofollow\">DNS Zone Transfer</a> or their zone files; if there are any <a href=\"http://en.wikipedia.org/wiki/Wildcard_DNS_record\" rel=\"nofollow\">wildcard DNS entries</a> in the zone, you'll have to also get the configurations (and potentially <em>code</em>) of the servers that respond to requests on the wildcard DNS entries. Don't forget that portions of the domain name space might be handled by other DNS servers -- you'll have to get data from them all.</p>\n\n<p>This is especially complicated because HTTP servers might have different handling for requests to different names baked into their server configuration files, or the application code running the servers, or perhaps the application code running the servers will perform database lookups to determine what to do with the given name. FTP does not provide for name-based virtual hosting, and whatever other services you're interested in may or may not provide name-based virtual hosting protocols.</p>\n", "creation_date": 1335136375, "is_accepted": false, "score": 0, "last_activity_date": 1335136375, "answer_id": 10272990}], "question_id": 10272920, "tags": ["url", "solr", "web-crawler", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/10272920/find-all-the-web-pages-in-a-domain-and-its-subdomains", "last_activity_date": 1335136776, "accepted_answer_id": 10272966, "body": "<p>I am looking for a way to find all the web pages and sub domains in a domain. For example, in the uoregon.edu domain, I would like to find all the web pages in this domain and in all the sub domains (e.g., cs.uoregon.edu).</p>\n\n<p>I have been looking at nutch, and I think it can do the job. But, it seems that nutch downloads entire web pages and indexes them for later search. But, I want a crawler that only scans a web page for URLs that belong to the same domain. Furthermore, it seems that nutch saves the linkdb in a serialized format. How can I read it? I tried solr, and it can read nutch's collected data. But, I dont think I need solr, since I am not performing any searches. All I need are the URLs that belong to a given domain.</p>\n\n<p>Thanks</p>\n", "creation_date": 1335135700, "score": 2},
{"title": "How to resolve dependencies for custom Nutch plug-in?", "view_count": 349, "is_answered": false, "answers": [{"question_id": 10271754, "owner": {"user_id": 1150329, "accept_rate": 89, "link": "http://stackoverflow.com/users/1150329/tejas-patil", "user_type": "registered", "reputation": 4793}, "body": "<p>Old way is to :\ndeclare the dependencies inside the <code>NUTCH_INSTALLATION/src/plugin/&lt;your_plugin&gt;/plugin.xml</code> file. </p>\n\n<p>If you want to use ivy : declare the dependencies inside <code>&lt;dependencies&gt;</code> tag in the <code>NUTCH_INSTALLATION/src/plugin/&lt;your_plugin&gt;/ivy.xml</code> file. </p>\n", "creation_date": 1335126921, "is_accepted": false, "score": 0, "last_activity_date": 1335126921, "answer_id": 10271891}, {"question_id": 10271754, "owner": {"user_id": 1275577, "accept_rate": 100, "link": "http://stackoverflow.com/users/1275577/%c4%b0smet-alkan", "user_type": "registered", "reputation": 3787}, "body": "<p>Take a look on the related <a href=\"http://wiki.apache.org/nutch/HowToMakeCustomSearch\" rel=\"nofollow\">page of Nutch wiki</a>.</p>\n", "creation_date": 1335127357, "is_accepted": false, "score": 0, "last_activity_date": 1335127357, "answer_id": 10271940}], "question_id": 10271754, "tags": ["java", "eclipse", "plugins", "ivy", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/10271754/how-to-resolve-dependencies-for-custom-nutch-plug-in", "last_activity_date": 1335127357, "owner": {"user_id": 1177059, "view_count": 3, "answer_count": 0, "creation_date": 1327878859, "reputation": 1}, "body": "<p>I am struggling with Ivy and dependency resolution in Eclipse. I have created a custom plug-in for Nutch, which has dependencies on another project in my Eclipse workspace. Now when I build it it complains about the classes from that project. I presume the standard Eclipse mechanism in build classpath settings (where I've added that project as dependency project) doesn't work, so I have to define dependency via ivy.xml. How can I do that?</p>\n", "creation_date": 1335125843, "score": 0},
{"title": "Why does Nutch only run the fetch step on one Hadoop node, when the cluster has 5 nodes total?", "view_count": 1203, "owner": {"age": 30, "answer_count": 40, "creation_date": 1317180947, "user_id": 968269, "accept_rate": 75, "view_count": 126, "location": "San Francisco Bay Area", "reputation": 1497}, "is_answered": true, "answers": [{"question_id": 10264183, "owner": {"user_id": 1150329, "accept_rate": 89, "link": "http://stackoverflow.com/users/1150329/tejas-patil", "user_type": "registered", "reputation": 4793}, "body": "<p>By default nutch partitions urls based in their hosts. The corresponding property in <code>nutch-default.xml</code> is:</p>\n\n<pre><code>&lt;property&gt;\n  &lt;name&gt;partition.url.mode&lt;/name&gt;\n  &lt;value&gt;byHost&lt;/value&gt;\n  &lt;description&gt;Determines how to partition URLs. Default value is 'byHost', \n  also takes 'byDomain' or 'byIP'. \n  &lt;/description&gt;\n&lt;/property&gt;\n</code></pre>\n\n<p>Please verify the value on your setup.</p>\n\n<p>I think that your problem can be diagnosed by getting answers for these questions:</p>\n\n<ol>\n<li>How many <strong>mappers were created for the fetch job</strong> ? it might be possible that there were multiple mappers spawned and all of them got finished early except for one.</li>\n<li>What was the <strong>topN value</strong> used in generate command ? If this is low, then despite of having 30K pages, very less will be sent to the fetch phase.</li>\n<li>Had you used <strong>numFetchers option</strong> in the generate command ? This controls the number of maps created for the fetch job.</li>\n<li><strong>How many reduces were generated for the generate-partition job</strong> ? If this value is 1, then only a single map will be created in fetch phase. The output of generate partition is given to fetch phase. Number of part files created by generate (ie. reducers for generate) is equal to the number of maps created for the fetch job.</li>\n<li>Whats the setting for mapred.map.tasks on your hadoop ? whats the corresponding value for reduce ?</li>\n</ol>\n", "creation_date": 1335086870, "is_accepted": true, "score": 2, "last_activity_date": 1335086870, "answer_id": 10266784}], "question_id": 10264183, "tags": ["hadoop", "nutch", "elastic-map-reduce"], "answer_count": 1, "link": "http://stackoverflow.com/questions/10264183/why-does-nutch-only-run-the-fetch-step-on-one-hadoop-node-when-the-cluster-has", "last_activity_date": 1335122043, "accepted_answer_id": 10266784, "body": "<p>I'm running Nutch on a Elastic MapReduce, with 3 worker nodes. I'm using Nutch 1.4, with the default configuration it ships with (after adding a user agent).</p>\n\n<p>However, even though I'm crawling a list of 30,000 domains the fetching step is only run from one worker node, even though the parsing step runs on all three.</p>\n\n<p>How do I get it to run the fetch step from all three nodes?</p>\n\n<p>*EDIT* The problem was that I needed to set the mapred.map.tasks property to the size of my Hadoop cluster. You can find this documented <a href=\"http://wiki.apache.org/nutch/NutchHadoopTutorial\" rel=\"nofollow\">here</a></p>\n", "creation_date": 1335053956, "score": 3},
{"title": "Can not compile Nutch1.4 with ant", "view_count": 804, "owner": {"age": 27, "answer_count": 19, "creation_date": 1331644819, "user_id": 1266556, "accept_rate": 75, "view_count": 41, "location": "Shanghai, China", "reputation": 362}, "is_answered": true, "answers": [{"last_edit_date": 1335084120, "owner": {"user_id": 1150329, "accept_rate": 89, "link": "http://stackoverflow.com/users/1150329/tejas-patil", "user_type": "registered", "reputation": 4793}, "body": "<p>For compiling nutch 1.4, all you have to do is run <code>ant clean deploy</code> from the nutch directory. The output is created in the directory named '<code>runtime</code>' with 2 folders: one for local mode and other one for cluster mode.</p>\n\n<p>please check the date settings and <code>ant</code> installation on your machine. I think that is casing the issue. Also have you tampered/ edited <code>/home/xenserver/apache-nutch-1.4/ivy/ivy.xml</code> ? Please check that file too.</p>\n\n<p><strong>EDIT:</strong></p>\n\n<p>There is some problem with the build file when executed on your Linux box. \nCheck these out: <a href=\"http://osdir.com/ml/search.nutch.devel/2005-02/msg00082.html\" rel=\"nofollow\">this</a> and <a href=\"http://comments.gmane.org/gmane.comp.search.nutch.devel/3436\" rel=\"nofollow\">this</a>.</p>\n\n<p>These are the things that you should verify on your setup:</p>\n\n<ol>\n<li><code>java version</code> and <code>ant version</code> : dont use old ones. get the latest ones or ones that are compatible with your nutch release. FYI: for nutch-1.4 I am using apache-ant-1.8.3 and java jdk1.6.0_18. This combination works perfectly fine with me.</li>\n<li>Check that you have installed a <strong>JDK and not a JRE</strong></li>\n<li>Check if your <code>JAVA_HOME</code> environment variable point to the JDK. System <code>PATH</code> variable must have <code>$JAVA_HOME/bin</code> and <code>$ANT_HOME/bin</code> appended to it. <code>ANT_HOM</code>E variable must point to the ant installation directory.</li>\n<li>Can you successfully run normal ant targets on any other build files ? try out with small ant build file.</li>\n<li><p>Still facing the same issue, run ant command with -v option. This will provide more information about the error faced.\neg. </p>\n\n<p>ant -v clean deploy</p></li>\n</ol>\n", "question_id": 10256948, "creation_date": 1335035065, "is_accepted": true, "score": 1, "last_activity_date": 1335084120, "answer_id": 10262056}], "question_id": 10256948, "tags": ["ant", "hadoop", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/10256948/can-not-compile-nutch1-4-with-ant", "last_activity_date": 1335084120, "accepted_answer_id": 10262056, "body": "<p>all\nI'm trying to deploy Nutch1.4 to Hadoop cluster(following <a href=\"http://www.rui-yang.com/develop/build-nutch-1-4-cluster-with-hadoop/\" rel=\"nofollow\">this page</a>). I got some problems when compiling Nutch with ant.</p>\n\n<h3>problem 1</h3>\n\n<p>When I run ant command, I got the following error:</p>\n\n<blockquote>\n  <p>/home/xenserver/apache-nutch-1.4-bin/build.xml:71: invalid Date syntax in \"01/25/1971 2:00 pm\"</p>\n</blockquote>\n\n<p>I remove attribute \"datetime\" from line 71 in file build.xml and run ant again. Then I got another problem.</p>\n\n<h3>problem 2</h3>\n\n<p>The error is:</p>\n\n<blockquote>\n  <p>/home/xenserver/apache-nutch-1.4/build.xml:412: syntax errors in ivy file: java.text.ParseException: <a href=\"http://java.sun.com/xml/jaxp/properties/schemaLanguage\" rel=\"nofollow\">http://java.sun.com/xml/jaxp/properties/schemaLanguage</a> in file:/home/xenserver/apache-nutch-1.4/ivy/ivy.xml\n     at org.apache.ivy.plugins.parser.xml.XmlModuleDescriptorParser$Parser.parse(XmlModuleDescriptorParser.java:273)</p>\n  \n  <p>........</p>\n</blockquote>\n\n<p>What's wrong with the steps above? Is there any tutorial for compiling Nutch1.4?\nNeed your help.Thanks in advance.</p>\n", "creation_date": 1334991316, "score": 0},
{"title": "What the outputs exactly are when integrating Nutch1.4 and Solr?", "view_count": 264, "owner": {"age": 28, "answer_count": 39, "creation_date": 1308279444, "user_id": 802585, "accept_rate": 55, "view_count": 641, "location": "China", "reputation": 5351}, "is_answered": true, "answers": [{"question_id": 10225239, "owner": {"user_id": 1150329, "accept_rate": 89, "link": "http://stackoverflow.com/users/1150329/tejas-patil", "user_type": "registered", "reputation": 4793}, "body": "<h2><strong>For question #1: What are these five folders exactly contains ?</strong></h2>\n\n<p>Here are the details from the <a href=\"http://wiki.apache.org/nutch/NutchTutorial\" rel=\"nofollow\">nutch wiki page</a>:</p>\n\n<p>The <strong>crawl database, or crawldb</strong>. This contains information about every URL known to Nutch, including whether it was fetched, and, if so, when.</p>\n\n<p>The <strong>link database, or linkdb</strong>. This contains the list of known links to each URL, including both the source URL and anchor text of the link.</p>\n\n<p>A <strong>set of segments</strong>. Each segment is a set of URLs that are fetched as a unit. Segments are directories with the following subdirectories:</p>\n\n<ol>\n<li>a crawl_generate names a set of URLs to be fetched</li>\n<li>a crawl_fetch contains the status of fetching each URL</li>\n<li>a content contains the raw content retrieved from each URL</li>\n<li>a parse_text contains the parsed text of each URL</li>\n<li>a parse_data contains outlinks and metadata parsed from each URL</li>\n<li>a crawl_parse contains the outlink URLs, used to update the crawldb</li>\n</ol>\n\n<p>The <strong>index</strong> folder contains the indexes created from the crawled content and the linkdb.</p>\n\n<p><strong>spellchecker</strong> : This is spell check index generated for improving queries. <a href=\"http://lucene.472066.n3.nabble.com/Spellcheck-with-Solr-td3315906.html\" rel=\"nofollow\">This</a> and <a href=\"http://stackoverflow.com/questions/3115422/spell-checker-in-nutch-1-0\">this</a> are worth reading if you want more knowledge about it. Also see <a href=\"http://stackoverflow.com/questions/3115422/spell-checker-in-nutch-1-0\">this</a>.</p>\n\n<h2><strong>For question #2: Where does the \"PageRank(or LinkRank)\" works ?</strong></h2>\n\n<p>Read <a href=\"http://wiki.apache.org/nutch/FixingOpicScoring\" rel=\"nofollow\">this</a> and <a href=\"http://wiki.apache.org/nutch/NewScoring\" rel=\"nofollow\">this</a>. Not sure if <a href=\"http://lucene.472066.n3.nabble.com/calcualting-Page-Rank-using-Nutch-Crawler-td1976998.html\" rel=\"nofollow\">this</a> and <a href=\"http://lucene.472066.n3.nabble.com/PageRank-LinkRank-in-Nutch-Opic-vs-NewScoring-in-Nutch-1-1-td938210.html\" rel=\"nofollow\">this</a> will be helpful but will add to your knowledge.</p>\n\n<h2><strong>For question #3: Does Nutch indexes the page and solr indexes them again ?</strong></h2>\n\n<p>Indexes for the crawled data are generated by Apache Solr not Nutch. </p>\n\n<p>This is <a href=\"http://www.atlantbh.com/apache-nutch-overview/\" rel=\"nofollow\">the internal working</a>:\nNutch delegates all data collected in parsing to the IndexingFilter extension which generates the data to be indexed. The output of the filter is a NutchDocument which again is delegated to Nutch. Nutch then decides if the data should be indexed based on the mapping file which defines which NutchDocument fields will be mapped to SolrDocument fields is read by Nutch.</p>\n", "creation_date": 1335036426, "is_accepted": true, "score": 2, "last_activity_date": 1335036426, "answer_id": 10262243}], "question_id": 10225239, "tags": ["solr", "lucene", "search-engine", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/10225239/what-the-outputs-exactly-are-when-integrating-nutch1-4-and-solr", "last_activity_date": 1335036426, "accepted_answer_id": 10262243, "body": "<p>When i integrating Nutch1.4 and solr, i notice there are two groups of outputs there.</p>\n\n<p><strong>I think the workflow may like this in my site:</strong></p>\n\n<p><strong>1\u3001</strong> Nutch-1.4 crawls the websites and generates three folders : \"crawler/crawldb\"\u3001\"crawler/linkdb\"\u3001\"crawler/segments\".</p>\n\n<p><strong>2\u3001</strong> Solr indexes the folder \"crawler/\" and generates its own folders \"data/index\"\u3001\"data/spellchecker\".</p>\n\n<p>Totally , there are five folders here.</p>\n\n<p><br/>\n<br/></p>\n\n<p><strong>What i want to know are:</strong></p>\n\n<p>1\u3001What are these five folders exactly contains ?</p>\n\n<p>2\u3001Where does the \"PageRank(or LinkRank)\" works ? </p>\n\n<p>3\u3001Does Nutch indexes the page and solr indexes them again ?</p>\n\n<p>Many Thanks.</p>\n", "creation_date": 1334827745, "score": 0},
{"title": "Nutch/Solr Indexing Sentences - Parser plugin or Indexing plugin?", "view_count": 344, "is_answered": false, "answers": [{"question_id": 10256598, "owner": {"user_id": 1150329, "accept_rate": 89, "link": "http://stackoverflow.com/users/1150329/tejas-patil", "user_type": "registered", "reputation": 4793}, "body": "<p>Both the places need some modifications.</p>\n\n<p>The nutch parsers chop off formatting from the crawled content. So the newlines are gone when the content is stored in the nutch segments. You need to modify that part.</p>\n\n<p>By default, nutch (its solr which does this) will consider words and not sentences for indexing. So you have to peek there too.</p>\n", "creation_date": 1335035373, "is_accepted": false, "score": 0, "last_activity_date": 1335035373, "answer_id": 10262103}], "question_id": 10256598, "tags": ["solr", "nutch", "opennlp"], "answer_count": 1, "link": "http://stackoverflow.com/questions/10256598/nutch-solr-indexing-sentences-parser-plugin-or-indexing-plugin", "last_activity_date": 1335035373, "owner": {"user_id": 1154870, "answer_count": 37, "creation_date": 1326834146, "accept_rate": 56, "view_count": 218, "reputation": 2386}, "body": "<p>Trying to index full sentences as their own field. If I do the sentence splitting in Nutch, using something like OpenNLP or LingPipe, where do I plug in the sentence detecting code? In the parsing stage or in the indexing stage? </p>\n", "creation_date": 1334987436, "score": 1},
{"title": "whats wrong in my nutch recrawl script", "view_count": 713, "is_answered": true, "answers": [{"question_id": 4827600, "owner": {"user_id": 1348655, "link": "http://stackoverflow.com/users/1348655/user1348655", "user_type": "registered", "reputation": 21}, "body": "<p>you are using the same directory for storing the indexes that are generated ie. crawl/indexes. Try to use different one or delete the old ones.</p>\n", "creation_date": 1335033753, "is_accepted": false, "score": 1, "last_activity_date": 1335033753, "answer_id": 10261873}], "question_id": 4827600, "tags": ["java", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/4827600/whats-wrong-in-my-nutch-recrawl-script", "last_activity_date": 1335033753, "owner": {"user_id": 593710, "answer_count": 0, "creation_date": 1296213398, "accept_rate": 0, "view_count": 3, "reputation": 31}, "body": "<p>hello i am using this script to recrawl my nutch but it gives an exception ..</p>\n\n<pre><code>Indexer: org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory file:/home/hat/crawl/indexes already exists\n    at org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:111)\n    at org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:772)\n    at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:730)\n    at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:1249)\n    at org.apache.nutch.indexer.Indexer.index(Indexer.java:76)\n    at org.apache.nutch.indexer.Indexer.run(Indexer.java:97)\n    at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n    at org.apache.nutch.indexer.Indexer.main(Indexer.java:106)\n</code></pre>\n\n<p>script </p>\n\n<pre><code>bin/nutch inject crawl/crawldb urls\nbin/nutch generate crawl/crawldb crawl/segments \ns1=`ls -d crawl/segments/2* | tail -1`\necho $s1\nbin/nutch fetch $s1 -threads 100 -depth 3 -topN 5\nbin/nutch updatedb crawl/crawldb $s1 \n\n\nbin/nutch invertlinks crawl/linkdb -dir crawl/segments \n\nbin/nutch index crawl/indexes crawl/crawldb crawl/linkdb crawl/segments/* \n</code></pre>\n\n<p>got this from <code>http://wiki.apache.org/nutch/NutchTutorial</code></p>\n\n<p>can any one tell me whats wrong....</p>\n", "creation_date": 1296213398, "score": 0},
{"title": "Nutch-Hadoop:- how can we crawl only the updates in the url going for recrawl?", "view_count": 377, "is_answered": true, "answers": [{"question_id": 10245642, "owner": {"user_id": 1150329, "accept_rate": 89, "link": "http://stackoverflow.com/users/1150329/tejas-patil", "user_type": "registered", "reputation": 4793}, "body": "<p>I think what you mean is that you want to re-crawl urls ONLY if the content is modified at the server end. You want nutch to identify it and thereby smartly decide to fetch the content or not. </p>\n\n<p>Nutch has this notion of maintaining \"Last modified\" time of a page and its been stored and NOT put into use while re-crawling pages. <a href=\"http://osdir.com/ml/search.nutch.devel/2004-11/msg00245.html\" rel=\"nofollow\">They knew</a> that it would save disk space and bandwidth but it didnt catch intrest due to other imp things. <a href=\"http://lucene.472066.n3.nabble.com/Last-Modified-metatag-in-nutch-td628964.html\" rel=\"nofollow\">People had raised</a> this issue but still i dont see any activity from nutch dev team. <a href=\"http://stackoverflow.com/questions/1252289/why-doesnt-nutch-seem-to-know-about-last-modified\">Efforts were taken</a> to improve, I still am not sure how precisely current version is using \"last modified\" field.</p>\n", "creation_date": 1334932922, "is_accepted": false, "score": 1, "last_activity_date": 1334932922, "answer_id": 10248388}, {"question_id": 10245642, "owner": {"user_id": 1348655, "link": "http://stackoverflow.com/users/1348655/user1348655", "user_type": "registered", "reputation": 21}, "body": "<p>you cannot tell nutch to get only the updated content of a page and forget the rest of unchanged data. It will  get the full content every time. You might set the recrawl frequency smartly so that pages will be recrawled after they get updated.</p>\n", "creation_date": 1335033511, "is_accepted": false, "score": 1, "last_activity_date": 1335033511, "answer_id": 10261846}], "question_id": 10245642, "tags": ["java", "hadoop", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/10245642/nutch-hadoop-how-can-we-crawl-only-the-updates-in-the-url-going-for-recrawl", "last_activity_date": 1335033511, "owner": {"user_id": 1343792, "view_count": 10, "answer_count": 0, "creation_date": 1334833738, "reputation": 6}, "body": "<p>please anybody let me know how can i identify updates in the url going for re-crawl? i want to crawl only the updated content of the page when the page is going for re-crawl not the older content which has already crawled.\nthanks in advance.\npragya..</p>\n", "creation_date": 1334922506, "score": 0},
{"title": "How to configure nutch in eclipse?", "view_count": 669, "is_answered": true, "answers": [{"question_id": 10229542, "owner": {"user_id": 1275577, "accept_rate": 100, "link": "http://stackoverflow.com/users/1275577/%c4%b0smet-alkan", "user_type": "registered", "reputation": 3787}, "body": "<p>I followed the instructions in that link and succeeded. The problem could be <strong>Eclipse</strong> version, try with <a href=\"http://www.eclipse.org/downloads/packages/eclipse-ide-java-ee-developers/indigosr2\" rel=\"nofollow\"><strong>Eclipse for Java EE Developers</strong></a>.</p>\n\n<p>Add these jars to your <strong>trunk project</strong> as external jar files : <strong>cyberneko.jar</strong>, <strong>rome-0.9.jar</strong> and <strong>tagsoup-1.2.jar</strong>. You can find all these jars by a simple <strong>google</strong> search.</p>\n", "creation_date": 1334933286, "is_accepted": false, "score": 1, "last_activity_date": 1334933286, "answer_id": 10248480}], "question_id": 10229542, "tags": ["linux", "eclipse", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/10229542/how-to-configure-nutch-in-eclipse", "last_activity_date": 1334933286, "owner": {"age": 36, "answer_count": 0, "creation_date": 1334842743, "user_id": 1344145, "view_count": 6, "location": "Bulgaria", "reputation": 1}, "body": "<p>I am trying to configure the environment so that I can test and run the latest trunk of nutch but nothing happens. My OS is Ubuntu 12.04, Java IDE Eclipse 3.7 Indigo and all the plugins that are required to run nutch. I will be grateful to give me links with detailed tutorials how to do it. I tried with this link: <a href=\"http://wiki.apache.org/nutch/RunNutchInEclipse\" rel=\"nofollow\">http://wiki.apache.org/nutch/RunNutchInEclipse</a> but I could not do it. Hope there are other sources where I can find proper instructions how to run nutch under Eclipse. </p>\n", "creation_date": 1334843444, "score": 0},
{"title": "Nutch Seed URLs", "view_count": 529, "owner": {"age": 26, "answer_count": 180, "creation_date": 1331977994, "user_id": 1275577, "accept_rate": 100, "view_count": 715, "location": "\u0130zmir", "reputation": 3787}, "is_answered": true, "answers": [{"last_edit_date": 1334930867, "owner": {"user_id": 1150329, "accept_rate": 89, "link": "http://stackoverflow.com/users/1150329/tejas-patil", "user_type": "registered", "reputation": 4793}, "body": "<p>No. This cannot be done directly with the default nutch codebase. You need to modify <a href=\"http://www.java-frameworks.com/java/nutch/org/apache/nutch/crawl/Injector.java.html\" rel=\"nofollow\">Injector.java</a> to achieve that.</p>\n\n<p><strong>EDIT:</strong></p>\n\n<p>Try using <a href=\"http://hadoop.apache.org/common/docs/r0.20.0/api/org/apache/hadoop/mapred/lib/db/DBInputFormat.html\" rel=\"nofollow\">DBInputFormat</a> : an <a href=\"http://hadoop.apache.org/common/docs/r0.20.0/api/org/apache/hadoop/mapred/InputFormat.html\" rel=\"nofollow\">InputFormat</a> that reads input data from an SQL table. You need to modify the <a href=\"http://www.java-frameworks.com/java/nutch/org/apache/nutch/crawl/Injector.java.html\" rel=\"nofollow\">Inject</a> code here (line 3 in snippet below):</p>\n\n<pre><code>JobConf sortJob = new NutchJob(getConf());\nsortJob.setJobName(\"inject \" + urlDir);\nFileInputFormat.addInputPath(sortJob, urlDir);\nsortJob.setMapperClass(InjectMapper.class);\n</code></pre>\n", "question_id": 10142953, "creation_date": 1334540641, "is_accepted": true, "score": 1, "last_activity_date": 1334930867, "answer_id": 10167833}], "question_id": 10142953, "tags": ["java", "nutch", "web-crawler"], "answer_count": 1, "link": "http://stackoverflow.com/questions/10142953/nutch-seed-urls", "last_activity_date": 1334930867, "accepted_answer_id": 10167833, "body": "<p>Is it possible to get URLs into <strong>Nutch</strong> directly from a database or a service etc. I'm not interested in the ways which data is taken from the database or service and written to seed.txt.</p>\n", "creation_date": 1334328116, "score": 0},
{"title": "Does Nutch automatically crawl my site when new pages are added?", "view_count": 473, "is_answered": true, "answers": [{"question_id": 1794886, "owner": {"user_id": 184223, "accept_rate": 86, "link": "http://stackoverflow.com/users/184223/svenkubiak", "user_type": "registered", "reputation": 318}, "body": "<p>No, you have to recrawl or create the index from scratch.</p>\n", "creation_date": 1273573837, "is_accepted": false, "score": 1, "last_activity_date": 1273573837, "answer_id": 2809866}, {"question_id": 1794886, "owner": {"user_id": 1150329, "accept_rate": 89, "link": "http://stackoverflow.com/users/1150329/tejas-patil", "user_type": "registered", "reputation": 4793}, "body": "<p>Tt wont re-crawl automatically. You can do either of these:</p>\n\n<ol>\n<li>make the parent page of the new url re-crawled so that the new url enters the crawldb and will be fetched in subsequent fetch round.</li>\n<li>Add the new url directly to the crawldb via inject command.</li>\n</ol>\n", "creation_date": 1333857141, "is_accepted": false, "score": 1, "last_activity_date": 1333857141, "answer_id": 10060254}, {"question_id": 1794886, "owner": {"user_id": 1275577, "accept_rate": 100, "link": "http://stackoverflow.com/users/1275577/%c4%b0smet-alkan", "user_type": "registered", "reputation": 3787}, "body": "<p>You should do scheduled crawling to keep your data up-to-date.</p>\n\n<p><a href=\"http://java-source.net/open-source/job-schedulers\" rel=\"nofollow\">Open source Java Job Schedulers</a></p>\n", "creation_date": 1334919279, "is_accepted": false, "score": 0, "last_activity_date": 1334919279, "answer_id": 10244853}], "question_id": 1794886, "tags": ["nutch"], "answer_count": 3, "link": "http://stackoverflow.com/questions/1794886/does-nutch-automatically-crawl-my-site-when-new-pages-are-added", "last_activity_date": 1334919279, "owner": {"age": 30, "answer_count": 3, "creation_date": 1258953861, "user_id": 216772, "accept_rate": 4, "view_count": 249, "location": "Bangalore, India", "reputation": 158}, "body": "<p>Does <a href=\"http://lucene.apache.org/nutch/\" rel=\"nofollow\">Nutch</a> crawl automatically when I add new pages to the website?</p>\n", "creation_date": 1259127520, "score": 0},
{"title": "Which Open Source Crawler is best?", "view_count": 2809, "is_answered": true, "answers": [{"last_edit_date": 1333505629, "owner": {"user_id": 1091789, "link": "http://stackoverflow.com/users/1091789/user1091789", "user_type": "registered", "reputation": 31}, "body": "<p>I haven't researched the crawlers you mention but I know that the one I wrote is extensible and can be modified. It can also be used with AJAX and \"javascript-only\" websites (i.e. sites using the Google Web Toolkit).</p>\n\n<p>The name is forklabs-javaxpcom and can be found at <a href=\"http://code.google.com/p/forklabs-javaxpcom/\" rel=\"nofollow\">http://code.google.com/p/forklabs-javaxpcom/</a>.</p>\n", "question_id": 8416065, "creation_date": 1325480941, "is_accepted": false, "score": 1, "last_activity_date": 1333505629, "answer_id": 8697589}, {"last_edit_date": 1334911577, "owner": {"user_id": 1275577, "accept_rate": 100, "link": "http://stackoverflow.com/users/1275577/%c4%b0smet-alkan", "user_type": "registered", "reputation": 3787}, "body": "<p><strong>Nutch</strong> is the most all around of them, extremely configurable. Tried with 100m documents. Trustworthy.</p>\n\n<p><strong>Heritrix</strong> works fine too, but not better than <strong>Nutch</strong>.</p>\n\n<p>You can give <strong>Crawler4j</strong> a try if you need to crawl fast. </p>\n\n<p>To do an introductory crawl and use and configure the crawler easily with a simple user interface, you can try <strong>websphinx</strong>.</p>\n\n<p><strong>Tika</strong> isn't a crawler : <a href=\"http://tika.apache.org/\" rel=\"nofollow\">it's a toolkit detects and extracts metadata and structured text content</a></p>\n\n<p>I had a job that required crawling, but <strong>OpenPipeLine</strong> wasn't in the list of favourite crawlers. It has an UI, job scheduler; it's used for enterprise solutions. As you just want to crawl some websites, you won't need such things.</p>\n", "question_id": 8416065, "creation_date": 1334910602, "is_accepted": false, "score": 3, "last_activity_date": 1334911577, "answer_id": 10242764}], "question_id": 8416065, "tags": ["web-crawler", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/8416065/which-open-source-crawler-is-best", "last_activity_date": 1334911577, "owner": {"user_id": 277581, "answer_count": 22, "creation_date": 1266658355, "accept_rate": 66, "view_count": 174, "location": "Lahore, Pakistan", "reputation": 1663}, "body": "<p>I am comparing these four Nutch / Heritrix / OpenPipeLine / Apache Tika\nWhich one is best? What are merits and demerits of each?\nI would like to have some extendible crawler that can crawl a list of websites and can be modified if needed. </p>\n", "creation_date": 1323264490, "score": 5},
{"title": "what is topN in nutch 1.3?", "view_count": 487, "is_answered": true, "answers": [{"last_edit_date": 1334392257, "owner": {"user_id": 764618, "accept_rate": 42, "link": "http://stackoverflow.com/users/764618/haya-aziz", "user_type": "registered", "reputation": 93}, "body": "<p>From another view <strong>topN</strong> means how many documents to get from each level of the <strong>depth</strong>.\n<strong>depth</strong> means how many generate/fetch/update cycles to carry out to get full page coverage.</p>\n", "question_id": 8222904, "creation_date": 1326702679, "is_accepted": false, "score": 2, "last_activity_date": 1334392257, "answer_id": 8877342}], "question_id": 8222904, "tags": ["nutch", "web-crawler", "top-n"], "answer_count": 1, "link": "http://stackoverflow.com/questions/8222904/what-is-topn-in-nutch-1-3", "last_activity_date": 1334392257, "owner": {"user_id": 1045194, "answer_count": 0, "creation_date": 1321258730, "accept_rate": 0, "view_count": 19, "reputation": 14}, "body": "<p>I read in sites that nutch fetch links from every link equls to topN.I have 4 links but when nutch crawl my links, all result that returned with nutch are equals topN. it means if i have 4 links and topN = 10, nutch fetch 10 links from all links not every link.end of crawl i have 10 links. help me.</p>\n", "creation_date": 1321943710, "score": 1},
{"title": "Nutch Raw Html Saving", "view_count": 1916, "owner": {"age": 26, "answer_count": 180, "creation_date": 1331977994, "user_id": 1275577, "accept_rate": 100, "view_count": 715, "location": "\u0130zmir", "reputation": 3787}, "is_answered": true, "answers": [{"question_id": 10142592, "owner": {"user_id": 1150329, "accept_rate": 89, "link": "http://stackoverflow.com/users/1150329/tejas-patil", "user_type": "registered", "reputation": 4793}, "body": "<p>The is no direct way to do that. You will have to do few code modifications.\nSee <a href=\"http://stackoverflow.com/questions/10007178/how-do-i-save-the-origin-html-file-with-apache-nutch/10060160#10060160\">this</a> and <a href=\"http://stackoverflow.com/questions/5123757/how-to-get-the-html-content-from-nutch\">this</a>.</p>\n", "creation_date": 1334369215, "is_accepted": true, "score": 2, "last_activity_date": 1334369215, "answer_id": 10150402}], "question_id": 10142592, "tags": ["nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/10142592/nutch-raw-html-saving", "last_activity_date": 1334369215, "accepted_answer_id": 10150402, "body": "<p>I'm trying to get raw html of crawled pages in different files, named as url of the page. Is it possible with <strong>Nutch</strong> to save the raw html pages in different files by ruling out the indexing part?</p>\n", "creation_date": 1334326851, "score": 2},
{"title": "Using Nutch to fetch particular HTML Tags", "view_count": 440, "is_answered": false, "answers": [{"question_id": 8652701, "owner": {"user_id": 1275577, "accept_rate": 100, "link": "http://stackoverflow.com/users/1275577/%c4%b0smet-alkan", "user_type": "registered", "reputation": 3787}, "body": "<p>Working on the same problem. However, it seems like there are some tiring steps to achieve the goal:</p>\n\n<p>1-crawl 2-index 3-get the raw html by -readseg 4-parse the file for information you want</p>\n\n<p>So, <strong>Nutch</strong> doesn't seem to be the good way to do this. Have you managed to find out an answer?</p>\n", "creation_date": 1334326656, "is_accepted": false, "score": 0, "last_activity_date": 1334326656, "answer_id": 10142534}], "question_id": 8652701, "tags": ["html", "parsing", "html-parsing", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/8652701/using-nutch-to-fetch-particular-html-tags", "last_activity_date": 1334326656, "owner": {"user_id": 669002, "answer_count": 1, "creation_date": 1292225568, "accept_rate": 36, "view_count": 59, "reputation": 359}, "body": "<p>I will be little verbose to clearly specify the problem, so please be patient :)</p>\n\n<p>Assume I have the following base URL: <a href=\"http://www.amazon.com/gp/goldbox/all-deals?ie=UTF8&amp;type=bd\" rel=\"nofollow\">http://www.amazon.com/gp/goldbox/all-deals?ie=UTF8&amp;type=bd</a><br>\nwhich lists a number of products with a concise description. Each listed product has a URL directing to a details product information page like: <a href=\"http://rads.stackoverflow.com/amzn/click/B000WU7RGS\" rel=\"nofollow\">http://www.amazon.com/dp/B000WU7RGS/ref=xs_gb_all-deals_center_rw_uk_A34K0C99MV3O0U?pf_rd_p=1261804642&amp;pf_rd_s=center-2&amp;pf_rd_t=701&amp;pf_rd_i=30&amp;pf_rd_m=ATVPDKIKX0DER&amp;pf_rd_r=1FE5R5X5XYMG6GDPHPK5</a></p>\n\n<p>Now my requirements is to (for example) fetch name, price and product information for each of these products. How can I achieve this using Nutch? Is Nutch required/good option for this or just a plain simple wget + own HTML parser is a better way to go?</p>\n\n<p>Note: I have to do this for multiple pages with very different layout and only input will be the URL and what html tags to fetch from the URL</p>\n", "creation_date": 1325054236, "score": 0},
{"title": "Extracting Fetched Web Pages from Nutch in a Map Reduce Friendly Format", "view_count": 1765, "owner": {"age": 31, "answer_count": 6, "creation_date": 1268719062, "user_id": 294508, "accept_rate": 100, "view_count": 57, "location": "Palo Alto, CA", "reputation": 345}, "is_answered": true, "answers": [{"question_id": 10098169, "owner": {"user_id": 1150329, "accept_rate": 89, "link": "http://stackoverflow.com/users/1150329/tejas-patil", "user_type": "registered", "reputation": 4793}, "body": "<p>The <code>bin/nutch readseg</code> command produces output in a human readable format and not map reduce format. The data is stored in segments in map-reduce format. I dont think that you can directly pull out that info from segements in map-reduce format.</p>\n\n<p>Few options for your concern:</p>\n\n<ol>\n<li>Segments are themselves map-reduce format files. Can you re-use those ? </li>\n<li>The output of <code>readseg</code> command can be converted to map-reduce form by writing a small map-reduce code. </li>\n</ol>\n", "creation_date": 1334209514, "is_accepted": false, "score": 0, "last_activity_date": 1334209514, "answer_id": 10118033}, {"question_id": 10098169, "owner": {"user_id": 294508, "accept_rate": 100, "link": "http://stackoverflow.com/users/294508/sid", "user_type": "registered", "reputation": 345}, "body": "<p>The answer lies in tweaking the source code of nutch. This turned out to be quite simple. Navigate to the <code>SegmentReader.java</code> file at <code>apache-nutch-1.4-bin/src/java/org/apache/nutch/segment</code></p>\n\n<p>Inside the <code>SegmentReader</code> class is a method <code>reduce</code> which is responsible for generating the human readable output the <code>bin/nutch readseg</code> command generates. Alter the <code>StringBuffer dump</code> variable as you see fit - this holds the entire output for a given url which is represented by the <code>key</code> variable.</p>\n\n<p>Make sure you to run <code>ant</code> to create a new binary and further calls to <code>bin/nutch readseg</code> shall generate the output in your custom format.</p>\n\n<p>These references were extremely useful in navigating the code: <br>\n[1] <a href=\"http://nutch.apache.org/apidocs-1.4/overview-summary.html\" rel=\"nofollow\">http://nutch.apache.org/apidocs-1.4/overview-summary.html</a> <br>\n[2] <a href=\"http://nutch.apache.org/apidocs-1.3/index-all.html\" rel=\"nofollow\">http://nutch.apache.org/apidocs-1.3/index-all.html</a></p>\n", "creation_date": 1334270479, "is_accepted": true, "score": 0, "last_activity_date": 1334270479, "answer_id": 10132902}], "question_id": 10098169, "tags": ["web-crawler", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/10098169/extracting-fetched-web-pages-from-nutch-in-a-map-reduce-friendly-format", "last_activity_date": 1334270479, "accepted_answer_id": 10132902, "body": "<p>After a Nutch crawl in distributed (deploy) mode as follows:    </p>\n\n<pre><code>bin/nutch crawl s3n://..... -depth 10 -topN 50000 -dir /crawl -threads 20 \n</code></pre>\n\n<p>I need to extract each URL fetched along with it's content in a map reduce friendly format. By using the readseg command below, the contents are fetched but the output format doesn't lend itself to being map reduced.</p>\n\n<pre><code>bin/nutch readseg -dump /crawl/segments/*  /output  -nogenerate -noparse -noparsedata -noparsetext\n</code></pre>\n\n<p>Ideally the output should be in this format:</p>\n\n<pre><code>http://abc.com/1     content of http://abc.com/1\nhttp://abc.com/2     content of http://abc.com/2\n</code></pre>\n\n<p>Any suggestions on how to achieve this?</p>\n", "creation_date": 1334102649, "score": 2},
{"title": "crawl websites out of java web application without using bin/nutch", "view_count": 1251, "is_answered": true, "answers": [{"question_id": 2847795, "owner": {"user_id": 183100, "link": "http://stackoverflow.com/users/183100/pascal-dimassimo", "user_type": "registered", "reputation": 5766}, "body": "<p>You probably have to add the nutch config files to your classpath. Normally, it is set via the <strong>NUTCH_CONF_DIR</strong> environment variable when calling the script bin/nutch. </p>\n\n<p>There is also the <strong>-Dhadoop.log.dir</strong> that might need to be set.</p>\n\n<p>Take the time to check the bin/nutch script to know more about those.</p>\n", "creation_date": 1274102341, "is_accepted": false, "score": 1, "last_activity_date": 1274102341, "answer_id": 2849316}, {"question_id": 2847795, "owner": {"user_id": 365719, "accept_rate": 70, "link": "http://stackoverflow.com/users/365719/mike-baranczak", "user_type": "registered", "reputation": 5678}, "body": "<p>You could fork a new process and run the bin/nutch script.</p>\n", "creation_date": 1276441752, "is_accepted": false, "score": 0, "last_activity_date": 1276441752, "answer_id": 3032690}, {"question_id": 2847795, "owner": {"user_id": 1150329, "accept_rate": 89, "link": "http://stackoverflow.com/users/1150329/tejas-patil", "user_type": "registered", "reputation": 4793}, "body": "<p>The inject job failed due to some reason which can't be diagnosed using the sysouts provided. More logs will be needed.</p>\n", "creation_date": 1333856594, "is_accepted": false, "score": 0, "last_activity_date": 1333856594, "answer_id": 10060217}], "question_id": 2847795, "tags": ["web-applications", "nutch", "web-crawler"], "answer_count": 3, "link": "http://stackoverflow.com/questions/2847795/crawl-websites-out-of-java-web-application-without-using-bin-nutch", "last_activity_date": 1333856594, "owner": {"user_id": 342885, "view_count": 2, "answer_count": 0, "creation_date": 1274087542, "reputation": 1}, "body": "<p>i am trying to using nutch (1.1) without bin/nutch from my (java) mojarra 2.0.2 webapp... i am searching at google for examples, but there are no examples how i can realize this :/ ... i get an exception and the job fails :/ (i think of cause something with hadoop)... here is my code:</p>\n\n<pre>\n  public void run() throws Exception {\n      final String[] args = new String[] {\n            String.format(\"%s%s%s%s\", JSFUtils.getWebAppRoot(), \"nutch\", File.separator, DIRECTORY_URLS),\n            \"-dir\", String.format(\"%s%s%s%s\", JSFUtils.getWebAppRoot(), \"nutch\", File.separator, DIRECTORY_CRAWL),\n            \"-threads\", this.preferences.get(\"threads\"),\n            \"-depth\", this.preferences.get(\"depth\"),\n            \"-topN\", this.preferences.get(\"topN\"),\n            \"-solr\", this.preferences.get(\"solr\")\n        };\n      Crawl.main(args);\n  }\n</pre>\n\n<p>and a part of the logging:</p>\n\n<pre>\n10/05/17 10:42:54 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n10/05/17 10:42:54 WARN mapred.JobClient: Use GenericOptionsParser for parsing the arguments. Applications should implement Tool for the same.\n10/05/17 10:42:54 INFO mapred.FileInputFormat: Total input paths to process : 1\n10/05/17 10:42:54 INFO mapred.JobClient: Running job: job_local_0001\n10/05/17 10:42:54 INFO mapred.FileInputFormat: Total input paths to process : 1\n10/05/17 10:42:55 INFO mapred.MapTask: numReduceTasks: 1\n10/05/17 10:42:55 INFO mapred.MapTask: io.sort.mb = 100\njava.io.IOException: Job failed!\n        at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:1232)\n        at org.apache.nutch.crawl.Injector.inject(Injector.java:211)\n        at org.apache.nutch.crawl.Crawl.main(Crawl.java:124)\n        at lan.localhost.process.NutchCrawling.run(NutchCrawling.java:108)\n        at lan.localhost.main.Index.indexing(Index.java:71)\n        at lan.localhost.bean.FeedingBean.actionStart(FeedingBean.java:25)\n        ....\n</pre>\n\n<p>can someone help me or tell me how i can crawling from a java application? i have increased the Xms to 256m and Xmx to 768m, but nothing changed...</p>\n\n<p>best regards marcel</p>\n", "creation_date": 1274087542, "score": 0},
{"title": "Is there any way to continue the Nutch Crawl task which has been killed unexpected?", "view_count": 252, "owner": {"age": 28, "answer_count": 39, "creation_date": 1308279444, "user_id": 802585, "accept_rate": 55, "view_count": 641, "location": "China", "reputation": 5351}, "is_answered": true, "answers": [{"question_id": 10055662, "owner": {"user_id": 1150329, "accept_rate": 89, "link": "http://stackoverflow.com/users/1150329/tejas-patil", "user_type": "registered", "reputation": 4793}, "body": "<p>After you started crawling, there might be some segments created in the output directory. Use <a href=\"http://wiki.apache.org/nutch/bin/nutch_crawl\" rel=\"nofollow\">bin/nutch</a> command and point <code>-dir</code> option to the output directory of previous run. For <code>urlDir</code> argument, create a dummy one with a single url (just for getting away from error if the urldir doesnt have any url in it.)</p>\n", "creation_date": 1333855483, "is_accepted": true, "score": 0, "last_activity_date": 1333855483, "answer_id": 10060142}], "question_id": 10055662, "tags": ["java", "lucene", "web-crawler", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/10055662/is-there-any-way-to-continue-the-nutch-crawl-task-which-has-been-killed-unexpect", "last_activity_date": 1333855483, "accepted_answer_id": 10060142, "body": "<p>I have a Nutch crawl task which has been running a whole day long until i killed the process by a mistake.</p>\n\n<p>I don't want to re-crawl the seeds (cost to much time), so i wonder whether there is a way or some Nutch Crawler parameters there, can make the crawler ignore those urls which has already been crawled.</p>\n\n<p>Many thanks !</p>\n", "creation_date": 1333811979, "score": 0},
{"title": "Is it possible to integrate Nutch Crawler with my existing Lucene project?", "view_count": 729, "owner": {"age": 28, "answer_count": 39, "creation_date": 1308279444, "user_id": 802585, "accept_rate": 55, "view_count": 641, "location": "China", "reputation": 5351}, "is_answered": true, "answers": [{"question_id": 10040510, "owner": {"user_id": 694426, "accept_rate": 80, "link": "http://stackoverflow.com/users/694426/csupnig", "user_type": "registered", "reputation": 2316}, "body": "<p>Yes, it is possible to search the index produced by nutch with your own lucene implementation. I wrote a short description in the wiki of our project, where we use nutch to crawl static content.</p>\n\n<p>You can have a look at it here: <a href=\"http://code.google.com/p/gtxcontentconnector/wiki/HowTo_Nutch\" rel=\"nofollow\">http://code.google.com/p/gtxcontentconnector/wiki/HowTo_Nutch</a></p>\n\n<p>br, \nChris</p>\n", "creation_date": 1333698902, "is_accepted": true, "score": 0, "last_activity_date": 1333698902, "answer_id": 10040726}], "question_id": 10040510, "tags": ["java", "lucene", "web-crawler", "nutch", "heritrix"], "answer_count": 1, "link": "http://stackoverflow.com/questions/10040510/is-it-possible-to-integrate-nutch-crawler-with-my-existing-lucene-project", "last_activity_date": 1333699318, "accepted_answer_id": 10040726, "body": "<p>I have a project using Lucene3.5 already.</p>\n\n<p>Now i need to provide web search function but i don't want to import the whole Nutch project.</p>\n\n<p>So i wonder , may be i can only use the crawler part of Nutch to crawl websites and index them into Lucene style.</p>\n\n<p>Then search the index files with my existing Lucene searcher.</p>\n\n<p>Is it possible to do this or do you have any suggestion (how about Heritrix)?</p>\n", "creation_date": 1333697437, "score": 0},
{"title": "limit the number of records produced by all reducers collectively", "view_count": 544, "owner": {"user_id": 1150329, "answer_count": 207, "creation_date": 1326629590, "accept_rate": 89, "view_count": 969, "location": "California", "reputation": 4793}, "is_answered": true, "answers": [{"question_id": 10022050, "owner": {"user_id": 709510, "link": "http://stackoverflow.com/users/709510/jdhok", "user_type": "registered", "reputation": 21}, "body": "<p>Hadoop has built in support for global <a href=\"http://hadoop.apache.org/common/docs/r0.20.2/mapred_tutorial.html#Counters\" rel=\"nofollow\">counters</a>. You can define your own counters, and increment/read them from your mapper or reducer code.</p>\n", "creation_date": 1333617272, "is_accepted": false, "score": 1, "last_activity_date": 1333617272, "answer_id": 10025837}, {"question_id": 10022050, "owner": {"user_id": 281362, "link": "http://stackoverflow.com/users/281362/root1982", "user_type": "registered", "reputation": 320}, "body": "<p>If you don't have to partition by hostname, you can solve your problem by a random partitioner. </p>\n\n<p>If you have to partition by hostname, I don't think there is any easy answers. Each reducer doesn't know how much of records are coming. Each reducer has to accumulate 100000 records or as much as it receives. You need to override the cleanup function in your reducer. Reducers need to talk to each other(through counter, maybe) in the \"cleanup\" function and decide how many of records are needed and only write out the records in the cleanup function. </p>\n\n<p>what do you think?</p>\n", "creation_date": 1333640029, "is_accepted": true, "score": 1, "last_activity_date": 1333640029, "answer_id": 10031598}], "question_id": 10022050, "tags": ["hadoop", "mapreduce", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/10022050/limit-the-number-of-records-produced-by-all-reducers-collectively", "last_activity_date": 1333640029, "accepted_answer_id": 10031598, "body": "<p>Here is the use case:</p>\n\n<p>input urls are read by maps and later emitted post some filtering. Then partitioners partition them based on their hostname. </p>\n\n<p>I have a global limit over the output urls after running the map-reduce job. I distribute this evenly across all reducers.\nie. if global limit is 1000 and number of reducers is 5, then every reducer will at most emit (1000/5 = 200) urls as output</p>\n\n<p>The problem is that if there are urls from only 2 hosts (due to user input) and there are 100000 urls of each of these 2 hosts,\nthe 2 reducers processing these urls (same host, same partition) will limit only 200 urls each to output. \nRest reducers dont get any data for processing due to partitioning and emit 0 records. </p>\n\n<p>So even though I had 100000 urls/host and global limit of 1000, output has 400 urls only (200 urls/host).</p>\n", "creation_date": 1333594311, "score": 1},
{"title": "Returning web page abstract with Solr", "view_count": 246, "is_answered": true, "answers": [{"question_id": 10007095, "owner": {"user_id": 907642, "link": "http://stackoverflow.com/users/907642/okke-klein", "user_type": "registered", "reputation": 2221}, "body": "<ol>\n<li><p>I haven't used Nutch in a long time, but I think it's pretty safe to assume that \"content\" is the field you want to highlight.</p></li>\n<li><p>You need to store the field to be able to use highlighting and if you want to use <a href=\"http://wiki.apache.org/solr/HighlightingParameters\" rel=\"nofollow\">FastVectorHighlighting</a> you need to enable the following attributes for that field: termVectors, termPositions and termOffsets.</p></li>\n</ol>\n\n<p>If you use FVH, you can also use boundaryScanner in Solr 3.5 and up.</p>\n", "creation_date": 1333529663, "is_accepted": false, "score": 1, "last_activity_date": 1333529663, "answer_id": 10007887}], "question_id": 10007095, "tags": ["solr", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/10007095/returning-web-page-abstract-with-solr", "last_activity_date": 1333529663, "owner": {"user_id": 1154870, "answer_count": 37, "creation_date": 1326834146, "accept_rate": 56, "view_count": 218, "reputation": 2386}, "body": "<p>I've crawled a site with Nutch successfully and am trying to return a highlighted abstract using Solr as the indexer/searcher. So, if I query \"ocean\" then I want to return a 20-30 word abstract from just the text of the web page (not the title or url) containing that query term. </p>\n\n<p>I've copied the Nutch schema.xml as my Solr schema.xml. </p>\n\n<p>So I have two questions:\n1. Is the \"content\" field in the Nutch schema.xml the field for body elements of a web page? \n2. If this field is not stored, is there a way to have Solr retrieve that field at search time so that it can be highlighted? </p>\n", "creation_date": 1333526442, "score": 0},
{"title": "Nutch Recrawl - Storing segments is necessory or not", "view_count": 176, "owner": {"user_id": 1187062, "answer_count": 3, "creation_date": 1328258401, "accept_rate": 64, "view_count": 28, "reputation": 88}, "is_answered": true, "answers": [{"question_id": 9225764, "owner": {"user_id": 1150329, "accept_rate": 89, "link": "http://stackoverflow.com/users/1150329/tejas-patil", "user_type": "registered", "reputation": 4793}, "body": "<p>The last fetch time is maintained by crawldb and not segments. Segments are useful just from indexing &amp; searching perspective. Storing in any from will NOT impact crawling rate.</p>\n", "creation_date": 1333469034, "is_accepted": true, "score": 0, "last_activity_date": 1333469034, "answer_id": 9997138}], "question_id": 9225764, "tags": ["nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/9225764/nutch-recrawl-storing-segments-is-necessory-or-not", "last_activity_date": 1333469034, "accepted_answer_id": 9997138, "body": "<p>I am deleting segments after they gets indexed then how nutch will get that last fetch time of pages while recrawling?  Do i need to store them to speedup the recrawl?</p>\n", "creation_date": 1328867242, "score": 0},
{"title": "How to index file names (and other file metadata) in nutch?", "view_count": 113, "owner": {"user_id": 726706, "answer_count": 7, "creation_date": 1303887958, "accept_rate": 68, "view_count": 92, "reputation": 1044}, "is_answered": true, "answers": [{"question_id": 9215842, "owner": {"user_id": 1150329, "accept_rate": 89, "link": "http://stackoverflow.com/users/1150329/tejas-patil", "user_type": "registered", "reputation": 4793}, "body": "<p>The details (file names, owner) will already be handled by code but i think that you need to do slight modification to parsers to store those details</p>\n", "creation_date": 1333468923, "is_accepted": true, "score": 0, "last_activity_date": 1333468923, "answer_id": 9997113}], "question_id": 9215842, "tags": ["java", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/9215842/how-to-index-file-names-and-other-file-metadata-in-nutch", "last_activity_date": 1333468923, "accepted_answer_id": 9997113, "body": "<p>It seems like nutch indexes only (some) parse results. It runs the indexing filters which  detremine what is indexed.</p>\n\n<p>These indexing filters get a Parse result as a parameter.</p>\n\n<p>How can I achieve file names and other file metadata like owner being indexed?</p>\n\n<p>Of course I need to add an indexing filter, but to do I also have to add a parser for parsing all filetypes and getting their metadata? </p>\n", "creation_date": 1328808979, "score": 0},
{"title": "use tika in nutch plugin", "view_count": 1649, "is_answered": true, "answers": [{"question_id": 9174078, "owner": {"user_id": 174982, "link": "http://stackoverflow.com/users/174982/jukka-zitting", "user_type": "registered", "reputation": 987}, "body": "<p>Since version 1.1 Nutch includes a <a href=\"http://wiki.apache.org/nutch/TikaPlugin\" rel=\"nofollow\">Tika plugin</a> (see also <a href=\"https://issues.apache.org/jira/browse/NUTCH-766\" rel=\"nofollow\">NUTCH-766</a>) that should cover your need. I don't know if there's more comprehensive documentation available. You might want to ask the <a href=\"http://nutch.apache.org/mailing_lists.html\" rel=\"nofollow\">Nutch users</a> mailing list for more details (or someone here on SO can fill in).</p>\n", "creation_date": 1328631573, "is_accepted": false, "score": 1, "last_activity_date": 1328631573, "answer_id": 9179796}, {"question_id": 9174078, "owner": {"user_id": 1150329, "accept_rate": 89, "link": "http://stackoverflow.com/users/1150329/tejas-patil", "user_type": "registered", "reputation": 4793}, "body": "<p>As Jukka Zitting said, <code>Tika</code> is already leveraged in nutch. In the code that you pasted, there is no place that you had set the <code>metadata</code> and <code>ParseStatus</code> to any nutch specific data structure. So you dont see the <code>ParseStatus</code> accordingly. </p>\n", "creation_date": 1333468779, "is_accepted": false, "score": 0, "last_activity_date": 1333468779, "answer_id": 9997065}], "question_id": 9174078, "tags": ["apache", "parsing", "nutch", "apache-tika"], "answer_count": 2, "link": "http://stackoverflow.com/questions/9174078/use-tika-in-nutch-plugin", "last_activity_date": 1333468779, "owner": {"user_id": 764618, "answer_count": 6, "creation_date": 1306052225, "accept_rate": 42, "view_count": 41, "reputation": 93}, "body": "<p>In nutch I'm implementing a plug-in that will get the content of webpages and process them in special way.</p>\n\n<p>My main problem is I want to convert webpages to plainText to be able to processed,, I read that tika toolkit can do that</p>\n\n<p>so, I found this code that use tika to parse urls, I write it under filter method</p>\n\n<pre><code> public ParseResult filter(Content content, ParseResult parseResult, HTMLMetaTags metaTags, DocumentFragment doc) \n  {\n byte[] raw = content.getContent();\n ContentHandler handler = new BodyContentHandler();\n Metadata metadata = new Metadata();\n Parser parser = new AutoDetectParser(); \n parser.parse(new ByteArrayInputStream(raw), handler, metadata, new ParseContext());\n String plainText = handler.toString(); \n LOG.info(\"Mime: \" + metadata.get(Metadata.CONTENT_TYPE));\n LOG.info(\"content: \" + handler.toString());\n\n      }\n</code></pre>\n\n<p>The result of metadata.get(Metadata.CONTENT_TYPE) is text/html</p>\n\n<p>but handler.toString() is empty !</p>\n\n<p>Update:\nAlso I try to use this line after the parser method</p>\n\n<pre><code> LOG.info (\"Status : \"+ new ParseStatus().toString());\n</code></pre>\n\n<p>and I get this result:\nStatus : notparsed(0,0)</p>\n", "creation_date": 1328608189, "score": 0},
{"title": "Using Nutch to crawl a specified URL list", "view_count": 3164, "owner": {"user_id": 561629, "answer_count": 2, "creation_date": 1294080535, "accept_rate": 61, "view_count": 48, "reputation": 215}, "is_answered": true, "answers": [{"question_id": 9154429, "owner": {"user_id": 1140725, "accept_rate": 94, "link": "http://stackoverflow.com/users/1140725/debaditya", "user_type": "registered", "reputation": 1506}, "body": "<ul>\n<li>Delete the crawl and urls directory (if created before)</li>\n<li>Create and Update the seed file ( where URLs are listed 1URL per row)</li>\n<li>Restart the crawling process</li>\n</ul>\n\n<p><strong>Command</strong>  </p>\n\n<pre><code>nutch crawl urllist -dir crawl -depth 3 -topN 1000000\n</code></pre>\n\n<ul>\n<li>urllist  - Directory where seed file (url list) is present  </li>\n<li>crawl    - Directory name</li>\n</ul>\n\n<p>Even if the problem persists, try to delete your nutch folder and restart the whole process.</p>\n", "creation_date": 1328507542, "is_accepted": false, "score": 1, "last_activity_date": 1328507542, "answer_id": 9156073}, {"question_id": 9154429, "owner": {"user_id": 1150329, "accept_rate": 89, "link": "http://stackoverflow.com/users/1150329/tejas-patil", "user_type": "registered", "reputation": 4793}, "body": "<p>Set this property in <code>nutch-site.xml</code>. (by default its true so it adds outlinks to the crawldb)</p>\n\n<pre><code>&lt;property&gt;\n  &lt;name&gt;db.update.additions.allowed&lt;/name&gt;\n  &lt;value&gt;false&lt;/value&gt;\n  &lt;description&gt;If true, updatedb will add newly discovered URLs, if false\n  only already existing URLs in the CrawlDb will be updated and no new\n  URLs will be added.\n  &lt;/description&gt;\n&lt;/property&gt;\n</code></pre>\n", "creation_date": 1333468463, "is_accepted": true, "score": 3, "last_activity_date": 1333468463, "answer_id": 9996967}], "question_id": 9154429, "tags": ["nutch", "web-crawler"], "answer_count": 2, "link": "http://stackoverflow.com/questions/9154429/using-nutch-to-crawl-a-specified-url-list", "last_activity_date": 1333468463, "accepted_answer_id": 9996967, "body": "<p>I have one million URL list to fetch. I use this list as nutch seeds and use the basic <strong>crawl</strong> command of Nutch to fetch them. However, I find that Nutch automatically fetches not-on-list URLs. I do set the crawl parameters as -depth 1 -topN 1000000. But it does not work. Does anyone know how to do this?</p>\n", "creation_date": 1328488342, "score": 0},
{"title": "db.max.anchor.length property in nutch-default.xml", "view_count": 120, "is_answered": false, "answers": [{"question_id": 9129305, "owner": {"user_id": 1150329, "accept_rate": 89, "link": "http://stackoverflow.com/users/1150329/tejas-patil", "user_type": "registered", "reputation": 4793}, "body": "<p>The property <code>db.max.anchor.length</code> is effective only if you create <code>linkdb</code> (inverted index). For normal crawling, it wont apply limit on size of anchor.</p>\n", "creation_date": 1333468254, "is_accepted": false, "score": 0, "last_activity_date": 1333468254, "answer_id": 9996906}], "question_id": 9129305, "tags": ["java", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/9129305/db-max-anchor-length-property-in-nutch-default-xml", "last_activity_date": 1333468254, "owner": {"user_id": 1187062, "answer_count": 3, "creation_date": 1328258401, "accept_rate": 64, "view_count": 28, "reputation": 88}, "body": "<p>I have db.max.anchor.length set to 5, but nutch is still fetching urls having anchor length greater that 5, \ne.g : <a href=\"http://mysite/somepage.htm\" rel=\"nofollow\">http://mysite/somepage.htm</a> , what is the reason for this?? I am using nutch-1.2.</p>\n", "creation_date": 1328275268, "score": 0},
{"title": "Nutch : Criteria for db_unfetched state of urls", "view_count": 651, "owner": {"user_id": 1187062, "answer_count": 3, "creation_date": 1328258401, "accept_rate": 64, "view_count": 28, "reputation": 88}, "is_answered": true, "answers": [{"question_id": 9125845, "owner": {"user_id": 1150329, "accept_rate": 89, "link": "http://stackoverflow.com/users/1150329/tejas-patil", "user_type": "registered", "reputation": 4793}, "body": "<p>Following urls are marked as db_<code>unfetched</code>:</p>\n\n<ol>\n<li>A freshly injected url, </li>\n<li>url which isnt fetched, </li>\n<li>a url which could not be fetched due to some exception</li>\n</ol>\n\n<p><code>crawlurlfilter.txt</code> doesnt allow the urls to be eligible for fetching. So the urls will remain <code>db_unfetched</code>.</p>\n", "creation_date": 1333467986, "is_accepted": true, "score": 2, "last_activity_date": 1333467986, "answer_id": 9996820}], "question_id": 9125845, "tags": ["solr", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/9125845/nutch-criteria-for-db-unfetched-state-of-urls", "last_activity_date": 1333467986, "accepted_answer_id": 9996820, "body": "<p>In which cases url goed to db_unfetched phase. does urls not satisfying crawlurlfilter.txt also goes to db_unfetched state.</p>\n", "creation_date": 1328258550, "score": 0},
{"title": "nutch - Is there a way to get http response header fields parsed?", "view_count": 485, "owner": {"user_id": 726706, "answer_count": 7, "creation_date": 1303887958, "accept_rate": 68, "view_count": 92, "reputation": 1044}, "is_answered": true, "answers": [{"question_id": 9077915, "owner": {"user_id": 1150329, "accept_rate": 89, "link": "http://stackoverflow.com/users/1150329/tejas-patil", "user_type": "registered", "reputation": 4793}, "body": "<p>See line 144 <a href=\"http://javasourcecode.org/html/open-source/nutch/nutch-1.3/org/apache/nutch/protocol/http/api/HttpBase.java.html\" rel=\"nofollow\">here</a> . You can see that http response headers can be obtained and you can use that info. </p>\n\n<p>For second question:\nFor parsing different file types, there are plugins provided by nutch. You need to study the same for the specific file type and get going.</p>\n", "creation_date": 1333467786, "is_accepted": true, "score": 0, "last_activity_date": 1333467786, "answer_id": 9996756}], "question_id": 9077915, "tags": ["java", "http-headers", "web-crawler", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/9077915/nutch-is-there-a-way-to-get-http-response-header-fields-parsed", "last_activity_date": 1333467786, "accepted_answer_id": 9996756, "body": "<p>Can I get http respone header fields parsed with nutch?</p>\n\n<p>Is it built-in capability that's need to be configured? </p>\n\n<p>I've looked the internet and I can't find any info about this.</p>\n\n<p>And also, if i do local file system crawling, is there a way to parse file's header? (size, description etc fields?)</p>\n", "creation_date": 1328005515, "score": 0},
{"title": "get the content of the page formated as it is in nutch", "view_count": 94, "is_answered": false, "answers": [{"question_id": 9060063, "owner": {"user_id": 1150329, "accept_rate": 89, "link": "http://stackoverflow.com/users/1150329/tejas-patil", "user_type": "registered", "reputation": 4793}, "body": "<p>No direct way to do that. </p>\n\n<p>Study and modify <code>src\\java\\org\\apache\\nutch\\segment\\ContentAsTextInputFormat.java</code> as per your needs.</p>\n", "creation_date": 1333467137, "is_accepted": false, "score": 0, "last_activity_date": 1333467137, "answer_id": 9996577}], "question_id": 9060063, "tags": ["http", "parsing", "tomcat", "solr", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/9060063/get-the-content-of-the-page-formated-as-it-is-in-nutch", "last_activity_date": 1333467137, "owner": {"user_id": 764618, "answer_count": 6, "creation_date": 1306052225, "accept_rate": 42, "view_count": 41, "reputation": 93}, "body": "<p>in  nutch, I'm looking for a way to get the content of the page formated as it is(with lines, new lines, and paragraphs). </p>\n\n<p>the coming code doesn't help because it removes all the format of the page.</p>\n\n<pre><code>Parse parse = parseResult.get(content.getUrl());    \nparse.getText()\n</code></pre>\n\n<p>even </p>\n\n<pre><code>BufferedReader br = new BufferedReader(new InputStreamReader(new   \nByteArrayInputStream(content.getContent())));\nwhile (br.readLine() != null) \nLOG.info(\"After br: \" +br.readLine());\n</code></pre>\n\n<p>is not the solution since it returns the content formatted but with the html tags.</p>\n\n<p>I really want it to be in its original format, to be able to send it to a method that it will extract the needed content.</p>\n\n<p>Thanks</p>\n", "creation_date": 1327904128, "score": 0},
{"title": "On which criteia nutch selects TopN docs while crawling?", "view_count": 121, "is_answered": true, "answers": [{"question_id": 9031354, "owner": {"user_id": 1150329, "accept_rate": 89, "link": "http://stackoverflow.com/users/1150329/tejas-patil", "user_type": "registered", "reputation": 4793}, "body": "<p>Here are the things that are taken into account:</p>\n\n<ol>\n<li>The score of the url</li>\n<li>how many urls belonging to the same host are allowed to be crawled.</li>\n<li>Is the re-fetch time of the url reached ?</li>\n</ol>\n", "creation_date": 1333466959, "is_accepted": false, "score": 1, "last_activity_date": 1333466959, "answer_id": 9996518}], "question_id": 9031354, "tags": ["nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/9031354/on-which-criteia-nutch-selects-topn-docs-while-crawling", "last_activity_date": 1333466959, "owner": {"user_id": 1147008, "answer_count": 0, "creation_date": 1326432403, "accept_rate": 50, "view_count": 20, "reputation": 28}, "body": "<p>On which criteia nutch selects TopN docs while crawling? And how nutch creates segments.?</p>\n", "creation_date": 1327657146, "score": 0},
{"title": "Nutch: get every url&#39;s seed url", "view_count": 277, "is_answered": false, "answers": [{"question_id": 9017794, "owner": {"user_id": 1150329, "accept_rate": 89, "link": "http://stackoverflow.com/users/1150329/tejas-patil", "user_type": "registered", "reputation": 4793}, "body": "<p>Try to generate linkdb (inverted index) from the crawled segments and you might get the urls' parents. If there url has one parent, then it will be the seed of that url.<br>\nRead <a href=\"http://wiki.apache.org/nutch/nutch-0.8-dev/bin/nutch_invertlinks\" rel=\"nofollow\">this</a> and <a href=\"http://wiki.apache.org/nutch/nutch-0.8-dev/bin/nutch_readlinkdb\" rel=\"nofollow\">this</a> for help on commands.</p>\n", "creation_date": 1333466651, "is_accepted": false, "score": 0, "last_activity_date": 1333466651, "answer_id": 9996443}], "question_id": 9017794, "tags": ["java", "nutch", "web-crawler"], "answer_count": 1, "link": "http://stackoverflow.com/questions/9017794/nutch-get-every-urls-seed-url", "last_activity_date": 1333466651, "owner": {"user_id": 1171186, "view_count": 4, "answer_count": 0, "creation_date": 1327578287, "reputation": 18}, "body": "<p>I try to configure Nutch as an image crawler and already could get the image urls after crawl.\nNow, I want to get the seed url of each image url, how can I do that?\nThank you!</p>\n", "creation_date": 1327579001, "score": 0},
{"title": "Unable to run the fetcher job in Nutch deploy mode", "view_count": 1073, "is_answered": true, "answers": [{"question_id": 9002511, "owner": {"user_id": 1231531, "link": "http://stackoverflow.com/users/1231531/falsealarm", "user_type": "registered", "reputation": 11}, "body": "<p>This is likely because you have not rebuilt yet. Can you run \"ant\" and see what happens? Obviously, you need to update the http.agent.name in nutch-site.xml if you have not done so yet.</p>\n", "creation_date": 1330112865, "is_accepted": false, "score": 1, "last_activity_date": 1330112865, "answer_id": 9437070}, {"question_id": 9002511, "owner": {"user_id": 1150329, "accept_rate": 89, "link": "http://stackoverflow.com/users/1150329/tejas-patil", "user_type": "registered", "reputation": 4793}, "body": "<p>Try this out:</p>\n\n<ol>\n<li><p>In the nutch source directory, modify the file <code>conf/nutch-site.xml</code> to set <code>http.agent.name</code> properly.</p></li>\n<li><p>re-build the code using <code>ant</code></p></li>\n<li><p>Go to <code>runtime/deploy</code> directory, set the required environment variables and try crawling again.</p></li>\n</ol>\n", "creation_date": 1333466359, "is_accepted": false, "score": 1, "last_activity_date": 1333466359, "answer_id": 9996355}], "question_id": 9002511, "tags": ["hadoop", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/9002511/unable-to-run-the-fetcher-job-in-nutch-deploy-mode", "last_activity_date": 1333466359, "owner": {"user_id": 1169127, "view_count": 3, "answer_count": 0, "creation_date": 1327492369, "reputation": 6}, "body": "<p>I've successfully run Nutch (v1.4) for a crawl using local mode on my Ubuntu 11.10 system. However, when switching over to \"deploy\" mode (all else being the same), I get an error during the fetch cycle.</p>\n\n<p>I have Hadoop running succesfully on the machine, in a pseudo-distributed mode (replication factor is 1 and I have just 1 map and 1 reduce job setup). \"jps\" shows that all Hadoop daemons are up and running.\n18920 Jps\n14799 DataNode\n15127 JobTracker\n14554 NameNode\n15361 TaskTracker\n15044 SecondaryNameNode</p>\n\n<p>I have also added the HADOOP_HOME/bin path to my PATH variable.</p>\n\n<blockquote>\n  <p>PATH=$PATH:/home/jimb/hadoop/bin</p>\n</blockquote>\n\n<p>Then I ran the crawl from the nutch/deploy directory, as below:</p>\n\n<blockquote>\n  <p>bin/nutch crawl /data/runs/ar/seedurls -dir /data/runs/ar/crawls</p>\n</blockquote>\n\n<p>Here is the output I get:</p>\n\n<pre><code>  12/01/25 13:55:49 INFO crawl.Crawl: crawl started in: /data/runs/ar/crawls\n  12/01/25 13:55:49 INFO crawl.Crawl: rootUrlDir = /data/runs/ar/seedurls\n  12/01/25 13:55:49 INFO crawl.Crawl: threads = 10\n  12/01/25 13:55:49 INFO crawl.Crawl: depth = 5\n  12/01/25 13:55:49 INFO crawl.Crawl: solrUrl=null\n  12/01/25 13:55:49 INFO crawl.Injector: Injector: starting at 2012-01-25 13:55:49\n  12/01/25 13:55:49 INFO crawl.Injector: Injector: crawlDb: /data/runs/ar/crawls/crawldb\n  12/01/25 13:55:49 INFO crawl.Injector: Injector: urlDir: /data/runs/ar/seedurls\n  12/01/25 13:55:49 INFO crawl.Injector: Injector: Converting injected urls to crawl db entries.\n  12/01/25 13:56:53 INFO mapred.FileInputFormat: Total input paths to process : 1\n...\n...\n  12/01/25 13:57:21 INFO crawl.Injector: Injector: Merging injected urls into crawl db.\n...\n  12/01/25 13:57:48 INFO crawl.Injector: Injector: finished at 2012-01-25 13:57:48, elapsed: 00:01:59\n  12/01/25 13:57:48 INFO crawl.Generator: Generator: starting at 2012-01-25 13:57:48\n  12/01/25 13:57:48 INFO crawl.Generator: Generator: Selecting best-scoring urls due for fetch.\n  12/01/25 13:57:48 INFO crawl.Generator: Generator: filtering: true\n  12/01/25 13:57:48 INFO crawl.Generator: Generator: normalizing: true\n  12/01/25 13:57:48 INFO mapred.FileInputFormat: Total input paths to process : 2\n...\n  12/01/25 13:58:15 INFO crawl.Generator: Generator: Partitioning selected urls for politeness.\n  12/01/25 13:58:16 INFO crawl.Generator: Generator: segment: /data/runs/ar/crawls/segments/20120125135816\n...\n  12/01/25 13:58:42 INFO crawl.Generator: Generator: finished at 2012-01-25 13:58:42, elapsed: 00:00:54\n  12/01/25 13:58:42 ERROR fetcher.Fetcher: Fetcher: No agents listed in 'http.agent.name' property.\n\nException in thread \"main\" java.lang.IllegalArgumentException: Fetcher: No agents listed in 'http.agent.name' property.\n        at org.apache.nutch.fetcher.Fetcher.checkConfiguration(Fetcher.java:1261)\n        at org.apache.nutch.fetcher.Fetcher.fetch(Fetcher.java:1166)\n        at org.apache.nutch.crawl.Crawl.run(Crawl.java:136)\n        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n        at org.apache.nutch.crawl.Crawl.main(Crawl.java:55)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n        at java.lang.reflect.Method.invoke(Method.java:597)\n        at org.apache.hadoop.util.RunJar.main(RunJar.java:156)\n</code></pre>\n\n<p>Now, the configuration files for the \"local\" mode are setup fine (since a crawl in local mode succeeded). For running in deploy mode, since the \"deploy\" folder did not have any \"conf\" subdirectory, I assumed that either:\na) the conf files need to be copied over under \"deploy/conf\", OR\nb) the conf files need to be placed onto HDFS.</p>\n\n<p>I have verified that option (a) above does not help. So, I'm assuming that the Nutch configuration files need to exist in HDFS, for the HDFS fetcher to run successfully? However, I don't know at what path within HDFS I should place these Nutch conf files, or perhaps I'm barking up the wrong tree?</p>\n\n<p>If Nutch reads config files during \"deploy\" mode from the files under \"local/conf\", then why is it that the local crawl worked fine, but the deploy-mode crawl isn't?</p>\n\n<p>What am I missing here?</p>\n\n<p>Thanks in advance!</p>\n", "creation_date": 1327493574, "score": 1},
{"title": "nutch - how to crawl a specific file type?", "view_count": 313, "is_answered": false, "answers": [{"question_id": 8971886, "owner": {"user_id": 1150329, "accept_rate": 89, "link": "http://stackoverflow.com/users/1150329/tejas-patil", "user_type": "registered", "reputation": 4793}, "body": "<p>In $NUTCH_HOME/conf/regex-urlfilter.txt file, delete exiting regex patterns and paste this:</p>\n\n<pre><code>+\\.doc$ \n-.\n</code></pre>\n\n<p>This will allow only .doc files to get crawled and filter out rest urls.</p>\n", "creation_date": 1333465999, "is_accepted": false, "score": 0, "last_activity_date": 1333465999, "answer_id": 9996251}], "question_id": 8971886, "tags": ["java", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/8971886/nutch-how-to-crawl-a-specific-file-type", "last_activity_date": 1333465999, "owner": {"user_id": 726706, "answer_count": 7, "creation_date": 1303887958, "accept_rate": 68, "view_count": 92, "reputation": 1044}, "body": "<p>Is it possible to define a specific file type that will be crawled?</p>\n\n<p>I'm trying to work around the regex-urlfildtr.txt file, but I only see how I can specify which type NOT to crawl.</p>\n\n<p>Is to possible to define the I want to crawl only, say .doc files?</p>\n", "creation_date": 1327323075, "score": 0},
{"title": "solr search with all documents being retrieved", "view_count": 271, "is_answered": false, "answers": [{"question_id": 9866105, "owner": {"user_id": 465710, "accept_rate": 100, "link": "http://stackoverflow.com/users/465710/marko-bonaci", "user_type": "registered", "reputation": 3833}, "body": "<p>Hmm, people usually have \"missing document\" type problem when they search with Solr. You have the opposite problem :)</p>\n\n<p>You should be able to see why that's happening immediately after you open your index with Luke.\nThe default search field with Nutch's schema is Content, so when you enter only search term into Solr that's the field that is going to be searched. Examine its contents using Luke.</p>\n\n<p>You are, of course, aware of these valuable resources:<br>\n<a href=\"http://www.lucidimagination.com/blog/2010/09/10/refresh-using-nutch-with-solr/\" rel=\"nofollow\">http://www.lucidimagination.com/blog/2010/09/10/refresh-using-nutch-with-solr/</a><br>\n<a href=\"http://groups.drupal.org/lucene-nutch-and-solr\" rel=\"nofollow\">http://groups.drupal.org/lucene-nutch-and-solr</a> <br>\n<a href=\"http://www.mail-archive.com/nutch-commits@lucene.apache.org/msg02227.html\" rel=\"nofollow\">http://www.mail-archive.com/nutch-commits@lucene.apache.org/msg02227.html</a></p>\n", "creation_date": 1332840921, "is_accepted": false, "score": 0, "last_activity_date": 1332840921, "answer_id": 9886864}], "question_id": 9866105, "tags": ["solr", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/9866105/solr-search-with-all-documents-being-retrieved", "last_activity_date": 1332840921, "owner": {"user_id": 1292056, "view_count": 2, "answer_count": 0, "creation_date": 1332729410, "reputation": 6}, "body": "<p>I used nutch 1.4 to crawl web sites, and indexed data to solr 3.5. this was successful. I used Luke to check the index data and found 1678 documents were fetched. but when I entered a query string (just a few key words) in the solr interface for search, all 1678 documents were retrieved. this is weird since most retrieved web pages did not contain these key words at all. </p>\n\n<p>Any idea for this problem?</p>\n\n<p>Thank you.</p>\n\n<p>Thunder</p>\n", "creation_date": 1332730928, "score": 0},
{"title": "UTF-8 characters not showing properly", "view_count": 1814, "owner": {"user_id": 1259550, "answer_count": 2, "creation_date": 1331304557, "accept_rate": 71, "view_count": 21, "reputation": 69}, "is_answered": true, "answers": [{"question_id": 9825793, "owner": {"user_id": 318174, "accept_rate": 71, "link": "http://stackoverflow.com/users/318174/adam-gent", "user_type": "registered", "reputation": 26279}, "body": "<p>I'm not as familiar with Nutch but I have seen this with other things. </p>\n\n<p>A couple of things you should check or do:</p>\n\n<ol>\n<li>Your new pages on the web server may not be content negotiating that its UTF-8</li>\n<li>Your charset meta tags for the new pages may still be iso8859-1</li>\n</ol>\n\n<p>What I recommend you do is take all the old pages of your old site and use a tool like iconv to convert them to UTF-8. Then in your web server configure it so that all text is treated as UTF-8 (that is the content-type header sent back says UTF-8).</p>\n", "creation_date": 1332431833, "is_accepted": false, "score": 0, "last_activity_date": 1332431833, "answer_id": 9825977}, {"question_id": 9825793, "owner": {"user_id": 1286291, "accept_rate": 83, "link": "http://stackoverflow.com/users/1286291/nikolay-spassov", "user_type": "registered", "reputation": 697}, "body": "<p>In nutch-default.xml \"parser.character.encoding.default\" value should be set accordingly. You just have to set it to utf-8. Its default value is \"windows-1252\".</p>\n", "creation_date": 1332439977, "is_accepted": true, "score": 3, "last_activity_date": 1332439977, "answer_id": 9828087}], "question_id": 9825793, "tags": ["solr", "lucene", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/9825793/utf-8-characters-not-showing-properly", "last_activity_date": 1332440003, "accepted_answer_id": 9828087, "body": "<p>I am using Nutch 1.4 and solr 3.3.0 to crawl and index my site which is in French. My site used to be in iso8859-1.</p>\n\n<p>Currently I have 2 indexes under solr. In the first one I store my old pages (in iso8859-1) and in the second one I store my new pages (in utf-8).</p>\n\n<p>I use the same nutch configurations for both of the crawl jobs to get and index the old and the new pages on my site. I have not added any settings about charters encodings on my own ( i think).</p>\n\n<p>I am facing problem when searching the new pages thats supposed to be in utf-8. The french characters doesn't display properly. But for the old pages thats in iso8859-1 everything seems to be fine.</p>\n\n<p>I was wondering if anyone could point me in the right direction for fixing this problem.</p>\n\n<p>I believe the problem comes from the nutch since when I created the dump of the segments I saw those funny character in the dump file.</p>\n\n<p>Thank you.</p>\n", "creation_date": 1332431268, "score": 1},
{"title": "Nutch path error", "view_count": 926, "is_answered": false, "answers": [{"question_id": 9520475, "owner": {"user_id": 135807, "accept_rate": 65, "link": "http://stackoverflow.com/users/135807/aftershock", "user_type": "registered", "reputation": 2494}, "body": "<p>I had similar problem.\nI deleted the db and directories . After that, it ran ok.</p>\n", "creation_date": 1332366035, "is_accepted": false, "score": 0, "last_activity_date": 1332366035, "answer_id": 9813289}], "question_id": 9520475, "tags": ["nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/9520475/nutch-path-error", "last_activity_date": 1332366035, "owner": {"user_id": 785148, "view_count": 4, "answer_count": 1, "creation_date": 1307309778, "reputation": 16}, "body": "<p>following this tutorial <a href=\"http://wiki.apache.org/nutch/NutchTutorial\" rel=\"nofollow\">http://wiki.apache.org/nutch/NutchTutorial</a>\nand <a href=\"http://www.nutchinstall.blogspot.com/\" rel=\"nofollow\">http://www.nutchinstall.blogspot.com/</a></p>\n\n<p>when i take the command</p>\n\n<pre><code>bin/nutch crawl urls -dir crawl -depth 3 -topN 5\n</code></pre>\n\n<p>i have this error</p>\n\n<pre><code>LinkDb: adding segment: file:/C:/cygwin/home/LeHung/apache-nutch-1.4-bin/runtime/local/crawl/segments/20120301233259\nLinkDb: adding segment: file:/C:/cygwin/home/LeHung/apache-nutch-1.4-bin/runtime/local/crawl/segments/20120301233337\nException in thread \"main\" org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/C:/cygwin/home/LeHung/apache-nutch-1.4-bin/runtime/local/crawl/segments/20120301221729/parse_data\nInput path does not exist: file:/C:/cygwin/home/LeHung/apache-nutch-1.4-bin/runtime/local/crawl/segments/20120301221754/parse_data\nInput path does not exist: file:/C:/cygwin/home/LeHung/apache-nutch-1.4-bin/runtime/local/crawl/segments/20120301221804/parse_data\n        at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:190)\n        at org.apache.hadoop.mapred.SequenceFileInputFormat.listStatus(SequenceFileInputFormat.java:44)\n        at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:201)\n        at org.apache.hadoop.mapred.JobClient.writeOldSplits(JobClient.java:810)\n        at org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:781)\n        at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:730)\n        at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:1249)\n        at org.apache.nutch.crawl.LinkDb.invert(LinkDb.java:175)\n        at org.apache.nutch.crawl.LinkDb.invert(LinkDb.java:149)\n        at org.apache.nutch.crawl.Crawl.run(Crawl.java:143)\n        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n        at org.apache.nutch.crawl.Crawl.main(Crawl.java:55)\n</code></pre>\n\n<p>i using cygwin, windows to run nutch</p>\n", "creation_date": 1330620816, "score": 1},
{"title": "nutch + mysql integration", "view_count": 1648, "owner": {"user_id": 389336, "answer_count": 5, "creation_date": 1278926991, "accept_rate": 75, "view_count": 17, "reputation": 17}, "is_answered": true, "answers": [{"question_id": 3227259, "owner": {"user_id": 183100, "link": "http://stackoverflow.com/users/183100/pascal-dimassimo", "user_type": "registered", "reputation": 5766}, "body": "<p>Create your own java class that manage the Nutch cycle. It should be similar to org.apache.nutch.crawl.Crawl but you will have to replace the call to the indexer by a call to your Mysql connector. Or you can call your Mysql connector during each cycle depending on whether you want to update Mysql at the end of the crawl or while it is happening.</p>\n", "creation_date": 1278939678, "is_accepted": true, "score": 4, "last_activity_date": 1278939678, "answer_id": 3228736}], "question_id": 3227259, "tags": ["nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/3227259/nutch-mysql-integration", "last_activity_date": 1332224230, "accepted_answer_id": 3228736, "body": "<p>When nutch finishes its cycle (that is crawl - fetch- parse - index) during index phase, I do not want nutch to index (lucene index), but I want nutch to place all the crawled data (I believe he keeps them as NutchDocument object) into mysql using my code.</p>\n\n<p>Is there any way to do this?</p>\n\n<p>Thanks</p>\n", "creation_date": 1278926992, "score": 1},
{"title": "Nutch-Cygwin How to set JAVA_HOME", "view_count": 13142, "owner": {"age": 25, "answer_count": 3, "creation_date": 1247872259, "user_id": 140477, "accept_rate": 60, "view_count": 114, "location": "Nigeria", "reputation": 738}, "is_answered": true, "answers": [{"question_id": 9345629, "owner": {"user_id": 1174910, "accept_rate": 100, "link": "http://stackoverflow.com/users/1174910/tony-rad", "user_type": "registered", "reputation": 1992}, "body": "<p>Try to use double quotes in:</p>\n\n<pre><code>export JAVA_HOME=\"/cygdrive/f/program files/java/jdk1.6.0_21\"\n</code></pre>\n\n<p>and wherever the script uses the JAVA_HOME variable, for example:</p>\n\n<pre><code>JAVA=\"$JAVA_HOME\"/bin/java\n</code></pre>\n\n<p>Regards</p>\n", "creation_date": 1329623236, "is_accepted": true, "score": 8, "last_activity_date": 1329623236, "answer_id": 9346510}], "question_id": 9345629, "tags": ["cygwin", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/9345629/nutch-cygwin-how-to-set-java-home", "last_activity_date": 1332104875, "accepted_answer_id": 9346510, "body": "<p>i am trying to run Nutch with Cygwin. I am having problems setting the JAVA_HOME.</p>\n\n<pre><code> $ export JAVA_HOME='/cygdrive/f/program files/java/jdk1.6.0_21'\n</code></pre>\n\n<p>When i run nutch command</p>\n\n<pre><code>$ bin/nutch crawl\n</code></pre>\n\n<p>i get</p>\n\n<pre><code>cygpath: can't convert empty path\nbin/nutch: line 268: /cygdrive/f/program: No such file or directory\nbin/nutch: line 268: exec: /cygdrive/f/program: cannot execute: No such file or directory\n</code></pre>\n\n<p>It appears that the space between program and files causes the problem <code>/cygdrive/f/**program files**/java/jdk1.6.0_21</code></p>\n\n<p>Is there a way that i can escape the space? Is there a better way to do this? Thanks</p>\n\n<p><strong>EDIT</strong></p>\n\n<p>At the end of the day, i had to re-install java in another directory with no space in its name. </p>\n", "creation_date": 1329612479, "score": 6},
{"title": "Nutch problem: java.lang.NoClassDefFoundError", "view_count": 4375, "owner": {"user_id": 253976, "answer_count": 23, "creation_date": 1263902980, "accept_rate": 88, "view_count": 4439, "reputation": 10123}, "is_answered": true, "answers": [{"last_edit_date": 1291371584, "owner": {"user_id": 203907, "accept_rate": 81, "link": "http://stackoverflow.com/users/203907/bozho", "user_type": "registered", "reputation": 386946}, "body": "<p>You must add the nutch jar to your classpath. <a href=\"http://download.oracle.com/javase/1.3/docs/tooldocs/win32/classpath.html\" rel=\"nofollow\">See here how</a></p>\n\n<p>It appears that the nutch distribution does not ship with a jar. You have to build it yourself using the ant <code>build.xml</code>. Type <code>ant jar</code> in the nutch directory (if you have ant installed), and it will make the jar.</p>\n", "question_id": 4343708, "creation_date": 1291365233, "is_accepted": true, "score": 4, "last_activity_date": 1291371584, "answer_id": 4343733}, {"question_id": 4343708, "owner": {"user_id": 139985, "accept_rate": 73, "link": "http://stackoverflow.com/users/139985/stephen-c", "user_type": "registered", "reputation": 391719}, "body": "<p>The JVM is telling you that it cannot find the <code>Crawl</code> class.  This is most likely because the command line arguments being passed to the <code>java</code> command are not specifying the correct classpath.  (The classpath is a search path used when searching for classes, and is typically set on the command line or using the CLASSPATH environment variable.) </p>\n", "creation_date": 1291365331, "is_accepted": false, "score": 0, "last_activity_date": 1291365331, "answer_id": 4343736}, {"question_id": 4343708, "owner": {"user_id": 484046, "accept_rate": 81, "link": "http://stackoverflow.com/users/484046/millebii", "user_type": "registered", "reputation": 976}, "body": "<p>What version are you using though ? Because some version include the .jar, however the recent ones like 1.2 does not and there is a warning somewhere telling you that you must build it yourself.</p>\n", "creation_date": 1291384618, "is_accepted": false, "score": 0, "last_activity_date": 1291384618, "answer_id": 4346213}, {"question_id": 4343708, "owner": {"user_id": 489048, "accept_rate": 79, "link": "http://stackoverflow.com/users/489048/hephestos", "user_type": "registered", "reputation": 145}, "body": "<p>I don't know how Old it is, I had the same problems with nutch 1.4, well my solution:</p>\n\n<ul>\n<li>set NUTCH_JAVA_HOME to the java home folder</li>\n<li>set NUTCH_HOME</li>\n<li>set JAVA_HOME</li>\n<li>set CLASSPATH to include all required jar files</li>\n</ul>\n\n<p>open nutch in bin\nand </p>\n\n<ol>\n<li>change cygwin manually to true</li>\n<li>DELETE the native folder from the lib folder</li>\n</ol>\n\n<p>99% Crawl should output something familiar.</p>\n", "creation_date": 1328003594, "is_accepted": false, "score": 0, "last_activity_date": 1328003594, "answer_id": 9077442}, {"question_id": 4343708, "owner": {"user_id": 1275661, "link": "http://stackoverflow.com/users/1275661/chitharanjan-das", "user_type": "unregistered", "reputation": 21}, "body": "<p>for nutch-1.4, running</p>\n\n<pre><code>ant job\n</code></pre>\n\n<p>from within the nutch directory, and then moving the resulting nutch-*.job file from the build subdirectory into the src subdirectory, did the trick for me.</p>\n", "creation_date": 1331983530, "is_accepted": false, "score": 2, "last_activity_date": 1331983530, "answer_id": 9749612}], "question_id": 4343708, "tags": ["java", "tomcat", "cygwin", "nutch"], "answer_count": 5, "link": "http://stackoverflow.com/questions/4343708/nutch-problem-java-lang-noclassdeffounderror", "last_activity_date": 1331983530, "accepted_answer_id": 4343733, "body": "<p>I'm trying to run Nutch on my Windows machine. I have Nutch, Java, Tomcat, and Cygwin installed. When I try to run the crawl command in Cygwin, I get the following error:</p>\n\n<pre><code>java.lang.NoClassDefFoundError: org/apache/nutch/crawl/Crawl\nCaused by: java.lang.ClassNotFoundException: org.apache.nutch.crawl.Crawl\n    at java.net.URLClassLoader$1.run(URLClassLoader.java:202)\n    at java.security.AccessController.doPrivileged(Native Method)\n    at java.net.URLClassLoader.findClass(URLClassLoader.java:190)\n    at java.lang.ClassLoader.loadClass(ClassLoader.java:307)\n    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301)\n    at java.lang.ClassLoader.loadClass(ClassLoader.java:248)\nCould not find the main class: org.apache.nutch.crawl.Crawl.  Program will exit.\nException in thread \"main\"\n</code></pre>\n\n<p>My Java is a bit rusty, but this seems to be telling me that the <code>Crawl</code> class doesn't exist. If this is the case, how do I go about finding it -- and making sure my system is aware of its existence?</p>\n", "creation_date": 1291364994, "score": 1},
{"title": "Regarding crawling of short URLs using nutch", "view_count": 1860, "is_answered": true, "answers": [{"question_id": 4796202, "owner": {"user_id": 183100, "link": "http://stackoverflow.com/users/183100/pascal-dimassimo", "user_type": "registered", "reputation": 5766}, "body": "<p>You will have to set a depth of 2 or more, because the first fetch returns a 301 (or 302) code. The redirection will be followed on the next iteration, so you have to allow more depth.</p>\n\n<p>Also, make sure that you allow all urls that will be followed in your regex-urlfilter.txt</p>\n", "creation_date": 1295974439, "is_accepted": false, "score": 0, "last_activity_date": 1295974439, "answer_id": 4796442}, {"question_id": 4796202, "owner": {"user_id": 435140, "accept_rate": 100, "link": "http://stackoverflow.com/users/435140/dirbacke", "user_type": "registered", "reputation": 675}, "body": "<p>Using nutch 1.2 try editing the file <b>conf/nutch-default.xml</b><br/>\nfind <i>http.redirect.max</i> and change the value to at least 1 instead of the default 0.</p>\n\n<pre><code>&lt;property&gt;\n  &lt;name&gt;http.redirect.max&lt;/name&gt;\n  &lt;value&gt;2&lt;/value&gt;&lt;!-- instead of 0 --&gt;\n  &lt;description&gt;The maximum number of redirects the fetcher will follow when\n  trying to fetch a page. If set to negative or 0, fetcher won't immediately\n  follow redirected URLs, instead it will record them for later fetching.\n  &lt;/description&gt;\n&lt;/property&gt;</code></pre>\n\n<p>Good luck</p>\n", "creation_date": 1303130012, "is_accepted": false, "score": 1, "last_activity_date": 1303130012, "answer_id": 5702934}], "question_id": 4796202, "tags": ["nutch", "web-crawler", "short-url"], "answer_count": 2, "link": "http://stackoverflow.com/questions/4796202/regarding-crawling-of-short-urls-using-nutch", "last_activity_date": 1331929116, "owner": {"user_id": 589317, "view_count": 12, "answer_count": 0, "creation_date": 1295973342, "reputation": 16}, "body": "<p>I am using nutch crawler for my application which needs to crawl a set of URLs which I give to the <code>urls</code> directory and fetch only the contents of that URL only.\nI am not interested in the contents of the internal or external links.\nSo I have used NUTCH crawler and have run the crawl command by giving depth as 1.</p>\n\n<pre>bin/nutch crawl urls -dir crawl -depth 1</pre>\n\n<p>Nutch crawls the urls and gives me the contents of the given urls.</p>\n\n<p>I am reading the content using readseg utility.</p>\n\n<pre>bin/nutch readseg -dump crawl/segments/* arjun -nocontent -nofetch -nogenerate -noparse -noparsedata</pre>\n\n<p>With this I am fetching the content of webpage.</p>\n\n<p>The problem I am facing is if I give direct urls like </p>\n\n<pre>http://isoc.org/wp/worldipv6day/\nhttp://openhackindia.eventbrite.com\nhttp://www.urlesque.com/2010/06/11/last-shot-ye-olde-twitter/\nhttp://www.readwriteweb.com/archives/place_your_tweets_with_twitter_locations.php\nhttp://bangalore.yahoo.com/labs/summerschool.html\nhttp://riadevcamp.eventbrite.com\nhttp://www.sleepingtime.org/</pre>\n\n<p>then I am able to get the contents of the webpage.\nBut when I give the set of URLs as short URLs like </p>\n\n<pre>http://is.gd/jOoAa9\nhttp://is.gd/ubHRAF\nhttp://is.gd/GiFqj9\nhttp://is.gd/H5rUhg\nhttp://is.gd/wvKINL\nhttp://is.gd/K6jTNl\nhttp://is.gd/mpa6fr\nhttp://is.gd/fmobvj\nhttp://is.gd/s7uZf***\n</pre>\n\n<p>I am not able to fetch the contents. </p>\n\n<p>When I read the segments, it is not showing any content. Please find below the content of dump file read from segments.</p>\n\n<pre>\n*Recno:: 0\nURL:: http://is.gd/0yKjO6\nCrawlDatum::\nVersion: 7\nStatus: 1 (db_unfetched)\nFetch time: Tue Jan 25 20:56:07 IST 2011\nModified time: Thu Jan 01 05:30:00 IST 1970\nRetries since fetch: 0\nRetry interval: 2592000 seconds (30 days)\nScore: 1.0\nSignature: null\nMetadata: _ngt_: 1295969171407\nContent::\nVersion: -1\nurl: http://is.gd/0yKjO6\nbase: http://is.gd/0yKjO6\ncontentType: text/html\nmetadata: Date=Tue, 25 Jan 2011 15:26:28 GMT nutch.crawl.score=1.0 Location=http://holykaw.alltop.com/the-twitter-cool-of-a-to-z?tu4=1 _fst_=36 nutch.segment.name=20110125205614 Content-Type=text/html; charset=UTF-8 Connection=close Server=nginx X-Powered-By=PHP/5.2.14\nContent:\nRecno:: 1\nURL:: http://is.gd/1tpKaN\nContent::\nVersion: -1\nurl: http://is.gd/1tpKaN\nbase: http://is.gd/1tpKaN\ncontentType: text/html\nmetadata: Date=Tue, 25 Jan 2011 15:26:28 GMT nutch.crawl.score=1.0 Location=http://holykaw.alltop.com/fighting-for-women-who-dont-want-a-voice?tu3=1 _fst_=36 nutch.segment.name=20110125205614 Content-Type=text/html; charset=UTF-8 Connection=close Server=nginx X-Powered-By=PHP/5.2.14\nContent:\nCrawlDatum::\nVersion: 7\nStatus: 1 (db_unfetched)\nFetch time: Tue Jan 25 20:56:07 IST 2011\nModified time: Thu Jan 01 05:30:00 IST 1970\nRetries since fetch: 0\nRetry interval: 2592000 seconds (30 days)\nScore: 1.0*\n</pre>\n\n<p>I have also tried by setting the max.redirects property in nutch-default.xml as 4 but dint find any progress.\nKindly provide me a solution for this problem.</p>\n\n<p>Thanks and regards,\nArjun Kumar Reddy</p>\n", "creation_date": 1295973342, "score": 1},
{"title": "Solr highlighted search", "view_count": 427, "is_answered": true, "answers": [{"question_id": 9600515, "owner": {"user_id": 1236090, "link": "http://stackoverflow.com/users/1236090/david-faber", "user_type": "registered", "reputation": 7844}, "body": "<p>If \"for some records title is not coming\" that suggests that <code>title</code> doesn't match your query in those circumstances. Highlighting will only return snippets from fields that actually contain matches. What I would suggest is the following:</p>\n\n<ol>\n<li>Run your query with <code>hl.fragsize = 0</code>. That will return the entire\ncontents of the field.</li>\n<li>If you don't have a title match in the highlighting structure,\ndisplay the title returned from the search w/o highlighting.</li>\n<li>If do do have a title match in highlighting, display that in place\nof the title.</li>\n</ol>\n\n<p>Hope this helps.</p>\n", "creation_date": 1331675646, "is_accepted": false, "score": 1, "last_activity_date": 1331675646, "answer_id": 9692793}], "question_id": 9600515, "tags": ["solr", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/9600515/solr-highlighted-search", "last_activity_date": 1331675646, "owner": {"age": 29, "answer_count": 0, "creation_date": 1322902565, "user_id": 1078790, "accept_rate": 48, "view_count": 102, "location": "Pakistan", "reputation": 153}, "body": "<p>I am using SOLR with Nutch </p>\n\n<p>I am using highlighted search in SOLR it works fine for me </p>\n\n<p>But it returns only one field that we give for highlighting </p>\n\n<p>If I give two fields like hl.fl = title,content then for some records title is not coming</p>\n\n<p>In schema.xml title field looks like </p>\n\n<pre><code>&lt;field name=\"title\" type=\"text\" stored=\"true\" indexed=\"true\"/&gt;\n</code></pre>\n\n<p>How can i get extra fields in highlighted search result.</p>\n\n<p>I need to display highlighted search results with all fields i have...</p>\n", "creation_date": 1331119319, "score": 1},
{"title": "Duplicates when using nutch -&gt; elasticsearch solution", "view_count": 747, "owner": {"user_id": 1228519, "view_count": 7, "answer_count": 0, "creation_date": 1330005670, "reputation": 13}, "is_answered": true, "answers": [{"question_id": 9414601, "owner": {"user_id": 976646, "accept_rate": 71, "link": "http://stackoverflow.com/users/976646/vineeth-mohan", "user_type": "registered", "reputation": 8015}, "body": "<p>One way , you can keep an index of check sum of all data you have entered into elasticSearch in some db and cross refer those before attempting to send data to elasticSearch.\nOr then you can run a \"more like this\" query to see similar documents and take decision based on that.</p>\n\n<p>LINK - <a href=\"http://www.elasticsearch.org/guide/reference/query-dsl/mlt-field-query.html\" rel=\"nofollow\">http://www.elasticsearch.org/guide/reference/query-dsl/mlt-field-query.html</a></p>\n", "creation_date": 1330167003, "is_accepted": false, "score": 2, "last_activity_date": 1330167003, "answer_id": 9443344}, {"question_id": 9414601, "owner": {"user_id": 40876, "accept_rate": 62, "link": "http://stackoverflow.com/users/40876/felipe-hummel", "user_type": "registered", "reputation": 1921}, "body": "<p>If you index each page/document crawled with the same id in ElasticSearch it won't duplicate it. You could use a checksum/hash function to turn the page's URL into a distinct ID.</p>\n\n<p>You can also use Operation_type to ensure that if that id is already indexed it should not reindex it:</p>\n\n<blockquote>\n  <p>The index operation also accepts an op_type that can be used to force\n  a create operation, allowing for \u201cput-if-absent\u201d behavior. When create\n  is used, the index operation will fail if a document by that id\n  already exists in the index.</p>\n</blockquote>\n\n<p><a href=\"http://www.elasticsearch.org/guide/reference/api/index_.html\" rel=\"nofollow\">ElasticSearch index API</a></p>\n", "creation_date": 1331656463, "is_accepted": true, "score": 2, "last_activity_date": 1331656463, "answer_id": 9688126}], "question_id": 9414601, "tags": ["indexing", "nutch", "elasticsearch", "duplicate-removal", "web-crawler"], "answer_count": 2, "link": "http://stackoverflow.com/questions/9414601/duplicates-when-using-nutch-elasticsearch-solution", "last_activity_date": 1331656463, "accepted_answer_id": 9688126, "body": "<p>I have crawled some data using nutch and managed to inject it into elasticsearch. But I have one problem: If I inject the crawled data again it will create duplicates. Is there any way of disallowing this? </p>\n\n<p>Has anyone managed to solve this or have any suggestions on how to solve it?</p>\n\n<p>/Samus</p>\n", "creation_date": 1330005995, "score": 2},
{"title": "Is there a way to run NUTCH with different configuration files?", "view_count": 956, "owner": {"user_id": 1259550, "answer_count": 2, "creation_date": 1331304557, "accept_rate": 71, "view_count": 21, "reputation": 69}, "is_answered": true, "answers": [{"question_id": 9673315, "owner": {"user_id": 633239, "accept_rate": 90, "link": "http://stackoverflow.com/users/633239/javanna", "user_type": "registered", "reputation": 38114}, "body": "<p><a href=\"http://wiki.apache.org/nutch/FAQ#How_can_I_force_fetcher_to_use_custom_nutch-config.3F\" rel=\"nofollow\">This nutch FAQ</a> should be useful. The answer describes how to create your custom conf directory and have nutch pointing to it through the <code>$NUTCH_CONF_DIR</code> environment variable.</p>\n", "creation_date": 1331581841, "is_accepted": true, "score": 3, "last_activity_date": 1331581841, "answer_id": 9673831}], "question_id": 9673315, "tags": ["solr", "lucene", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/9673315/is-there-a-way-to-run-nutch-with-different-configuration-files", "last_activity_date": 1331581841, "accepted_answer_id": 9673831, "body": "<p>I was wondering if its possible to run the same NUTCH instance with different set of configuration files ? I can't see of any options in the arguments list to allow such thing.</p>\n\n<p>I want to run NUTCH on only 1 computer and I don't want to duplicate the nutch instance.</p>\n\n<p>Does anyone know of a easy way of doing this or do i need to modify the bin/nutch script myself to get it done.</p>\n\n<p>Thanks.</p>\n", "creation_date": 1331579563, "score": 2},
{"title": "using nutch 1.4 in ubuntu", "view_count": 291, "owner": {"user_id": 648896, "answer_count": 42, "creation_date": 1299534994, "accept_rate": 47, "view_count": 751, "reputation": 3059}, "is_answered": true, "answers": [{"question_id": 9660227, "owner": {"user_id": 633239, "accept_rate": 90, "link": "http://stackoverflow.com/users/633239/javanna", "user_type": "registered", "reputation": 38114}, "body": "<p>The problem isn't about nutch: the space within the <code>search engine</code> folder name creates problems. As you can see from the <code>ClassNotFoundException</code> the part after the space is taken as name of the class to be executed. Can't you just rename <code>search engine</code> to something else like <code>search-engine</code>?</p>\n", "creation_date": 1331540171, "is_accepted": true, "score": 0, "last_activity_date": 1331540171, "answer_id": 9663633}], "question_id": 9660227, "tags": ["java", "ubuntu", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/9660227/using-nutch-1-4-in-ubuntu", "last_activity_date": 1331540171, "accepted_answer_id": 9663633, "body": "<p>I try to use nutch 1.4 crawler in ubuntu however when I try to execute nutcg with all the setting that are suggested in nutch wiki it gives this error:</p>\n\n<pre><code>erogol@erogol-G50V:~/Desktop/search engine/apache-nutch-1.4-bin/runtime/local$\nbin/nutch crawl urls -dir crawl -depth 1\nbin/nutch: line 108: [: /home/erogol/Desktop/search: binary operator expected\nException in thread \"main\" java.lang.NoClassDefFoundError: engine/apache-nutch-1/4-bin \n/runtime/local/logs\nCaused by: java.lang.ClassNotFoundException: engine.apache-nutch-    \n1.4-bin.runtime.local.logs\n    at java.net.URLClassLoader$1.run(URLClassLoader.java:202)\n    at java.security.AccessController.doPrivileged(Native Method)\n    at java.net.URLClassLoader.findClass(URLClassLoader.java:190)\n    at java.lang.ClassLoader.loadClass(ClassLoader.java:306)\n    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301)\n    at java.lang.ClassLoader.loadClass(ClassLoader.java:247)\nCould not find the main class: engine/apache-nutch-1.4-bin/runtime/local/logs.      \nProgram will exit.\n</code></pre>\n\n<p>Do you have any suggestion or idea to solve the using problem for nutch?</p>\n\n<p>Thanks in advance... all nutch knowers :)</p>\n", "creation_date": 1331511013, "score": 0},
{"title": "Nutch crawler only finds a subset of links on a given page?", "view_count": 237, "is_answered": true, "answers": [{"question_id": 8292495, "owner": {"user_id": 1254556, "link": "http://stackoverflow.com/users/1254556/michael-fritsch", "user_type": "unregistered", "reputation": 11}, "body": "<p>I had a similar problem. In my case the property <code>http.content.limit</code> was the problem. The default value is \"65536\". If your page is bigger, it will be truncated and links may be lost.</p>\n", "creation_date": 1331120061, "is_accepted": false, "score": 1, "last_activity_date": 1331120061, "answer_id": 9600709}], "question_id": 8292495, "tags": ["java", "web-crawler", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/8292495/nutch-crawler-only-finds-a-subset-of-links-on-a-given-page", "last_activity_date": 1331120061, "owner": {"user_id": 1068828, "answer_count": 3, "creation_date": 1304792231, "accept_rate": 9, "view_count": 28, "reputation": 268}, "body": "<p>I'm using the following command to crawl one single page with 788 links on it:</p>\n\n<pre><code>nutch crawl urls/ -dir crawls -depth 1 -topN 1000\n</code></pre>\n\n<p>The above command only is able to find 72 urls! Here is the output for\n<code>nutch readdb  ./crawls/crawldb/ -stats</code>:</p>\n\n<pre><code>CrawlDb statistics start: ./crawls/crawldb/\n\nStatistics for CrawlDb: ./crawls/crawldb/\n\nTOTAL urls: 72\n\nretry 0:    72\n\nmin score:  0.009\n\navg score:  0.026777778\n\nmax score:  1.279\n\nstatus 1 (db_unfetched):    71\n\nstatus 2 (db_fetched):  1\n\nCrawlDb statistics: done\n</code></pre>\n\n<p>My <code>regex-urlfilter.txt</code> has the default settings and I'm using Nutch 1.4. </p>\n\n<p>Any help is appreciated.</p>\n", "creation_date": 1322464101, "score": 0},
{"title": "Post processing of pages crawled using nutch", "view_count": 327, "is_answered": true, "answers": [{"question_id": 9585340, "owner": {"user_id": 131433, "accept_rate": 76, "link": "http://stackoverflow.com/users/131433/bmargulies", "user_type": "registered", "reputation": 65222}, "body": "<p>Turning raw web pages into information is not a trivial task. One tool used for this job is Boilerpipe. However, it won't give you a solution on a plate.</p>\n\n<p>If you are working on a fixed target, you might just write your own procedural code to find the data you need. If you need to find this sort of thing in arbitrary HTML, you are facing a very hard problem with no off-the-shelf solutions.</p>\n", "creation_date": 1331045192, "is_accepted": false, "score": 1, "last_activity_date": 1331045192, "answer_id": 9585913}], "question_id": 9585340, "tags": ["solr", "lucene", "mapreduce", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/9585340/post-processing-of-pages-crawled-using-nutch", "last_activity_date": 1331045192, "owner": {"age": 26, "answer_count": 8, "creation_date": 1314202051, "user_id": 910032, "accept_rate": 62, "view_count": 56, "reputation": 149}, "body": "<p>I have a set of pages crawled using nutch. And I understand that this crawled pages are saved as segments. I want to extract certain key values from this pages and feed it to solr as xml.</p>\n\n<p>A sample situation is that I have crawled a shopping website with many product listings. I want to extract key infos like Name, Price, Specs of the product and ignore rest of the data. So that I may provide to solr some xml like \nqwerty123qwerty\nThis is so that using solr I should be able to do sorting of different product listings based on the price.</p>\n\n<p>Now how this extraction part can be done? Does map reduce come anywhere in picture?</p>\n", "creation_date": 1331043241, "score": 0},
{"title": "java.net.UnknownHostException during fetching in crawl(with nutch 1.4)", "view_count": 550, "is_answered": false, "question_id": 9554821, "tags": ["fetch", "nutch", "web-crawler"], "answer_count": 0, "link": "http://stackoverflow.com/questions/9554821/java-net-unknownhostexception-during-fetching-in-crawlwith-nutch-1-4", "last_activity_date": 1330863774, "owner": {"user_id": 924923, "answer_count": 1, "creation_date": 1314953369, "accept_rate": 12, "view_count": 58, "reputation": 79}, "body": "<p>I have one link with many external link inside it,when the fetching process start, many external link failed with: java.net.UnknownHostException, i use nutch 1.4 and i set the below setting in nutch-site.xml, is this any misconfiguration?</p>\n\n<pre><code> &lt;property&gt;\n        &lt;name&gt;parser.timeout&lt;/name&gt;\n        &lt;value&gt;30&lt;/value&gt;       \n    &lt;/property&gt;\n    &lt;property&gt;\n        &lt;name&gt;db.fetch.interval.default&lt;/name&gt;\n        &lt;value&gt;36000&lt;/value&gt;       \n    &lt;/property&gt;\n    &lt;property&gt;\n        &lt;name&gt;db.ignore.external.links&lt;/name&gt;\n        &lt;value&gt;false&lt;/value&gt;        \n    &lt;/property&gt;\n    &lt;property&gt;\n        &lt;name&gt;http.timeout&lt;/name&gt;\n        &lt;value&gt;30000&lt;/value&gt;       \n    &lt;/property&gt;\n    &lt;property&gt;\n        &lt;name&gt;db.max.outlinks.per.page&lt;/name&gt;\n        &lt;value&gt;-1&lt;/value&gt;\n    &lt;/property&gt;\n    &lt;property&gt;\n        &lt;name&gt;db.fetch.interval.max&lt;/name&gt;\n        &lt;value&gt;7776000&lt;/value&gt;        \n    &lt;/property&gt;\n   &lt;property&gt;\n        &lt;name&gt;fetcher.threads.fetch&lt;/name&gt;\n        &lt;value&gt;10&lt;/value&gt;  \n   &lt;/property&gt; \n</code></pre>\n", "creation_date": 1330863774, "score": 1},
{"title": "Why do I get &quot;security.Groups: Group mapping impl=org.apache.hadoop.security.ShellBasedUnixGroupsMapping; cacheTimeout=300000&quot;?", "view_count": 644, "owner": {"user_id": 300248, "answer_count": 60, "creation_date": 1269371954, "accept_rate": 74, "view_count": 602, "location": "Bolzano, Italy", "reputation": 3706}, "is_answered": true, "answers": [{"question_id": 5685026, "owner": {"user_id": 540873, "link": "http://stackoverflow.com/users/540873/thomas-jungblut", "user_type": "registered", "reputation": 15761}, "body": "<p>It's just an INFO what's your problem?</p>\n", "creation_date": 1302942288, "is_accepted": false, "score": 0, "last_activity_date": 1302942288, "answer_id": 5685324}, {"question_id": 5685026, "owner": {"user_id": 131460, "link": "http://stackoverflow.com/users/131460/jakob-homan", "user_type": "registered", "reputation": 1984}, "body": "<p>It's not a warning, just a standard message.  It shouldn't have been output at INFO level, however, because it does get printed with every message.  In trunk it has been moved to DEBUG so you don't normally see it (<a href=\"https://issues.apache.org/jira/browse/HADOOP-6763\" rel=\"nofollow\">in this JIRA</a>).  This will be incorporated into the next release.</p>\n", "creation_date": 1302979065, "is_accepted": true, "score": 0, "last_activity_date": 1302979065, "answer_id": 5688676}], "question_id": 5685026, "tags": ["hadoop", "nutch", "hdfs"], "answer_count": 2, "link": "http://stackoverflow.com/questions/5685026/why-do-i-get-security-groups-group-mapping-impl-org-apache-hadoop-security-she", "last_activity_date": 1330701849, "accepted_answer_id": 5688676, "body": "<pre><code>$hdfs dfs -rmr crawl\n    11/04/16 08:49:33 INFO security.Groups: Group mapping impl=org.apache.hadoop.security.ShellBasedUnixGroupsMapping; cacheTimeout=300000\n</code></pre>\n\n<p>I'm using hadoop-0.21.0 with the <a href=\"http://hadoop.apache.org/common/docs/current/single_node_setup.html#Local\" rel=\"nofollow\">default Single Node Setup configuration</a>.</p>\n", "creation_date": 1302937074, "score": 2},
{"title": "Setting up nutch 1.3 and Hadoop 0.20.2", "view_count": 349, "is_answered": false, "question_id": 7753031, "tags": ["hadoop", "nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/7753031/setting-up-nutch-1-3-and-hadoop-0-20-2", "last_activity_date": 1330657369, "owner": {"age": 25, "answer_count": 7, "creation_date": 1315727263, "user_id": 938959, "accept_rate": 70, "view_count": 80, "location": "Bangalore, India", "reputation": 394}, "body": "<p>I have a multi-node cluster running on UEC(Ubuntu enterprise cloud) and i thought it will be a good idea to set up nutch with it .\nHowever, i found this tutorial unhelpful <a href=\"http://wiki.apache.org/nutch/NutchHadoopTutorial\" rel=\"nofollow\">http://wiki.apache.org/nutch/NutchHadoopTutorial</a> as this is for nucth release 0.8 and it doesn't support latest versions . I'm stuck .\nCan somebody tell me how i can configure nutch 1.3 with hadoop? Thank you for your time .</p>\n", "creation_date": 1318503294, "score": 0},
{"title": "Django Integration with SOLR and NUTCH", "view_count": 401, "owner": {"age": 29, "answer_count": 0, "creation_date": 1322902565, "user_id": 1078790, "accept_rate": 48, "view_count": 102, "location": "Pakistan", "reputation": 153}, "is_answered": true, "answers": [{"question_id": 9499437, "owner": {"user_id": 754484, "accept_rate": 94, "link": "http://stackoverflow.com/users/754484/samuele-mattiuzzo", "user_type": "registered", "reputation": 5902}, "body": "<p>it depends on what you need:</p>\n\n<p><strong>IF YOUR SOLR INSTANCE IS BOUND TO THE DJANGO'S MODEL</strong>  </p>\n\n<p>you probably are looking for <a href=\"http://haystacksearch.org/\" rel=\"nofollow\">django-haystack</a>: it's pretty easy to setup and to use inside your django views, but it's not suited for huge amount of data</p>\n\n<p><strong>IF YOU'RE USING SOLR WITHOUT ANY RELATION TO DJANGO MODELS</strong>  </p>\n\n<p>this means that your results are retrieved as pure array, and you just need a python interface to solr. you have some choices</p>\n\n<ul>\n<li><a href=\"http://code.google.com/p/pysolr/\" rel=\"nofollow\">pysolr</a></li>\n<li><a href=\"http://code.google.com/p/solrpy/\" rel=\"nofollow\">solrpy</a></li>\n<li><a href=\"https://github.com/tow/sunburnt\" rel=\"nofollow\">sunburnt</a> (i'm using this)</li>\n</ul>\n\n<p>you have to try them out and find out which one satisfies you more (check their websites too, to see if the project is mantained. also, stumble into their source code, to check if there's all you need). being a python interface implies that you have to do a little bit more of work inside your views</p>\n\n<p>i hope this can help you</p>\n", "creation_date": 1330521620, "is_accepted": true, "score": 2, "last_activity_date": 1330521620, "answer_id": 9499900}], "question_id": 9499437, "tags": ["django", "solr", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/9499437/django-integration-with-solr-and-nutch", "last_activity_date": 1330521664, "accepted_answer_id": 9499900, "body": "<p>I am using SOLR with NUTCH </p>\n\n<p>I have successfully configure both and Solr is giving me the desired results</p>\n\n<p>Now i want to integrate this in my Django project </p>\n\n<p>Can anyone suggest me  how to do it ... </p>\n", "creation_date": 1330519620, "score": 0},
{"title": "Index all the intranet with nutch", "view_count": 144, "is_answered": false, "answers": [{"question_id": 9174300, "owner": {"user_id": 1174910, "accept_rate": 100, "link": "http://stackoverflow.com/users/1174910/tony-rad", "user_type": "registered", "reputation": 1992}, "body": "<p>If you know all the urls of the intranet, then write a robots.txt (or an equivalent page with  all the urls and point the crawler to it). </p>\n\n<p>If you don't then you cannot be never secure that you'll have crawled all the urls, because you cannot verify it after the crawling.</p>\n\n<p>In the last case the best chance is to do the crawl at the maximum depth.</p>\n\n<p>Regards</p>\n", "creation_date": 1329623824, "is_accepted": false, "score": 0, "last_activity_date": 1329623824, "answer_id": 9346543}], "question_id": 9174300, "tags": ["nutch", "intranet"], "answer_count": 1, "link": "http://stackoverflow.com/questions/9174300/index-all-the-intranet-with-nutch", "last_activity_date": 1329623824, "owner": {"user_id": 715860, "answer_count": 1, "creation_date": 1295599530, "accept_rate": 47, "view_count": 47, "reputation": 145}, "body": "<p>I use Nutch  and i would like index an intranet, but how to make sure everything on the intranet will be indexed ?</p>\n\n<p>Thanks.</p>\n", "creation_date": 1328609172, "score": 0},
{"title": "Integrate Nutch with Solr For Advance Search Options", "view_count": 305, "owner": {"age": 29, "answer_count": 0, "creation_date": 1322902565, "user_id": 1078790, "accept_rate": 48, "view_count": 102, "location": "Pakistan", "reputation": 153}, "is_answered": true, "answers": [{"question_id": 9289702, "owner": {"user_id": 907642, "link": "http://stackoverflow.com/users/907642/okke-klein", "user_type": "registered", "reputation": 2221}, "body": "<p>First you have to store Cancer and Colorectal &amp; Digestive in a category field. You can use <a href=\"http://wiki.apache.org/solr/AnalyzersTokenizersTokenFilters#solr.PathHierarchyTokenizerFactory\" rel=\"nofollow\">http://wiki.apache.org/solr/AnalyzersTokenizersTokenFilters#solr.PathHierarchyTokenizerFactory</a> for that. Then \nthe URL's could look like mysite:8983/solr/select/?q=bone&amp;fq=category:Cancer</p>\n\n<p><a href=\"http://wiki.apache.org/solr/CommonQueryParameters#fq\" rel=\"nofollow\">http://wiki.apache.org/solr/CommonQueryParameters#fq</a></p>\n", "creation_date": 1329603902, "is_accepted": true, "score": 0, "last_activity_date": 1329603902, "answer_id": 9344731}], "question_id": 9289702, "tags": ["solr", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/9289702/integrate-nutch-with-solr-for-advance-search-options", "last_activity_date": 1329603902, "accepted_answer_id": 9344731, "body": "<p>I am using <strong>apache-nutch-1.4</strong> with <strong>apache-solr-3.2.0</strong></p>\n\n<p>I have successfully integrated <strong>NUTCH</strong> with <strong>SOLR</strong></p>\n\n<p>when i have queried the following</p>\n\n<p>mysite/solr/select/?q=bone&amp;version=2.2&amp;start=0&amp;rows=10&amp;indent=on</p>\n\n<p>It gives me following result</p>\n\n<pre><code>&lt;doc&gt;\n&lt;float name=\"boost\"&gt;1.0117649&lt;/float&gt;\n&lt;str name=\"cache\"&gt;content&lt;/str&gt;\n&lt;str name=\"content\"&gt;&lt;/str&gt;\n&lt;str name=\"digest\"&gt;9bf016ea547cf50be81e468553c483de&lt;/str&gt;\n&lt;str name=\"id\"&gt;http://107.21.107.118:8000/&lt;/str&gt;\n&lt;str name=\"segment\"&gt;20120214151903&lt;/str&gt;\n&lt;str name=\"title\"&gt;Home&lt;/str&gt;\n&lt;date name=\"tstamp\"&gt;2012-02-14T10:19:08.215Z&lt;/date&gt;\n&lt;str name=\"url\"&gt;mysite:8000/&lt;/str&gt;\n&lt;/doc&gt;\n</code></pre>\n\n<p>Problem is when i have to search bone in particular category like <strong>cancer</strong> or <strong>Colorectal &amp; Digestive</strong> </p>\n\n<p>then what <strong>param</strong> i need to add in above query to get records for this specific category only</p>\n\n<p>mysite:8983/solr/select/?q=bone&amp;????????</p>\n\n<p>i have urls like </p>\n\n<p>mysite:8000/Encyclopedia/Patient Centers/</p>\n\n<p>mysite:8000/Encyclopedia/Patient Centers/Cancer/</p>\n\n<p>mysite:8000/Encyclopedia/Patient Centers/Cancer/Colorectal &amp; Digestive/</p>\n\n<p>my <strong>schema.xml</strong> file looks like this which i have added in <strong>NUTCH</strong> directory also....</p>\n\n<p><a href=\"http://dpaste.org/MTDF2/\" rel=\"nofollow\">http://dpaste.org/MTDF2/</a></p>\n\n<p>my reputation is not 10 so i can not make any attachment here thats why i needed to paste schema.xml on dpaste.org...</p>\n\n<p>sorry for the inconvenience it may have caused.</p>\n\n<p>i will realy apreciate your advice and sugessions ...</p>\n", "creation_date": 1329292604, "score": 0},
{"title": "Calling system() or IPC::Run3 commands from perl don&#39;t seem to pass environment variable ($ENV{JAVA_HOME})", "view_count": 654, "owner": {"user_id": 1214836, "answer_count": 13, "creation_date": 1329426377, "accept_rate": 75, "view_count": 32, "reputation": 315}, "is_answered": true, "answers": [{"last_edit_date": 1329452349, "owner": {"user_id": 139985, "accept_rate": 73, "link": "http://stackoverflow.com/users/139985/stephen-c", "user_type": "registered", "reputation": 391719}, "body": "<blockquote>\n  <p>The root of the problem is that the java process is missing the JAVA_HOME environment variable causing a ClassNotFoundException.</p>\n</blockquote>\n\n<p><strong>REVISED</strong></p>\n\n<p>That is not the root of the problem.  In fact, <em>Java itself</em> does not require JAVA_HOME to be set.</p>\n\n<p>The immediate cause of the problem is one of the following:</p>\n\n<ul>\n<li><p>The wrapper is not setting the <strong>classpath</strong> correctly for the application that you are trying to execute.  </p></li>\n<li><p>The wrapper using the wrong class name.  The class name \"nutch\" is unusual and suspicious - there's no package name.</p></li>\n</ul>\n\n<p>It seems likely that the real root cause is that you are assembling the argument list incorrectly.  Each of those arguments with a space inside them should really be two arguments; i.e.</p>\n\n<pre><code>        my @nutch_command = (\"$Config{nutch_binary}\",\n             \"crawl\", \"$Config{nutch_seed_dir}\",\n             \"-solr\", \"$Config{solr_url}\",\n             \"-d\", \"$Config{nutch_crawl_dir}\",\n             \"-threads\", \"1\",\n             \"-depth\", \"1\");\n</code></pre>\n\n<p>I suspect that this has confused the nutch wrapper script, and caused it to use the wrong classname (among other things).  When you pass the entire command as one string and let the shell parse it, the problem (naturally) goes away.</p>\n", "question_id": 9319745, "creation_date": 1329436139, "is_accepted": true, "score": 0, "last_activity_date": 1329452349, "answer_id": 9320904}], "question_id": 9319745, "tags": ["java", "perl", "nutch", "env"], "answer_count": 1, "link": "http://stackoverflow.com/questions/9319745/calling-system-or-ipcrun3-commands-from-perl-dont-seem-to-pass-environment", "last_activity_date": 1329505584, "accepted_answer_id": 9320904, "body": "<p>I've been struggling with launching a java process from perl. The root of the problem is that the java process is missing the <code>JAVA_HOME</code> environment variable causing a <code>ClassNotFoundException</code>.</p>\n\n<p>I started by using <code>IPC::Run3</code> because of its relatively elegant redirection of STDIN/STDOUT. </p>\n\n<p>Assuming <code>IPC::Run3</code> would use <code>%ENV</code>, I tried adding <code>$ENV{JAVA_HOME}</code>. </p>\n\n<p>When that didn't work I tried doing <code>system()</code>. That didn't work, so finally, I got it to work using <code>system(\"JAVA_HOME=/path/to/java &amp;&amp; /path/to/java_program\");</code></p>\n\n<p>My test program is below. Naturally I'd uncomment the proper block to test the appropriate invocation.</p>\n\n<pre><code>#!/usr/bin/perl -w\nuse strict;\n\nuse IPC::Run3;\n\nuse vars qw(%Config $nutch_stdout $nutch_stderr);\n\n%Config = (\n  'nutch_binary'       =&gt; q[/home/crawl/nutch/runtime/local/bin/nutch],\n  'nutch_crawl_dir'    =&gt; q[/home/crawl/nutch-crawl/crawl/crawldb/current/part-00000],\n  'nutch_seed_dir'     =&gt; q[/home/crawl/urls],\n  'solr_url'           =&gt; q[http://localhost:8080/solr],\n);\n\nmy @nutch_command = (\"$Config{nutch_binary}\",\n                 \"crawl $Config{nutch_seed_dir}\",\n                 \"-solr $Config{solr_url}\",\n                 \"-d    $Config{nutch_crawl_dir}\",\n                 \"-threads 1\",\n                 \"-depth 1\");\n\n$ENV{JAVA_HOME}       = '/usr/lib/jvm/java-1.6.0';\n\nwhile ((my $key,my $value) = each %ENV) {\n    print \"$key=$value\\n\";\n}\n\nprint \"Running @nutch_command\\n\";\n\n# My original code. Next few lines are shown in first batch of output below.\n#run3 \\@nutch_command, undef, \\$nutch_stdout, \\$nutch_stderr;\n#print \"Output from Nutch:\\n\";\n#print $nutch_stdout;\n#print \"Errors from Nutch:\\n\";\n#print $nutch_stderr;\n\n# Second try. The next line's output is the second batch of output.\n#system(@nutch_command);\n\n# Third try. Despite setting and displaying %ENV, this is the only thing I tried that worked\nsystem(\"JAVA_HOME=/usr/lib/jvm/java-1.6.0 &amp;&amp; @nutch_command\");\n</code></pre>\n\n<p>Here's the output of running the run3:</p>\n\n<pre><code>    -bash-3.2$ ./test.pl \n    ... [snip] ...\n    JAVA_HOME=/usr/lib/jvm/java-1.6.0\n    ... [snip] ...\n    Running /home/crawl/nutch/runtime/local/bin/nutch crawl /home/crawl/urls -solr http://localhost:8080/solr -d    /home/crawl/nutch-crawl/crawl/crawldb/current/part-00000 -threads 1 -depth 1\n    Output from Nutch:\n    Errors from Nutch:\n    Exception in thread \"main\" java.lang.NoClassDefFoundError: crawl\n    Caused by: java.lang.ClassNotFoundException: crawl\nat java.net.URLClassLoader$1.run(URLClassLoader.java:217)\nat java.security.AccessController.doPrivileged(Native Method)\nat java.net.URLClassLoader.findClass(URLClassLoader.java:205)\nat java.lang.ClassLoader.loadClass(ClassLoader.java:321)\nat sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:294)\nat java.lang.ClassLoader.loadClass(ClassLoader.java:266)\n    Could not find the main class: crawl. Program will exit.\n</code></pre>\n\n<p>And the output of the first system() call:</p>\n\n<pre><code>    -bash-3.2$ ./test.pl\n    ... [snip] ...\n    JAVA_HOME=/usr/lib/jvm/java-1.6.0\n    ... [snip] ...\n    Running /home/crawl/nutch/runtime/local/bin/nutch crawl /home/crawl/urls -solr http://localhost:8080/solr -d    /home/crawl/nutch-crawl/crawl/crawldb/current/part-00000 -threads 1 -depth 1\n    Exception in thread \"main\" java.lang.NoClassDefFoundError: crawl\n    Caused by: java.lang.ClassNotFoundException: crawl\nat java.net.URLClassLoader$1.run(URLClassLoader.java:217)\nat java.security.AccessController.doPrivileged(Native Method)\nat java.net.URLClassLoader.findClass(URLClassLoader.java:205)\nat java.lang.ClassLoader.loadClass(ClassLoader.java:321)\nat sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:294)\nat java.lang.ClassLoader.loadClass(ClassLoader.java:266)\n    Could not find the main class: crawl. Program will exit.\n</code></pre>\n\n<p>Finally, the third system call-- the only one that worked!-- with the environment variable set inline:</p>\n\n<pre><code>    -bash-3.2$ ./test.pl\n    ... [snip] ...\n    JAVA_HOME=/usr/lib/jvm/java-1.6.0\n    ... [snip] ...\n    Running /home/crawl/nutch/runtime/local/bin/nutch crawl /home/crawl/urls -solr http://localhost:8080/solr -d    /home/crawl/nutch-crawl/crawl/crawldb/current/part-00000 -threads 1 -depth 1\n    crawl started in: crawl-20120216133832\n    ... continue success stdout output\n</code></pre>\n\n<p>Finally to the question: Aside from having to set the environment in-line with the system() call, what's the appropriate way to pass an environment var to a IPC::Run3 or a system() call? </p>\n\n<p>(Note: output of %ENV is truncated to only relevant lines... lines like PATH, SHELL, _, etc. not relevant to the question omitted)</p>\n\n<p>In case it's relevant:</p>\n\n<pre><code>-bash-3.2$ uname -a\nLinux hostname 2.6.18-238.el5xen #1 SMP Thu Jan 13 16:41:45 EST 2011 x86_64 x86_64 x86_64 GNU/Linux\n-bash-3.2$ perl --version\nThis is perl, v5.8.8 built for x86_64-linux-thread-multi\n</code></pre>\n", "creation_date": 1329429676, "score": 0},
{"title": "Nutch web spider, index entire web", "view_count": 275, "owner": {"age": 22, "answer_count": 5, "creation_date": 1275852339, "user_id": 359844, "accept_rate": 73, "view_count": 383, "location": "United States", "reputation": 1345}, "is_answered": true, "answers": [{"last_edit_date": 1300240360, "owner": {"user_id": 40527, "accept_rate": 90, "link": "http://stackoverflow.com/users/40527/cody", "user_type": "registered", "reputation": 2473}, "body": "<p>I'm not framiliar with Nutch but this is just a regular expression.</p>\n\n<pre><code>+^http://([a-z0-9\\.])*\n</code></pre>\n\n<p>Would probably work just fine, or some variation thereof. Its just matching a pattern. The one I just wrote above should match anything starting with http:// and then any number of letters, numbers or dots.</p>\n", "question_id": 5320093, "creation_date": 1300239871, "is_accepted": true, "score": 0, "last_activity_date": 1300240360, "answer_id": 5320120}], "question_id": 5320093, "tags": ["regex", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/5320093/nutch-web-spider-index-entire-web", "last_activity_date": 1327174967, "accepted_answer_id": 5320120, "body": "<p>Alright, I've been messing around with <a href=\"http://nutch.apache.org/\" rel=\"nofollow\">Nutch</a> and need to know what parameter inside the <code>crawl-urlfilter.txt</code> file I edit so the spider has <strong>no boundaries</strong>. In other words I want it to roam around the web outside of a specified domain. </p>\n\n<p>I'm assuming it has to do with this line, but I don't know how to edit it correctly to do as I want it to:</p>\n\n<pre><code>+^http://([a-z0-9]*\\.)*urlz.net/\n</code></pre>\n", "creation_date": 1300239569, "score": -1},
{"title": "How to log execution of a nutch plugin", "view_count": 1384, "owner": {"user_id": 764618, "answer_count": 6, "creation_date": 1306052225, "accept_rate": 42, "view_count": 41, "reputation": 93}, "is_answered": true, "answers": [{"question_id": 8877272, "owner": {"user_id": 525590, "accept_rate": 75, "link": "http://stackoverflow.com/users/525590/jpee", "user_type": "registered", "reputation": 111}, "body": "<p>if your plugin has the class Variable</p>\n\n<pre><code>public static final Log LOG = LogFactory.getLog(YourClass.class\n        .getName());\n</code></pre>\n\n<p>And your called method has:</p>\n\n<pre><code>LOG.info(\"Your Logmessage\");\n</code></pre>\n\n<p>And you have built your plugin and configured that Nutch uses your plugin when fetching/crawling/... then the message is logged in the hadoop.log. </p>\n\n<p>When you built Nutch, does it say it is building your plugin?</p>\n\n<p>If yes, you can check your your plugin configuration, which is deployed to NutchHome/runtime/local/conf/nutch-site.xml:</p>\n\n<pre><code>&lt;property&gt;\n&lt;name&gt;plugin.includes&lt;/name&gt;\n&lt;value&gt;protocol-http|urlfilter-regex|parse-(html)|yourplugin&lt;/value&gt;\n&lt;description&gt;The plugins which are used in every crawl ordered by call-  order&lt;/description&gt;\n&lt;/property&gt;\n</code></pre>\n\n<p>If your plugin is configured in the runtime environment. If not change the configfile an rebuild Nutch. If that doesn't help, you can give me some more information.</p>\n", "creation_date": 1326795676, "is_accepted": true, "score": 0, "last_activity_date": 1326795676, "answer_id": 8892931}, {"question_id": 8877272, "owner": {"user_id": 764618, "accept_rate": 42, "link": "http://stackoverflow.com/users/764618/haya-aziz", "user_type": "registered", "reputation": 93}, "body": "<p>Solved by this code </p>\n\n<pre><code> import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n public static final Logger LOG = LoggerFactory.getLogger(\"org.apache.nutch.parse.html\");\n</code></pre>\n", "creation_date": 1327135115, "is_accepted": false, "score": 0, "last_activity_date": 1327135115, "answer_id": 8951705}], "question_id": 8877272, "tags": ["java", "plugins", "solr", "nutch", "web-crawler"], "answer_count": 2, "link": "http://stackoverflow.com/questions/8877272/how-to-log-execution-of-a-nutch-plugin", "last_activity_date": 1327135115, "accepted_answer_id": 8892931, "body": "<p>I am working hard in building custom <a href=\"http://wiki.apache.org/nutch/\" rel=\"nofollow\">nutch</a> plugin with special requirements.</p>\n\n<p>I found my plugin mentioned in <code>hadoop.log</code> but it was not relevant.</p>\n\n<p>I added <code>LOG.debug(\"test\")</code> and <code>LOG.info(\"test2\")</code> in different places of my code and I wrote in <code>log4j.properties</code></p>\n\n<pre><code># Logging for development\nlog4j.logger.org.apache.nutch.parse.html=DEBUG\n</code></pre>\n\n<p>My question is: where can I find these messages, I've checked <code>hadoop.log</code> but I couldn't find anything?</p>\n\n<p>How can I log / trace my java code? (I'm not using eclipse.)</p>\n\n<p>Thanks</p>\n", "creation_date": 1326702190, "score": 2},
{"title": "LanguageIdentifierUpdateProcessor solr", "view_count": 94, "is_answered": false, "question_id": 8910687, "tags": ["solr", "lucene", "full-text-search", "nutch", "apache-tika"], "answer_count": 0, "link": "http://stackoverflow.com/questions/8910687/languageidentifierupdateprocessor-solr", "last_activity_date": 1326892315, "owner": {"user_id": 1147008, "answer_count": 0, "creation_date": 1326432403, "accept_rate": 50, "view_count": 20, "reputation": 28}, "body": "<p>I am using LanguageIdentifierUpdateProcessor for detecting languge and accordingly applying analysers to fields in solr, in my solrconfig.xml i have whitelisted two languages en and hi, and threshold is 0.8. but i my case languages detected are 'fr' and 'it', and these are completely wrong . it detect hindi and english for very few docs. what could be the posiible reason for this. i tried using both tika's and googles cld .</p>\n", "creation_date": 1326892315, "score": 0},
{"title": "Nutch- how to delete old segments?", "view_count": 368, "is_answered": false, "answers": [{"question_id": 8563568, "owner": {"user_id": 1150329, "accept_rate": 89, "link": "http://stackoverflow.com/users/1150329/tejas-patil", "user_type": "registered", "reputation": 4793}, "body": "<pre><code>I can't know for sure that only the segments that were created in the latest crawl are used and all the others can be deleted, can I?\n</code></pre>\n\n<p>The segments created in the last crawl are useful and others can be deleted.</p>\n", "creation_date": 1326742499, "is_accepted": false, "score": 0, "last_activity_date": 1326742499, "answer_id": 8885315}], "question_id": 8563568, "tags": ["solr", "hadoop", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/8563568/nutch-how-to-delete-old-segments", "last_activity_date": 1326742499, "owner": {"user_id": 726706, "answer_count": 7, "creation_date": 1303887958, "accept_rate": 68, "view_count": 92, "reputation": 1044}, "body": "<p>In nutch, when I crawl and then re-crawl, duplicated segments are created.\nhow can I delete the old ones?</p>\n\n<p>I can't know for sure that only the segments that were created in the latest crawl are used and all the others can be deleted, can I?</p>\n", "creation_date": 1324308746, "score": 0},
{"title": "recrawl URLs in nutch 1.3", "view_count": 560, "is_answered": false, "answers": [{"question_id": 7872169, "owner": {"user_id": 525590, "accept_rate": 75, "link": "http://stackoverflow.com/users/525590/jpee", "user_type": "registered", "reputation": 111}, "body": "<p>I think you have found a solution by yourself in the last months but here is an answer for the community. The nutch-default.xml has 3 properties defined:</p>\n\n<pre><code>&lt;property&gt;\n &lt;name&gt;db.default.fetch.interval&lt;/name&gt;\n &lt;value&gt;30&lt;/value&gt;\n &lt;description&gt;(DEPRECATED) The default number of days between re-fetches of a page.\n &lt;/description&gt;\n&lt;/property&gt;\n\n&lt;property&gt;\n &lt;name&gt;db.fetch.interval.default&lt;/name&gt;\n &lt;value&gt;2592000&lt;/value&gt;\n &lt;description&gt;The default number of seconds between re-fetches of a page (30 days).\n &lt;/description&gt;\n&lt;/property&gt;\n\n&lt;property&gt;\n &lt;name&gt;db.fetch.interval.max&lt;/name&gt;\n &lt;value&gt;7776000&lt;/value&gt;\n &lt;description&gt;The maximum number of seconds between re-fetches of a page\n (90 days). After this period every page in the db will be re-tried, no\n matter what is its status.\n&lt;/description&gt;\n</code></pre>\n\n<p></p>\n\n<p>Which can be overridden in the nutch-site.xml.</p>\n", "creation_date": 1326729475, "is_accepted": false, "score": 0, "last_activity_date": 1326729475, "answer_id": 8882599}], "question_id": 7872169, "tags": ["nutch", "web-crawler"], "answer_count": 1, "link": "http://stackoverflow.com/questions/7872169/recrawl-urls-in-nutch-1-3", "last_activity_date": 1326729475, "owner": {"age": 29, "answer_count": 0, "creation_date": 1316882275, "user_id": 962809, "view_count": 10, "location": "Iran", "reputation": 1}, "body": "<p>I set re_crawler to fetch a site every day. but it fetch this site for 3 times.\nwhat property i should set in nutch? thanks.</p>\n", "creation_date": 1319439303, "score": 0},
{"title": "Using CrawlDbReader to read Nutch Crawl Data", "view_count": 789, "owner": {"user_id": 1138148, "answer_count": 0, "creation_date": 1326092481, "accept_rate": 80, "view_count": 20, "reputation": 35}, "is_answered": true, "answers": [{"question_id": 8785391, "owner": {"user_id": 726706, "accept_rate": 68, "link": "http://stackoverflow.com/users/726706/aaaa", "user_type": "registered", "reputation": 1044}, "body": "<p>Is there something special with these APIs that make this more than \"pass data from server to client\" issue?\nYou can use the APIs to get the data. Just look how they are used by nutch.sh, and how the main() is built and do something similar. Then pass the data to the client wither by XML or by JSON or any other way.</p>\n", "creation_date": 1326406236, "is_accepted": true, "score": 0, "last_activity_date": 1326406236, "answer_id": 8843065}], "question_id": 8785391, "tags": ["nutch", "web-crawler"], "answer_count": 1, "link": "http://stackoverflow.com/questions/8785391/using-crawldbreader-to-read-nutch-crawl-data", "last_activity_date": 1326406236, "accepted_answer_id": 8843065, "body": "<p>I am using nutch 1.4 to implement a focused crawler. Can anyone tell me how to use the nutch <strong>CrawlDbReader, LinkDbReader</strong> and <strong>SegmentReader APIs</strong> in my <strong>JSP</strong> program so that I can create custom UI for my project.\nSpecifically, I need to issue commands like <strong>readdb</strong>, <strong>readseg</strong> etc to the crawl data and get the output through a browser.</p>\n", "creation_date": 1326096101, "score": 0},
{"title": "Crawling Version Control System", "view_count": 374, "is_answered": true, "answers": [{"question_id": 8816740, "owner": {"user_id": 105672, "accept_rate": 86, "link": "http://stackoverflow.com/users/105672/pmr", "user_type": "registered", "reputation": 39851}, "body": "<p>Github comes with a JSON API. Use the repository API to get the list of repositories for a specific user and then clone them. Should be a matter of a few lines shell.</p>\n\n<p>See the API documentation <a href=\"http://develop.github.com/p/repo.html\" rel=\"nofollow\">here</a>.</p>\n", "creation_date": 1326304385, "is_accepted": false, "score": 2, "last_activity_date": 1326304385, "answer_id": 8824160}, {"question_id": 8816740, "owner": {"user_id": 3408, "accept_rate": 50, "link": "http://stackoverflow.com/users/3408/rjmunro", "user_type": "registered", "reputation": 15071}, "body": "<p>Nutch is a search engine, made by Apache, based on a Lucene backend.</p>\n\n<p>Take a look at github's robots.txt file:\n<a href=\"https://github.com/robots.txt\" rel=\"nofollow\">https://github.com/robots.txt</a></p>\n\n<p>Apart from specific engines, (e.g. google), it says:</p>\n\n<pre><code>User-agent: *\nDisallow: /\n</code></pre>\n\n<p>Therefore you cannot crawl GitHub with Nutch.</p>\n\n<p>Crawling github with a search engine seems like a bad idea. There will be a lot of similar pages that you would be downloading for no reason. What's wrong with GitHub's search?</p>\n\n<p>Please try to generalise your question. What do you hope to achieve by crawling github with Nutch? What kind of searches are you wanting to perform?</p>\n", "creation_date": 1326304919, "is_accepted": false, "score": 1, "last_activity_date": 1326304919, "answer_id": 8824289}], "question_id": 8816740, "tags": ["git", "svn", "version-control", "nutch", "web-crawler"], "answer_count": 2, "link": "http://stackoverflow.com/questions/8816740/crawling-version-control-system", "last_activity_date": 1326304919, "owner": {"age": 32, "answer_count": 8, "creation_date": 1250768160, "user_id": 159972, "accept_rate": 0, "view_count": 94, "location": "Karnataka, India", "reputation": 330}, "body": "<p>I want to crawl some kind of project on GitHub say I want to crawl source code which are created by particular author and bla bla constraints. Is there any plugin for Nutch to crawl this information or best way to get the whole repositories crawled.</p>\n\n<p>I even want to crawl version of publicly hosted version control system using Nutch. Is there any plugin available for the same.</p>\n", "creation_date": 1326273307, "score": 0},
{"title": "sample code in java by using apache nutch to get all links from a website", "view_count": 2000, "is_answered": false, "question_id": 8755774, "tags": ["java", "web-crawler", "nutch"], "answer_count": 0, "link": "http://stackoverflow.com/questions/8755774/sample-code-in-java-by-using-apache-nutch-to-get-all-links-from-a-website", "last_activity_date": 1325863582, "owner": {"age": 27, "answer_count": 10, "creation_date": 1312468915, "user_id": 878818, "view_count": 90, "location": "Turkey", "reputation": 246}, "body": "<p>I want  to get all links from any web site by using NUTCH in JAVA.\nIs there any code example that is writtten in java?</p>\n\n<p>for the example code my input is a domain name and output will be all links of the web site.</p>\n", "creation_date": 1325842225, "score": 0},
{"title": "Apache nutch: Manipulating the DOM before parsing", "view_count": 624, "owner": {"age": 31, "answer_count": 7, "creation_date": 1302685089, "user_id": 705638, "accept_rate": 56, "view_count": 99, "location": "Germany", "reputation": 516}, "is_answered": true, "answers": [{"question_id": 8576735, "owner": {"user_id": 222077, "link": "http://stackoverflow.com/users/222077/josegil", "user_type": "registered", "reputation": 317}, "body": "<p>You have some alternativer for doing that:</p>\n\n<ul>\n<li><p>You can write a plugin for nutch for doing that. This blog have an execelent example of doing a plugin in nutch: <a href=\"http://sujitpal.blogspot.com/2009/07/nutch-custom-plugin-to-parse-and-add.html\" rel=\"nofollow\">http://sujitpal.blogspot.com/2009/07/nutch-custom-plugin-to-parse-and-add.html</a></p></li>\n<li><p>Using an extractor content: Here <a href=\"http://tomazkovacic.com/blog/122/evaluating-text-extraction-algorithms/\" rel=\"nofollow\">http://tomazkovacic.com/blog/122/evaluating-text-extraction-algorithms/</a> have some algorithmics. Maybe the best way of doing that it\u00b4s also in a pluggin in nutch.</p></li>\n</ul>\n", "creation_date": 1324671551, "is_accepted": true, "score": 3, "last_activity_date": 1324671551, "answer_id": 8620277}], "question_id": 8576735, "tags": ["java", "search", "indexing", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/8576735/apache-nutch-manipulating-the-dom-before-parsing", "last_activity_date": 1324671551, "accepted_answer_id": 8620277, "body": "<p>I want to remove specific elements from the page response, before it is handed down to nutch.\nSpecifically, I want to mark parts of my pages with i.e.</p>\n\n<pre><code> &lt;div class=\"noindex\"&gt;I shall not be indexed&lt;/div&gt;\n</code></pre>\n\n<p>And want to remove them before nutch parse, so that \"I shall not be indexed\" is not present in the NutchDocument afterwards. I plan die surround my navigation, header, footer content with this because right now, they are present in every document in the index.</p>\n\n<p>Thanks,\nPaul</p>\n", "creation_date": 1324390190, "score": 0},
{"title": "Search implementation on classified site", "view_count": 203, "is_answered": true, "answers": [{"question_id": 8579612, "owner": {"user_id": 266143, "accept_rate": 20, "link": "http://stackoverflow.com/users/266143/codecaster", "user_type": "registered", "reputation": 77595}, "body": "<p>Use Google's <a href=\"http://www.google.com/cse/\" rel=\"nofollow\">Custom Search</a>. </p>\n\n<p>Or, simply redirect your users to Google:</p>\n\n<pre><code>&lt;form method=\"get\" action=\"http://www.google.com/search\"&gt;\n    &lt;input type=\"text\" name=\"q\" /&gt;\n    &lt;input type=\"submit\" value=\"Google Search\" /&gt;\n&lt;/form&gt;\n</code></pre>\n", "creation_date": 1324403519, "is_accepted": false, "score": 1, "last_activity_date": 1324403519, "answer_id": 8579707}, {"last_edit_date": 1324405818, "owner": {"user_id": 1108371, "accept_rate": 100, "link": "http://stackoverflow.com/users/1108371/prym", "user_type": "registered", "reputation": 348}, "body": "<p>You can use <a href=\"http://developer.yahoo.com/search/boss/\" rel=\"nofollow\">Yahoo BOSS api</a> for web search if you are willing to pay, since it is a paid api. It is a great way to customize your search</p>\n\n<p>For free APIs you can have a look at <a href=\"http://www.programmableweb.com/apis/directory/1?apicat=Search\" rel=\"nofollow\">programmableweb</a>. Not sure whether there is a free web search API or not.</p>\n", "question_id": 8579612, "creation_date": 1324404735, "is_accepted": false, "score": 1, "last_activity_date": 1324405818, "answer_id": 8579982}], "question_id": 8579612, "tags": ["php", "search", "solr", "nutch", "web-search"], "answer_count": 2, "link": "http://stackoverflow.com/questions/8579612/search-implementation-on-classified-site", "last_activity_date": 1324492124, "owner": {"user_id": 1004350, "view_count": 14, "answer_count": 0, "creation_date": 1319077791, "reputation": 67}, "body": "<p>I am presently working on a classified site which is developed in PHP. I want to implement two types of search in our site: </p>\n\n<ol>\n<li>Search from our own site </li>\n<li>Search the web. </li>\n</ol>\n\n<p>I was about to implement Solr for the first search, but am not sure how to implement 'Search the web' search. So can you please provide me suggestions on that, I have come to know about Nutch crawler, but dont know if that is the right choice. www.scrubtheweb.com  is the sample site with the implementation of both kinds(search the web and site search) of search. I want to implement search exactly same as that of www.scrubtheweb.com .</p>\n\n<p>So please provide me suggestions on that</p>\n", "creation_date": 1324402987, "score": 0},
{"title": "Solr: I have set `hl=true` but no summaries are being output", "view_count": 447, "is_answered": true, "answers": [{"question_id": 8356746, "owner": {"user_id": 1108393, "link": "http://stackoverflow.com/users/1108393/d-whelan", "user_type": "registered", "reputation": 667}, "body": "<p>Looks like you aren't specifying the field to highlight (hl.fl). You should create a text field to use for highlighting (don't use string type) and have it stored/indexed.</p>\n", "creation_date": 1324407569, "is_accepted": false, "score": 1, "last_activity_date": 1324407569, "answer_id": 8580532}], "question_id": 8356746, "tags": ["solr", "lucene", "search-engine", "information-retrieval", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/8356746/solr-i-have-set-hl-true-but-no-summaries-are-being-output", "last_activity_date": 1324407569, "owner": {"age": 33, "answer_count": 20, "creation_date": 1291812750, "user_id": 534994, "accept_rate": 93, "view_count": 246, "location": "Seattle, WA", "reputation": 3887}, "body": "<p>I need to get snippets from documents where the query terms are matched to be able to output results similar to Google's snippet beneath the website URL. For example:</p>\n\n<blockquote>\n  <p>Snippet - Wikipedia, the free encyclopedia<br>\n  en.wikipedia.org/wiki/Snippet<br>\n  A snippet is defined as a small piece of something, it may in more  specific contexts refer to: Sampling (music), the use of a short phrase  of a recording as an ...</p>\n</blockquote>\n\n<p>I have set <code>hl=true</code> and even <code>hl.fl='*'</code> in the query URL and but no summaries are being output.</p>\n\n<p><a href=\"http://wiki.apache.org/solr/FAQ#I_have_set_.60hl.3Dtrue.60_but_no_summaries_are_being_output\" rel=\"nofollow\">Solr FAQs</a> say: </p>\n\n<blockquote>\n  <p>For a field to be summarizable it must be both stored and indexed.</p>\n</blockquote>\n\n<p>I'm using Nutch and Solr and have set them up using <a href=\"http://wiki.apache.org/nutch/NutchTutorial#A4._Setup_Solr_for_search\" rel=\"nofollow\">this tutorial</a>. What additional steps to I need to take to be able to do this?</p>\n\n<hr>\n\n<p>Adding sample query and output:</p>\n\n<blockquote>\n  <p><a href=\"http://localhost:8983/solr/select/?q=test&amp;version=2.2&amp;start=0&amp;rows=10&amp;indent=on&amp;hl=true\" rel=\"nofollow\">http://localhost:8983/solr/select/?q=test&amp;version=2.2&amp;start=0&amp;rows=10&amp;indent=on&amp;hl=true</a></p>\n</blockquote>\n\n<pre><code>&lt;response&gt;\n&lt;lst name=\"responseHeader\"&gt;\n&lt;int name=\"status\"&gt;0&lt;/int&gt;\n&lt;int name=\"QTime\"&gt;57&lt;/int&gt;\n&lt;lst name=\"params\"&gt;\n&lt;str name=\"indent\"&gt;on&lt;/str&gt;\n&lt;str name=\"start\"&gt;0&lt;/str&gt;\n&lt;str name=\"q\"&gt;test&lt;/str&gt;\n&lt;str name=\"hl\"&gt;true&lt;/str&gt;\n&lt;str name=\"version\"&gt;2.2&lt;/str&gt;\n&lt;str name=\"rows\"&gt;10&lt;/str&gt;\n&lt;/lst&gt;\n&lt;/lst&gt;\n&lt;result name=\"response\" numFound=\"94\" start=\"0\"&gt;\n&lt;doc&gt;\n&lt;arr name=\"anchor\"&gt;\n&lt;str&gt;User:Sir Lestaty de Lioncourt&lt;/str&gt;\n&lt;/arr&gt;\n&lt;float name=\"boost\"&gt;0.0&lt;/float&gt;\n&lt;str name=\"digest\"&gt;6c27160d0b08068f3873bb2c063508b3&lt;/str&gt;\n&lt;str name=\"id\"&gt;\nhttp://aa.wikibooks.org/wiki/User:Sir_Lestaty_de_Lioncourt\n&lt;/str&gt;\n&lt;str name=\"segment\"&gt;20111029223245&lt;/str&gt;\n&lt;str name=\"title\"&gt;User:Sir Lestaty de Lioncourt - Wikibooks&lt;/str&gt;\n&lt;date name=\"tstamp\"&gt;2011-10-29T21:34:27.055Z&lt;/date&gt;\n&lt;str name=\"url\"&gt;\nhttp://aa.wikibooks.org/wiki/User:Sir_Lestaty_de_Lioncourt\n&lt;/str&gt;\n&lt;/doc&gt;\n...\n&lt;/result&gt;\n&lt;lst name=\"highlighting\"&gt;\n&lt;lst name=\"http://aa.wikibooks.org/wiki/User:Sir_Lestaty_de_Lioncourt\"/&gt;\n&lt;lst name=\"http://aa.wikipedia.org/wiki/User:PipepBot\"/&gt;\n&lt;lst name=\"http://aa.wikipedia.org/wiki/User:Purodha\"/&gt;\n...\n&lt;/lst&gt;\n&lt;/response&gt;\n</code></pre>\n", "creation_date": 1322831223, "score": 2},
{"title": "rss feeds in nutch", "view_count": 1765, "is_answered": false, "answers": [{"question_id": 1091530, "owner": {"user_id": 37379, "accept_rate": 89, "link": "http://stackoverflow.com/users/37379/sam", "user_type": "registered", "reputation": 6017}, "body": "<p>You can find many <a href=\"http://lucene.apache.org/nutch/mailing%5Flists.html\" rel=\"nofollow\">nutch experts here</a></p>\n", "creation_date": 1247164864, "is_accepted": false, "score": 0, "last_activity_date": 1247164864, "answer_id": 1105764}, {"question_id": 1091530, "owner": {"user_id": 222077, "link": "http://stackoverflow.com/users/222077/josegil", "user_type": "registered", "reputation": 317}, "body": "<p>Nutch has a plugin for parsing that kind of data. You have only two things to do: Active that plugin in your nutch-site.xml file config adding the plugin \"feed\". This make the pareser necesary for updating the solr index with metadata related feeds.</p>\n", "creation_date": 1323947923, "is_accepted": false, "score": 0, "last_activity_date": 1323947923, "answer_id": 8519234}], "question_id": 1091530, "tags": ["nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/1091530/rss-feeds-in-nutch", "last_activity_date": 1323947923, "owner": {"user_type": "does_not_exist"}, "body": "<p>Actually i ma newbie to nutch. i want to khnow is there any way we crawl a rss feed then customize the parse data so that index can hv different fields from rss.\nlike\nSuppose the rss feed hav a field source in item. i want to index this field..</p>\n\n<p>thanxx\nvibs </p>\n", "creation_date": 1246962172, "score": -1},
{"title": "Nutch 1.4 integration with Solr 3.5 http.agent.name property", "view_count": 953, "is_answered": true, "answers": [{"question_id": 8397160, "owner": {"user_id": 1086057, "link": "http://stackoverflow.com/users/1086057/jonny", "user_type": "unregistered", "reputation": 31}, "body": "<p>As per the Nutch tutorial you should add an agent name in the following file:\n    ./conf/nutch-default.xml\ne.g.\n    vi /opt/nutch/conf/nutch-default.xml\nChange:\nhttp.agent.name to:</p>\n\n<pre><code>    &lt;name&gt;http.agent.name&lt;/name&gt;\n    &lt;value&gt;nutch&lt;/value&gt;\n</code></pre>\n\n<p>Also set values for http.agent.url and http.agent.email\nfor http.robots.agents - make sure the first entry matches your http.agent.name i.e.\n        http.robots.agents\n        nutch,*</p>\n", "creation_date": 1323274649, "is_accepted": false, "score": 3, "last_activity_date": 1323274649, "answer_id": 8418612}], "question_id": 8397160, "tags": ["java", "apache", "search", "solr", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/8397160/nutch-1-4-integration-with-solr-3-5-http-agent-name-property", "last_activity_date": 1323274649, "owner": {"age": 29, "answer_count": 3, "creation_date": 1314253775, "user_id": 911336, "view_count": 6, "reputation": 36}, "body": "<p>I have been trying to setup Nutch with Solr, but getting the following exception</p>\n\n<p>Fetcher: No agents listed in 'http.agent.name' property.</p>\n\n<blockquote>\n  <p>Exception in thread \"main\" java.lang.IllegalArgumentException: Fetcher: No agents listed in http.agent.name' property.</p>\n</blockquote>\n", "creation_date": 1323160806, "score": 1},
{"title": "Getting snippets in Solr", "view_count": 1794, "owner": {"age": 33, "answer_count": 20, "creation_date": 1291812750, "user_id": 534994, "accept_rate": 93, "view_count": 246, "location": "Seattle, WA", "reputation": 3887}, "is_answered": true, "answers": [{"last_edit_date": 1322594455, "owner": {"user_id": 167980, "accept_rate": 100, "link": "http://stackoverflow.com/users/167980/paige-cook", "user_type": "registered", "reputation": 18612}, "body": "<p>You need to specify the fields that you want highlight results returned for by passing field names to the <code>hl.fl</code> parameter in your query. Please see <a href=\"http://wiki.apache.org/solr/HighlightingParameters\" rel=\"nofollow\">HighlightingParameters</a> on the Solr Wiki for more details and examples.</p>\n", "question_id": 8309507, "creation_date": 1322568937, "is_accepted": true, "score": 1, "last_activity_date": 1322594455, "answer_id": 8310527}], "question_id": 8309507, "tags": ["solr", "lucene", "search-engine", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/8309507/getting-snippets-in-solr", "last_activity_date": 1322594455, "accepted_answer_id": 8310527, "body": "<p>I'm running Solr + Nutch and need to get a snippet of each result. I tried setting <code>hl</code> to <code>true</code> in the query URL but I still get the same XML result (without snippets). Any ideas on how to get this done?</p>\n", "creation_date": 1322563785, "score": 1},
{"title": "delete url from crawldb in nutch 1.3?", "view_count": 801, "is_answered": false, "answers": [{"question_id": 8118983, "owner": {"user_id": 608167, "accept_rate": 50, "link": "http://stackoverflow.com/users/608167/varshith", "user_type": "registered", "reputation": 252}, "body": "<p>To read from crawlDb you can use the CrawlDBReader class(org.apache.nutch.crawl package). To delete/remove a url from the crawlDb you can use try using the CrawlDBMerger class(org.apache.nutch.crawl package) with the \"-filter\" option. But I suggest writing a Mapreduce to delete urls according to your needs.</p>\n", "creation_date": 1321335842, "is_accepted": false, "score": 0, "last_activity_date": 1321335842, "answer_id": 8132005}], "question_id": 8118983, "tags": ["nutch", "web-crawler", "urlfetch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/8118983/delete-url-from-crawldb-in-nutch-1-3", "last_activity_date": 1321335842, "owner": {"user_id": 1045194, "answer_count": 0, "creation_date": 1321258730, "accept_rate": 0, "view_count": 19, "reputation": 14}, "body": "<p>I crawl sites in nutch 1.3. now I want to delete a url from crawldb, how can I do this? how I read from crawldb? I want to see urls that exist in crawldb.</p>\n", "creation_date": 1321259454, "score": 0},
{"title": "Sentences as documents in Nutch", "view_count": 291, "owner": {"age": 33, "answer_count": 20, "creation_date": 1291812750, "user_id": 534994, "accept_rate": 93, "view_count": 246, "location": "Seattle, WA", "reputation": 3887}, "is_answered": true, "answers": [{"question_id": 7948481, "owner": {"user_id": 629122, "accept_rate": 69, "link": "http://stackoverflow.com/users/629122/marcorossi", "user_type": "registered", "reputation": 889}, "body": "<p>Yes, you can check out Nutch for your task.</p>\n\n<p>1) configuration files alone will not do the job for you. see points above.</p>\n\n<p>2) you'd need to write your own Parser plugin that hooks to nutch parsing phase after crawls, split your HTMLpage into sentences and return N results from a single page. This is quite odd as usually one page is one result. Check out the FeedParser to see how to return multiple results from one page.</p>\n\n<p>3) in principle, you could iterate over the pages fetched by nutch, get the text, split them in sentences and use SOLR api to index your sentences as if they were docs. this could even be a mapreduce job quite easily.</p>\n\n<p>As a general reference I suggest you have a look at this article for splitting your text in sentences:</p>\n\n<p><a href=\"http://sujitpal.blogspot.com/2011/04/uima-sentence-annotator-using-opennlp.html\" rel=\"nofollow\">http://sujitpal.blogspot.com/2011/04/uima-sentence-annotator-using-opennlp.html</a></p>\n", "creation_date": 1320288527, "is_accepted": true, "score": 1, "last_activity_date": 1320288527, "answer_id": 7989740}], "question_id": 7948481, "tags": ["search", "lucene", "indexing", "web-crawler", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/7948481/sentences-as-documents-in-nutch", "last_activity_date": 1320288527, "accepted_answer_id": 7989740, "body": "<p>I need Nutch to split web pages into sentences when saving the crawl results. The reason is so that Solr sees each sentence as a document when indexing.</p>\n\n<p>The result I need is to be able to do a search for, say, \"one word\" and get a list of all sentences that contain \"one\" and/or \"word\".</p>\n\n<p>I'm new to Nutch so some pointers would really be useful...</p>\n\n<ol>\n<li>Should I look into Nutch configuration files?</li>\n<li>Do I need to change Nutch source code?</li>\n<li>Or can I write a separate app that can edit the crawl results once Nutch is done crawling?</li>\n</ol>\n", "creation_date": 1320014993, "score": 1},
{"title": "Apache Nutch on Windows", "view_count": 2658, "owner": {"age": 33, "answer_count": 28, "creation_date": 1233349032, "user_id": 60810, "accept_rate": 88, "view_count": 315, "reputation": 1475}, "is_answered": true, "answers": [{"last_edit_date": 1317547698, "owner": {"user_id": 60810, "accept_rate": 88, "link": "http://stackoverflow.com/users/60810/pablote", "user_type": "registered", "reputation": 1475}, "body": "<p>Well I've found the solution to this: uname was installed that was not the problem. The weird thing is that the shell scripts are in DOS text format instead of Unix text format. Using the 'd2u' command to convert this files fixed the problem.</p>\n", "question_id": 546215, "creation_date": 1234549371, "is_accepted": true, "score": 1, "last_activity_date": 1317547698, "answer_id": 547110}, {"question_id": 546215, "owner": {"user_id": 84855, "link": "http://stackoverflow.com/users/84855/dstine", "user_type": "registered", "reputation": 251}, "body": "<p>The problem isn't that uname is missing.  The problem is that the nutch script has incorrect line termination.  I do the following:</p>\n\n<p>1) open nutch in vi</p>\n\n<p>2) :set ff=unix</p>\n\n<p>3) :wq</p>\n", "creation_date": 1238901375, "is_accepted": false, "score": 1, "last_activity_date": 1238901375, "answer_id": 718277}], "question_id": 546215, "tags": ["windows", "cygwin", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/546215/apache-nutch-on-windows", "last_activity_date": 1317547698, "accepted_answer_id": 547110, "body": "<p>Has anyone tryed to install Nutch on Windows? I'm following this installation guide: <a href=\"http://zillionics.com/resources/articles/NutchGuideForDummies.htm\" rel=\"nofollow\">http://zillionics.com/resources/articles/NutchGuideForDummies.htm</a></p>\n\n<p>After a few bumps I'm stuck trying to run the crawler. It gives me this error: </p>\n\n<p>bin/nutch: line 15: syntax error near unexpected token '$'in\\r''\n'in/nutch: line 15: 'case \"'uname'\" in</p>\n\n<p>Apparently I need to install the uname utility in cygwin but I can't find it anywhere. Does anyone know in which package is it in or if there another way to fix this?</p>\n", "creation_date": 1234536623, "score": 2},
{"title": "Exploring nutch over hadoop", "view_count": 415, "owner": {"age": 25, "answer_count": 7, "creation_date": 1315727263, "user_id": 938959, "accept_rate": 70, "view_count": 80, "location": "Bangalore, India", "reputation": 394}, "is_answered": true, "answers": [{"question_id": 7564545, "owner": {"user_id": 484046, "accept_rate": 81, "link": "http://stackoverflow.com/users/484046/millebii", "user_type": "registered", "reputation": 976}, "body": "<p>If you want to only do Map/Reduce jobs you don't need Nutch but Hadoop only.  Hadoop brings you a cluster file system and a scheduler for map/reduce jobs on the filesystem.</p>\n\n<p>As Nutch builds on top of Hadoop you can create your own map/reduce jobs on Nutch data as long as you understand the data structure and what the crawler is doing.</p>\n\n<p>However if you only wanted to run some map/reduce jobs, just install hadoop and off you go.</p>\n", "creation_date": 1317381880, "is_accepted": true, "score": 1, "last_activity_date": 1317381880, "answer_id": 7609669}], "question_id": 7564545, "tags": ["hadoop", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/7564545/exploring-nutch-over-hadoop", "last_activity_date": 1317381880, "accepted_answer_id": 7609669, "body": "<p>What possibly can i do with Hadoop and Nutch used as a search engine ? I know that nutch is used to build a web crawler . But i'm not finding the perfect picture . Can i use mapreduce with nutch and do some mapreduce job ? Any ideas are welcome . Few links will be greatly appreciated . Thanks.</p>\n", "creation_date": 1317100958, "score": 0},
{"title": "What Nutch is all about?", "view_count": 293, "is_answered": true, "answers": [{"question_id": 4464561, "owner": {"user_id": 3333, "accept_rate": 83, "link": "http://stackoverflow.com/users/3333/paul-tomblin", "user_type": "registered", "reputation": 113417}, "body": "<p>Nutch is a full featured search engine - it can crawl external web sites, and it understands and respects robots.txt.</p>\n\n<p><a href=\"http://nutch.apache.org/about.html\" rel=\"nofollow\">http://nutch.apache.org/about.html</a></p>\n\n<blockquote>\n  <p>Overview Nutch is open source\n  web-search software. It builds on\n  Lucene and Solr, adding web-specifics,\n  such as a crawler, a link-graph\n  database, parsers for HTML and other\n  document formats, etc.</p>\n  \n  <p>Nutch can run on a single machine, but\n  gains a lot of its strength from\n  running in a Hadoop cluster</p>\n  \n  <p>The system can be enhanced (eg other\n  document formats can be parsed) using\n  a plugin mechanism.</p>\n  \n  <p>For more information about Nutch,\n  please see the Nutch wiki.</p>\n</blockquote>\n", "creation_date": 1292527551, "is_accepted": false, "score": 2, "last_activity_date": 1292527551, "answer_id": 4464592}, {"question_id": 4464561, "owner": {"user_id": 446591, "accept_rate": 67, "link": "http://stackoverflow.com/users/446591/brad-mace", "user_type": "registered", "reputation": 16406}, "body": "<p>Nutch is a ready-made, configurable web crawler with a Java Servlet for performing searches.  If you wanted to do this as a project, Nutch probably does too much since all that's left is creating the pages for entering searches and displaying results.</p>\n", "creation_date": 1292527940, "is_accepted": false, "score": 0, "last_activity_date": 1292527940, "answer_id": 4464637}], "question_id": 4464561, "tags": ["search-engine", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/4464561/what-nutch-is-all-about", "last_activity_date": 1317381769, "owner": {"user_id": 545234, "answer_count": 0, "creation_date": 1292527294, "accept_rate": 0, "view_count": 34, "reputation": 915}, "body": "<p>Im going to make my own search engine.</p>\n\n<p>When searching about search engine, crawler, and so on, I confused about <a href=\"http://nutch.apache.org/\" rel=\"nofollow\">Nutch</a>.</p>\n\n<p>I don\u2019t understand what is Nutch. Is it for internal use like Lucene (correct me if Im wrong) or a framework for creating a search engine (example:google, bing, yahoo)?</p>\n", "creation_date": 1292527294, "score": -1},
{"title": "Parsing html data with nutch 1.0 and a custom plugin", "view_count": 2066, "is_answered": true, "answers": [{"question_id": 800059, "owner": {"user_id": 259705, "accept_rate": 62, "link": "http://stackoverflow.com/users/259705/mlathe", "user_type": "registered", "reputation": 1917}, "body": "<p>I remember making a nutch HTML parsing plugin for a past work. I don't have access to how I did it exactly, but here are the basic points. We wanted to do the following:</p>\n\n<ol>\n<li>parse an HTML page but conditionally use a H1 tag or a tag with a certain class as the page title rather than the actual //html/head/title</li>\n<li>There were some special pieces of data that were sometimes on the page (ie what tab was selected, which would tell us if this was a retail customer, a bank customer, or a corporate customer).</li>\n<li>etc.</li>\n</ol>\n\n<p>What I did was just find the html-parse plugin class (I'm having trouble finding the actual class name), and extend it. Then override the parsing function. The new function should call the <code>super</code> function and then can walk the DOM tree to find the special data you are looking for. In my case I'd look for a better title and then override the value that the <code>super</code> function came up with. </p>\n\n<p>For your second question, I'm not clear what you are asking about. I think you are asking what happens when the DOM isn't well formed? I would just dig through the nutch code (http://grepcode.com/snapshot/repo1.maven.org/maven2/org.apache.nutch/nutch/1.3/) and find out how the parsing is done (i'm sure they use a library to do it). That should tell you more about if things are greedy, or what.</p>\n\n<p>Holler if you have questions.</p>\n", "creation_date": 1317056754, "is_accepted": false, "score": 1, "last_activity_date": 1317056754, "answer_id": 7558587}], "question_id": 800059, "tags": ["html-parsing", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/800059/parsing-html-data-with-nutch-1-0-and-a-custom-plugin", "last_activity_date": 1317056754, "owner": {"age": 37, "answer_count": 6, "creation_date": 1235171831, "user_id": 69150, "view_count": 68, "location": "Sarpsborg, Norway", "reputation": 166}, "body": "<p>I am currently trying to write a custom plugin for nutch 1.0. This plugin is supposed to parse html data and filter out relevant information from documents. I have a basic plugin working, it extends the HtmlParserResult object and is executed each time I do a parse.</p>\n\n<p>My problems are two faced at the moment: </p>\n\n<ol>\n<li><p>I do not understand the workflow/pipline of the nutch parsing good enough. I do not find the information about this on the nutch site. </p></li>\n<li><p>I do not understand how the DOM parsing is done, I see that Nutch have set of DOM objects and that the HtmlParser plugin does some DOM parsing, still I have not figured out how this is best done.</p></li>\n</ol>\n", "creation_date": 1240956805, "score": 3},
{"title": "How can I ensure certain hadoop jobs dont end up running in the same datanode when using Fair Scheduler?", "view_count": 117, "is_answered": false, "answers": [{"question_id": 7500780, "owner": {"user_id": 352268, "accept_rate": 94, "link": "http://stackoverflow.com/users/352268/donald-miner", "user_type": "registered", "reputation": 24349}, "body": "<p>I'm not sure if you want to do something like this because it will impact the rest of your Hadoop cluster...</p>\n\n<p>You can set the number of reduce slots per node to 1. The configuration parameter you want to change for this is <code>mapred.tasktracker.reduce.tasks.maximum</code>.</p>\n", "creation_date": 1316617834, "is_accepted": false, "score": 0, "last_activity_date": 1316617834, "answer_id": 7502185}], "question_id": 7500780, "tags": ["hadoop", "scheduling", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/7500780/how-can-i-ensure-certain-hadoop-jobs-dont-end-up-running-in-the-same-datanode-wh", "last_activity_date": 1316617834, "owner": {"user_id": 608167, "answer_count": 5, "creation_date": 1297170796, "accept_rate": 50, "view_count": 14, "reputation": 252}, "body": "<p>When using the nutch crawler, the fetch jobs are created such that URLs from same host end up in a single data node to maintain crawl politeness(1 QPS). However, certain hosts allow more than 1 QPS and so the URLs are partitioned accordingly. For such hosts, the URLs will be in two fetch jobs meant to be run on two different data nodes. But sometimes Fair scheduler schedules those jobs(reduce tasks) to the same data node. </p>\n\n<p>So is there any way to work around this issue?</p>\n\n<p>Any help is greatly appreciated.</p>\n\n<p>Thanks</p>\n", "creation_date": 1316612560, "score": 2},
{"title": "Exclude duplicate results from Solr query based on highlight snippets?", "view_count": 836, "owner": {"age": 41, "answer_count": 5, "creation_date": 1292948849, "user_id": 550199, "view_count": 16, "location": "Naples, ME", "reputation": 65}, "is_answered": true, "answers": [{"question_id": 7418778, "owner": {"user_id": 484046, "accept_rate": 81, "link": "http://stackoverflow.com/users/484046/millebii", "user_type": "registered", "reputation": 976}, "body": "<p>In the base shipping Nutch (not Solr) there is a clustering mechanism, I don't really know how it works but it does something which I had to remove. Have you looked at that ? </p>\n\n<p>Another idea popping to mind would be : to  index separetely real content, from navigational snippets. And at search time you apply a heigher query weight to 'real content' field.</p>\n\n<p>Which would pull forward pages with 'solder' as content as opposed to pages with only 'solder' as navigation and yet you keep all pages just in case.</p>\n\n<p>Hope I understood your problem correctly.</p>\n", "creation_date": 1316182596, "is_accepted": false, "score": 0, "last_activity_date": 1316182596, "answer_id": 7446059}, {"question_id": 7418778, "owner": {"user_id": 914705, "accept_rate": 100, "link": "http://stackoverflow.com/users/914705/mike-sokolov", "user_type": "registered", "reputation": 4825}, "body": "<p>I don't thinkthere is any way of doing exactly what you are asking, except post-processing that would be up to you, and not very efficient for larger result sets.</p>\n\n<p>Maybe you should ask a different question if the documents being returned are actually quite different, even though the snippets are identical.  If the documents are different, presumably there is value in showing them all, rather than de-duplicating.  </p>\n\n<p>You could try enhancing the search result display to show more information about the documents so that the user can discriminate amongst them - maybe not relying on highlighting, but showing some other parts of the document as well?</p>\n\n<p>I really do think though that at the heart of the problem is the need to make matches found in site boilerplate less relevant than matches found elsewhere.  Usually relevance ranking does a good job of this because the common terms are much less important for relevance ranking, but if you are mixing documents from a wide range of different sites you might find the effect less pronounced - since oft-repeated terms on one site could be very unique on another site.  If your results are truly segmented by site, you might consider creating separate indexes (cores) for each site - this would have the effect of performing the relevance calculations in a site-specific way, and might help with this problem.</p>\n", "creation_date": 1316194067, "is_accepted": true, "score": 0, "last_activity_date": 1316194067, "answer_id": 7448330}], "question_id": 7418778, "tags": ["html", "search", "solr", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/7418778/exclude-duplicate-results-from-solr-query-based-on-highlight-snippets", "last_activity_date": 1316194067, "accepted_answer_id": 7448330, "body": "<p><strong>The scene:</strong></p>\n\n<p>I have indexed many websites using Nutch and Solr.  I've implemented result grouping by site.  My results output includes the page title, highlight snippets and URL. My issue is with the page navigation/copyright/company info bits that appear on many company sites.</p>\n\n<p>A query for \"solder\", for example, may return 200+ results for a particular site -- but only a handful of the results are actually appropriate; perhaps the company's site structure includes \"solder\" on every page as part of their core business description, site navigation, etc.  There are relevant results to see, but they're flooded by the irrelevant, repetitive matches from the other pages on the site.</p>\n\n<p><strong>The problem:</strong></p>\n\n<p>I've seen other postings asking how to prevent Nutch and Solr from indexing site headers, footers, navigation and others but with such a diverse group of sites, this approach just isn't feasible.  What I'm observing, however, is that although the content for each result is significantly different, the highlighted snippets returned are 90-100% identical for the results I don't want. Observe:</p>\n\n<pre><code>Products | Alloy Information || --------\n-Free Solutions Halogen-Free Products Sales Contacts Technical Articles Industry Links Terms &amp; Conditions Products Support Site Map Lead-Free Solutions Halogen-Free Products Sales   Contacts Technical Articles Industry\nhttp://www.--------.com/Products/AlloyInformation.aspx\n\nProducts | Chemicals &amp; Cleaners || --------\n-Free Solutions Halogen-Free Products Sales Contacts Technical Articles Industry Links Terms &amp; Conditions Products Industrial Division   Products Services News Support Site Map Lead-Free Solutions Halogen-Free Products Sales\nhttp://www.--------.com/Products/ChemicalsCleaners.aspx\n\nProducts | Rosin Based || --------\n-Free Solutions Halogen-Free Products Sales Contacts Technical Articles Industry Links Terms &amp; Conditions Products   Products Services News Support Site Map Lead-Free Solutions Halogen-Free Products Sales Contacts Technical\nhttp://www.--------.com/Products/RosinBased.aspx\n\nSupport | Engineering Guide || --------\n-Free Solutions Halogen-Free Products Sales Contacts Technical Articles Industry Links Terms &amp; Conditions Support   Products Services News Support Site Map Lead-Free Solutions   Halogen-Free Products Sales Contacts Technical\nhttp://www.--------.com/Support/EngineeringGuide.aspx\n</code></pre>\n\n<p><strong>The Big Idea:</strong></p>\n\n<p>This leads me to the question of if I can filter or group results based on the highlighted snippets that are returned.  I can't just group on the content because 1) the field is huge; and 2) the content is very different from page to page. If I could group, exclude or deduplicate results whose snippets were >85% identical, that would probably solve the problem.  Perhaps some sort of post-processing step or some kind of tokenizer factory? Or a sort of idf for the search results rather than the entire document set?</p>\n\n<p>This seems like it would be a fairly common problem, and perhaps I've just missed how to do it.   Essentially this is Google's \"To blah blah your search, we have hidden xxx similar results. Click here to show them\" feature.</p>\n\n<p>Thoughts?</p>\n", "creation_date": 1316013461, "score": 1},
{"title": "Simple Nutch 1.3/Solr index explanation", "view_count": 993, "owner": {"age": 33, "answer_count": 8, "creation_date": 1221619062, "user_id": 14758, "accept_rate": 62, "view_count": 108, "location": "Minneapolis, MN", "reputation": 446}, "is_answered": true, "answers": [{"question_id": 7422949, "owner": {"user_id": 565296, "link": "http://stackoverflow.com/users/565296/umar", "user_type": "registered", "reputation": 2041}, "body": "<p>The nutch schema defines id (= url) as teh unique key. If you re-crawl the url teh document will be replaced in solr index when nutch posts the data to solr.</p>\n", "creation_date": 1316072655, "is_accepted": true, "score": 1, "last_activity_date": 1316072655, "answer_id": 7427422}, {"question_id": 7422949, "owner": {"user_id": 70211, "accept_rate": 80, "link": "http://stackoverflow.com/users/70211/mt3", "user_type": "registered", "reputation": 1212}, "body": "<p>Try Lucidworks' enterprise Solr for testing/prototyping, which has a webcrawler builtin.</p>\n\n<p><a href=\"http://www.lucidimagination.com/products/lucidworks-search-platform/enterprise\" rel=\"nofollow\">http://www.lucidimagination.com/products/lucidworks-search-platform/enterprise</a></p>\n\n<p>It'll give you a feel for the whole Lucene stack. It has a MUCH better interface than any other Java software I've ever used. It's a joy to use.</p>\n", "creation_date": 1316106123, "is_accepted": false, "score": -1, "last_activity_date": 1316106123, "answer_id": 7434700}, {"last_edit_date": 1316181540, "owner": {"user_id": 484046, "accept_rate": 81, "link": "http://stackoverflow.com/users/484046/millebii", "user_type": "registered", "reputation": 976}, "body": "<p>Well you need to implement incremental crawling in Nutch...   which is dependent on your application. Some people want to recrawl every day, others every 3 month. The max is 90 days in any case.</p>\n\n<p>The general idea is to delete crawl segments that are older than your max time for recrawl, since they will be redundant at that time. And produce a fresh <code>solrindex</code> for use in Solr.</p>\n\n<p>I'm afraid that you have to do that yourself in scripting. One day I may put on the wiki some scripts I did for that, but they are not ready for publish as it stands.</p>\n", "question_id": 7422949, "creation_date": 1316176677, "is_accepted": false, "score": 1, "last_activity_date": 1316181540, "answer_id": 7444770}], "question_id": 7422949, "tags": ["solr", "nutch"], "answer_count": 3, "link": "http://stackoverflow.com/questions/7422949/simple-nutch-1-3-solr-index-explanation", "last_activity_date": 1316181540, "accepted_answer_id": 7427422, "body": "<p>After much searching, it doesn't seem like there's any straightforward explanation of how to use Nutch 1.3 with Solr.</p>\n\n<p>I have a Solr index with other content in it that I'll be using on a website for search.</p>\n\n<p>I'd like to add Nutch results to the index, which will add external sites to the website's search.</p>\n\n<p>All of this is working just fine.</p>\n\n<p>The question is, how do you freshen the index? Do you have to delete all of the Nutch results from Solr first? Or does Nutch take care of that? Does Nutch remove results that are no longer valid from the Solr index?</p>\n\n<p>Shell scripts with no documentation or explanation of what they are doing haven't been helpful with answering these questions.</p>\n", "creation_date": 1316034246, "score": 0},
{"title": "whether method cancel() and method interrupt() do the duplicate job?", "view_count": 1127, "is_answered": false, "answers": [{"question_id": 7412491, "owner": {"user_id": 40342, "accept_rate": 62, "link": "http://stackoverflow.com/users/40342/joachim-sauer", "user_type": "registered", "reputation": 185366}, "body": "<p>They don't usually do the same thing, as they act on different abstraction levels (tasks being a higher abstraction levels than threads). In this case, however the calls seem to be redundant.</p>\n\n<p><a href=\"http://download.oracle.com/javase/7/docs/api/java/util/concurrent/FutureTask.html#cancel%28boolean%29\" rel=\"nofollow\"><code>FutureTask.cancel()</code></a> tells the task that it no longer needs to run and (if <code>true</code> is passed as the argument) will attempt to interrupt the <code>Thread</code> on which the task is currently running (if any).</p>\n\n<p><a href=\"http://download.oracle.com/javase/7/docs/api/java/lang/Thread.html#interrupt%28%29\" rel=\"nofollow\"><code>t.interrupt()</code></a> attempts to interrupt the <code>Thread</code> <code>t</code>.</p>\n\n<p>In this case it seems to be redundant. <em>If</em> the <code>Task</code> is still running, then <code>cancel(true)</code> <em>should</em> interrupt the thread, in which case the duplicate <code>interrupt()</code> call is unnecessary (unless the code running in the thread somehow ignores <em>one</em> interruption, but halts on <em>two</em> interrupts, which is unlikely).</p>\n\n<p>If the Task is already complete at that point, then both <code>cancel()</code> and <code>interrupt()</code> will have no effet.</p>\n", "creation_date": 1315985165, "is_accepted": false, "score": 0, "last_activity_date": 1315985165, "answer_id": 7412704}, {"question_id": 7412491, "owner": {"user_id": 930190, "accept_rate": 43, "link": "http://stackoverflow.com/users/930190/kaiwii-ho", "user_type": "registered", "reputation": 529}, "body": "<p>Here,I'd like to make up a conclusion\uff1a\nwhen we pass the true as the argument of the FutureTask.cancel(),we can get the same effect as the interupt() does yet.\nwhy?\nLet's peek into the src of cancel() method.\nwe got that the cancel() method call the method:</p>\n\n<pre><code>innerCancel(mayInterruptIfRunning);\n</code></pre>\n\n<p>when inside the method:<code>innerCancel(mayInterruptIfRunning);</code>,we can have the instructions below:</p>\n\n<pre><code>if (mayInterruptIfRunning) {\n                Thread r = runner;\n                if (r != null)\n                    r.interrupt();\n            }\n</code></pre>\n\n<p>So,in my case,the cancel() actually call the interrupt() indeed.</p>\n", "creation_date": 1316044882, "is_accepted": false, "score": 0, "last_activity_date": 1316044882, "answer_id": 7424407}], "question_id": 7412491, "tags": ["java", "hadoop", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/7412491/whether-method-cancel-and-method-interrupt-do-the-duplicate-job", "last_activity_date": 1316044882, "owner": {"user_id": 930190, "answer_count": 5, "creation_date": 1315295521, "accept_rate": 43, "view_count": 176, "location": "China", "reputation": 529}, "body": "<p>I read the source of <code>org.apache.nutch.parse.ParseUtil.runParser(Parser p, Content content)</code>.</p>\n\n<p>Do these two method calls do the same thing:</p>\n\n<p>Instruction 1:</p>\n\n<pre><code>t.interrupt();\n</code></pre>\n\n<p>Instruction 2:</p>\n\n<pre><code>task.cancel(true);\n</code></pre>\n\n<p>The source  of the <code>org.apache.nutch.parse.ParseUtil.runParser(Parser p, Content content)</code> is:</p>\n\n<pre><code>ParseCallable pc = new ParseCallable(p, content);\nFutureTask&lt;ParseResult&gt; task = new FutureTask&lt;ParseResult&gt;(pc);\nParseResult res = null;\nThread t = new Thread(task);\nt.start();\ntry {\n  res = task.get(MAX_PARSE_TIME, TimeUnit.SECONDS);\n} catch (TimeoutException e) {\n  LOG.warn(\"TIMEOUT parsing \" + content.getUrl() + \" with \" + p);\n} catch (Exception e) {\n  task.cancel(true);\n  res = null;\n  t.interrupt();\n} finally {\n  t = null;\n  pc = null;\n}\nreturn res;\n</code></pre>\n", "creation_date": 1315983929, "score": 2},
{"title": "Nutch and save crawl data to Amazon S3", "view_count": 2412, "owner": {"user_id": 331029, "answer_count": 0, "creation_date": 1272841574, "accept_rate": 100, "view_count": 14, "reputation": 32}, "is_answered": true, "answers": [{"question_id": 7391500, "owner": {"user_id": 438427, "link": "http://stackoverflow.com/users/438427/patrick-salami", "user_type": "registered", "reputation": 421}, "body": "<p>Hadoop can use S3 as its underlying file system natively. I have had very good results with this approach when running Hadoop in EC2, either using EMR or your own / third-party Hadoop AMIs. I would not recommend using S3 as the underlying file system when using Hadoop outside of EC2, as bandwidth limitations would likely negate any performance gains Hadoop would give you. The S3 adapter for Hadoop was developed by Amazon and is part of the Hadoop core. Hadoop treats S3 just like HDFS. See <a href=\"http://wiki.apache.org/hadoop/AmazonS3\" rel=\"nofollow\">http://wiki.apache.org/hadoop/AmazonS3</a> for more info on using Hadoop with S3.</p>\n\n<p>Nutch is designed to run as a job on a Hadoop cluster (when in \"deploy\" mode) and therefore does not include the Hadoop jars in its distribution. Because it runs as a Hadoop job, however, it can access any underlying data store that Hadoop supports, such as HDFS or S3. When run in \"local\" mode, you will provide your own local Hadoop installation. Once crawling is finished in \"deploy\" mode, the data will be stored in the distributed file system. It is recommended that you wait for indexing to finish and then download the index to a local machine for searching, rather than searching in the DFS, for performance reasons. For more on using Nutch with Hadoop, see <a href=\"http://wiki.apache.org/nutch/NutchHadoopTutorial\" rel=\"nofollow\">http://wiki.apache.org/nutch/NutchHadoopTutorial</a>. </p>\n\n<p>Regarding HBase, I have had good experiences using it, although not for your particular use case. I can imagine that for random searches, Solr may be faster and more feature-rich than HBase, but this is debatable. HBase is probably worth a try. Until 2.0 comes out, you may want to write your own Nutch-to-HBase connector or simply stick with Solr for now.</p>\n", "creation_date": 1316033877, "is_accepted": true, "score": 1, "last_activity_date": 1316033877, "answer_id": 7422889}], "question_id": 7391500, "tags": ["solr", "amazon-s3", "hadoop", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/7391500/nutch-and-save-crawl-data-to-amazon-s3", "last_activity_date": 1316033877, "accepted_answer_id": 7422889, "body": "<p>I am trying to evaluate if Nutch/Solr/Hadoop are the right technologies for my task.</p>\n\n<p>PS: Previously I was trying to integrate Nutch (1.4) and Hadoop to see how it works.</p>\n\n<p>Here is what I am trying to achieve overall, \na) Start with a Seed URL(s) and crawl and parse/save data/links \n   --Which Nutch crawler does anyway.</p>\n\n<p>b) Then be able to query the crawled indexes from a Java client\n   --- (may be either using SolrJ client)</p>\n\n<p>c) Since Nutch (as of 1.4.x) already uses Hadoop internally. I will just install Hadoop and configure in the nutch-**.xml</p>\n\n<p>d) I would like Nutch to save the crawled indexes to Amazon S3 and also Hadoop to use S3 as file system.\n   Is this even possible? or even worth it?</p>\n\n<p>e) I read in one of the forums, that in Nutch 2.0, there is a data layer using GORA that can save indexes to HBase etc. I don't when 2.0 release is due. :-(\nDoes anyone suggest to grab 2.0 \"inprogress\" trunk and start using it, hoping to get a released lib sooner or later?</p>\n\n<p>PS: I am still trying to figure out how/when/why/where Nutch uses Hadoop internally. I just cannot find any written documentation or tutorials..Any help on this aspect is also much appreciated.</p>\n\n<p>If you are reading this line, then thank you so much for reading this post up to this point :-)</p>\n", "creation_date": 1315847923, "score": 1},
{"title": "How to get file extension in nutch?", "view_count": 385, "is_answered": false, "answers": [{"question_id": 7358952, "owner": {"user_id": 51986, "link": "http://stackoverflow.com/users/51986/hkn", "user_type": "registered", "reputation": 839}, "body": "<p>Don't forget to request the newly added field <code>contentType</code> in your query.\nUse a query like `q=text&amp;fl=content,contentType'.</p>\n\n<p>Check <a href=\"http://wiki.apache.org/solr/CommonQueryParameters#fl\" rel=\"nofollow\">http://wiki.apache.org/solr/CommonQueryParameters#fl</a></p>\n", "creation_date": 1315557677, "is_accepted": false, "score": 0, "last_activity_date": 1315557677, "answer_id": 7359126}], "question_id": 7358952, "tags": ["solr", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/7358952/how-to-get-file-extension-in-nutch", "last_activity_date": 1315557677, "owner": {"user_id": 924923, "answer_count": 1, "creation_date": 1314953369, "accept_rate": 12, "view_count": 58, "reputation": 79}, "body": "<p>I add the following changes in solr-index maping.xml and add the related field in solr scheama but the content_type doesn't appear in xml result,please help me.</p>\n\n<p>feild i have added in solr-index maping.xml:</p>\n\n<pre><code> &lt;fields&gt;\n    &lt;field dest=\"content\" source=\"content\"/&gt;        \n    &lt;field dest=\"contentType\" source=\"contentType\"/&gt;        \n  &lt;/fields&gt;\n</code></pre>\n\n<p>feild i have added in solr schema:</p>\n\n<pre><code>&lt;field name=\"contentType\" type=\"string\" indexed=\"true\" stored=\"true\" multiValued=\"true\"/&gt;\n</code></pre>\n", "creation_date": 1315556675, "score": 0},
{"title": "i don&#39;t known what does the symbol,&quot;#&quot; mean in the following src of the nutch&#39;s HttpBase.java", "view_count": 104, "is_answered": true, "answers": [{"question_id": 7358029, "owner": {"user_id": 22656, "accept_rate": 86, "link": "http://stackoverflow.com/users/22656/jon-skeet", "user_type": "registered", "reputation": 904775}, "body": "<p>It's just shorthand for \"number\" or \"count\". So \"Find out the # of users we need to support\" would be shorthand for \"Find out the number of users we need to support\" in the same way.</p>\n\n<p>See the wikipedia page for <a href=\"http://en.wikipedia.org/wiki/Number_sign\" rel=\"nofollow\">\"number sign\"</a> for more information.</p>\n", "creation_date": 1315550729, "is_accepted": false, "score": 12, "last_activity_date": 1315550729, "answer_id": 7358045}], "question_id": 7358029, "tags": ["java", "hadoop", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/7358029/i-dont-known-what-does-the-symbol-mean-in-the-following-src-of-the-nutchs", "last_activity_date": 1315551040, "owner": {"user_id": 930190, "answer_count": 5, "creation_date": 1315295521, "accept_rate": 43, "view_count": 176, "location": "China", "reputation": 529}, "body": "<p>When I come to the following src of the nutch's <code>HttpBase.java</code>, I don't known what does the symbol,\"#\" mean in the author's desription:</p>\n\n<pre><code>// get # of threads already accessing this addr\nInteger counter = (Integer)THREADS_PER_HOST_COUNT.get(host);\n</code></pre>\n", "creation_date": 1315550660, "score": 0},
{"title": "Setup Nutch 1.3 and Hadoop", "view_count": 946, "owner": {"user_id": 331029, "answer_count": 0, "creation_date": 1272841574, "accept_rate": 100, "view_count": 14, "reputation": 32}, "is_answered": true, "answers": [{"question_id": 7265831, "owner": {"user_id": 484046, "accept_rate": 81, "link": "http://stackoverflow.com/users/484046/millebii", "user_type": "registered", "reputation": 976}, "body": "<p>Well hadoop is not included anymore in Nutch since 1.3 ... I have complained in the mailing list. But the goal of Nutch group seem to have changed to a crawler component only. To make use of it you need to install hadoop <a href=\"http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-single-node-cluster\" rel=\"nofollow\">here is good tutorial</a> &amp; solr (for search).<br>\nSome people announced they are going to fix that but for Nutch1.4 only. Not sure where it will end up.</p>\n", "creation_date": 1315400209, "is_accepted": true, "score": 1, "last_activity_date": 1315400209, "answer_id": 7334253}], "question_id": 7265831, "tags": ["hadoop", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/7265831/setup-nutch-1-3-and-hadoop", "last_activity_date": 1315547226, "accepted_answer_id": 7334253, "body": "<p>I am a newbie to Nutch and Hadoop and trying to follow the tutorial here at <a href=\"http://wiki.apache.org/nutch/NutchHadoopTutorial\" rel=\"nofollow\">http://wiki.apache.org/nutch/NutchHadoopTutorial</a>.</p>\n\n<p>So I started with Nutch 1.3 release.</p>\n\n<p>Even though Hadoop is included in Nutch, I did not see any of these .sh or .xml files referred in the tutorial under /nutch/search/conf after the build.</p>\n\n<p>I was wondering if I have to setup hadoop first in the same directory structure or copy over hadoop config files before proceeding to Nutch setup.</p>\n\n<p>Can anyone please put me in the right direction. I am pretty sure that I am lost :-(</p>\n\n<p>Thanks very much in advance </p>\n", "creation_date": 1314847452, "score": 1},
{"title": "How to write a java code for crawling sites with apache nutch 1.3 api?", "view_count": 819, "is_answered": true, "answers": [{"question_id": 7281141, "owner": {"user_id": 484046, "accept_rate": 81, "link": "http://stackoverflow.com/users/484046/millebii", "user_type": "registered", "reputation": 976}, "body": "<p>Well you don't need to write any Java code to that... just install Nutch and off you go. However you should install Nutch 1.2 which is self contained.<br>\nWith Nutch1.3 you need to install hadoop yourself and you need to install &amp; user Solr for the webseach too.</p>\n", "creation_date": 1315307356, "is_accepted": false, "score": 2, "last_activity_date": 1315307356, "answer_id": 7318915}], "question_id": 7281141, "tags": ["web-crawler", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/7281141/how-to-write-a-java-code-for-crawling-sites-with-apache-nutch-1-3-api", "last_activity_date": 1315307356, "owner": {"user_id": 924923, "answer_count": 1, "creation_date": 1314953369, "accept_rate": 12, "view_count": 58, "reputation": 79}, "body": "<p>I want to write a program with java and nutch 1.3 api to crawl the the sites\ni searched the web but there is no sample code \nhow can i do that?\nthanks</p>\n", "creation_date": 1314953369, "score": 1},
{"title": "nutch crawl path", "view_count": 328, "owner": {"user_id": 133932, "answer_count": 47, "creation_date": 1246918932, "accept_rate": 65, "view_count": 292, "location": "Toronto, Canada", "reputation": 654}, "is_answered": true, "answers": [{"question_id": 7152122, "owner": {"user_id": 484046, "accept_rate": 81, "link": "http://stackoverflow.com/users/484046/millebii", "user_type": "registered", "reputation": 976}, "body": "<p>This should crawl only the domain/path you want : </p>\n\n<pre><code>+.*www\\.domain\\.com/yourpath/.*  \n#skip everything else  \n-.*\n</code></pre>\n", "creation_date": 1314201128, "is_accepted": true, "score": 2, "last_activity_date": 1314201128, "answer_id": 7178481}], "question_id": 7152122, "tags": ["nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/7152122/nutch-crawl-path", "last_activity_date": 1314201128, "accepted_answer_id": 7178481, "body": "<p>I would like to know how to make nutch crawl not only the domain that I specified, but also the dir path within the domain that I specified.  I know that you can configure this information on regex-urlfilter.txt</p>\n", "creation_date": 1314039151, "score": 2},
{"title": "use nutch to index my local HTML files", "view_count": 293, "owner": {"age": 27, "answer_count": 13, "creation_date": 1300097129, "user_id": 658560, "accept_rate": 80, "view_count": 92, "location": "Tehran, Iran", "reputation": 650}, "is_answered": true, "answers": [{"question_id": 7130092, "owner": {"user_id": 636354, "link": "http://stackoverflow.com/users/636354/yavuz", "user_type": "registered", "reputation": 48}, "body": "<p>Check this post;\n<a href=\"http://wiki.apache.org/nutch/FAQ#How_do_I_index_my_local_file_system.3F\" rel=\"nofollow\">http://wiki.apache.org/nutch/FAQ#How_do_I_index_my_local_file_system.3F</a></p>\n", "creation_date": 1313890454, "is_accepted": true, "score": 1, "last_activity_date": 1313890454, "answer_id": 7135803}], "question_id": 7130092, "tags": ["indexing", "nutch", "local-files"], "answer_count": 1, "link": "http://stackoverflow.com/questions/7130092/use-nutch-to-index-my-local-html-files", "last_activity_date": 1313890454, "accepted_answer_id": 7135803, "body": "<p>I have a lot of HTML files on my hard disk and want to index them with Nutch, but as I know nutch only get URLs and index them and pages that linked in that URLs.\nDoes any body know how can I use nutch to index my local files??</p>\n", "creation_date": 1313820964, "score": 1},
{"title": "Solr and Nutch - How to take control over Facets?", "view_count": 1232, "is_answered": false, "answers": [{"question_id": 6937943, "owner": {"user_id": 226568, "accept_rate": 79, "link": "http://stackoverflow.com/users/226568/bob-yoplait", "user_type": "registered", "reputation": 1435}, "body": "<p>I haven't used Nutch (I use Heritrix), but at the end of the day, Nutch need to extract the \"meta\" tag values and index them in Solr (using SolrJ for ex), with different solr fields \"price\", \"categories\", etc</p>\n\n<p>Then you do <li>\n<a href=\"http://localhost:8080/solr/fnac/select?q=yourquery&amp;facet=true&amp;facet.limit=10&amp;facet.field=categories\" rel=\"nofollow\">http://localhost:8080/solr/myrep/select?q=mobile&amp;facet=true&amp;facet.limit=10&amp;facet.field=categories</a></p>\n\n<p>to get facets per categories. Here is a page on facets:\n<li> <a href=\"http://wiki.apache.org/solr/SolrFacetingOverview\" rel=\"nofollow\">http://wiki.apache.org/solr/SolrFacetingOverview</a></p>\n", "creation_date": 1313686140, "is_accepted": false, "score": 0, "last_activity_date": 1313686140, "answer_id": 7111087}, {"question_id": 6937943, "owner": {"user_id": 123498, "accept_rate": 25, "link": "http://stackoverflow.com/users/123498/sushant", "user_type": "registered", "reputation": 419}, "body": "<p>One of the options is to use nutch with <a href=\"http://wiki.apache.org/nutch/WritingPluginExample-0.9\" rel=\"nofollow\">metadata plugin</a></p>\n\n<p>Although it is given as an example, it is very much included with the distribution.\nAssuming you know the other processes of configuring, and crawling data using nutch\nBefore indexing, you need to configure nutch to use metadata plugin like this. \nEdit conf/nutch-site.xml</p>\n\n<pre><code>      &lt;property&gt;\n         &lt;name&gt;plugin.includes&lt;/name&gt;\n         &lt;value&gt;urlmeta|(rest of the plugins)&lt;/value&gt;\n     &lt;/property&gt;\n</code></pre>\n\n<p>The metadata tags that need to be indexed, like price can be supplied as another property</p>\n\n<pre><code>    &lt;property&gt;\n       &lt;name&gt;urlmeta.tags&lt;/name&gt;\n         &lt;value&gt;price&lt;/value&gt;\n    &lt;/property&gt;$\n</code></pre>\n\n<p>Now, you can run the nutch crawl command. After crawling and indexing with solr, you should see a field price in the index. The facet search can be used by adding facet.field in your query.</p>\n\n<p>Here are some links of interest.</p>\n\n<ol>\n<li>Using Solr to index nutch data link :<a href=\"http://wiki.apache.org/nutch/RunningNutchAndSolr\" rel=\"nofollow\">Link</a></li>\n<li>Help on Solr faceting queries link :<a href=\"http://wiki.apache.org/solr/SolrFacetingOverview\" rel=\"nofollow\">Link</a></li>\n</ol>\n", "creation_date": 1313688116, "is_accepted": false, "score": 0, "last_activity_date": 1313688116, "answer_id": 7111488}], "question_id": 6937943, "tags": ["solr", "nutch", "facets"], "answer_count": 2, "link": "http://stackoverflow.com/questions/6937943/solr-and-nutch-how-to-take-control-over-facets", "last_activity_date": 1313688116, "owner": {"age": 35, "answer_count": 79, "creation_date": 1269446131, "user_id": 300963, "accept_rate": 81, "view_count": 151, "location": "Sweden", "reputation": 3101}, "body": "<p>Sorry if this question might be too general. I'd be happy with good links to documentation, if there are any. Google won't help me find them.</p>\n\n<p>I need to understand how facets can be extracted from a web site crawled by Nutch then indexed by Solr. On the web site, pages have meta tags, like <code>&lt;meta name=\"price\" content=\"123.45\"/&gt;</code> or <code>&lt;meta name=\"categories\" content=\"category1, category2\"/&gt;</code>. Can I tell Nutch to extract those and Solr to treat them as facets?</p>\n\n<p>In the example above, I want to specify manually that the meta name \"categories\" is to be treated as a facet, but the content should be dynamically used as categories.</p>\n\n<p>Does it make sense? Is it possible to do with Nutch and Solr, or should I rethink my way of using it?</p>\n", "creation_date": 1312444384, "score": 0},
{"title": "Increase Java heap space for language-identifier plugin-in in nutch", "view_count": 1006, "owner": {"user_id": 200340, "answer_count": 6, "creation_date": 1257033732, "accept_rate": 68, "view_count": 153, "reputation": 966}, "is_answered": true, "answers": [{"question_id": 7104854, "owner": {"user_id": 575766, "accept_rate": 96, "link": "http://stackoverflow.com/users/575766/mikaveli", "user_type": "registered", "reputation": 4841}, "body": "<p>Assuming you're developing on a Windows box, edit nutch.bat and add the following after the <code>rem NUTCH_OPTS</code> line:</p>\n\n<pre><code>set NUTCH_OPTS=%NUTCH_OPTS% -Xmx1024m\n</code></pre>\n\n<p>Obviously set the amount of RAM within the physical limit of your machine - note that Nutch can easily require 4g, depending on what you're doing with it.</p>\n", "creation_date": 1313661621, "is_accepted": true, "score": 1, "last_activity_date": 1313661621, "answer_id": 7105625}], "question_id": 7104854, "tags": ["java", "hadoop", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/7104854/increase-java-heap-space-for-language-identifier-plugin-in-in-nutch", "last_activity_date": 1313663665, "accepted_answer_id": 7105625, "body": "<p>I am trying to add a new language To Automatic Language Detection tool Apache's tika. It needs to build a language profile for adding a new language. So i am using nutch language-identifier plug-in to build this profile.</p>\n\n<p>The command is the following:</p>\n\n<pre>\nbin/nutch plugin language-identifier org.apache.nutch.analysis.lang.NGramProfile -create ./language-detection-profile/jp ./language-detection-profile/japanese4ngram-1.txt utf-8\n</pre>\n\n<p>Where ./language-detection-profile/japanese4ngram-1.txt is the new language corpus. </p>\n\n<p>I have tested on a small size corpus (1 MB), and everything is fine, the profile is created as I expected.</p>\n\n<p>However, when the corpus is large (> 1 GB). I have the problem of out of memory (heap space), like</p>\n\n<pre>\nException in thread \"main\" java.lang.reflect.InvocationTargetException\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n    at java.lang.reflect.Method.invoke(Method.java:597)\n    at org.apache.nutch.plugin.PluginRepository.main(PluginRepository.java:421)\nCaused by: java.lang.OutOfMemoryError: Java heap space\n    at java.util.Arrays.copyOf(Arrays.java:2882)\n    at java.lang.AbstractStringBuilder.expandCapacity(AbstractStringBuilder.java:100)\n    at java.lang.AbstractStringBuilder.append(AbstractStringBuilder.java:390)\n    at java.lang.StringBuilder.append(StringBuilder.java:119)\n    at org.apache.nutch.analysis.lang.NGramProfile.create(NGramProfile.java:374)\n    at org.apache.nutch.analysis.lang.NGramProfile.main(NGramProfile.java:484)\n    ... 5 more\n\n</pre>\n\n<p>Does anyone know how to specify heap space size for nutch's plugin? Thanks.</p>\n\n<p>Edit:\nWith the help from Mikaveli.\nIn Ubuntu:\nset </p>\n\n<pre>\nif [ \"x$JAVA_LIBRARY_PATH\" != \"x\" ]; then\n  NUTCH_OPTS=\"$NUTCH_OPTS -Djava.library.path=$JAVA_LIBRARY_PATH -Xmx2048m\"\nfi\n</pre>\n", "creation_date": 1313658210, "score": 1},
{"title": "Adding URL parameter to Nutch/Solr index and search results", "view_count": 2577, "is_answered": true, "answers": [{"question_id": 6495468, "owner": {"user_id": 81194, "link": "http://stackoverflow.com/users/81194/claude", "user_type": "registered", "reputation": 827}, "body": "<p>You could create a custom field in a Nutch filter to save the entire URL. As long as you define the same field in the Solr schema with store=\"true\" it will show up in your results. See <a href=\"http://wiki.apache.org/nutch/WritingPluginExample-1.2\" rel=\"nofollow\">WritingPluginExample-1.2</a>.</p>\n\n<p>Let me know if you'd like some help.</p>\n", "creation_date": 1309476397, "is_accepted": false, "score": 1, "last_activity_date": 1309476397, "answer_id": 6542180}], "question_id": 6495468, "tags": ["solr", "web-crawler", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/6495468/adding-url-parameter-to-nutch-solr-index-and-search-results", "last_activity_date": 1313516652, "owner": {"age": 32, "answer_count": 85, "creation_date": 1221569545, "user_id": 12016, "accept_rate": 77, "view_count": 290, "location": "Delaware", "reputation": 2720}, "body": "<p>I can't find any hint on how to setup nutch to NOT filter/remove my URL parameters. I want to crawl and index some pages where lots of content is hidden behind the same base URLs (like <em>/news.jsp?id=1 /news.jsp?id=2 /news.jsp?id=3</em> and so on).</p>\n\n<ul>\n<li>the <strong>regex-normalize.xml</strong> only removes redundant stuff from the URL (like session id, and trailing ?)</li>\n<li>the <strong>regex-urlfilter.txt</strong> seems to have a wildcard for my host <em>(+^http://$myHost/)</em></li>\n</ul>\n\n<p>The crawling works fine so far. Any ideas?</p>\n\n<p>cheers,\nmana</p>\n\n<p>EDIT:</p>\n\n<p>A part of the solution is hidden here:</p>\n\n<p><a href=\"http://stackoverflow.com/questions/1751597/configuring-nutch-regex-normalize-xml\">configuring nutch regex-normalize.xml</a></p>\n\n<pre><code># skip URLs containing certain characters as probable queries, etc.\n-[?*!@=]\n</code></pre>\n\n<p>has to be modfied. One has to allow all chars that may exist in a URL parameter like '?' and '='. The new line looks like</p>\n\n<pre><code>-[*!@]\n</code></pre>\n\n<p>And pages are crawled now with params. But they are not yet send to Solr with parameters (Solr still cuts the parameters from the links)</p>\n\n<p>EDIT2:</p>\n\n<p>Nutch has some issues on how to handle relative urls ('?param=value'). Still stuck on that Parameter thing:</p>\n\n<p>see maling list: <a href=\"http://search.lucidimagination.com/search/document/b6011a942b323ba3/problem_with_href_param_value_links\" rel=\"nofollow\">http://search.lucidimagination.com/search/document/b6011a942b323ba3/problem_with_href_param_value_links</a></p>\n", "creation_date": 1309189702, "score": 3},
{"title": "how to crawl a specific URL using nutch 1.2", "view_count": 781, "is_answered": true, "answers": [{"question_id": 7038546, "owner": {"user_id": 12016, "accept_rate": 77, "link": "http://stackoverflow.com/users/12016/mana", "user_type": "registered", "reputation": 2720}, "body": "<p>I thought that I read somewhere, that this config is evaluated from top to bottom.</p>\n\n<p>And since the </p>\n\n<pre><code>-. \n</code></pre>\n\n<p>matches everything - nutch will ignore the further entries. My crawl-urlfilter.txt looks something like that at the bottom (in exactly that order) and is working pretty well:</p>\n\n<pre><code># accept hosts in MY.DOMAIN.NAME\n+^http://localhost:8080/\n\n# skip everything else\n-.\n</code></pre>\n", "creation_date": 1313145411, "is_accepted": false, "score": 1, "last_activity_date": 1313145411, "answer_id": 7038902}], "question_id": 7038546, "tags": ["web-crawler", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/7038546/how-to-crawl-a-specific-url-using-nutch-1-2", "last_activity_date": 1313145411, "owner": {"age": 29, "answer_count": 26, "creation_date": 1293577911, "user_id": 556540, "accept_rate": 75, "view_count": 178, "location": "Mumbai, India", "reputation": 1450}, "body": "<p>I'm using nutch-1.2 but not able to restrict my config file to crawl only given urls<br>\nmy crawl-urlfilter.txt file is</p>\n\n<pre><code>    # Each non-comment, non-blank line contains a regular expression\n    # prefixed by '+' or '-'. The first matching pattern in the file\n    # determines whether a URL is included or ignored. If no pattern\n    # matches, the URL is ignored.\n\n    # skip file:, ftp:, &amp; mailto: urls\n    -^(file|ftp|mailto):\n\n    # skip image and other suffixes we can't yet parse\n    -\\.(gif|GIF|js|jpg|JPG|png|PNG|ico|ICO|css|sit|eps|wmf|zip|ppt|mpg|xls|gz|rpm|tgz|mov|MOV|exe|jpeg|JPEG|bmp|BMP)$\n\n    # skip URLs containing certain characters as probable queries, etc.\n    -[?*!@=]\n\n    # skip URLs with slash-delimited segment that repeats 3+ times, to break loops\n    -.*(/[^/]+)/[^/]+\\1/[^/]+\\1/\n\n # accept hosts in MY.DOMAIN.NAME\n    -.*\n    +^http://([a-z0-9]*\\.)abc.com/[0-9].*[a-z]*.html$\n    -^http://([a-z0-9]*\\.)*abc.com/[-a-z/]*/$\n    -^http://([a-z0-9]*\\.)*abc.com/[-a-z]*.php$\n</code></pre>\n\n<p>Thanks</p>\n", "creation_date": 1313143404, "score": 0},
{"title": "How to use Solr search that is included with Nutch-1.2?", "view_count": 375, "is_answered": false, "answers": [{"question_id": 4244390, "owner": {"user_id": 565296, "link": "http://stackoverflow.com/users/565296/umar", "user_type": "registered", "reputation": 2041}, "body": "<p>You need to copy the nutch solr schema from NUTCH_HOME/conf/schema.xml to SOLR_HOME/solr/conf/\nThen you can use the nutch solr index command to update the solr index from nutch.\nIt is better explained here: <a href=\"http://www.lucidimagination.com/blog/2009/03/09/nutch-solr/\" rel=\"nofollow\">http://www.lucidimagination.com/blog/2009/03/09/nutch-solr/</a></p>\n", "creation_date": 1297920583, "is_accepted": false, "score": 0, "last_activity_date": 1297920583, "answer_id": 5025416}], "question_id": 4244390, "tags": ["solr", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/4244390/how-to-use-solr-search-that-is-included-with-nutch-1-2", "last_activity_date": 1312283111, "owner": {"age": 35, "answer_count": 13, "creation_date": 1270842308, "user_id": 313127, "accept_rate": 75, "view_count": 424, "location": "Algiers, Algeria", "reputation": 2433}, "body": "<p>There's some good (outdated though) tutorials on how to integrate Nutch with Solr to get a full web search experience. In the latest Nutch release (1.2), Solr is integrated out-of-the-box in the Nutch distribution. Question is: How to use it?</p>\n\n<p>Thanks</p>\n", "creation_date": 1290421364, "score": 0},
{"title": "Get text snippet from search index generated by solr and nutch", "view_count": 1601, "owner": {"age": 35, "answer_count": 79, "creation_date": 1269446131, "user_id": 300963, "accept_rate": 81, "view_count": 151, "location": "Sweden", "reputation": 3101}, "is_answered": true, "answers": [{"question_id": 6870557, "owner": {"user_id": 644817, "accept_rate": 90, "link": "http://stackoverflow.com/users/644817/the-bndr", "user_type": "registered", "reputation": 5682}, "body": "<p>don't forget: indexed is not the same as stored.</p>\n\n<p>You can search words in an document, if all field are indexed, but no field is stored.\nTo get the content of a specific field, it must be also stored=true in schema.xml</p>\n\n<p>If your fulltext-field is \nstored, so probably the default \"field-list-settings\" does not include the fulltext-field.\nYou can add this by using the <code>fl</code> parameter:</p>\n\n<pre><code>http://&lt;solr-url&gt;:port/select/?......&amp;fl=mytext,*\n</code></pre>\n\n<p>...this example, if your fulltext is stored in the field called mytext</p>\n\n<p>Finally, if you like to have only a snippet of the text with the searched words (not the whole text) look at the highlight-component from solr/lucene</p>\n", "creation_date": 1311936029, "is_accepted": true, "score": 1, "last_activity_date": 1311936029, "answer_id": 6871905}], "question_id": 6870557, "tags": ["solr", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/6870557/get-text-snippet-from-search-index-generated-by-solr-and-nutch", "last_activity_date": 1311936029, "accepted_answer_id": 6871905, "body": "<p>I have just configured nutch and solr to successfully crawl and index text on a web site, by following the geting started tutorials. Now I am trying to make a search page by modifying the example velocity templates.</p>\n\n<p>Now to my question. How can I tell solr to provide a relevant text snippet of the content of the hits? I only get the following fields associated with each hit:</p>\n\n<p>score, boost, digest, id, segment, title, date, tstamp and url.</p>\n\n<p>The content is really indexed, because I can search for words that I know only is in the fulltext, but I still don't get the fulltext back associated with the hit.</p>\n", "creation_date": 1311928340, "score": 1},
{"title": "Apache Nutch: No URLs to fetch - check your seed list and URL filters", "view_count": 3303, "is_answered": false, "answers": [{"question_id": 6648975, "owner": {"user_id": 484046, "accept_rate": 81, "link": "http://stackoverflow.com/users/484046/millebii", "user_type": "registered", "reputation": 976}, "body": "<p>Your URL filter rules look weird and I don't think they match valid URLs, something like this should be better no ?</p>\n\n<p><code>\n+^http://152\\.111\\.1\\.87/<br>\n+^http://152\\.111\\.1\\.88/\n</code></p>\n", "creation_date": 1310803273, "is_accepted": false, "score": 0, "last_activity_date": 1310803273, "answer_id": 6716176}], "question_id": 6648975, "tags": ["apache", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/6648975/apache-nutch-no-urls-to-fetch-check-your-seed-list-and-url-filters", "last_activity_date": 1310803273, "owner": {"user_id": 1087234, "answer_count": 1, "creation_date": 1302257984, "accept_rate": 67, "view_count": 88, "location": "South Africa", "reputation": 387}, "body": "<p>I'm using nutch 1.2. When I run the crawl command like so: </p>\n\n<pre><code>bin/nutch crawl urls -dir crawl -depth 2 -topN 1000\n\nInjector: starting at 2011-07-11 12:18:37\nInjector: crawlDb: crawl/crawldb\nInjector: urlDir: urls\nInjector: Converting injected urls to crawl db entries.\nInjector: Merging injected urls into crawl db.\nInjector: finished at 2011-07-11 12:18:44, elapsed: 00:00:07\nGenerator: starting at 2011-07-11 12:18:45\nGenerator: Selecting best-scoring urls due for fetch.\nGenerator: filtering: true\nGenerator: normalizing: true\nGenerator: topN: 1000\nGenerator: jobtracker is 'local', generating exactly one partition.\nGenerator: 0 records selected for fetching, exiting ...\nStopping at depth=0 - no more URLs to fetch.\n**No URLs to fetch - check your seed list and URL filters.**\ncrawl finished: crawl\n</code></pre>\n\n<p>The problem is that it keeps complaining about the: No URLs to fetch - check your seed list and URL filters.</p>\n\n<p>I have a list of urls to crawl under the nutch_root/urls/nutch file. my crawl-urlfilter.txt is also set.</p>\n\n<p>Why would it complain about my url list and filters? it never did this before.</p>\n\n<p>Here is my crawl-urlfilter.txt</p>\n\n<pre><code># skip file:, ftp:, &amp; mailto: urls\n-^(file|ftp|mailto):\n\n# skip image and other suffixes we can't yet parse\n-\\.(gif|GIF|jpg|JPG|png|PNG|ico|ICO|css|sit|eps|wmf|zip|ppt|mpg|xls|gz|rpm|tgz|mov|MOV|exe|jpeg|JPEG|bmp|BMP)$\n\n# skip URLs containing certain characters as probable queries, etc.\n\n\n# skip URLs with slash-delimited segment that repeats 3+ times, to break loops\n-.*(/[^/]+)/[^/]+\\1/[^/]+\\1/\n\n# accept hosts in MY.DOMAIN.NAME\n+^http://([a-z0-9]*\\.)*152.111.1.87/\n+^http://([a-z0-9]*\\.)*152.111.1.88/\n\n# skip everything else\n-.\n</code></pre>\n", "creation_date": 1310381281, "score": 2},
{"title": "Nutch solrindex command not indexing all URLs in Solr", "view_count": 2331, "is_answered": true, "answers": [{"question_id": 6421642, "owner": {"user_id": 12016, "accept_rate": 77, "link": "http://stackoverflow.com/users/12016/mana", "user_type": "registered", "reputation": 2720}, "body": "<p>I can only guess what happend from my experiences:</p>\n\n<p>There is a component called url-normalizer (with its configuration url-normalizer.xml) which is truncating some urls (removing URL parameters, SessionIds, ...)</p>\n\n<p>Additionally, Nutch uses a unique constraint, by default each url is only saved once.</p>\n\n<p>So, if the normalizer truncates 2 or more URLs ('foo.jsp?param=value', 'foo.jsp?param=value2', 'foo.jsp?param=value3', ...) to the exactly same one ('foo.jsp'), they get only saved once. So Solr will only see a subset of all your crawled URLs.</p>\n\n<p>cheers</p>\n", "creation_date": 1310463487, "is_accepted": false, "score": 2, "last_activity_date": 1310463487, "answer_id": 6662139}], "question_id": 6421642, "tags": ["solr", "nutch", "web-crawler"], "answer_count": 1, "link": "http://stackoverflow.com/questions/6421642/nutch-solrindex-command-not-indexing-all-urls-in-solr", "last_activity_date": 1310463487, "owner": {"age": 31, "answer_count": 1, "creation_date": 1308640417, "user_id": 807956, "view_count": 33, "location": "Cape Town, South Africa", "reputation": 41}, "body": "<p>I have a Nutch index crawled from a specific domain and I am using the solrindex command to push the crawled data to my Solr index. The problem is that it seems that only some of the crawled URLs are actually being indexed in Solr. I had the Nutch crawl output to a text file so I can see the URLs that it crawled, but when I search for some of the crawled URLs in Solr I get no results.</p>\n\n<p>Command I am using to do the Nutch crawl: <code>bin/nutch crawl urls -dir crawl -depth 20 -topN 2000000</code></p>\n\n<p>This command is completing successfully and the output displays URLs that I cannot find in the resulting Solr index.</p>\n\n<p>Command I am using to push the crawled data to Solr: <code>bin/nutch solrindex <a href=\"http://localhost:8983/solr/\" rel=\"nofollow\">http://localhost:8983/solr/</a> crawl/crawldb crawl/linkdb crawl/segments/*</code></p>\n\n<p>The output for this command says it is also completing successfully, so it does not seem to be an issue with the process terminating prematurely (which is what I initially thought it might be).</p>\n\n<p>One final thing that I am finding strange is that the entire Nutch &amp; Solr config is identical to a setup I used previously on a different server and I had no problems that time. It is literally the same config files copied onto this new server.</p>\n\n<p><strong>TL;DR:</strong> I have a set of URLs successfully crawled in Nutch, but when I run the solrindex command only some of them are pushed to Solr. Please help.</p>\n\n<p><strong>UPDATE:</strong> I've re-run all these commands and the output still insists it's all working fine. I've looked into any blockers for indexing that I can think of, but still no luck. The URLs being passed to Solr are all active and publicly accessible, so that's not an issue. I'm really banging my head against a wall here so would love some help.</p>\n", "creation_date": 1308640417, "score": 2},
{"title": "Generating db_gone urls for fetch", "view_count": 466, "is_answered": false, "answers": [{"last_edit_date": 1310388231, "owner": {"user_id": 484046, "accept_rate": 81, "link": "http://stackoverflow.com/users/484046/millebii", "user_type": "registered", "reputation": 976}, "body": "<p>You just need to configure nutch-site.xml with a different refetch interval.</p>\n\n<p><strong>ADDITION</strong></p>\n\n<p><code>&lt;property&gt;\n  &lt;name&gt;db.fetch.interval.max&lt;/name&gt;<br>\n  &lt;value&gt;7776000&lt;/value&gt;<br>\n  &lt;description&gt;The maximum number of seconds between re-fetches of a page\n  (90 days). After this period every page in the db will be re-tried, no\n  matter what is its status.\n  &lt;/description&gt;<br>\n&lt;/property&gt;</code></p>\n", "question_id": 6621302, "creation_date": 1310332992, "is_accepted": false, "score": 0, "last_activity_date": 1310388231, "answer_id": 6643789}], "question_id": 6621302, "tags": ["nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/6621302/generating-db-gone-urls-for-fetch", "last_activity_date": 1310388231, "owner": {"age": 28, "answer_count": 7, "creation_date": 1305531285, "user_id": 755269, "accept_rate": 43, "view_count": 54, "reputation": 357}, "body": "<p>In my crawler system, I have set the fetch interval as 30 days. I initially set my user agent as say \"....\" then many urls are getting rejected. But after changing my user agent to appropriate name, I want to fetch those urls which are rejected initially. \nBut the thing is those urls with the db_gone status will have retry interval as 45 days. So generator wont pick that.Hence in this case how would I fetch those urls with db_gone status?</p>\n\n<p>Is nutch by default has any options to crawl those db_gone urls alone?</p>\n\n<p>Or do I need to write a seperate map-reduce program to collect those urls and use freegen to generate segments for them?</p>\n", "creation_date": 1310110163, "score": 0},
{"title": "Nutch Fetcher: Number of urls fetched persecond", "view_count": 501, "owner": {"age": 28, "answer_count": 7, "creation_date": 1305531285, "user_id": 755269, "accept_rate": 43, "view_count": 54, "reputation": 357}, "is_answered": true, "answers": [{"question_id": 6518331, "owner": {"user_id": 755269, "accept_rate": 43, "link": "http://stackoverflow.com/users/755269/sriram", "user_type": "registered", "reputation": 357}, "body": "<p>I figured out this problem myself. Actual fetch rate is dependent on the number of map tasks that run throughout the fetcher time limit and the thread per host property. Also the number of fetcher threads play a part here. </p>\n\n<p>For example, if the total number of map tasks is 8 and the number of domains in the input urls is 3, then only 3 map process will run in that fetch cycle. And if each map process has enough urls from the same domain that it would run for the fetcher.timelimit.mins then the total fetch rate of the system will be the sum of the fetch rates of these individual map tasks.</p>\n", "creation_date": 1310109840, "is_accepted": true, "score": 2, "last_activity_date": 1310109840, "answer_id": 6621240}], "question_id": 6518331, "tags": ["nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/6518331/nutch-fetcher-number-of-urls-fetched-persecond", "last_activity_date": 1310109840, "accepted_answer_id": 6621240, "body": "<p>I would like to know a property which denotes how many urls to be fetched per second. I have seen from net that it is actually number of map tasks * number of threads. But in my case the numbers don't match when I verify them. Hence is there any property to specify the number of urls to be fetched per second?</p>\n\n<p>regards,</p>\n\n<p>V.Sriram</p>\n", "creation_date": 1309339298, "score": 1},
{"title": "Nutch No agents listed in &#39;http.agent.name&#39;", "view_count": 5769, "owner": {"user_id": 696523, "answer_count": 2, "creation_date": 1302169441, "accept_rate": 100, "view_count": 51, "reputation": 229}, "is_answered": true, "answers": [{"question_id": 6582934, "owner": {"user_id": 432844, "link": "http://stackoverflow.com/users/432844/julien-nioche", "user_type": "registered", "reputation": 939}, "body": "<p>using 1.3? If so make sure you changed nutch-site.xml (and not default) in runtime/local/conf\nChanging the conf in NUTCH_HOME/conf won't be copied to the runtime dirs unless you rebuild with ant. \nBTW why don't you ask on the mailing list instead? You are more likely to get some help there</p>\n", "creation_date": 1310071106, "is_accepted": true, "score": 11, "last_activity_date": 1310071106, "answer_id": 6616880}, {"question_id": 6582934, "owner": {"user_id": 755269, "accept_rate": 43, "link": "http://stackoverflow.com/users/755269/sriram", "user_type": "registered", "reputation": 357}, "body": "<p>Try giving the agent name for http.robots.agents also. It worked for me. I didn't get that message thereafter!!!</p>\n", "creation_date": 1310107922, "is_accepted": false, "score": 0, "last_activity_date": 1310107922, "answer_id": 6620923}], "question_id": 6582934, "tags": ["nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/6582934/nutch-no-agents-listed-in-http-agent-name", "last_activity_date": 1310107922, "accepted_answer_id": 6616880, "body": "<pre><code>Exception in thread \"main\" java.lang.IllegalArgumentException: Fetcher: No agents listed in 'http.agent.name' property.\n        at org.apache.nutch.fetcher.Fetcher.checkConfiguration(Fetcher.java:1166)\n        at org.apache.nutch.fetcher.Fetcher.fetch(Fetcher.java:1068)\n        at org.apache.nutch.crawl.Crawl.run(Crawl.java:135)\n        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n        at org.apache.nutch.crawl.Crawl.main(Crawl.java:54)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:616)\n        at org.apache.hadoop.util.RunJar.main(RunJar.java:156)\n</code></pre>\n\n<p>Every time i run ./nutch crawl urls -dir crawl -depth 3 -topN 5 . nutch decides to throw this error. I have both my nutch-site.xml &amp; nutch-default.xml set with. </p>\n\n<pre><code> &lt;property&gt;\n  &lt;name&gt;http.agent.name&lt;/name&gt;\n  &lt;value&gt;blah&lt;/value&gt;\n  &lt;/property&gt;\n</code></pre>\n\n<p>Took the description out to make its easier to read. But I fail to see where else the agent name can be specified. if anybody has any advice I would be grateful. </p>\n", "creation_date": 1309870273, "score": 9},
{"title": "crawler distributed over different geographical locations", "view_count": 190, "owner": {"user_id": 46920, "answer_count": 8, "creation_date": 1229486566, "accept_rate": 88, "view_count": 187, "reputation": 600}, "is_answered": true, "answers": [{"question_id": 6565329, "owner": {"user_id": 823435, "accept_rate": 100, "link": "http://stackoverflow.com/users/823435/anjunatl", "user_type": "registered", "reputation": 561}, "body": "<p>If you use Nutch like buffer suggested, there is a script on the <a href=\"http://wiki.apache.org/nutch/MergeCrawl\" rel=\"nofollow\">Nutch Wiki</a> that may be able to help you. You would just need to get the linkdb, crawldb, and segments from each system to the central server before doing this - I think attempting to access those resources remotely would take a long time during the indexing process.</p>\n", "creation_date": 1310059706, "is_accepted": true, "score": 1, "last_activity_date": 1310059706, "answer_id": 6614625}], "question_id": 6565329, "tags": ["python", "web-crawler", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/6565329/crawler-distributed-over-different-geographical-locations", "last_activity_date": 1310059706, "accepted_answer_id": 6614625, "body": "<p>I have a few desktop machines at different geographical locations.  I need to create a crawler with clients at each desktop machine and a central server where data is indexed.  Is it possible to create such a crawler in Nutch? Are there any alternatives. Python based crawlers would be preferable.</p>\n", "creation_date": 1309724440, "score": 0},
{"title": "web crawling tools which support interacting with target sites before begining to crawl", "view_count": 621, "is_answered": true, "answers": [{"question_id": 6507040, "owner": {"user_id": 781695, "accept_rate": 87, "link": "http://stackoverflow.com/users/781695/medorator", "user_type": "registered", "reputation": 6821}, "body": "<p>You could try hooking up selenium to a python based crawler like <a href=\"http://scrapy.org/\" rel=\"nofollow\">scrapy</a> . Whenever AJAX needs to be handled, it'll fire up an external process for scraping with selenium.</p>\n", "creation_date": 1309313707, "is_accepted": false, "score": 0, "last_activity_date": 1309313707, "answer_id": 6515079}, {"question_id": 6507040, "owner": {"user_id": 432844, "link": "http://stackoverflow.com/users/432844/julien-nioche", "user_type": "registered", "reputation": 939}, "body": "<p>Nutch does not handle AJAX, cookies or any of the user interactions that you described. </p>\n", "creation_date": 1309537244, "is_accepted": false, "score": 1, "last_activity_date": 1309537244, "answer_id": 6550572}], "question_id": 6507040, "tags": ["web-crawler", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/6507040/web-crawling-tools-which-support-interacting-with-target-sites-before-begining-t", "last_activity_date": 1309537244, "owner": {"user_id": 216575, "answer_count": 17, "creation_date": 1258916527, "accept_rate": 55, "view_count": 198, "location": "Australian Capital Territory, Australia", "reputation": 2600}, "body": "<p>I am looking for a crawler which is capable of handling pages with Ajax and being able to perform certain user interactions with the target site before starting to crawl the site (e.g., clicking on certain menu items, filling some forms, etc...).I tried webdriver/selenium (which are really web scraping tools) and now I am want to know if there is any crawler available that supports emulating certain user interactions before starting to crawl ? (In Java or Python or Ruby ...)</p>\n\n<p>Thanks</p>\n\n<p>ps - Can nutch do this ? If yes, I appreciate any link describing this.</p>\n", "creation_date": 1309266957, "score": 0},
{"title": "Custom Parser for Nutch (or open source .NET Crawler)", "view_count": 1020, "owner": {"age": 32, "answer_count": 148, "creation_date": 1257340902, "user_id": 202609, "accept_rate": 87, "view_count": 415, "location": "Johannesburg", "reputation": 3055}, "is_answered": true, "answers": [{"question_id": 6106994, "owner": {"user_id": 432844, "link": "http://stackoverflow.com/users/432844/julien-nioche", "user_type": "registered", "reputation": 939}, "body": "<p>See <a href=\"https://issues.apache.org/jira/browse/NUTCH-585\" rel=\"nofollow\">https://issues.apache.org/jira/browse/NUTCH-585</a>\nand <a href=\"https://issues.apache.org/jira/browse/NUTCH-961\" rel=\"nofollow\">https://issues.apache.org/jira/browse/NUTCH-961</a></p>\n\n<p>BTW you'd get a more relevant audience by posting to the Nutch user list</p>\n", "creation_date": 1306336726, "is_accepted": true, "score": 1, "last_activity_date": 1306336726, "answer_id": 6126823}, {"question_id": 6106994, "owner": {"user_id": 81194, "link": "http://stackoverflow.com/users/81194/claude", "user_type": "registered", "reputation": 827}, "body": "<p>You can implement a Nutch filter (I like <a href=\"http://jericho.htmlparser.net/docs/index.html\" rel=\"nofollow\">Jericho HTML Parser</a>) to extract only the parts of the page you need to index using DOM manipulation. You can use the <a href=\"http://jericho.htmlparser.net/docs/javadoc/net/htmlparser/jericho/TextExtractor.html\" rel=\"nofollow\">TextExtractor</a> class to grab clean text (sans HTML tags) to be used in your index. I usually save that data in custom fields.</p>\n", "creation_date": 1309477399, "is_accepted": false, "score": 1, "last_activity_date": 1309477399, "answer_id": 6542253}], "question_id": 6106994, "tags": ["asp.net", "solr", "web-crawler", "nutch", "solrnet"], "answer_count": 2, "link": "http://stackoverflow.com/questions/6106994/custom-parser-for-nutch-or-open-source-net-crawler", "last_activity_date": 1309477399, "accepted_answer_id": 6126823, "body": "<p>I have been using Nutch/Solr/SolrNet for my search solutions, I must say, it works a treat. On a new site I'm working on, I am using Master pages, as a result, content in the header and footer is getting indexed and distorts the results. For example, I have a link to the Contact Us page in the header. Now, when I search for 'Contact' the result returns all the pages in the site.</p>\n\n<p>Is there a customizable Nutch parser that i can maybe pass a div id and then it only indexes content inside the div.</p>\n\n<p>Or if there are .NET based crawlers that I can customize.</p>\n", "creation_date": 1306221449, "score": 0},
{"title": "Run Nutch on existing Hadoop cluster", "view_count": 3976, "is_answered": true, "answers": [{"question_id": 5301883, "owner": {"user_id": 435820, "link": "http://stackoverflow.com/users/435820/catosmandros", "user_type": "registered", "reputation": 144}, "body": "<p>I ran Nutch on an existing hadoop cluster modifying the bin/nutch script and then copying the nutch config files on the hadoop folders, modifying the TS and NS parameters. Did you try it that way?</p>\n", "creation_date": 1300349988, "is_accepted": false, "score": 0, "last_activity_date": 1300349988, "answer_id": 5336538}, {"question_id": 5301883, "owner": {"user_id": 246692, "accept_rate": 56, "link": "http://stackoverflow.com/users/246692/mihaela", "user_type": "registered", "reputation": 129}, "body": "<p>Finally I ran Nutch MapReduce jobs (Injector, Generator and Fetcher) using the bin/hadoop script with no modification with respect of Nutch.<br/><br/>\nThe problem is with <code>org.apache.hadoop.util.RunJar</code> class (the class which runs a hadoop job jar when calling <code>hadoop jar &lt;jobfile&gt; jobClass</code>) that adds to the classpath from the job jar file only the <code>classes/</code> and <code>lib/</code> subdirectories and Nutch jobs have a <code>plugins</code> subfolder also which containes the plugins used at runtime. I tried overriding the property <code>mapreduce.job.jar.unpack.pattern</code> to value <code>(?:classes/|lib/|plugins/).*</code> so that the RunJar class add also the plugins to the classpath but it didn't work.<br/><br>\nAfter looking in Nutch code I saw that it uses a property <code>plugin.folders</code> which controls where can be found the plugins. So what I have done and it worked was to copy the plugins subfolder from the job jar to a shared drive and set the property <code>plugin.folders</code> to that path each time I run a Nutch job. For example: <br></p>\n\n<pre><code> hadoop jar &lt;path to nutch job file&gt; org.apache.nutch.fetcher.Fetcher -conf ../conf/nutch-default.xml -Dplugin.folders=&lt;path to plugins folder&gt; &lt;segment path&gt;\n</code></pre>\n\n<p>In the <code>conf/nutch-default.xml</code> file I have set some properties like the agent name, proxy host and port, timeout, content limit, etc.</p>\n\n<p>I have also tried creating the Nutch job jar with the plugins subfolder in the lib subfolder and then setting the <code>plugin.folders</code> property to value <code>lib/plugins</code> but it didn't work....</p>\n", "creation_date": 1300376514, "is_accepted": false, "score": 4, "last_activity_date": 1300376514, "answer_id": 5341415}], "question_id": 5301883, "tags": ["hadoop", "cluster-computing", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/5301883/run-nutch-on-existing-hadoop-cluster", "last_activity_date": 1307491005, "owner": {"user_id": 246692, "answer_count": 3, "creation_date": 1262981141, "accept_rate": 56, "view_count": 52, "reputation": 129}, "body": "<p>We have a Hadoop cluster (Hadoop 0.20) and I want to use Nutch 1.2 to import some files over HTTP into HDFS, but I couldn't get Nutch running on the cluster.</p>\n\n<p>I've updated the <b>$HADOOP_HOME/bin/hadoop</b> script to add the Nutch jars to the classpath (actually I've copied the classpath setup from <b>$NUTCH_HOME/bin/nutch</b> script without the part that adds the $NUTCH_HOME/lib/* to the classpath) and then I tried running the following command to inject URLS:</p>\n\n<p><code>hadoop jar nutch*.jar org.apache.nutch.crawl.Injector -conf conf/nutch-site.xml crawl_path urls_path</code></p>\n\n<p>but I got <code>java.lang.RuntimeException: x point  org.apache.nutch.net.URLNormalizer not found.</code></p>\n\n<p>The <b>$NUTCH_HOME/conf/nutch-site.xml</b> configuration file sets the property </p>\n\n<pre><code>&lt;property&gt;\n    &lt;name&gt;mapreduce.job.jar.unpack.pattern&lt;/name&gt;\n    &lt;value&gt;(?:classes/|lib/|plugins/).*&lt;/value&gt;\n&lt;/property&gt;\n</code></pre>\n\n<p>as workaround to force unpacking of the /plugin directory as suggested at: <a href=\"https://issues.apache.org/jira/browse/NUTCH-937?page=com.atlassian.streams.streams-jira-plugin%3aactivity-stream-issue-tab#issue-tabs\" rel=\"nofollow\">When nutch is run on hadoop > 0.20.2 (or cdh) it will not find plugins because MapReduce will not unpack plugin/ directory from the job's pack (due to MAPREDUCE-967)</a> but it seems that for me it didn't work.</p>\n\n<p>Has anybody encountered this problem? Do you have a step by step tutorial on how to run Nutch on existing Hadoop?</p>\n\n<p>Thanks in advance, \n<br>mihaela</p>\n", "creation_date": 1300122457, "score": 0},
{"title": "Nutch versus Solr", "view_count": 4609, "owner": {"age": 28, "answer_count": 5, "creation_date": 1271253009, "user_id": 316563, "accept_rate": 72, "view_count": 95, "location": "Ukraine", "reputation": 2363}, "is_answered": true, "answers": [{"question_id": 2818112, "owner": {"user_id": 183100, "link": "http://stackoverflow.com/users/183100/pascal-dimassimo", "user_type": "registered", "reputation": 5766}, "body": "<p>Nutch is a framework to build web crawler and search engines. Nutch can do the whole process from collecting the web pages to building the inverted index. It can also push those indexes to Solr.</p>\n\n<p>Solr is mainly a search engine with support for faceted searches and many other neat features. But Solr doesn't fetch the data, you have to feed it.</p>\n\n<p>So maybe the first thing you have to ask in order to choose between the two is whether or not you have the data to be indexed already available (in XML, in a CMS or a database.). In that case, you should probably just use Solr and feed it that data. On the other hand, if you have to fetch the data from the web, you are probably better of with Nutch.</p>\n", "creation_date": 1273668381, "is_accepted": true, "score": 15, "last_activity_date": 1273668381, "answer_id": 2818860}], "question_id": 2818112, "tags": ["solr", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/2818112/nutch-versus-solr", "last_activity_date": 1307345039, "accepted_answer_id": 2818860, "body": "<p>Currently collecting information where I should use Nutch/Solr/Nutch with Solr (domain - vertical web search). Could you suggest me?</p>\n", "creation_date": 1273662048, "score": 7},
{"title": "Nutch Newbie - JSP with html problem", "view_count": 308, "owner": {"user_id": 544006, "answer_count": 3, "creation_date": 1292452039, "accept_rate": 79, "view_count": 73, "reputation": 157}, "is_answered": true, "answers": [{"question_id": 6089504, "owner": {"user_id": 484046, "accept_rate": 81, "link": "http://stackoverflow.com/users/484046/millebii", "user_type": "registered", "reputation": 976}, "body": "<p>I have my search totaly modified.  However I  have my <code>&lt;html&gt;...</code> tags after the second scriptlet ie <code>&lt;% %&gt;</code> not <code>&lt;%@ page</code>.<br>\nAs for your index.jsp modified it has a redirection <code>response.sendRedirect</code> and therefore it looks normal to me that you see nothing.<br>\nAlso I presume you took care of loading the jsp pages at the right place under the tomcat/webapps tree, because the standard ant make file doesn't. So I ended up adding some Ant task to patch my test website.<br>\nBeware if you are going to change the .jar files you also need to restart Tomcat.</p>\n", "creation_date": 1306091266, "is_accepted": true, "score": 1, "last_activity_date": 1306091266, "answer_id": 6090102}], "question_id": 6089504, "tags": ["java", "jsp", "website", "lucene", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/6089504/nutch-newbie-jsp-with-html-problem", "last_activity_date": 1306091266, "accepted_answer_id": 6090102, "body": "<p>System: Mac OSX</p>\n\n<p>I have set up nutch so that it crawls and indexes my site. It also returns search results. My problem is that I want to customise the Nutch index.jsp and search.jsp pages to fit with my site. Ive read up and on jsp and it says its just a matter of putting in the html tags and then using &lt;% %> to enclose the Java scriplets you want. For some reason nothing changes when i edit the files (index and search)</p>\n\n<p>Here is what the original file displays:</p>\n\n<pre><code>&lt;%@ page\n  session=\"false\"\n  import=\"java.io.*\"\n  import=\"java.util.*\"\n%&gt;&lt;%\n  String language =\n    ResourceBundle.getBundle(\"org.nutch.jsp.search\", request.getLocale())\n    .getLocale().getLanguage();\n  String requestURI = HttpUtils.getRequestURL(request).toString();\n  String base = requestURI.substring(0, requestURI.lastIndexOf('/'));\n  response.sendRedirect(language + \"/\");\n%&gt;\n</code></pre>\n\n<p>Here is my edited version with sum gibberish test added to test it:</p>\n\n<pre><code>&lt;html&gt;\n&lt;head&gt;\n&lt;/head&gt;\n&lt;body&gt;\n\ngigigyigig\n\n\n&lt;%@ page\n  session=\"false\"\n  import=\"java.io.*\"\n  import=\"java.util.*\"\n%&gt;&lt;%\n  String language =\n    ResourceBundle.getBundle(\"org.nutch.jsp.search\", request.getLocale())\n    .getLocale().getLanguage();\n  String requestURI = HttpUtils.getRequestURL(request).toString();\n  String base = requestURI.substring(0, requestURI.lastIndexOf('/'));\n  response.sendRedirect(language + \"/\");\n%&gt;\n\nghjgjkbkhb\nhjgjvjhvj\n\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre>\n\n<p>Nothing has changed tho and the nutch homepage/index.jsp still displays the same as original. This is my first encounter with JSP so its just what ive picked up so far. Can anyone tell me why the page isnt displaying the html with gibberish typed??</p>\n", "creation_date": 1306084619, "score": 1},
{"title": "Do ivy dependency revisions have anything to do with svn&#39;s?", "view_count": 338, "owner": {"user_id": 300248, "answer_count": 60, "creation_date": 1269371954, "accept_rate": 74, "view_count": 602, "location": "Bolzano, Italy", "reputation": 3706}, "is_answered": true, "answers": [{"question_id": 5941789, "owner": {"user_id": 300248, "accept_rate": 74, "link": "http://stackoverflow.com/users/300248/simpatico", "user_type": "registered", "reputation": 3706}, "body": "<blockquote>\n  <p>Neither do they correspond to maven versions (the version would be 4.0-SNAPSHOT). So where would I typically find the available ivy revisions?</p>\n</blockquote>\n\n<p>False, they correspond.</p>\n\n<pre><code>So where would I typically find the available ivy revisions? \n</code></pre>\n\n<p>Maven central repo (<a href=\"http://search.maven.org/#search%7Cga%7C1%7Cg%3a%22org.apache.solr.solr%22\" rel=\"nofollow\">solr in maven central repo</a>)</p>\n\n<blockquote>\n  <p>Also, is it possible to checkout the dependency against a given svn revision? \n  mvn install locally, and then refer to your snapshot. \n  For Solr:</p>\n</blockquote>\n\n<pre><code>&lt;dependency org=\"org.apache.solr\" name=\"solr-solrj\" rev=\"4.0-SNAPSHOT\"\n            conf=\"*-&gt;default\"/&gt;\n</code></pre>\n", "creation_date": 1305016254, "is_accepted": true, "score": 2, "last_activity_date": 1305016254, "answer_id": 5947321}], "question_id": 5941789, "tags": ["svn", "solr", "ivy", "dependency-management", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/5941789/do-ivy-dependency-revisions-have-anything-to-do-with-svns", "last_activity_date": 1305016254, "accepted_answer_id": 5947321, "body": "<p>With no background on <a href=\"http://ant.apache.org/ivy/history/2.1.0/ivyfile/dependency.html\" rel=\"nofollow\">ivy dependencies</a> I'm trying to <a href=\"http://www.mail-archive.com/user@nutch.apache.org/msg02576.html\" rel=\"nofollow\">build nutch with solr 4.0</a>, but I'm not sure how to change the nutch ivy dependency on solr in the ivy.xml:</p>\n\n<pre><code>&lt;dependency org=\"org.apache.solr\" name=\"solr-solrj\" *rev=\"**1.4.1**\"*\nconf=\"*-&gt;default\" /&gt;\n</code></pre>\n\n<p>I've no clue on where those revisions are set, as they certainly don't correspond to svn revisions. Neither do they correspond to maven versions (the version would be 4.0-SNAPSHOT). So where would I typically find the available ivy revisions?\nAlso, is it possible to checkout the dependency against a given svn revision? At that revision it works as I want, but I'm not sure about the latest instead.</p>\n", "creation_date": 1304970808, "score": 1},
{"title": "Nutch 1.2 (crawl or parse) mp3", "view_count": 492, "is_answered": true, "answers": [{"question_id": 5896942, "owner": {"user_id": 154146, "link": "http://stackoverflow.com/users/154146/brian", "user_type": "registered", "reputation": 1203}, "body": "<p>There's a parse-mp3 plugin available with Nutch for parsing out the headers of MP3 audio files.</p>\n\n<p>For a complete list of Nutch Plugins, checkout <a href=\"http://wiki.apache.org/nutch/PluginCentral\" rel=\"nofollow\">Plugin Central</a></p>\n", "creation_date": 1304716385, "is_accepted": false, "score": 1, "last_activity_date": 1304716385, "answer_id": 5917072}], "question_id": 5896942, "tags": ["solr", "mp3", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/5896942/nutch-1-2-crawl-or-parse-mp3", "last_activity_date": 1304718054, "owner": {"user_id": 739805, "view_count": 2, "answer_count": 0, "creation_date": 1304594546, "reputation": 6}, "body": "<p>I have a question about Nutch v1.2.</p>\n\n<p>Can someone explain to me how I can go about crawling/parsing an MP3 and indexing the result into Solr 1.4.</p>\n\n<p>Is there a specific plugin I can use to do this?</p>\n\n<p>Thanks for your help!</p>\n", "creation_date": 1304594909, "score": 1},
{"title": "nutch field problem", "view_count": 154, "is_answered": true, "answers": [{"question_id": 1010661, "owner": {"user_id": 77308, "accept_rate": 67, "link": "http://stackoverflow.com/users/77308/shashikant-kore", "user_type": "registered", "reputation": 3934}, "body": "<p>I think, you are using StandardAnalyzer for parsing the input query, which will tokenize your input query \"irn_CA\" into two tokens - \"irn\" and \"CA\". Since the index has \"irn_CA\" as single token, it won't match.</p>\n\n<p>Try using <a href=\"http://lucene.apache.org/java/2%5F4%5F0/api/org/apache/lucene/analysis/KeywordAnalyzer.html\" rel=\"nofollow\">KeywordAnalyzer</a> for while searching. It will generate single token for the query string and match the indexed token correctly.</p>\n", "creation_date": 1245303738, "is_accepted": false, "score": 2, "last_activity_date": 1245303738, "answer_id": 1010949}, {"question_id": 1010661, "owner": {"user_id": 260541, "link": "http://stackoverflow.com/users/260541/rhu", "user_type": "registered", "reputation": 766}, "body": "<p>I think the searcher bean forces everything to lowercase...so make the state is in lower case when adding to the index:</p>\n\n<pre><code>Field stateField = new Field(\"state\",\"irn_\" + state.toLowerCase(), Field.Store.NO, Field.Index.UN_TOKENIZED);\n</code></pre>\n\n<p>and when you query: 'state:irn_ca' instead of 'state:irn_CA'.</p>\n\n<p>I also note you prefixed with 'irn_' - good call, otherwise the highlighter flags up the the query.</p>\n", "creation_date": 1304525237, "is_accepted": false, "score": 0, "last_activity_date": 1304525237, "answer_id": 5886435}], "question_id": 1010661, "tags": ["lucene", "field", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/1010661/nutch-field-problem", "last_activity_date": 1304525237, "owner": {"user_id": 104015, "answer_count": 1, "creation_date": 1241863152, "accept_rate": 44, "view_count": 8383, "reputation": 34802}, "body": "<p>I was using something like:</p>\n\n<pre><code>Field notdirectory = new Field(\"notdirectory\",\"1\", Field.Store.NO, Field.Index.UN_TOKENIZED);\n</code></pre>\n\n<p>and queries like \"notdirectory:1\" can be processed quite well all the time.</p>\n\n<p>But recently I've changed the \"Field.Store.NO, Field.Index.UN_TOKENIZED\" to index a non-numeric string:</p>\n\n<pre><code>Field stateField = new Field(\"state\",\"irn_\" + state, Field.Store.NO, Field.Index.UN_TOKENIZED);\n</code></pre>\n\n<p>and queries like \"state:irn_CA\" can never fetch any results any more,even though I watch through hadoop logs that \"irn_CA\" is added to \"state\" field in fact.</p>\n\n<p>So I doubt for Fields that satisfy \"Field.Store.NO, Field.Index.UN_TOKENIZED\",only numeric Fields can searchable,but I didn't see any documents about that.</p>\n\n<p>So what's the true reason for this?</p>\n", "creation_date": 1245296354, "score": 0},
{"title": "Boosting in Solr based on date with conditions", "view_count": 560, "is_answered": true, "answers": [{"question_id": 5820925, "owner": {"user_id": 21339, "accept_rate": 76, "link": "http://stackoverflow.com/users/21339/bajafresh4life", "user_type": "registered", "reputation": 7899}, "body": "<p>During indexing, apply your logic and add the date to a \"date_boost\" field.  During search time, boost using the date_boost field.</p>\n", "creation_date": 1304038053, "is_accepted": false, "score": 0, "last_activity_date": 1304038053, "answer_id": 5826691}, {"question_id": 5820925, "owner": {"user_id": 347165, "accept_rate": 77, "link": "http://stackoverflow.com/users/347165/xodarap", "user_type": "registered", "reputation": 6997}, "body": "<p>You probably do want to do it in the business layer, as baja fresh suggested. However, there is a pure Solr way to do it: just use the <a href=\"http://wiki.apache.org/solr/FunctionQuery#map\" rel=\"nofollow\">map</a> function to map anything in the range <code>(timestamp,timestamp)</code> to <code>now</code>. </p>\n", "creation_date": 1304132850, "is_accepted": false, "score": 1, "last_activity_date": 1304132850, "answer_id": 5839197}], "question_id": 5820925, "tags": ["lucene", "solr", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/5820925/boosting-in-solr-based-on-date-with-conditions", "last_activity_date": 1304132850, "owner": {"user_id": 729595, "answer_count": 0, "creation_date": 1304003446, "view_count": 4, "location": "New York", "reputation": 6}, "body": "<p>I am trying to boost newer documents in Solr queries. The ms function <a href=\"http://wiki.apache.org/solr/SolrRelevancyFAQ#How_can_I_boost_the_score_of_newer_documents\" rel=\"nofollow\">Solr FAQ</a> seems to be the right way to go, but I need to add an additional condition:\nI am using the last-Modified-Date from crawled web pages as the date to consider, and that does not always provide a meaningful date. Therefore I would like the function to only boost documents where the date (not time) found in the last-Modified-Date is different from the timestamp, eliminating results that just return the current date as the last-Modified-Date. Suggestions are appreciated!</p>\n", "creation_date": 1304003826, "score": 0},
{"title": "changing the url domain in nutch index programmatically", "view_count": 186, "owner": {"age": 28, "answer_count": 16, "creation_date": 1296141388, "user_id": 592427, "accept_rate": 82, "view_count": 27, "location": "Singapore, Singapore", "reputation": 342}, "is_answered": true, "answers": [{"question_id": 5248965, "owner": {"user_id": 271458, "link": "http://stackoverflow.com/users/271458/slick86", "user_type": "registered", "reputation": 1772}, "body": "<p>If you are only working with the index after the crawl, you can open up the index with a Lucene IndexReader and add new records with an IndexModifier. Your can page through each document, create a copy of the document with the new url, and then add the new document back to the index. You will need to delete the original document if you do not with it to persist in the index. </p>\n\n<p>Lucene does not allow index updating but rather the deletion of a old record and the insertion of a new one.</p>\n", "creation_date": 1302883529, "is_accepted": true, "score": 1, "last_activity_date": 1302883529, "answer_id": 5679485}], "question_id": 5248965, "tags": ["nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/5248965/changing-the-url-domain-in-nutch-index-programmatically", "last_activity_date": 1302883529, "accepted_answer_id": 5679485, "body": "<p>i'm currently making search engine for a website content (only for searching within that website). however, i'm thinking of building the index in the staging server. it's something like this:\n1. i stage my code at www.staging_server.com\n2. i index the pages at www.staging_server.com\n3. i copy codes at www.staging_server.com to www.production_server.com\n4. i copy the index to www.production_server.com index???</p>\n\n<p>the problem with step 4 is that the urls in the index created in step 2 is in the form of www.staging_server.com/index, www.staging_server.com/whatever, www.staging_server/anything. but what i need is www.production_server.com/index, www.production_server.com/whatever, www.production_server.com/anything</p>\n\n<p>i'm wondering whether the urls in the index can be changed programmatically. and if so, how to do that?</p>\n\n<p>note: i'm nutch beginner, so please be merciful to me</p>\n", "creation_date": 1299688573, "score": 0},
{"title": "Indexing HTML with solr", "view_count": 1449, "is_answered": false, "answers": [{"question_id": 5595193, "owner": {"user_id": 608436, "link": "http://stackoverflow.com/users/608436/karthik", "user_type": "registered", "reputation": 118}, "body": "<p>have you looked at the HTML different HTML tokenizers available within solr ?</p>\n\n<p><a href=\"http://wiki.apache.org/solr/AnalyzersTokenizersTokenFilters#solr.HTMLStripWhitespaceTokenizerFactory\" rel=\"nofollow\">http://wiki.apache.org/solr/AnalyzersTokenizersTokenFilters#solr.HTMLStripWhitespaceTokenizerFactory</a></p>\n\n<p>they should help you resolve this issue. you should not index the html tags themselves. however if you need to uniquely identify certain tags then you will need to create individual fields and store the contents of those special tags in those fields.</p>\n", "creation_date": 1302284976, "is_accepted": false, "score": -1, "last_activity_date": 1302284976, "answer_id": 5598866}], "question_id": 5595193, "tags": ["solr", "design-patterns", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/5595193/indexing-html-with-solr", "last_activity_date": 1302503700, "owner": {"user_id": 698651, "view_count": 3, "answer_count": 0, "creation_date": 1302266586, "reputation": 6}, "body": "<p>I am crawling our large website(s) with nutch and then indexing with solr and the results a pretty good.  However, there are several menu structures across the site that index and spoil the results of a query. </p>\n\n<p>Each of these menus is clearly defined in a DIV so <code>&lt;div id=\"RHBOX\"&gt; ... &lt;/div&gt; or &lt;div id=\"calendar\"&gt; ...&lt;/div&gt;</code> and several others.</p>\n\n<p>I need to, at some point, delete the content in these DIVS.  </p>\n\n<p>I am guessing that the right place is during indexing by solr but cannot work out how.</p>\n\n<p>A pattern would look something like <code>(&lt;div id=\"calendar\"&gt;).*?(&lt;\\/div&gt;)</code> but i cannot get that to work in <code>&lt;tokenizer class=\"solr.PatternTokenizerFactory\" pattern=\"(&lt;div id=\"calendar\"&gt;).*?(&lt;\\/div&gt;)\" /&gt;</code>  and I am not really sure where to put it in schema.xml.</p>\n\n<p>When I do put that pattern in schema.xml does not parse.</p>\n\n<p>I am adding this line so the edit sticks</p>\n", "creation_date": 1302266586, "score": 1},
{"title": "wrote code in java for nutch", "view_count": 584, "is_answered": true, "answers": [{"question_id": 5610238, "owner": {"user_id": 577423, "link": "http://stackoverflow.com/users/577423/howard", "user_type": "registered", "reputation": 27615}, "body": "<p>The line</p>\n\n<pre><code>char[] parse.getData() = input.trim().toCharArray();\n</code></pre>\n\n<p>will give you a compile error because the left hand side is not a variable. Please replace <code>parse.getData()</code> by a unique variable name (e.g. <code>parsedData</code>) in this line and the following lines.</p>\n\n<p>Second the import of</p>\n\n<pre><code>import org.apache.nutch.parse.getData().parse.getData();\n</code></pre>\n\n<p>will also fail. Looks a lot like a text replace issue.</p>\n", "creation_date": 1302418857, "is_accepted": false, "score": 1, "last_activity_date": 1302418857, "answer_id": 5610263}], "question_id": 5610238, "tags": ["java", "parsing", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/5610238/wrote-code-in-java-for-nutch", "last_activity_date": 1302418857, "owner": {"user_id": 647856, "answer_count": 0, "creation_date": 1299488823, "accept_rate": 33, "view_count": 23, "reputation": 35}, "body": "<p>hello:\nI'm writing code in java for nutch(open source search engine) to remove the movments from arabic words in the indexer.\nI don't know what is the error in it.\nTthis is the code:</p>\n\n<pre><code>package com.mycompany.nutch.indexing;\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.io.Text;\nimport org.apache.log4j.Logger;\nimport org.apache.nutch.crawl.CrawlDatum;\nimport org.apache.nutch.crawl.Inlinks;\nimport org.apache.nutch.indexer.IndexingException;\nimport org.apache.nutch.indexer.IndexingFilter;\nimport org.apache.nutch.indexer.NutchDocument;\nimport org.apache.nutch.parse.getData().parse.getData();\n\n\npublic class InvalidUrlIndexFilter implements IndexingFilter {\n\n  private static final Logger LOGGER = \n    Logger.getLogger(InvalidUrlIndexFilter.class);\n\n  private Configuration conf;\n\n  public void addIndexBackendOptions(Configuration conf) {\n    // NOOP\n    return;\n  }\n\n  public NutchDocument filter(NutchDocument doc, Parse parse, Text url,\n      CrawlDatum datum, Inlinks inlinks) throws IndexingException {\n    if (url == null) {\n      return null;\n    }\n\n\n\n    char[] parse.getData() = input.trim().toCharArray();\n        for(int p=0;p&lt;parse.getData().length;p++)\n          if(!(parse.getData()[p]=='\u064e'||parse.getData()[p]=='\u064b'||parse.getData()[p]=='\u064f'||parse.getData()[p]=='\u0650'||parse.getData()[p]=='\u064d'||parse.getData()[p]=='\u064c' ||parse.getData()[p]=='\u0651'||parse.getData()[p]=='\u0652' ||parse.getData()[p]=='\"' ))\n            new String.append(parse.getData()[p]);\n\n    return doc;\n  }\n\n  public Configuration getConf() {\n    return conf;\n  }\n\n  public void setConf(Configuration conf) {\n    this.conf = conf;\n  }\n}\n</code></pre>\n\n<p>I think that the error is in using <code>parse.getdata()</code> but I don't know what I should use instead of it?</p>\n", "creation_date": 1302418480, "score": 0},
{"title": "How to omit JavaScript and comments using nutch crawl?", "view_count": 409, "owner": {"age": 43, "answer_count": 23, "creation_date": 1283184567, "user_id": 435140, "accept_rate": 100, "view_count": 42, "location": "Stockholm, Sweden", "reputation": 675}, "is_answered": true, "answers": [{"question_id": 5513936, "owner": {"user_id": 484046, "accept_rate": 81, "link": "http://stackoverflow.com/users/484046/millebii", "user_type": "registered", "reputation": 976}, "body": "<p>beware there are two different filter files, one for the one stop crawl command and the other for the step-by-step commands.\nFor the rest just build a regex that will match the urls you want to skip, add minus before and you shoulb de done.</p>\n", "creation_date": 1302186215, "is_accepted": true, "score": 0, "last_activity_date": 1302186215, "answer_id": 5582644}], "question_id": 5513936, "tags": ["nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/5513936/how-to-omit-javascript-and-comments-using-nutch-crawl", "last_activity_date": 1302186215, "accepted_answer_id": 5582644, "body": "<p>I am a newbie at this, trying to use <strong><a href=\"http://nutch.apache.org/\" rel=\"nofollow\">Nutch 1.2</a></strong> to fetch a site. I'm using only a Linux console to work with <strong>Nutch</strong> as I don't need anything else. My command looks like this <code><pre>\nbin/nutch crawl urls -dir crawled -depth 3\n</pre></code>\nWhere the folder <em>urls</em> is were I have my links and I do get the results to the folder <em>crawled</em>.\nAnd when I would like to see the results I type:<code><pre>bin/nutch readseg -dump crawled/segments/20110401113805 /home/nutch/dumpfiles</pre></code>\nThis works very fine, but I get a lot of broken links.\nNow, I do <strong>not</strong> want <strong>Nutch</strong> to follow JavaScript links, only regular links, could anyone give me a hint/help on how to do that?\nI've tried to edit the <em>conf/crawl-urlfilter.txt</em> with no results. I might have typed wrong commands!</p>\n\n<p>Any help appreciated! </p>\n", "creation_date": 1301663955, "score": 0},
{"title": "Zend lucene content field", "view_count": 528, "is_answered": true, "answers": [{"question_id": 4246995, "owner": {"user_id": 13803, "accept_rate": 100, "link": "http://stackoverflow.com/users/13803/neil-aitken", "user_type": "registered", "reputation": 6190}, "body": "<p>I don't know the nutch index format, but whenever I need to check a lucene index I use <a href=\"http://www.getopt.org/luke/\" rel=\"nofollow\">Luke - Lucene Index Toolbox</a></p>\n\n<p>It allows you to open an index directory, browse fields and run queries. Very helpful if you're using an unfamiliar index.</p>\n", "creation_date": 1290441356, "is_accepted": false, "score": 2, "last_activity_date": 1290441356, "answer_id": 4247339}], "question_id": 4246995, "tags": ["php", "lucene", "nutch", "zend-search-lucene"], "answer_count": 1, "link": "http://stackoverflow.com/questions/4246995/zend-lucene-content-field", "last_activity_date": 1301505326, "owner": {"user_id": 330936, "answer_count": 8, "creation_date": 1272825795, "accept_rate": 67, "view_count": 75, "reputation": 878}, "body": "<p>I have indexed a site using Nutch and now I am searching the index using the Zend Lucene library.</p>\n\n<p>I've actually pulled the Zend libraries into Codeigniter but it is all Zend doing the work.</p>\n\n<p>I can display the title, score and url fine but I can't find the name of the field to display the content from the page.</p>\n\n<p>So far I have the following code</p>\n\n<pre><code>$index = new Zend_Search_Lucene('C:\\nutch\\nutch-0.9\\my-search\\index');\n\n$query = $this-&gt;input-&gt;post('searchQuery');\n\n$hits = $index-&gt;find($query);\n\necho \"&lt;p&gt;Index contains \" . $index-&gt;count() . \" documents.&lt;/p&gt;\";\n\necho \"&lt;p&gt;Search for '\" . $query . \"' returned \" . count($hits) . \" hits&lt;/p&gt;\";\n\nforeach ($hits as $hit) \n{\n\n  echo \"&lt;h4&gt;\" . $hit-&gt;title . \"&lt;/h4&gt;\";\n\n  echo \"&lt;p&gt;&lt;b&gt;Score:&lt;/b&gt; \" . sprintf('%.2f', $hit-&gt;score) . \"&lt;/p&gt;\";\n\n  echo \"&lt;p&gt;&lt;b&gt;Url:&lt;/b&gt; \" .\"&lt;a href='\" . $hit-&gt;url . \"'&gt;\" . $hit-&gt;url. \"&lt;/a&gt;&lt;/p&gt;\";\n\n}\n</code></pre>\n\n<p>Can anyone help out with the name of the field to display the content or a content summary?</p>\n\n<p>Thanks</p>\n", "creation_date": 1290439670, "score": 2},
{"title": "Will Nutch, the spider, index webpages it already has in it&#39;s index?", "view_count": 195, "is_answered": true, "answers": [{"question_id": 5320189, "owner": {"user_id": 484046, "accept_rate": 81, "link": "http://stackoverflow.com/users/484046/millebii", "user_type": "registered", "reputation": 976}, "body": "<p>Yes and no. By default Nutch will reindex pages only after a certain period 1 month (from memory), if the page hasn't change it will delay increase the re-indexing time too a maximum which is 3 month by default.\nAll settings are configurable in nutch-site.xml</p>\n", "creation_date": 1300463881, "is_accepted": false, "score": 3, "last_activity_date": 1300463881, "answer_id": 5354587}], "question_id": 5320189, "tags": ["nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/5320189/will-nutch-the-spider-index-webpages-it-already-has-in-its-index", "last_activity_date": 1300463881, "owner": {"age": 22, "answer_count": 5, "creation_date": 1275852339, "user_id": 359844, "accept_rate": 73, "view_count": 383, "location": "United States", "reputation": 1345}, "body": "<p>Does Nutch index pages again if they're already in the index? If so, how do I change this?</p>\n", "creation_date": 1300240439, "score": 2},
{"title": "Is there a way to end the Nutch spider without losing the information you spidered?", "view_count": 151, "is_answered": false, "answers": [{"question_id": 5320249, "owner": {"user_id": 484046, "accept_rate": 81, "link": "http://stackoverflow.com/users/484046/millebii", "user_type": "registered", "reputation": 976}, "body": "<p>Spider crawls and searching are independent so once you have produced an index you can make it available for search and start crawling your next segment.\nSo what is it really you are trying to achieve ?</p>\n", "creation_date": 1300462446, "is_accepted": false, "score": 0, "last_activity_date": 1300462446, "answer_id": 5354279}], "question_id": 5320249, "tags": ["nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/5320249/is-there-a-way-to-end-the-nutch-spider-without-losing-the-information-you-spider", "last_activity_date": 1300462446, "owner": {"age": 22, "answer_count": 5, "creation_date": 1275852339, "user_id": 359844, "accept_rate": 73, "view_count": 383, "location": "United States", "reputation": 1345}, "body": "<p>If I'm in the middle of a spider session and I close the spider all the data won't show up in the index. I would have to wait until the indexing has completed it's self. Is there a way I can end the spider and still be able to search through that data with the Nutch search? If so, how?</p>\n", "creation_date": 1300240995, "score": 0},
{"title": "crawl 1000 url per recrawl in nutch", "view_count": 395, "is_answered": false, "answers": [{"question_id": 5151207, "owner": {"user_id": 484046, "accept_rate": 81, "link": "http://stackoverflow.com/users/484046/millebii", "user_type": "registered", "reputation": 976}, "body": "<p>From the top of my head you should use </p>\n\n<pre><code>bin/nutch generate ... -topN 1000\n</code></pre>\n\n<p>Fetch only uses the result of generate.</p>\n", "creation_date": 1299604568, "is_accepted": false, "score": 0, "last_activity_date": 1299604568, "answer_id": 5235883}], "question_id": 5151207, "tags": ["nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/5151207/crawl-1000-url-per-recrawl-in-nutch", "last_activity_date": 1299604568, "owner": {"user_id": 614600, "answer_count": 0, "creation_date": 1297549172, "accept_rate": 47, "view_count": 39, "reputation": 281}, "body": "<p>hello\n i wrote an crawl script to crawl the url and i need to fetch 1000 urls per crawl session if i use this \n<code>bin/nutch fetch $s1 -threads 100 -topN 1000</code>\nit crawls more than 1000 url i have no idea y it happens can any one tell me how can i crawl exactly 1000 urls per crawl session in nutch1.2 </p>\n", "creation_date": 1298960671, "score": 0},
{"title": "Should I deploy on GAE or AWS?", "view_count": 678, "owner": {"user_id": 300248, "answer_count": 60, "creation_date": 1269371954, "accept_rate": 74, "view_count": 602, "location": "Bolzano, Italy", "reputation": 3706}, "is_answered": true, "answers": [{"question_id": 5203921, "owner": {"user_id": 413735, "accept_rate": 86, "link": "http://stackoverflow.com/users/413735/tobiasbayer", "user_type": "registered", "reputation": 7117}, "body": "<p>The number 1 limitation in GAE (no SQL support and thus, unbearable JPA restrictions) is gonna fall soon: <a href=\"https://spreadsheets0.google.com/viewform?hl=en&amp;hl=en&amp;formkey=dHBwRmpHV2VicFVVNi1PaFBvUGgydXc6MA&amp;ndplr=1\" rel=\"nofollow\">Google SQL Service Preview</a></p>\n", "creation_date": 1299343821, "is_accepted": true, "score": 2, "last_activity_date": 1299343821, "answer_id": 5205201}, {"last_edit_date": 1299370353, "owner": {"user_id": 350615, "link": "http://stackoverflow.com/users/350615/juha-palom%c3%a4ki", "user_type": "registered", "reputation": 17210}, "body": "<p>Amazon Elastic Beanstalk provides you with a managed and scalable Tomcat environment (more app servers may be coming in future) <a href=\"http://aws.amazon.com/elasticbeanstalk/\" rel=\"nofollow\">http://aws.amazon.com/elasticbeanstalk/</a></p>\n\n<p>Choice between AWS and GAE also depends a lot on your application. The design considerations behind the services are very much different. The goal of GAE is to provide ultimate scalability, but on the other hand eliminate fixed costs (if your application is serving only few customers, running it is very cheap). </p>\n\n<p>GAE maybe be good if your application is lightweight (does not depend on heavy frameworks) and if you can adapt to the datastore limitations. For generic Java applications I would recommend AWS, if the costs associated with Beanstalk and RDS are not too high.</p>\n", "question_id": 5203921, "creation_date": 1299369446, "is_accepted": false, "score": 2, "last_activity_date": 1299370353, "answer_id": 5207652}], "question_id": 5203921, "tags": ["java", "google-app-engine", "openid", "amazon-web-services", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/5203921/should-i-deploy-on-gae-or-aws", "last_activity_date": 1299370353, "accepted_answer_id": 5205201, "body": "<p>GAE:\n+1 Servlet Container ready (+ JVM6)\n+2 openid out-of-the-box support /API\n-1 JPA2.0 restrictions (inc. - no criteria API)\n-2 I cannot use nutch</p>\n\n<p>AWS:\n-1 I've to manage a server.\n-2 I've to implement OpenID myself.\n-3 I cannot use JPA2 with AWS.</p>\n\n<p>The points above illustrate how much/little I know, esp. of AWS. Is there how I could get an AWS Java-ready, JPA2-ready, and OpenID-ready with little-hassle as is the case with GAE.</p>\n", "creation_date": 1299329517, "score": 1},
{"title": "how to make search result group by host in nutch", "view_count": 122, "is_answered": false, "answers": [{"question_id": 4291918, "owner": {"user_id": 484046, "accept_rate": 81, "link": "http://stackoverflow.com/users/484046/millebii", "user_type": "registered", "reputation": 976}, "body": "<p>You could make 3 queries using site:\"foo.example.com\" field. Is that good enough for your use case ?</p>\n", "creation_date": 1291580535, "is_accepted": false, "score": 0, "last_activity_date": 1291580535, "answer_id": 4360941}], "question_id": 4291918, "tags": ["java", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/4291918/how-to-make-search-result-group-by-host-in-nutch", "last_activity_date": 1298266679, "owner": {"user_id": 522267, "view_count": 8, "answer_count": 0, "creation_date": 1290865469, "reputation": 8}, "body": "<p>i am making an small intranet search i use nutch for crawling and searching\ni have sub-domains like</p>\n\n<pre><code>z.example.com\na.example.com\nm.example.com\n</code></pre>\n\n<p>and if i search some value using nutch i am getting search result from all domains i need one result per sub domain </p>\n", "creation_date": 1290865469, "score": 0},
{"title": "Re-crawling websites fast", "view_count": 858, "is_answered": true, "answers": [{"question_id": 4618530, "owner": {"user_id": 183100, "link": "http://stackoverflow.com/users/183100/pascal-dimassimo", "user_type": "registered", "reputation": 5766}, "body": "<p>For Nutch, I have written a blog post on <a href=\"http://pascaldimassimo.com/2010/06/11/how-to-re-crawl-with-nutch/\" rel=\"nofollow\">how to re-crawl with Nutch</a>. Basically, you should set a low value for the <strong>db.fetch.interval.default</strong> setting. On the next fetch of a url, Nutch will use the last fetch time as the value for the <strong>If-Modified-Since</strong> HTTP header.  </p>\n", "creation_date": 1294340410, "is_accepted": false, "score": 0, "last_activity_date": 1294340410, "answer_id": 4618657}, {"question_id": 4618530, "owner": {"user_id": 105669, "link": "http://stackoverflow.com/users/105669/stefaan-colman", "user_type": "registered", "reputation": 3229}, "body": "<p>I recommend using <a href=\"http://curl.haxx.se/\" rel=\"nofollow\">curl</a> to fetch only the head and check if the Last-Modified header has changed.</p>\n\n<p>Example:</p>\n\n<pre><code> curl --head www.bankier.pl\n</code></pre>\n", "creation_date": 1294340607, "is_accepted": false, "score": 1, "last_activity_date": 1294340607, "answer_id": 4618686}], "question_id": 4618530, "tags": ["wget", "web-crawler", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/4618530/re-crawling-websites-fast", "last_activity_date": 1297992894, "owner": {"user_id": 565905, "view_count": 0, "answer_count": 0, "creation_date": 1294339577, "reputation": 11}, "body": "<p>I am developing a system that has to track content of few portals and check changes every night (for example download and index new sites that have been added during the day). Content of this portals will be indexed for searching. The problem is in re-crawling this portals - first crawling of portal takes very long (examples of portals: www.onet.pl, www.bankier.pl, www.gazeta.pl ) <strong>and I want to re-crawl it faster (as fast as it is possible)</strong> for example by checking date of modification but I have used <em>wget</em> to download www.bankier.pl but in response it complains that there is no last-modification header.\nIs there any way to re-crawl so many sites? I have also tried using Nutch but script for re-clawing seems not work properly - or it also depends on this headers (last-modified).\nMaybe there is a tool, crawler (like Nutch or something) that can update already downloaded sites by adding new one??</p>\n\n<p>Best regards,\nWojtek</p>\n", "creation_date": 1294339577, "score": 2},
{"title": "stopwords and synonyms in nutch", "view_count": 216, "is_answered": false, "answers": [{"question_id": 4878814, "owner": {"user_id": 484046, "accept_rate": 81, "link": "http://stackoverflow.com/users/484046/millebii", "user_type": "registered", "reputation": 976}, "body": "<p>It's no very easy but if you are going to be ready of writing your own Indexer plugin I think yes.</p>\n", "creation_date": 1297408581, "is_accepted": false, "score": 0, "last_activity_date": 1297408581, "answer_id": 4966513}], "question_id": 4878814, "tags": ["nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/4878814/stopwords-and-synonyms-in-nutch", "last_activity_date": 1297408581, "owner": {"user_id": 593710, "answer_count": 0, "creation_date": 1296213398, "accept_rate": 0, "view_count": 3, "reputation": 31}, "body": "<p>is there is any option to configure stop words and synonyms in nutch crawler</p>\n\n<pre><code>synonyms \n\ngov--&gt;government\n</code></pre>\n\n<p>some thing similar to this`</p>\n", "creation_date": 1296674197, "score": 0},
{"title": "Nutch querying on the fly", "view_count": 205, "owner": {"user_id": 163373, "answer_count": 2, "creation_date": 1251279453, "accept_rate": 56, "view_count": 293, "reputation": 2649}, "is_answered": true, "answers": [{"question_id": 4941459, "owner": {"user_id": 484046, "accept_rate": 81, "link": "http://stackoverflow.com/users/484046/millebii", "user_type": "registered", "reputation": 976}, "body": "<p>This is not possible.\nWhat you could do though is chunk the crawl cycle in a smaller number of URL's such that it will publish result more often whith this command<br>\n<code>nutch generate crawl/crawldb crawl/segments -topN  &lt;the limit&gt;</code><br>\nIf you are using the onestop command <code>craw</code>l it should be the same.</p>\n\n<p>I typically have a 24hours chunking scheme.</p>\n", "creation_date": 1297408327, "is_accepted": true, "score": 0, "last_activity_date": 1297408327, "answer_id": 4966482}], "question_id": 4941459, "tags": ["solr", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/4941459/nutch-querying-on-the-fly", "last_activity_date": 1297408327, "accepted_answer_id": 4966482, "body": "<p>I am a newbie to nutch and solr. Well relatively much newer to Solr than Nutch :)</p>\n\n<p>I have been using nutch for past two weeks, and I wanted to know if I can query or search on my nutch crawls on the fly(before it completes). I am asking this because the websites I am crawling are really huge and it takes around 3-4 days for a crawl to complete. I want to analyze some quick results while the nutch crawler is still crawling the URLs. Some one suggested me that Solr would make it possible.</p>\n\n<p>I followed the steps in <a href=\"http://www.lucidimagination.com/blog/2009/03/09/nutch-solr/\" rel=\"nofollow\">http://www.lucidimagination.com/blog/2009/03/09/nutch-solr/</a> for this. I see only the injected URLs are shown in the Solr search. I know I did something really foolish and the crawl never happened, I feel I am missing some information here. But I did all the steps mentioned in the link. I think somewhere in the process there should be a crawling happening and which is missed.</p>\n\n<p>Just wanted to see if some one could help me pointing this out and where I went wrong in the process. Forgive my foolishness and thanks for your patience.</p>\n\n<p>Cheers,\nAbi  </p>\n", "creation_date": 1297227365, "score": 0},
{"title": "SOLR AND+OR Query - how to do?", "view_count": 437, "is_answered": false, "question_id": 3423254, "tags": ["solr", "nutch", "boolean-expression"], "answer_count": 0, "link": "http://stackoverflow.com/questions/3423254/solr-andor-query-how-to-do", "last_activity_date": 1295942199, "owner": {"user_id": 412959, "view_count": 3, "answer_count": 0, "creation_date": 1281093599, "reputation": 6}, "body": "<p>In Nutch I'm using Solr as a search server.</p>\n\n<p>I would like to perform something query like \n(hillary AND clinton) OR (barack AND obama) OR (..)</p>\n\n<p>How to do it?</p>\n\n<p>For me single OR query works, like\nindia OR pakistan OR china\nquery.AddNotRequiredTerm(term);</p>\n\n<p>single AND query works\nindia AND paksitan AND China\nquery.AddRequiredTerm(term);</p>\n\n<p>But the combination doesn't give appropriate result. Any help is highly appreciated.</p>\n\n<p>Thanks in advance!!!</p>\n\n<p>-Subas</p>\n", "creation_date": 1281093599, "score": 1},
{"title": "How do we create a simple search engine using Lucene, Solr or Nutch?", "view_count": 7588, "owner": {"user_type": "does_not_exist"}, "is_answered": true, "answers": [{"question_id": 223536, "owner": {"user_id": 27308, "accept_rate": 73, "link": "http://stackoverflow.com/users/27308/jamie-love", "user_type": "registered", "reputation": 3700}, "body": "<p>If you've a Linux server, you could use <a href=\"http://www.beagle-project.org/\" rel=\"nofollow\">Beagle</a> to index them, and then just use the search functionality that comes with it. It has an (experimental) web search interface, and it can be hooked into the FireFox search box as well.</p>\n\n<p>It automatically indexes files as they're included, and I'd suspect that you'll find it much more efficient to enhance or fix beagle than to write your own search interface to Lucene.</p>\n", "creation_date": 1224624470, "is_accepted": false, "score": 0, "last_activity_date": 1224624470, "answer_id": 223574}, {"last_edit_date": 1224625259, "owner": {"user_id": 3474, "accept_rate": 83, "link": "http://stackoverflow.com/users/3474/erickson", "user_type": "registered", "reputation": 181723}, "body": "<p>Answering such a broad question in this forum will be tough. I'd recommend you check out the book <em><a href=\"http://www.manning.com/hatcher2/\" rel=\"nofollow\">Lucene in Action</a>,</em> which covers the basics of indexing and searching in a quite readable fashion.</p>\n\n<p>Given your application, it sounds like Nutch and Solr probably will not be necessary. Since all of your documents are available locally, Nutch probably won't be helpful. Solr may help you manage a cluster of searchers if you have a high query load, but Lucene is highly performant, and handles large document sets in a very scalable manner.</p>\n\n<p>The one area that might consume a lot of your effort is the use of PDF. It's possible to index PDF documents, and there are <a href=\"http://wiki.apache.org/lucene-java/LuceneFAQ#head-c45f8b25d786f4e384936fa93ce1137a23b7e422\" rel=\"nofollow\">Lucene contributions to facilitate the extraction of raw text from PDFs</a>, but depending on the document, the quality of results can vary. Often, the context of a keyword in a PDF document is unclear because of formatting instructions, and that can make it hard to do proximity searches or show the context of a hit.</p>\n", "question_id": 223536, "creation_date": 1224624776, "is_accepted": false, "score": 1, "last_activity_date": 1224625259, "answer_id": 223585}, {"question_id": 223536, "owner": {"user_id": 993, "accept_rate": 90, "link": "http://stackoverflow.com/users/993/guy", "user_type": "registered", "reputation": 6820}, "body": "<p>Take a look at <a href=\"http://www.eprints.org\" rel=\"nofollow\">eprints</a>. It includes a workflow for adding new documents, automatically indexes and thumbnails PDF's and has fairly comprehensive full text search functionality.  It can also be easily customised and branded.</p>\n\n<p>Why re-invent the wheel. Again.</p>\n", "creation_date": 1224625171, "is_accepted": false, "score": 2, "last_activity_date": 1224625171, "answer_id": 223607}, {"question_id": 223536, "owner": {"user_id": 18565, "accept_rate": 89, "link": "http://stackoverflow.com/users/18565/kris", "user_type": "registered", "reputation": 19211}, "body": "<p>Having the (imho) distinct advantage of being on a Mac, I use <a href=\"http://www.gravityapps.com/searchlight/overview/\" rel=\"nofollow\">SearchLight</a> on a somewhat older G5. nice web interface to spotlight, the Mac OS' built-in indexing service.</p>\n", "creation_date": 1224625242, "is_accepted": false, "score": -4, "last_activity_date": 1224625242, "answer_id": 223608}, {"question_id": 223536, "owner": {"user_id": 22767, "link": "http://stackoverflow.com/users/22767/craig-wohlfeil", "user_type": "registered", "reputation": 396}, "body": "<p>Google Search Appliance <a href=\"http://www.google.com/enterprise/gsa/\" rel=\"nofollow\">http://www.google.com/enterprise/gsa/</a></p>\n", "creation_date": 1224625307, "is_accepted": false, "score": 3, "last_activity_date": 1224625307, "answer_id": 223610}, {"question_id": 223536, "owner": {"user_id": 80075, "link": "http://stackoverflow.com/users/80075/tony-benbrahim", "user_type": "registered", "reputation": 4649}, "body": "<p>I have had good luck with lucene, but it is not click, install and search, it does require a bit of work.<br />\nIf you need something that yo can download and install and be searching within 10 minutes, look at the free Ominifind Yahoo Edition <a href=\"http://omnifind.ibm.yahoo.net/\" rel=\"nofollow\">http://omnifind.ibm.yahoo.net/</a>, it uses Lucene, but is packaged such that it is configured and ready to run upon install, a much easier way to try Lucene.</p>\n", "creation_date": 1224626804, "is_accepted": false, "score": 8, "last_activity_date": 1224626804, "answer_id": 223695}, {"question_id": 223536, "owner": {"user_id": 29903, "accept_rate": 91, "link": "http://stackoverflow.com/users/29903/james-brady", "user_type": "registered", "reputation": 14808}, "body": "<p>None of the projects in the Lucene family can natively process PDFs, but there are utilities you can drop in and well written examples on how to roll your own.</p>\n\n<p>Lucene will do pretty much whatever you need it to do, but there is overhead in terms of your time, as Tony said above. Thousands of documents really isn't <em>that</em> many, so you might be able to get away with a lighter weight alternative.</p>\n\n<p>That said, I would still recommend looking at Solr - it's much, much easier to set up than Lucene, has support for backups, replication, etc., as well as a nifty JSON interface which would fit your use case very well: <a href=\"http://wiki.apache.org/solr/SolJSON\" rel=\"nofollow\">http://wiki.apache.org/solr/SolJSON</a></p>\n", "creation_date": 1229481543, "is_accepted": true, "score": 3, "last_activity_date": 1229481543, "answer_id": 373506}, {"last_edit_date": 1251362234, "owner": {"user_type": "does_not_exist"}, "body": "<p>I think you want a system to manage your PDF file. Please try to use dspace system. Dspace is a digital library, it supports Lucene based on. www.dspace.org.</p>\n", "question_id": 223536, "creation_date": 1229483447, "is_accepted": false, "score": 3, "last_activity_date": 1251362234, "answer_id": 373549}, {"question_id": 223536, "owner": {"user_id": 56150, "accept_rate": 98, "link": "http://stackoverflow.com/users/56150/sumit-ghosh", "user_type": "registered", "reputation": 1728}, "body": "<p>Nutch + Lucene + Pdf plugin enabled in Nutch is your solution. Nutch allows you to parse pdfs by enabling the pdf plugin. </p>\n\n<p>Lucene will allow you to index the crawled and parsed data and Nutch has servelet which gives  you a search interface.</p>\n\n<p>We use the same for our internal lans.</p>\n", "creation_date": 1244830972, "is_accepted": false, "score": 7, "last_activity_date": 1244830972, "answer_id": 988214}, {"question_id": 223536, "owner": {"user_type": "does_not_exist"}, "body": "<p>A great free search technology you might look at is the IBM Yahoo! free search. I'm not sure whether they followed through on plans to use Lucene under the covers, but it remains one of the really great, east to use free search technologies. It handles up to 500K documents, I believe, and it supports PDF and other non-text formats as well. Graphic user interface; easy to customize search results, and basic search analytics. Basic thesaurus, and powerful API so you can do pretty much whatever you want if the out of the box results are not to your liking. We've suggested this to a number of clients where there were fewer than half a million documents, and they love it.</p>\n", "creation_date": 1251098219, "is_accepted": false, "score": 1, "last_activity_date": 1251098219, "answer_id": 1320892}], "question_id": 223536, "tags": ["lucene", "solr", "nutch"], "answer_count": 10, "link": "http://stackoverflow.com/questions/223536/how-do-we-create-a-simple-search-engine-using-lucene-solr-or-nutch", "last_activity_date": 1295834542, "accepted_answer_id": 373506, "body": "<p>Our company has thousands of PDF documents.  How do we create a simple search engine using Lucene, Solr or Nutch?  We'll provide a basic Java/JSP web page were people can type in words and perform basic and/or queries then show them the document links of all matching PDF's.</p>\n", "creation_date": 1224623717, "score": 7},
{"title": "Suggestion for building search engine using Django", "view_count": 1782, "is_answered": true, "answers": [{"last_edit_date": 1294423647, "owner": {"user_id": 462954, "accept_rate": 100, "link": "http://stackoverflow.com/users/462954/dominique-guardiola", "user_type": "registered", "reputation": 2854}, "body": "<p>The best known pluggable app for that is <a href=\"http://haystacksearch.org/\" rel=\"nofollow\">Django-Haystack</a> which allows you to connect to several search backends :  </p>\n\n<ul>\n<li><a href=\"http://lucene.apache.org/solr/\" rel=\"nofollow\">Solr / Lucene</a> the buzzword-compliant Apache foundation project</li>\n<li><a href=\"http://whoosh.ca/\" rel=\"nofollow\">Whoosh</a> a native python search library</li>\n<li><a href=\"http://xapian.org/\" rel=\"nofollow\">Xapian</a> another very good semantic search engine</li>\n</ul>\n\n<p>haystack allows you to use an API which looks like Django's own Queryset syntax to use directly these search engines (which all happens to have their own API and dialects).</p>\n\n<p><strong>If you're juste after scraping tools</strong>, whatever tool you'll use : <a href=\"http://www.crummy.com/software/BeautifulSoup/documentation.html\" rel=\"nofollow\">BeautifulSoup</a> or Scrappy, you'll be on your own, writing python code that will parse what you want to parse, and then populate your django models.<br>\nThis can even be separate python scripts , available in the commands.py module.</p>\n\n<p>If you have a lot of files to search, you will probably need an index, which is rebuilt frequently and allows fast searches without hitting the django ORM.<br>\nUsing a Solr index (for example) enables you to create other fields on-the-fly, like virtual fields based on your real model's fields (ex : splitting author firstname and lastname, adding an uppercased file title field, whatever) </p>\n\n<p>Of course, f you don't need speedy indexation, keyword boost or semantic analysis, you still can do a classic full-text search over a couple of django model fields i :  </p>\n\n<ul>\n<li><a href=\"http://docs.djangoproject.com/en/dev/ref/models/querysets/\" rel=\"nofollow\">Django native QuerySet</a> see the \"__search('something')\" field lookup</li>\n<li><a href=\"http://barryp.org/blog/entries/postgresql-full-text-search-django/?utm_medium=twitter&amp;utm_source=twitterfeed\" rel=\"nofollow\">PostGreSQL-specific full text search</a> with Django</li>\n</ul>\n", "question_id": 4626863, "creation_date": 1294420863, "is_accepted": false, "score": 6, "last_activity_date": 1294423647, "answer_id": 4628304}, {"question_id": 4626863, "owner": {"user_id": 87451, "link": "http://stackoverflow.com/users/87451/pablo-hoffman", "user_type": "registered", "reputation": 1285}, "body": "<p>Have you checked <a href=\"http://doc.scrapy.org/experimental/djangoitems.html\" rel=\"nofollow\">DjangoItem</a>? It's an experimental Scrapy feature, but it's known to work</p>\n", "creation_date": 1294801142, "is_accepted": false, "score": 0, "last_activity_date": 1294801142, "answer_id": 4664967}], "question_id": 4626863, "tags": ["django", "search-engine", "nutch", "scrapy"], "answer_count": 2, "link": "http://stackoverflow.com/questions/4626863/suggestion-for-building-search-engine-using-django", "last_activity_date": 1294801142, "owner": {"user_id": 567045, "view_count": 4, "answer_count": 0, "creation_date": 1294412731, "reputation": 11}, "body": "<p>Im new in web crawling. I'm going to build a search engine which the crawler saves Rapidshare links including URL where that Rapidshare links found... </p>\n\n<p>In other words, I'm going to build a website similar to <code>filestube.com</code></p>\n\n<p>After some searching, I've found <a href=\"http://scrapy.org/\" rel=\"nofollow\">Scrapy</a> works with Django. I've tried to find about nutch integration with Django, but found nothing</p>\n\n<p>I hope you can give me suggestion for building this kind of website... especially the crawler</p>\n", "creation_date": 1294412731, "score": 2},
{"title": "Writing MetaData inside HDFS", "view_count": 404, "is_answered": true, "answers": [{"question_id": 4625195, "owner": {"user_id": 432844, "link": "http://stackoverflow.com/users/432844/julien-nioche", "user_type": "registered", "reputation": 939}, "body": "<blockquote>\n  <p>We are extracting the meta data in xml file, in the indexing phase(We modified the code of indexer.java), and when ran in local mode it gave us the required metadata.</p>\n</blockquote>\n\n<p>modifying the indexer is not the best option as illustrated by the issue you've encountered</p>\n\n<p>you could : </p>\n\n<ul>\n<li>add the metadata as part of the injection (if you want to do that for the seeds only)</li>\n<li>or write a custom indexing plugin : and e.g. get it to load the XML md from a file in conf/</li>\n</ul>\n\n<p>the content of conf/ is added to the job file and is distributed across the nodes of the cluster. There are quite a few examples of indexing plugins in the code.</p>\n\n<p>Maybe you should use the Nutch user list to get a broader audience? </p>\n", "creation_date": 1294673895, "is_accepted": false, "score": 1, "last_activity_date": 1294673895, "answer_id": 4648470}], "question_id": 4625195, "tags": ["hadoop", "nutch", "indexer"], "answer_count": 1, "link": "http://stackoverflow.com/questions/4625195/writing-metadata-inside-hdfs", "last_activity_date": 1294673895, "owner": {"user_id": 566825, "view_count": 2, "answer_count": 0, "creation_date": 1294401141, "reputation": 6}, "body": "<p>We are using nutch to crawl our intranet site.</p>\n\n<p>We are extracting the meta data in xml file, in the indexing phase(We modified the code of indexer.java), and when ran in local mode it gave us the required metadata.</p>\n\n<p>Now, we thought of using nutch in cluster mode(using hadoop), when we crawled nutch in cluster, we are able to get the index but not the metadata which we used to get previously, in local mode we used(java's IO classes to write meta to files). For hadoop we have changed this to hadoop file system io classes. Yet we are not able to get the meta.</p>\n\n<p>Are there any solution, or are we missing something?</p>\n\n<p>Thanks in advance,\nGeo</p>\n", "creation_date": 1294401141, "score": 1},
{"title": "Building vertical crawler using Bixo", "view_count": 1580, "owner": {"age": 33, "answer_count": 17, "creation_date": 1254641796, "user_id": 183871, "accept_rate": 94, "view_count": 234, "location": "Hyderabad, India", "reputation": 1282}, "is_answered": true, "answers": [{"question_id": 3276808, "owner": {"user_id": 396231, "link": "http://stackoverflow.com/users/396231/erich-nachbar", "user_type": "registered", "reputation": 156}, "body": "<p>I used Bixo in production at a large social networking site (100M page views/day) for user content classification (basically anything user produced with a link in it). </p>\n\n<p>It was a fairly complex workflow using Cascading to </p>\n\n<ul>\n<li>dedupe URLs, </li>\n<li>make Bixo retrieve the page content, </li>\n<li>push the page content through classifiers and </li>\n<li>trigger account revocations for spammy accounts, run spam reports, etc.</li>\n</ul>\n\n<p>If you know Cascading then Bixo works really like any other Cascading component essentially expecting URLs as input and emitting a bunch of page related information as output.</p>\n\n<p>One thing that I underestimated in the beginning is that for a lot of vertical crawlers is that the crawling aspect is \"only\" one small piece in the puzzle. The entire workflow around it can become very complex and if you go with another isolated crawler product you need to find a way to integrate it. Bixo using Cascading becomes just another input to your workflow.</p>\n\n<p>Bixo itself seems to be very solid. Ken Krugler (lead dev) is super responsive and was able to fix some hanging issues I had in the beginning within a day (my dataset contained lots of \"dirty\" URLs). He has a very comprehensive automated test suite making sure Bixo works as designed.</p>\n\n<p>Overall I can't recommend it highly enough. The entire system was built by me in 6-9 months and I don't think I could have done it w/o it in that timeframe.</p>\n", "creation_date": 1279574475, "is_accepted": true, "score": 8, "last_activity_date": 1279574475, "answer_id": 3285194}], "question_id": 3276808, "tags": ["java", "web-crawler", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/3276808/building-vertical-crawler-using-bixo", "last_activity_date": 1294598885, "accepted_answer_id": 3285194, "body": "<p>I came across an an open source crawler Bixo.\nHas anyone tried it? Could you please share the learning? Could we build directed crawler with enough ease (compared to Nutch/Heritrix) ?\nThanks\nNayn</p>\n", "creation_date": 1279479366, "score": 3},
{"title": "How to make sub engines with SOLR &amp; NUTCH?", "view_count": 213, "owner": {"age": 28, "answer_count": 124, "creation_date": 1274187838, "user_id": 429377, "accept_rate": 92, "view_count": 2886, "location": "Cairo, Egypt", "reputation": 12221}, "is_answered": true, "answers": [{"question_id": 4555144, "owner": {"user_id": 484046, "accept_rate": 81, "link": "http://stackoverflow.com/users/484046/millebii", "user_type": "registered", "reputation": 976}, "body": "<p>There is no concept of sub-engines in NUTCH/SOLR... you may use different plug-ins to index your different data in different fields. You can develop a different web front end for the different data type/fields.\nHope this helps</p>\n", "creation_date": 1294042917, "is_accepted": true, "score": 1, "last_activity_date": 1294042917, "answer_id": 4582986}], "question_id": 4555144, "tags": ["solr", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/4555144/how-to-make-sub-engines-with-solr-nutch", "last_activity_date": 1294042917, "accepted_answer_id": 4582986, "body": "<p>greetings all\nI am making a search engine with a template like google\nthat contains news sub engine &amp; images sub engine &amp; videos sub engine\nand I was wondering about how to make the sub engines with SOLR &amp; NUTCH \nI am really new to them, and don't have any idea how to do so, so please advise.</p>\n", "creation_date": 1293634731, "score": 0},
{"title": "crawler get external website search result", "view_count": 558, "owner": {"user_id": 108869, "answer_count": 13, "creation_date": 1240802616, "accept_rate": 83, "view_count": 1330, "reputation": 11463}, "is_answered": true, "answers": [{"question_id": 1912668, "owner": {"user_id": 203907, "accept_rate": 81, "link": "http://stackoverflow.com/users/203907/bozho", "user_type": "registered", "reputation": 386946}, "body": "<p>you can use:</p>\n\n<ul>\n<li>The Selenium API</li>\n<li>HtmlUnit</li>\n<li>Htmlparser</li>\n</ul>\n\n<p>etc.</p>\n", "creation_date": 1260946698, "is_accepted": true, "score": 1, "last_activity_date": 1260946698, "answer_id": 1912812}], "question_id": 1912668, "tags": ["java", "selenium", "web-crawler", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/1912668/crawler-get-external-website-search-result", "last_activity_date": 1293920180, "accepted_answer_id": 1912812, "body": "<ol>\n<li>What is the best practice and library I can use to key in search textbox on external website and collect the search result?<br /></li>\n<li>How do tackle website with different search box and checkbox and collect the result?<br /></li>\n<li>Can Selenium be used to automate this?<br /></li>\n<li>Should I use Heritrix or nutch? Which one is better? I heard nutch comes with plugins. Which one has a bigger community?</li>\n</ol>\n", "creation_date": 1260944138, "score": 0},
{"title": "Running a web crawler for selected sites on google app engine?", "view_count": 1645, "is_answered": true, "answers": [{"question_id": 4364950, "owner": {"user_id": 280474, "link": "http://stackoverflow.com/users/280474/robert-kluin", "user_type": "registered", "reputation": 7693}, "body": "<p>Just glancing over the <a href=\"http://nutch.apache.org/\" rel=\"nofollow\">nutch docs</a>, I see comments like \"[t]his is the second release of Nutch based entirely on the underlying Hadoop platform\" \nwhich make me suspect this will not run on <a href=\"http://code.google.com/appengine/docs/\" rel=\"nofollow\">App Engine</a>.  App Engine apps run in a <a href=\"http://code.google.com/appengine/docs/python/runtime.html#The_Sandbox\" rel=\"nofollow\">Python</a> or <a href=\"http://code.google.com/appengine/docs/java/runtime.html#The_Sandbox\" rel=\"nofollow\">Java</a> sandbox.</p>\n\n<p>That said, you should be able to put a basic crawler together on App Egnine.  I basic implementation would probably involve launching <a href=\"http://code.google.com/appengine/docs/java/taskqueue/\" rel=\"nofollow\">tasks</a> that use <a href=\"http://code.google.com/appengine/docs/java/urlfetch/\" rel=\"nofollow\">urlfetch</a> to grab pages, and then, optionally, insert additional tasks to process links the document links to.  You can kick the crawl off using <a href=\"http://code.google.com/appengine/docs/java/config/cron.html\" rel=\"nofollow\">scheduled tasks</a>. </p>\n", "creation_date": 1291701287, "is_accepted": false, "score": 4, "last_activity_date": 1291701287, "answer_id": 4373795}], "question_id": 4364950, "tags": ["google-app-engine", "web-crawler", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/4364950/running-a-web-crawler-for-selected-sites-on-google-app-engine", "last_activity_date": 1293919408, "owner": {"age": 28, "answer_count": 9, "creation_date": 1247604072, "user_id": 189206, "accept_rate": 68, "view_count": 457, "location": "Fayetteville, PA", "reputation": 1295}, "body": "<p>I need to write a crawler to extract some info from few pre-slected websites only.</p>\n\n<p>I know this is a straightway job but am thinking of using google app engine to get this done.</p>\n\n<p>May be I can try Nutch to do this for me.</p>\n\n<p>How feasible is this way of getting it done?</p>\n\n<p>1) hosting a crawler on google infrastructure\n2) Nutch + app engine- will it be possible?</p>\n", "creation_date": 1291628069, "score": 3},
{"title": "Nutch crawler picture download", "view_count": 815, "owner": {"age": 31, "answer_count": 1714, "creation_date": 1283310779, "user_id": 436560, "accept_rate": 86, "view_count": 2755, "location": "Arad, Romania", "reputation": 16749}, "is_answered": true, "answers": [{"question_id": 4479854, "owner": {"user_id": 356472, "link": "http://stackoverflow.com/users/356472/jmmata", "user_type": "registered", "reputation": 64}, "body": "<p>See <a href=\"http://stackoverflow.com/questions/3247589\">how to get the images in Nutch results? - Stack Overflow</a></p>\n", "creation_date": 1293120179, "is_accepted": true, "score": 1, "last_activity_date": 1293120179, "answer_id": 4520400}], "question_id": 4479854, "tags": ["java", "download", "nutch", "image"], "answer_count": 1, "link": "http://stackoverflow.com/questions/4479854/nutch-crawler-picture-download", "last_activity_date": 1293120179, "accepted_answer_id": 4520400, "body": "<p>How can I download pictures using Nutch in Eclipse?</p>\n", "creation_date": 1292701984, "score": 0},
{"title": "Nutch - Lucene - capture the content of the pages", "view_count": 765, "is_answered": false, "answers": [{"question_id": 4356504, "owner": {"user_id": 484046, "accept_rate": 81, "link": "http://stackoverflow.com/users/484046/millebii", "user_type": "registered", "reputation": 976}, "body": "<p>You need to give more details about what you want to achieve... because Nutch already includes a Lucene Index so I wonder why you want another one????\nNutch has a jsp front-end where you can look at, and find how to query for some field content. There is a cache system implemented so you can retrieve the cached data of page, but then you have to parse it again and index it again.</p>\n", "creation_date": 1291579146, "is_accepted": false, "score": 0, "last_activity_date": 1291579146, "answer_id": 4360836}], "question_id": 4356504, "tags": ["lucene", "nutch", "web-crawler"], "answer_count": 1, "link": "http://stackoverflow.com/questions/4356504/nutch-lucene-capture-the-content-of-the-pages", "last_activity_date": 1291675022, "owner": {"user_id": 530850, "view_count": 2, "answer_count": 0, "creation_date": 1291505796, "reputation": 6}, "body": "<p>I have crawled a few pages with Java Nutch\nAlso I  have made a module with Lucene in Java which allows execute queries on indexed documents.\nI know I created Nutch fields like url, weight and the title. But I am interested in capturing the content of each page. How I can do it using Lucene and knowing I have crawled with nutch?</p>\n\n<p>Thanks </p>\n", "creation_date": 1291506567, "score": 1},
{"title": "Can&#39;t run nutch from a Tomcat webapp on Windows", "view_count": 421, "owner": {"age": 55, "answer_count": 1209, "creation_date": 1219883520, "user_id": 3333, "accept_rate": 83, "view_count": 12303, "location": "Rochester, NY", "reputation": 113417}, "is_answered": true, "answers": [{"question_id": 4357103, "owner": {"user_id": 484046, "accept_rate": 81, "link": "http://stackoverflow.com/users/484046/millebii", "user_type": "registered", "reputation": 976}, "body": "<p>I run Nutch on my Windows PC using Vista... almost daily, with cygwin installed have you tried ?</p>\n", "creation_date": 1291549169, "is_accepted": false, "score": 0, "last_activity_date": 1291549169, "answer_id": 4358597}, {"question_id": 4357103, "owner": {"user_id": 157882, "accept_rate": 93, "link": "http://stackoverflow.com/users/157882/balusc", "user_type": "registered", "reputation": 687614}, "body": "<blockquote>\n  <p><em>The problem is that Tomcat runs as a service, so the script it spawns is running as this \"nt authority\\system\" user, which is what is confusing hadoop because it evidently expects whoami to return a single word, not two words separated by a space.</em></p>\n</blockquote>\n\n<p>Then change the account. <em>Start > Run > services.msc</em>, doubleclick the Tomcat service, open <em>Log On</em> tab and set the account there.</p>\n", "creation_date": 1291553660, "is_accepted": true, "score": 1, "last_activity_date": 1291553660, "answer_id": 4358878}], "question_id": 4357103, "tags": ["windows", "tomcat", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/4357103/cant-run-nutch-from-a-tomcat-webapp-on-windows", "last_activity_date": 1291553660, "accepted_answer_id": 4358878, "body": "<p>I have a web app that spawns off a script that runs a Nutch crawl. It's all working really well, except now my client wants it running on a Windows PC.  The Windows PC she gave me is running Windows 7 Home Premium.</p>\n\n<p>I've got nearly everything running, except when the crawl script - when it fires off nutch, nutch (which runs Hadoop, which for some bizarre reason does a <code>whoami</code>) fails because <code>whoami</code> returns \"nt authority\\system\" instead of a single string.  The error:</p>\n\n<blockquote>\n  <p>javax.security.auth.login.LoginException:\n  Login failed: Expect one token as the\n  result of whoami: nt authority\\system</p>\n</blockquote>\n\n<p>Is there some way to change the name that whoami returns in that case?</p>\n\n<p><strong>Update</strong>: Just to clarify, the exact same script runs fine when I run it from the command line.  The problem is that Tomcat runs as a service, so the script it spawns is running as this \"nt authority\\system\" user, which is what is confusing hadoop because it evidently expects <code>whoami</code> to return a single word, not two words separated by a space.</p>\n", "creation_date": 1291517656, "score": 1},
{"title": "Hadoop to create an Index and Add() it to distributed SOLR... is this possible? Should I use Nutch? ..Cloudera?", "view_count": 1963, "owner": {"age": 40, "answer_count": 141, "creation_date": 1272498882, "user_id": 328397, "accept_rate": 63, "view_count": 2380, "location": "New York, NY", "reputation": 16083}, "is_answered": true, "answers": [{"last_edit_date": 1290466684, "owner": {"user_id": 487064, "accept_rate": 66, "link": "http://stackoverflow.com/users/487064/greycat", "user_type": "registered", "reputation": 5808}, "body": "<p>Generally, you what you've described is almost exactly how Nutch works. Nutch is an crawling, indexing, index merging and query answering toolkit that's based on Hadoop core.</p>\n\n<p>You shouldn't mix Cloudera, Hadoop, Nutch and Lucene. You'll most likely end up using all of them:</p>\n\n<ul>\n<li><strong>Nutch</strong> is the name of indexing / answering (like Solr) machinery.</li>\n<li>Nutch itself runs using a <strong>Hadoop</strong> cluster (which heavily uses it's own distributed file system, HDFS)</li>\n<li>Nutch uses <strong>Lucene</strong> format of indexes</li>\n<li>Nutch includes a query answering frontend, which you can use, or you can attach a <strong>Solr</strong> frontend and use Lucene indexes from there.</li>\n<li>Finally, <strong>Cloudera Hadoop Distribution</strong> (or CDH) is just a Hadoop distribution with several dozens of patches applied to it, to make it more stable and backport some useful features from development branches. Yeah, you'd most likely want to use it, unless you have a reason not to (for example, if you want a bleeding edge Hadoop 0.22 trunk).</li>\n</ul>\n\n<p>Generally, if you're just looking into a ready-made crawling / search engine solution, then Nutch is a way to go. Nutch already includes a lot of plugins to parse and index various crazy types of documents, include MS Word documents, PDFs, etc, etc.</p>\n\n<p>I personally don't see much point in using .NET technologies here, but if you feel comfortable with it, you can do front-ends in .NET. However, working with Unix technologies might feel fairly awkward for Windows-centric team, so if I'd managed such a project, I'd considered alternatives, especially if your task of crawling &amp; indexing is limited (i.e. you don't want to crawl the whole internet for some purpose).</p>\n", "question_id": 4235892, "creation_date": 1290465129, "is_accepted": true, "score": 3, "last_activity_date": 1290466684, "answer_id": 4250881}, {"question_id": 4235892, "owner": {"user_id": 241770, "link": "http://stackoverflow.com/users/241770/joe-stein", "user_type": "registered", "reputation": 964}, "body": "<p>Have you looked at Lucandra <a href=\"https://github.com/tjake/Lucandra\" rel=\"nofollow\">https://github.com/tjake/Lucandra</a> a Cassandra based back end for Lucense/Solr which you can use Hadoop to populate the Cassandra store with the index of your data.</p>\n", "creation_date": 1291425048, "is_accepted": false, "score": 0, "last_activity_date": 1291425048, "answer_id": 4351329}], "question_id": 4235892, "tags": ["solr", "hadoop", "nutch", "solrnet", "faceted-search"], "answer_count": 2, "link": "http://stackoverflow.com/questions/4235892/hadoop-to-create-an-index-and-add-it-to-distributed-solr-is-this-possible", "last_activity_date": 1291425048, "accepted_answer_id": 4250881, "body": "<p>Can I use a MapReduce framework to create an index and somehow add it to a distributed Solr?</p>\n\n<p>I have a burst of information (logfiles and documents) that will be transported over the internet and stored in my datacenter (or Amazon).  It needs to be parsed, indexed, and finally searchable by our replicated Solr installation.  </p>\n\n<p>Here is my proposed architecture:</p>\n\n<ul>\n<li>Use a MapReduce framework (Cloudera, Hadoop, Nutch, even <a href=\"http://research.microsoft.com/en-us/projects/dryadlinq/default.aspx\" rel=\"nofollow\">DryadLinq</a>) to prepare those documents for indexing</li>\n<li>Index those documents into a Lucene.NET / Lucene (java) compatible file format</li>\n<li>Deploy that file to all my Solr instances </li>\n<li>Activate that replicated index</li>\n</ul>\n\n<p>If that above is possible, I need to choose a MapReduce framework.  Since Cloudera is vendor supported and has a ton of patches not included in the Hadoop install, I think it may be worth looking at.</p>\n\n<p>Once I choose the MatpReduce framework, I need to tokenize the documents (PDF, DOCx, DOC, OLE, etc...), index them, copy the index to my Solr instances, and somehow \"activate\" them so they are searchable in the running instance.  I believe this methodolgy is better that submitting documents via the REST interface to Solr. </p>\n\n<p>The reason I bring .NET into the picture is because we are mostly a .NET shop.  The only Unix / Java we will have is Solr and have a front end that leverages the REST interface via Solrnet.  </p>\n\n<blockquote>\n  <p>Based on your experience, how does\n  this architecture look?  Do you see\n  any issues/problems?  What advice can\n  you give?</p>\n</blockquote>\n\n<p>What should I <em>not</em> do to lose faceting search?  After reading the Nutch documentation, I believe it said that it does not do faceting, but I may not have enough background in this software to understand what it's saying.</p>\n", "creation_date": 1290301610, "score": 2},
{"title": "Need plugin to overwrite default title", "view_count": 86, "is_answered": false, "answers": [{"question_id": 4294157, "owner": {"user_id": 484046, "accept_rate": 81, "link": "http://stackoverflow.com/users/484046/millebii", "user_type": "registered", "reputation": 976}, "body": "<p>Why don't you put your title in a different field, thus it will be handled properly ?</p>\n", "creation_date": 1291384843, "is_accepted": false, "score": 0, "last_activity_date": 1291384843, "answer_id": 4346239}], "question_id": 4294157, "tags": ["solr", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/4294157/need-plugin-to-overwrite-default-title", "last_activity_date": 1291384843, "owner": {"user_id": 522567, "view_count": 2, "answer_count": 2, "creation_date": 1290895456, "reputation": 31}, "body": "<p>Im trying to write a plugin for Nutch based on <a href=\"http://sujitpal.blogspot.com/2009/07/nutch-custom-plugin-to-parse-and-add.html\" rel=\"nofollow\">http://sujitpal.blogspot.com/2009/07/nutch-custom-plugin-to-parse-and-add.html</a> to get a custom title finder.</p>\n\n<p>This works well, and storing extracted titles in new field is no problem. But I want to use it in Solr instead of default title. The problem is Solr needs multivalued fields as I have 2 title fields.</p>\n\n<p>metadata.remove(\"title\");</p>\n\n<p>didnt work.</p>\n\n<p>I really want to use the new title instead of the default one created by Nutch. Any suggestions?</p>\n", "creation_date": 1290895456, "score": 0},
{"title": "Nutch API advice", "view_count": 1774, "owner": {"age": 32, "answer_count": 50, "creation_date": 1251151325, "user_id": 370481, "accept_rate": 72, "view_count": 427, "location": "Bucharest, Romania", "reputation": 3017}, "is_answered": true, "answers": [{"question_id": 4340222, "owner": {"user_id": 484046, "accept_rate": 81, "link": "http://stackoverflow.com/users/484046/millebii", "user_type": "registered", "reputation": 976}, "body": "<p>Nutch is very different than what you have ever practiced most probably. \nBecause it is something like a framework it not only has front for query &amp; search, athough solr seems more powerfull than the native Nutch search front end. It also has the crawling part and the indexing (into a Lucene indexe).</p>\n\n<p>If you want to use the crawled for other purposes than search, you will need to developp your own programms and be familiar with Hadoop and MapReduce programming.</p>\n\n<p>Not sure what you want to do with your crawling, but it doesn't look like Nutch is the solution</p>\n", "creation_date": 1291384074, "is_accepted": true, "score": 0, "last_activity_date": 1291384074, "answer_id": 4346137}], "question_id": 4340222, "tags": ["java", "web-crawler", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/4340222/nutch-api-advice", "last_activity_date": 1291384074, "accepted_answer_id": 4346137, "body": "<p>I'm working on a project where I need a mature crawler to do some work, and I'm evaluating Nutch for this purpose. \nMy current needs are relatively straightforward: I need a crawler that is able to save the data to disk and I need it to be able to recrawl only the updated resources of a site and skip the parts that are already crawled. \nDoes anyone have any experience working with the Nutch code directly in Java, not via the command line. I would like to start simple: create a crawler (or similar), minimally configure it and start it, nothing fancy. \nIs there some example for this, or some resource I should be looking at? I'm going over the Nutch documentation, but most of it is about command line, search and other stuff. \nHow usable is the Nutch crawling module without the need to index and search?\nAny help is appreciated. \nThanks. </p>\n", "creation_date": 1291325860, "score": 6},
{"title": "Profiling Lucene in Nutch", "view_count": 310, "owner": {"user_id": 364711, "answer_count": 2, "creation_date": 1276272451, "accept_rate": 85, "view_count": 94, "reputation": 525}, "is_answered": true, "answers": [{"last_edit_date": 1291271819, "owner": {"user_id": 1702, "accept_rate": 100, "link": "http://stackoverflow.com/users/1702/yuval-f", "user_type": "registered", "reputation": 17594}, "body": "<p>I had the same experience trying to locate Lucene time inside Tomcat calls.\nWhat you have to do is:</p>\n\n<ol>\n<li>Use VisualVM 1.2.2.</li>\n<li>Choose the relevant process and press \"Profile\".</li>\n<li>Check the \"Settings\" checkbox. This should open a \"CPU settings\" tab, with fields you can fill.</li>\n<li>Under \"Start profiling From classes:\" write an entrance point in your code\n(e.g. com.my.company.NutchUser) </li>\n<li>Uncheck \"Profile new runnables\".</li>\n<li>Choose \"Profile only classes:\" and under it write:\norg.apache.lucene.*\norg.apache.nutch.*</li>\n<li>Press the \"Profile CPU\" button.\nI believe if you do all that, then run your process and take occasional snapshots, you will be fine.</li>\n</ol>\n\n<p>Alternatively, <a href=\"http://stackoverflow.com/questions/1777556/alternatives-to-gprof/1779343#1779343\">This guy suggests doing stack sampling instead of profiling</a>. I have never done it, but it sounds interesting.</p>\n", "question_id": 4116996, "creation_date": 1289822450, "is_accepted": true, "score": 0, "last_activity_date": 1291271819, "answer_id": 4184111}], "question_id": 4116996, "tags": ["search", "lucene", "profile", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/4116996/profiling-lucene-in-nutch", "last_activity_date": 1291271819, "accepted_answer_id": 4184111, "body": "<p>I'm trying to profile Nutch using VisualVM.  Lucene is the part of the Nutch core responsible for generating url indexes and for searching these indexes due to some query.  I'm running Nutch through Apache Tomcat and I would like to determine how much time Nutch spends in various function calls (including Lucene calls) but when I try to profile using VisualVM I get a bunch of profiling data about Tomcat and not Nutch or Lucene.  What am I doing wrong here?</p>\n", "creation_date": 1289118553, "score": 1},
{"title": "Getting nutch to prioritize frequently updated pages?", "view_count": 245, "owner": {"user_id": 188923, "answer_count": 8, "creation_date": 1255420799, "accept_rate": 88, "view_count": 164, "reputation": 1494}, "is_answered": true, "answers": [{"question_id": 3202878, "owner": {"user_id": 183100, "link": "http://stackoverflow.com/users/183100/pascal-dimassimo", "user_type": "registered", "reputation": 5766}, "body": "<p>What you need is the <a href=\"http://nutch.apache.org/apidocs-1.1/org/apache/nutch/crawl/AdaptiveFetchSchedule.html\" rel=\"nofollow\">Adaptive Fetch Schedule</a>. I have written <a href=\"http://pascaldimassimo.com/2010/06/11/how-to-re-crawl-with-nutch/\" rel=\"nofollow\">a blog post</a> about how it works. Basically what this scheduler does is gradually makes the pages that change more often to be visited more and more regularly.</p>\n", "creation_date": 1278591995, "is_accepted": true, "score": 1, "last_activity_date": 1278591995, "answer_id": 3203503}], "question_id": 3202878, "tags": ["web-crawler", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/3202878/getting-nutch-to-prioritize-frequently-updated-pages", "last_activity_date": 1290995066, "accepted_answer_id": 3203503, "body": "<p>Is there a way to get Nutch to increase the crawling of pages that gets updated frequently? </p>\n\n<p>E.g. index pages and feeds.</p>\n\n<p>It would also be of value to refresh fresh pages that contains comments more frequently the first date after the page was created. Any tips are appreciated.   </p>\n", "creation_date": 1278586958, "score": 1},
{"title": "nutch 1.1 schema.xml", "view_count": 402, "owner": {"user_id": 389336, "answer_count": 5, "creation_date": 1278926991, "accept_rate": 75, "view_count": 17, "reputation": 17}, "is_answered": true, "answers": [{"question_id": 3252980, "owner": {"user_id": 523202, "link": "http://stackoverflow.com/users/523202/nsrtechrecipes", "user_type": "unregistered", "reputation": 26}, "body": "<ol>\n<li>Access and inspect the Nutch Index in question\n<a href=\"http://netscienceresearch.com/techrecipes.php\" rel=\"nofollow\">View How-To recipe</a></li>\n</ol>\n", "creation_date": 1290978270, "is_accepted": true, "score": 1, "last_activity_date": 1290978270, "answer_id": 4299073}], "question_id": 3252980, "tags": ["plugins", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/3252980/nutch-1-1-schema-xml", "last_activity_date": 1290978270, "accepted_answer_id": 4299073, "body": "<p>I recently downloaded latest version of nutch. (nutch-1.1) While going through its code, I noticed that there is a conf/schema.xml file which defines schema for solr part bundled with nutch.</p>\n\n<p>This schema.xml has <em>fields</em> for every plugin.\n My question is, How do I find out, what values a particular plugin is retuning? In other words, if I use a third party plugin (say plugin X) with nutch and wants to add few <em>fields</em> in schema.xml, how do I figure out what \"plugin X\" is returning and if it is string, int, array?</p>\n\n<p>My second question is that, I see conf/solrindex-mapping.xml which is been used by solrIndexer of nutch. This makes me more confused, since not all <em>fields</em> in schema.xml are in solrindex-mapping.xml</p>\n\n<p>For simplicity of explaining answer, lets say Plugin X is feed plugin bundled with nutch.</p>\n", "creation_date": 1279175076, "score": 1},
{"title": "Why is nutch parsing application/x-javascript files?", "view_count": 713, "is_answered": false, "answers": [{"question_id": 4154259, "owner": {"user_id": 183100, "link": "http://stackoverflow.com/users/183100/pascal-dimassimo", "user_type": "registered", "reputation": 5766}, "body": "<p>I use Nutch 1.1 (not trunk) to parse rss feeds. I use the parse-rss plugin. Feeds are only parsed if I activate the plugin. If not, they are ignored. So to answer your question, yes, Nutch should only be using the plugins defined in plugin.includes.</p>\n", "creation_date": 1289832903, "is_accepted": false, "score": 0, "last_activity_date": 1289832903, "answer_id": 4185534}], "question_id": 4154259, "tags": ["plugins", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/4154259/why-is-nutch-parsing-application-x-javascript-files", "last_activity_date": 1289832903, "owner": {"age": 55, "answer_count": 1209, "creation_date": 1219883520, "user_id": 3333, "accept_rate": 83, "view_count": 12303, "location": "Rochester, NY", "reputation": 113417}, "body": "<p>I configured nutch with the following in my <code>conf/nutch-site.xml</code></p>\n\n<pre><code>&lt;property&gt;\n  &lt;name&gt;plugin.includes&lt;/name&gt;\n  &lt;value&gt;urlfilter-regex|protocol-(http|file)|parse-(text|html|pdf|msword)|in\ndex-(basic|anchor|more)|query-(basic|site|url)|response-(json|xml)|summary-basic\n|scoring-opic|urlnormalizer-(pass|regex|basic)&lt;/value&gt;\n  &lt;description&gt;Regular expression naming plugin directory names to\n  include.  Any plugin not matching this expression is excluded.\n  In any case you need at least include the nutch-extensionpoints plugin. By\n  default Nutch includes crawling just HTML and plain text via HTTP,\n  and basic indexing and search plugins. In order to use HTTPS please enable \n  protocol-httpclient, but be aware of possible intermittent problems with the \n  underlying commons-httpclient library.\n  &lt;/description&gt;\n&lt;/property&gt;\n</code></pre>\n\n<p>Note the list of parsers - only text, html, pdf and msword.  But for some strange reason I just discovered some application/x-javascript files in my index.  Why would that be?  Is it using what's in the plugins directory and disregarding my plugin.includes?</p>\n", "creation_date": 1289477565, "score": 0},
{"title": "Fetching web pages with javascript links from Java", "view_count": 1153, "is_answered": false, "answers": [{"question_id": 4134933, "owner": {"user_id": 304024, "accept_rate": 100, "link": "http://stackoverflow.com/users/304024/alois-cochard", "user_type": "registered", "reputation": 6705}, "body": "<p>Seems possible to extract usefull code from Nutch:</p>\n\n<ul>\n<li><a href=\"http://www.docjar.com/html/api/org/apache/nutch/parse/js/JSParseFilter.java.html\" rel=\"nofollow\">http://www.docjar.com/html/api/org/apache/nutch/parse/js/JSParseFilter.java.html</a></li>\n</ul>\n\n<p>Look at how the main method can be used as standalone JS link extractor.</p>\n", "creation_date": 1289314162, "is_accepted": false, "score": 0, "last_activity_date": 1289314162, "answer_id": 4134985}], "question_id": 4134933, "tags": ["java", "parsing", "web-crawler", "html-parsing", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/4134933/fetching-web-pages-with-javascript-links-from-java", "last_activity_date": 1289314162, "owner": {"user_id": 40876, "answer_count": 33, "creation_date": 1227662263, "accept_rate": 62, "view_count": 281, "location": "Manaus, Brazil", "reputation": 1921}, "body": "<p>I have a web crawler application in Java that needs to access all links in a web page. The problem is that in some pages, links are generated by a javascript function. Something like:</p>\n\n<pre><code>&lt;a href=\"someJavascriptFunction()\"&gt; Lorem Ipsum &lt;/a&gt;\n</code></pre>\n\n<p>I'm aware of <a href=\"http://htmlunit.sourceforge.net/\" rel=\"nofollow\">HtmlUnit</a>. But in my tests, it was just way too slow for my purposes. A local page (in <a href=\"http://localhost/test.html\" rel=\"nofollow\">http://localhost/test.html</a>) took almost 2 seconds to be fetched. Other remote web pages took much more time.</p>\n\n<p>I would like the simpliest/fastest way to find all links in a web page, even the javascript ones in Java. (Solutions in C/C++ are welcome). \nI'm also aware that <a href=\"http://nutch.apache.org/\" rel=\"nofollow\">Nutch</a> (the crawler) has a link extractor from Javascript, but I'm not sure if that code could be \"extracted\" out of Nutch to be used in another context.</p>\n", "creation_date": 1289313847, "score": 0},
{"title": "How do you crawl external links on a found page?", "view_count": 792, "owner": {"user_id": 239375, "answer_count": 18, "creation_date": 1261969180, "accept_rate": 97, "view_count": 164, "location": "Dallas, TX", "reputation": 1192}, "is_answered": true, "answers": [{"question_id": 4019115, "owner": {"user_id": 183100, "link": "http://stackoverflow.com/users/183100/pascal-dimassimo", "user_type": "registered", "reputation": 5766}, "body": "<p>First, make sure that the parameter 'db.ignore.external.links' is set to false. Also, in the file 'regex-urlfilter.txt', add rules for the external links you wish to be crawled OR add <code>+.</code> as the last rule. The <code>+.</code> rule will make the crawler follow ALL links. If you use that last option, beware that you risk crawling all the Web!</p>\n", "creation_date": 1288183422, "is_accepted": true, "score": 2, "last_activity_date": 1288183422, "answer_id": 4033309}], "question_id": 4019115, "tags": ["nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/4019115/how-do-you-crawl-external-links-on-a-found-page", "last_activity_date": 1288183422, "accepted_answer_id": 4033309, "body": "<p>I used the example on installing nutch from their wiki. I was able to crawl multiple pages pulled from dmoz easily. But is there a configuration that can be done to crawl external links it finds on a page, or write those external links to a file to be crawled next?</p>\n\n<p>What is the best way to follow links on a page to index that page as well with nutch? If I were executing the bin/nutch via python, could I get back all the external links it found, and create a new crawl list to run again? What would you do?</p>\n", "creation_date": 1288042857, "score": 1},
{"title": "Architecture with 3 servers for solr search engine", "view_count": 444, "is_answered": true, "answers": [{"question_id": 3447797, "owner": {"user_id": 158763, "link": "http://stackoverflow.com/users/158763/eric-pugh", "user_type": "registered", "reputation": 1333}, "body": "<p>I think try both.   Read up on what the HathiTrust has done.   I would start out with a single master for and two slaves, that is the simplest approach.  And if you only have 13mln documents, I am guessing the load will be on the indexing/crawling side.....   But 13mln is only ~300 pages a minute.  I think you nutch crawler will be the bottle neck....</p>\n", "creation_date": 1281470907, "is_accepted": false, "score": 1, "last_activity_date": 1281470907, "answer_id": 3452975}, {"question_id": 3447797, "owner": {"user_id": 280795, "accept_rate": 60, "link": "http://stackoverflow.com/users/280795/nick-lothian", "user_type": "registered", "reputation": 136}, "body": "<p>I'd tend towards using two servers for search and one for indexing. </p>\n\n<p>As a general rule you want to keep search as fast as possible, at the expense of indexing performance. Also, two search servers gives you some natural redundancy.</p>\n\n<p>I'd use the third server for searching, too, when it's not actually doing the indexing. (13 million docs isn't a huge index, and indexing it shouldn't take very long compared to how often you reindex it)</p>\n", "creation_date": 1286860587, "is_accepted": false, "score": 0, "last_activity_date": 1286860587, "answer_id": 3911868}], "question_id": 3447797, "tags": ["search", "full-text-search", "nutch", "solr"], "answer_count": 2, "link": "http://stackoverflow.com/questions/3447797/architecture-with-3-servers-for-solr-search-engine", "last_activity_date": 1286860587, "owner": {"user_id": 183038, "answer_count": 3, "creation_date": 1254481801, "accept_rate": 17, "view_count": 37, "reputation": 268}, "body": "<p>I'm going to build a search engine on solr, and nutch as a crawler. I have to index about 13mln documents.\nI have 3 servers for this job:</p>\n\n<ol>\n<li>4 core Xeon 3Ghz, 20Gb ram, 1.5Tb sata</li>\n<li>2*4 core Xeon 3Ghz, 16Gb ram, 500Gb ide</li>\n<li>2*4 core Xeon 3Ghz, 16Gb ram, 500Gb ide</li>\n</ol>\n\n<p>One of the servers I can use as a master for crawling and indexing, other twos as a slave for searching, or I can use one for searching, and another two for indexing with two shards.\nWhat architecture can you recommend? Should I use sharding, how much shards, and which of the servers should I use for what?</p>\n", "creation_date": 1281434516, "score": 0},
{"title": "nutch crawler - how to set maximum number of inlinks per host", "view_count": 1070, "is_answered": false, "answers": [{"question_id": 3870492, "owner": {"user_id": 183100, "link": "http://stackoverflow.com/users/183100/pascal-dimassimo", "user_type": "registered", "reputation": 5766}, "body": "<p>With depth=10 and topN=1000, you will not have more than 10000 documents in your index (if you don't re-crawl). The 'depth' parameter indicates how many iterations Nutch will run. The 'topN' parameter controls how much urls at maximum will be fetched during one iteration. So multiplying 'depth' by 'topN' gives an approximation of how many urls will be indexed. It is an approximation because you might have urls that will timed-out or return a 404.</p>\n\n<p>If you don't want to re-crawl, make sure the 'db.fetch.interval.default' is set with a high enough value for the crawl job to complete. If the crawl job is not completed when that interval expires, then you will start re-crawling some urls and so the number of urls indexed will be less than depth*topN.</p>\n", "creation_date": 1286381757, "is_accepted": false, "score": 0, "last_activity_date": 1286381757, "answer_id": 3874641}], "question_id": 3870492, "tags": ["full-text-search", "solr", "search-engine", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/3870492/nutch-crawler-how-to-set-maximum-number-of-inlinks-per-host", "last_activity_date": 1286381757, "owner": {"user_id": 183038, "answer_count": 3, "creation_date": 1254481801, "accept_rate": 17, "view_count": 37, "reputation": 268}, "body": "<p>How can i set maximum number of pages to index per host?\ni don't want to index all million pages of site, i want to index only first 100000 found pages.</p>\n", "creation_date": 1286352181, "score": 1},
{"title": "nutch crawler relative urls problem", "view_count": 388, "is_answered": true, "answers": [{"question_id": 3820683, "owner": {"user_id": 7412, "accept_rate": 96, "link": "http://stackoverflow.com/users/7412/dogbane", "user_type": "registered", "reputation": 138145}, "body": "<p>A <a href=\"https://issues.apache.org/jira/browse/NUTCH-566\" rel=\"nofollow\">bug</a> has already been logged for this. Take a look.</p>\n", "creation_date": 1285757870, "is_accepted": false, "score": 3, "last_activity_date": 1285757870, "answer_id": 3820928}], "question_id": 3820683, "tags": ["java", "lucene", "solr", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/3820683/nutch-crawler-relative-urls-problem", "last_activity_date": 1285757870, "owner": {"user_id": 183038, "answer_count": 3, "creation_date": 1254481801, "accept_rate": 17, "view_count": 37, "reputation": 268}, "body": "<p>Has any one experience a problem with the way the standard html parser plugin handles relative urls? There is a site - <a href=\"http://xxxx/asp/list_books.asp?id_f=11327\" rel=\"nofollow\">http://xxxx/asp/list_books.asp?id_f=11327</a>\nand when browsing a link with its href set to\n'?id_r=442&amp;id=41&amp;order='\na browser will naturally take you to\n<a href=\"http://xxxx/asp/list_books.asp?id_r=442&amp;id=41&amp;order=\" rel=\"nofollow\">http://xxxx/asp/list_books.asp?id_r=442&amp;id=41&amp;order=</a></p>\n\n<p>However, in nutch when the outlinks are parsed from the page the link ends up being\n<a href=\"http://xxxx/asp/?id_r=442&amp;id=41&amp;order=\" rel=\"nofollow\">http://xxxx/asp/?id_r=442&amp;id=41&amp;order=</a></p>\n\n<p>which of course is broken. So why is the list_books.asp gone?</p>\n", "creation_date": 1285755649, "score": 1},
{"title": "problem by integration of apache nutch (release 1.2) in apach solr (trunk) - got solr exception", "view_count": 792, "is_answered": false, "answers": [{"question_id": 3692410, "owner": {"user_id": 167980, "accept_rate": 100, "link": "http://stackoverflow.com/users/167980/paige-cook", "user_type": "registered", "reputation": 18612}, "body": "<p>Nutch 1.2 does not work with Solr trunk... </p>\n\n<p>From the Nutch mailing list (original post <a href=\"http://lucene.472066.n3.nabble.com/RESULT-VOTE-Apache-Nutch-1-2-Release-Candidate-4-td1576749.html#a1576749\" rel=\"nofollow\">here</a>)...</p>\n\n<blockquote>\n  <p>Do you all know if 1.2 works with current Solr trunk? </p>\n  \n  <p>It doesn't, it uses Solr 1.4.x. Solr trunk uses incompatible API. </p>\n</blockquote>\n", "creation_date": 1285691532, "is_accepted": false, "score": 0, "last_activity_date": 1285691532, "answer_id": 3814980}], "question_id": 3692410, "tags": ["solr", "integration", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/3692410/problem-by-integration-of-apache-nutch-release-1-2-in-apach-solr-trunk-got", "last_activity_date": 1285691532, "owner": {"user_id": 445255, "view_count": 0, "answer_count": 0, "creation_date": 1284234405, "reputation": 1}, "body": "<p><br>\nI have configured the <code>solrindex-mapping.xml</code> (nutch) and configured my solr <code>schema.xml</code> and <code>solrconfig.xml</code> too. Both working well on single run, but if I use the <code>bin/nutch solrindex ...</code> I get an exception:</p>\n\n<pre><code>org.apache.solr.common.SolrException: Document [null] missing required field: id\n</code></pre>\n\n<p>I have configured the <code>id</code> in all config-files. At <code>solrindex-mapping.xml</code> it maps from <code>url</code> to <code>id</code> and at <code>schema.xml</code> of solr I configured the id too. I don't know what's wrong. I add some logging outputs into <code>org.apache.nutch.indexer.solr.SolrWriter.java</code>. I add one loginfo at these line, when the read fields are added to SolrInputDocument. The result after building and running is:</p>\n\n<pre><code>2010-09-11 21:31:06,326 INFO  solr.SolrWriter - write()\n2010-09-11 21:31:06,327 INFO  solr.SolrWriter - Key: segment, value: 20100911212934\n2010-09-11 21:31:06,327 INFO  solr.SolrWriter - Key: boost, value: 1.0\n2010-09-11 21:31:06,327 INFO  solr.SolrWriter - Key: digest, value: bc315927b7c01c7a2905d5b6872bc35b\n2010-09-11 21:31:06,327 INFO  solr.SolrWriter - close()\n</code></pre>\n\n<p>You will see only 3 read fields O_o. Does anyone know if there is something wrong in my configuration? <strong>I need the running nutch really fast, because I am currently writing on my bachelor thesis :/</strong> (on information integration of heterogenous data sources at the local network)</p>\n\n<p>Best regards<br>\nmarcel =)</p>\n\n<p>The rest of the log:</p>\n\n<pre><code>2010-09-11 21:31:06,079 INFO  solr.SolrWriter - open()\n2010-09-11 21:31:06,280 INFO  solr.SolrMappingReader - source: content dest: content\n2010-09-11 21:31:06,280 INFO  solr.SolrMappingReader - source: site dest: site\n2010-09-11 21:31:06,280 INFO  solr.SolrMappingReader - source: title dest: metadata_title\n2010-09-11 21:31:06,281 INFO  solr.SolrMappingReader - source: host dest: host\n2010-09-11 21:31:06,281 INFO  solr.SolrMappingReader - source: segment dest: segment\n2010-09-11 21:31:06,281 INFO  solr.SolrMappingReader - source: boost dest: boost\n2010-09-11 21:31:06,281 INFO  solr.SolrMappingReader - source: digest dest: digest\n2010-09-11 21:31:06,281 INFO  solr.SolrMappingReader - source: tstamp dest: metadata_last_modified\n2010-09-11 21:31:06,281 INFO  solr.SolrMappingReader - source: lastModified dest: metadata_last_modified\n2010-09-11 21:31:06,281 INFO  solr.SolrMappingReader - source: url dest: url\n2010-09-11 21:31:06,281 INFO  solr.SolrMappingReader - source: url dest: id\n2010-09-11 21:31:06,281 INFO  solr.SolrMappingReader - source: url dest: id\n2010-09-11 21:31:06,281 INFO  solr.SolrMappingReader - source: url dest: url\n2010-09-11 21:31:06,281 INFO  solr.SolrMappingReader - uniqueKey = id\n2010-09-11 21:31:06,291 INFO  solr.SolrWriter - write()\n2010-09-11 21:31:06,294 INFO  solr.SolrWriter - Key: segment, value: 20100911212934\n2010-09-11 21:31:06,294 INFO  solr.SolrWriter - Key: boost, value: 1.0\n2010-09-11 21:31:06,294 INFO  solr.SolrWriter - Key: digest, value: 18abadd34a2bd71a8336fa5e8c6dbedb\n2010-09-11 21:31:06,306 INFO  solr.SolrWriter - write()\n2010-09-11 21:31:06,306 INFO  solr.SolrWriter - Key: segment, value: 20100911212934\n2010-09-11 21:31:06,306 INFO  solr.SolrWriter - Key: boost, value: 1.0\n2010-09-11 21:31:06,306 INFO  solr.SolrWriter - Key: digest, value: 3267fd5ea03852cdc83383635d133fad\n2010-09-11 21:31:06,310 INFO  solr.SolrWriter - write()\n2010-09-11 21:31:06,310 INFO  solr.SolrWriter - Key: segment, value: 20100911212934\n2010-09-11 21:31:06,310 INFO  solr.SolrWriter - Key: boost, value: 1.0\n2010-09-11 21:31:06,311 INFO  solr.SolrWriter - Key: digest, value: b61607602ab99eda5684adc9966349d6\n2010-09-11 21:31:06,314 INFO  solr.SolrWriter - write()\n2010-09-11 21:31:06,314 INFO  solr.SolrWriter - Key: segment, value: 20100911212851\n2010-09-11 21:31:06,314 INFO  solr.SolrWriter - Key: boost, value: 1.0\n2010-09-11 21:31:06,314 INFO  solr.SolrWriter - Key: digest, value: 9bdb8df3d1addf254203542dd22096d3\n2010-09-11 21:31:06,316 INFO  solr.SolrWriter - write()\n2010-09-11 21:31:06,316 INFO  solr.SolrWriter - Key: segment, value: 20100911212934\n2010-09-11 21:31:06,316 INFO  solr.SolrWriter - Key: boost, value: 1.0\n2010-09-11 21:31:06,317 INFO  solr.SolrWriter - Key: digest, value: 66eb3639ae15655bf91dc53208f95167\n2010-09-11 21:31:06,319 INFO  solr.SolrWriter - write()\n2010-09-11 21:31:06,319 INFO  solr.SolrWriter - Key: segment, value: 20100911212934\n2010-09-11 21:31:06,319 INFO  solr.SolrWriter - Key: boost, value: 1.0\n2010-09-11 21:31:06,319 INFO  solr.SolrWriter - Key: digest, value: 6e0501b52e204c2a68d9caa70dd0dfa9\n2010-09-11 21:31:06,326 INFO  solr.SolrWriter - write()\n2010-09-11 21:31:06,327 INFO  solr.SolrWriter - Key: segment, value: 20100911212934\n2010-09-11 21:31:06,327 INFO  solr.SolrWriter - Key: boost, value: 1.0\n2010-09-11 21:31:06,327 INFO  solr.SolrWriter - Key: digest, value: bc315927b7c01c7a2905d5b6872bc35b\n2010-09-11 21:31:06,327 INFO  solr.SolrWriter - close()\n2010-09-11 21:31:06,687 WARN  mapred.LocalJobRunner - job_local_0001\norg.apache.solr.common.SolrException: Document [null] missing required field: id\nDocument [null] missing required field: id\nrequest: http://127.0.0.1:8983/solr/update?wt=javabin&amp;version=1\n        at org.apache.solr.client.solrj.impl.CommonsHttpSolrServer.request(CommonsHttpSolrServer.java:424)\n        at org.apache.solr.client.solrj.impl.CommonsHttpSolrServer.request(CommonsHttpSolrServer.java:243)\n        at org.apache.solr.client.solrj.request.AbstractUpdateRequest.process(AbstractUpdateRequest.java:105)\n        at org.apache.solr.client.solrj.SolrServer.add(SolrServer.java:49)\n        at org.apache.nutch.indexer.solr.SolrWriter.close(SolrWriter.java:98)\n        at org.apache.nutch.indexer.IndexerOutputFormat$1.close(IndexerOutputFormat.java:48)\n        at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:474)\n        at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:411)\n        at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:216)\n2010-09-11 21:31:07,556 ERROR solr.SolrIndexer - java.io.IOException: Job failed!\n</code></pre>\n", "creation_date": 1284234405, "score": 0},
{"title": "solrindex way of mapping nutch schema to solr", "view_count": 898, "is_answered": true, "answers": [{"question_id": 3582740, "owner": {"user_id": 183100, "link": "http://stackoverflow.com/users/183100/pascal-dimassimo", "user_type": "registered", "reputation": 5766}, "body": "<p>What I would do is use a tool like <a href=\"https://tcpmon.dev.java.net/\" rel=\"nofollow\">tcpmon</a> to monitor exactly what Nutch is sending to Solr. By examing the xml payload, you could determine if Nutch is correctly sending those custom fields to Solr. If Nutch is sending them correctly, there is something going on on the Solr side. On the opposite, re-check your Nutch code.</p>\n", "creation_date": 1282914636, "is_accepted": false, "score": 2, "last_activity_date": 1282914636, "answer_id": 3584463}], "question_id": 3582740, "tags": ["solr", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/3582740/solrindex-way-of-mapping-nutch-schema-to-solr", "last_activity_date": 1282914636, "owner": {"user_id": 432743, "view_count": 15, "answer_count": 0, "creation_date": 1282900385, "reputation": 11}, "body": "<p>We have several custom nutch fields that the crawler picks up and indexes. Transferring this to solr via solrindex (using the mapping file) works fine. The log shows everything is fine, however the index in solr environment does not reflect this. \nAny help will be much appreciated,</p>\n\n<p>Thanks,\nAshok</p>\n", "creation_date": 1282900385, "score": 1},
{"title": "Identifying strings in documents, with nutch+solr?", "view_count": 472, "owner": {"user_id": 188923, "answer_count": 8, "creation_date": 1255420799, "accept_rate": 88, "view_count": 164, "reputation": 1494}, "is_answered": true, "answers": [{"question_id": 3507262, "owner": {"user_id": 183100, "link": "http://stackoverflow.com/users/183100/pascal-dimassimo", "user_type": "registered", "reputation": 5766}, "body": "<p>Nutch works with Solr by indexing the crawled data to Solr via the Solr HTTP API. You trigger the indexation by calling the solrindex command. See <a href=\"http://wiki.apache.org/nutch/RunningNutchAndSolr\" rel=\"nofollow\">this page</a> for details on how to setup this.</p>\n\n<p>To be able to extract the company names, I would add the necessary code in Solr. I would use a <a href=\"http://wiki.apache.org/solr/UpdateRequestProcessor\" rel=\"nofollow\">UpdateRequestProcessor</a>. It allows to add an extra step in the indexing process to add extra fields in the document being indexed. Your UpdateRequestProcessor would be used to examine to document sent to Solr by Nutch, extract the company names from the text and add them as new fields in the document. Solr would them index the document + the fields that you add.</p>\n", "creation_date": 1282138972, "is_accepted": true, "score": 1, "last_activity_date": 1282138972, "answer_id": 3512720}, {"question_id": 3507262, "owner": {"user_id": 432844, "link": "http://stackoverflow.com/users/432844/julien-nioche", "user_type": "registered", "reputation": 939}, "body": "<p>You could embed a NER library (see opennlp, lingpipe, gate) in to a custom parser, generate new fields and create an indexingfilter accordingly. This is not particularly difficult and the advantage compared to doing this on the SOLR side is that you'd gain from the scalability of mapreduce (NLP tasks are often CPU-hungry).\nSee <a href=\"http://github.com/jnioche/behemoth\" rel=\"nofollow\">Behemoth</a> for an example of how to embed GATE in mapreduce</p>\n", "creation_date": 1282907205, "is_accepted": false, "score": 3, "last_activity_date": 1282907205, "answer_id": 3583537}], "question_id": 3507262, "tags": ["solr", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/3507262/identifying-strings-in-documents-with-nutchsolr", "last_activity_date": 1282907205, "accepted_answer_id": 3512720, "body": "<p>I'm looking into a search solution that will identify strings (company names) and use these strings for search and facets in Solr. </p>\n\n<p>I'm new to Nutch and Solr so I wonder if this is best done in Nutch or in Solr. One solution would be to generate a Parser in Nutch that identifies the strings in question and then index the name of the company, later mapped to a Solr value. I'm not sure on how, but I guess this could also be done inside Solr directly from the text? </p>\n\n<p>Does it make sense to do this string identification in Nutch or in Solr and is there some functionality in Solr or Nutch that could help me here? </p>\n\n<p>Thanks. </p>\n", "creation_date": 1282081826, "score": 0},
{"title": "Nutch : get current crawl depth in the plugin", "view_count": 923, "owner": {"age": 33, "answer_count": 17, "creation_date": 1254641796, "user_id": 183871, "accept_rate": 94, "view_count": 234, "location": "Hyderabad, India", "reputation": 1282}, "is_answered": true, "answers": [{"question_id": 3530850, "owner": {"user_id": 183100, "link": "http://stackoverflow.com/users/183100/pascal-dimassimo", "user_type": "registered", "reputation": 5766}, "body": "<p>With Nutch, \"depth\" represents the number of generate/fetch/update cycles run successively. Per example, if you are at depth 4, it means your are in the fourth cycle. When you say that you want to go no further than depth 10, it means that you want to stop after 10 cycles. </p>\n\n<p>Within each cycle, the number or previous cycles run before it (the \"depth\") is unknown. That information is not kept. </p>\n\n<p>If you have your own version of Crawl.java, you could keep track of the current \"depth\" and pass that information to your HTML parser plugin.</p>\n", "creation_date": 1282309926, "is_accepted": false, "score": 0, "last_activity_date": 1282309926, "answer_id": 3531102}, {"question_id": 3530850, "owner": {"user_id": 183871, "accept_rate": 94, "link": "http://stackoverflow.com/users/183871/nayn", "user_type": "registered", "reputation": 1282}, "body": "<p>Crawl.java has NutchConfiguration object. This object is passed while initializing all the components. I set the property for crawl-depth before creating new Fetcher.</p>\n\n<pre><code>conf.setInt(\"crawl.depth\", i+1);\nnew Fetcher(conf).fetch(segs[0], threads,\n          org.apache.nutch.fetcher.Fetcher.isParsing(conf));  // fetch it\n</code></pre>\n\n<p>The HtmlParser plugin can access it as below:</p>\n\n<pre><code>LOG.info(\"Current depth: \" + getConf().getInt(\"crawl.depth\", -1));\n</code></pre>\n\n<p>This doesn't force me to break map-reduce.\nThanks\nNayn</p>\n", "creation_date": 1282562338, "is_accepted": true, "score": 2, "last_activity_date": 1282562338, "answer_id": 3547015}], "question_id": 3530850, "tags": ["nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/3530850/nutch-get-current-crawl-depth-in-the-plugin", "last_activity_date": 1282562338, "accepted_answer_id": 3547015, "body": "<p>I want to write my own HTML parser plugin for nutch.\nI am doing focused crawling by generating outlinks falling only in specific xpath.\nIn my use case, I want to fetch different data from the html pages depending on the current depth of the crawl. So I need to know the current depth in HtmlParser plugin for each content that I am parsing. </p>\n\n<p>Is it possible with Nutch? I see CrawlDatum does not have crawl_depth information.\nI was thinking of having map of  information in another data structure.\nDoes anybody have better idea?</p>\n\n<p>Thanks</p>\n", "creation_date": 1282307677, "score": 0},
{"title": "how to add &quot;did you mean&quot; in nutch-lucene search engine", "view_count": 232, "is_answered": true, "answers": [{"question_id": 3345866, "owner": {"user_id": 272861, "accept_rate": 95, "link": "http://stackoverflow.com/users/272861/mikos", "user_type": "registered", "reputation": 6427}, "body": "<p>Look at <a href=\"http://lucene.apache.org/solr/\" rel=\"nofollow\">Apache Solr</a> (built using Lucene), it offers you this <a href=\"http://www.ibm.com/developerworks/java/library/j-solr-update/\" rel=\"nofollow\">functionality built-in</a>. </p>\n\n<p>To do this by yourself using Lucene would need considerable effort and knowledge of n-grams, string distances etc. So why reinvent the wheel, if solr does it for you.</p>\n", "creation_date": 1280322736, "is_accepted": false, "score": 1, "last_activity_date": 1280322736, "answer_id": 3353151}, {"question_id": 3345866, "owner": {"user_id": 347165, "accept_rate": 77, "link": "http://stackoverflow.com/users/347165/xodarap", "user_type": "registered", "reputation": 6997}, "body": "<p>Look in the <a href=\"http://svn.apache.org/repos/asf/lucene/dev/trunk/lucene/contrib/spellchecker/\" rel=\"nofollow\">contrib/spellchecker</a> folder. I don't know that I would agree with Mikos that it takes \"considerable effort\" to set it up, but as with everything else in Lucene, it's easier to just use Solr.</p>\n", "creation_date": 1280328794, "is_accepted": false, "score": 0, "last_activity_date": 1280328794, "answer_id": 3354106}, {"question_id": 3345866, "owner": {"user_id": 395681, "accept_rate": 86, "link": "http://stackoverflow.com/users/395681/guillaume-lebourgeois", "user_type": "registered", "reputation": 2714}, "body": "<p>Beware of the Solr spellchecker, the last version is not fully stable and can provoke grave exceptions.</p>\n\n<p>The ticket is open on <a href=\"https://issues.apache.org/jira/browse/SOLR-1630?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel\" rel=\"nofollow\">https://issues.apache.org/jira/browse/SOLR-1630?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel</a></p>\n", "creation_date": 1280330043, "is_accepted": false, "score": 0, "last_activity_date": 1280330043, "answer_id": 3354315}], "question_id": 3345866, "tags": ["lucene", "nutch"], "answer_count": 3, "link": "http://stackoverflow.com/questions/3345866/how-to-add-did-you-mean-in-nutch-lucene-search-engine", "last_activity_date": 1280330043, "owner": {"user_id": 403642, "view_count": 5, "answer_count": 0, "creation_date": 1280248937, "reputation": 6}, "body": "<p>i am having problem of implementing this suggestion to my bangla search engine.\ncould anyone kindly help me out?</p>\n", "creation_date": 1280248937, "score": 1},
{"title": "Directed crawl using Nutch or Heritrix", "view_count": 988, "owner": {"age": 33, "answer_count": 17, "creation_date": 1254641796, "user_id": 183871, "accept_rate": 94, "view_count": 234, "location": "Hyderabad, India", "reputation": 1282}, "is_answered": true, "answers": [{"question_id": 3254903, "owner": {"user_id": 183871, "accept_rate": 94, "link": "http://stackoverflow.com/users/183871/nayn", "user_type": "registered", "reputation": 1282}, "body": "<p>I tried to create a POC with both of these. I needed the outlinks to start the next phase of the crawl with diff set of rules. With heritrix, there is no way to retain outlinks on the last hop since all the outlinks are discarded. With Nutch, there is no way to incorporate my own scraper which does not return outlink etc which are required by its internal data structures like ParseData etc. Moreover it is tightly coupled with lucene and related indexing system.\nThanks\nNayn</p>\n", "creation_date": 1280317275, "is_accepted": true, "score": 0, "last_activity_date": 1280317275, "answer_id": 3352454}], "question_id": 3254903, "tags": ["java", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/3254903/directed-crawl-using-nutch-or-heritrix", "last_activity_date": 1280317275, "accepted_answer_id": 3352454, "body": "<p>I have seen Nutch and Heritrix way of crawling. They both have the concept of generate/fetch/update cycles which start with some seed urls and iterate over the result urls after fetching step.</p>\n\n<p>The scoping/filtering logic works on regular expression applied to the URLs extracted.</p>\n\n<p>I want to do something very specific.\nI don't want to extract all urls from the page but I'd rather fetch urls based on some xpath.\nThe reasons being: \n  - Not all urls could be classified with precise regular expression\n  - I might miss some urls which fall outside given reg ex\n  - I might want to follow 'Next Page' sequence as well\n  - A specific crawl cycle might have different xpath based filters in each depth.</p>\n\n<p>Has anybody done such thing with Nutch of Heritrix?</p>\n\n<p>Thanks\nNayn</p>\n", "creation_date": 1279191677, "score": 0},
{"title": "Nutch search always returns 0 results", "view_count": 249, "is_answered": true, "answers": [{"question_id": 952703, "owner": {"user_type": "does_not_exist"}, "body": "<p>You may want to restart Tomcat.  If you have not since you changed the files.  </p>\n", "creation_date": 1253600628, "is_accepted": false, "score": 1, "last_activity_date": 1253600628, "answer_id": 1458413}], "question_id": 952703, "tags": ["search", "tomcat", "hadoop", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/952703/nutch-search-always-returns-0-results", "last_activity_date": 1279538215, "owner": {"user_type": "does_not_exist"}, "body": "<p>I have set up nutch 1.0 on a cluster. It has been setup and has successfully crawled, I copied the crawl directory using the dfs -copyToLocal and set the value of searcher.dir in the nutch-site.xml file located in the tomcat directory to point to that directory. Still when I try to search I receive 0 results.</p>\n\n<p>Any help would be greatly appreciated. </p>\n", "creation_date": 1244144662, "score": 1},
{"title": "Give comparision of Nutch Vs Heritrix", "view_count": 2361, "owner": {"age": 33, "answer_count": 17, "creation_date": 1254641796, "user_id": 183871, "accept_rate": 94, "view_count": 234, "location": "Hyderabad, India", "reputation": 1282}, "is_answered": true, "answers": [{"question_id": 3262786, "owner": {"user_id": 58811, "accept_rate": 100, "link": "http://stackoverflow.com/users/58811/upul-bandara", "user_type": "registered", "reputation": 4135}, "body": "<p>Your main task is scrape specific pages from the web site.</p>\n\n<p><strong>Nutch</strong>: Open-source web-search software, built on Lucene Java</p>\n\n<p><strong>Heritrix</strong>: is the Internet Archive's open-source, extensible, web-scale, archival-quality web crawler project</p>\n\n<p>So I think Heritrix is much better than Nutch for your project.</p>\n\n<p>Learning a framework/library is a valuable exercise. But it takes some time. Since you task is not very complex one, sometimes it would be less painful to write a simple crawler from the scratch in Java</p>\n", "creation_date": 1279265939, "is_accepted": true, "score": 0, "last_activity_date": 1279265939, "answer_id": 3262840}], "question_id": 3262786, "tags": ["java", "web-crawler", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/3262786/give-comparision-of-nutch-vs-heritrix", "last_activity_date": 1279265939, "accepted_answer_id": 3262840, "body": "<p>I want to select one of the above for building a crawling framework for specific web sites. This is not an internet-wide crawl. I am not building a search index, and rather interested in scraping specific pages from the web site.</p>\n\n<p>Could somebody please detail about the pros and cons of above?\nThanks\nNayn</p>\n", "creation_date": 1279265446, "score": 2},
{"title": "How to Index Only Pages with Certain Urls with Nutch?", "view_count": 378, "is_answered": true, "answers": [{"question_id": 3253525, "owner": {"user_id": 183100, "link": "http://stackoverflow.com/users/183100/pascal-dimassimo", "user_type": "registered", "reputation": 5766}, "body": "<p>One way is to use the -filter switch of the mergedb command. The command takes a crawl db as input and created a new crawl db with some urls filtered. Just use that filtered crawl db for indexing.</p>\n\n<p>The only drawback to this is that I have not found a way for the mergedb command to use another file than regex-urlfilter.txt, which is the file used by the generator. You will have to maintain two files like regex-urlfilter.txt: one used for the generator with <em>abc.com</em> and another one used for the mergedb command that excludes urls not like <em>car.abc.com</em>. But since both command try to load the same file, you will have to rename the appropriate file to regex-urlfilter.txt before calling one of the two commands.</p>\n\n<p>If someone knows a way to configure the mergedb command to use another file, I'd be happy to hear it!</p>\n", "creation_date": 1279213763, "is_accepted": false, "score": 1, "last_activity_date": 1279213763, "answer_id": 3258081}], "question_id": 3253525, "tags": ["indexing", "nutch", "restrict", "url"], "answer_count": 1, "link": "http://stackoverflow.com/questions/3253525/how-to-index-only-pages-with-certain-urls-with-nutch", "last_activity_date": 1279213763, "owner": {"user_id": 392431, "view_count": 0, "answer_count": 0, "creation_date": 1279180386, "reputation": 6}, "body": "<p>I want nutch to crawl abc.com, but  I want to index only car.abc.com.  car.abc.com links can in any levels in abc.com.  So, basically, I want nutch to keep crawl abc.com normally, but index only pages that start as car.abc.com.  e.g. car.abc.com/toyota...car.abc.com/honda... </p>\n\n<p>I set the regex-urlfilter.txt to include only car.abc.com and run the command \"generate crawl/crawldb crawl/segments\", but it just say \"Generator: 0 records selected for fetching, exiting ...\" .  I guess car.abc.com links exist only in several levels deep.   </p>\n\n<p>How to do this? \nThanks. </p>\n", "creation_date": 1279180386, "score": 1},
{"title": "Spell Checker in Nutch 1.0", "view_count": 181, "is_answered": false, "answers": [{"question_id": 3115422, "owner": {"user_id": 105224, "accept_rate": 77, "link": "http://stackoverflow.com/users/105224/andreas-d", "user_type": "registered", "reputation": 80449}, "body": "<p><a href=\"http://wiki.apache.org/nutch/InstallingWeb2\" rel=\"nofollow\">Can anyone tell me how to use the spell-check query plugin available in the contrib \\ web2 dir (and even the rest of the plugins too)? Is it similar to enabling the nutch-plugins?</a></p>\n\n<p>First hit at google (`nutch spell checker') and located on the apache nutch project pages...</p>\n", "creation_date": 1277452534, "is_accepted": false, "score": 0, "last_activity_date": 1277452534, "answer_id": 3116399}], "question_id": 3115422, "tags": ["java", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/3115422/spell-checker-in-nutch-1-0", "last_activity_date": 1277452534, "owner": {"age": 30, "answer_count": 2, "creation_date": 1277201769, "user_id": 373031, "view_count": 8, "location": "Chengdu", "reputation": 14}, "body": "<p>Can anyone tell me how to implement spell checker in nutch 1.0?</p>\n", "creation_date": 1277437665, "score": 0},
{"title": "which Distribution of Linux is best suited for Nutch-Hadoop?", "view_count": 1019, "is_answered": true, "answers": [{"question_id": 2948423, "owner": {"user_id": 312026, "accept_rate": 86, "link": "http://stackoverflow.com/users/312026/wojtek", "user_type": "registered", "reputation": 3654}, "body": "<p>There is no much difference between any major Linux distribution in this case. But I'd recommend you one that has hadoop packages prepared. I'm using Cloudera's Hadoop distribution on debian and it works very well.</p>\n", "creation_date": 1276865809, "is_accepted": false, "score": 1, "last_activity_date": 1276865809, "answer_id": 3069739}, {"question_id": 2948423, "owner": {"user_id": 240976, "accept_rate": 22, "link": "http://stackoverflow.com/users/240976/thomas-koch", "user_type": "registered", "reputation": 1515}, "body": "<p>hadoop and hbase packages will be in the next Debian Stable version:</p>\n\n<p><a href=\"http://packages.debian.org/search?keywords=hadoop\" rel=\"nofollow\">http://packages.debian.org/search?keywords=hadoop</a></p>\n", "creation_date": 1277033492, "is_accepted": false, "score": 1, "last_activity_date": 1277033492, "answer_id": 3079063}], "question_id": 2948423, "tags": ["hadoop", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/2948423/which-distribution-of-linux-is-best-suited-for-nutch-hadoop", "last_activity_date": 1277033492, "owner": {"user_id": 162767, "answer_count": 3, "creation_date": 1251208205, "accept_rate": 57, "view_count": 62, "location": "Bangalore, India", "reputation": 658}, "body": "<p>we are Trying to figure out which Distribution of Linux be best suited for the Nutch-Hadoop Integration?.\nwe are planning to Use Clusters for Crawling  large  contents through Nutch.\nLet me Know if You  need more clarification on this question?.</p>\n\n<p>Thanks you.</p>\n", "creation_date": 1275382959, "score": 2},
{"title": "Problem with running the Nutch command from PHP exec()", "view_count": 361, "owner": {"age": 35, "answer_count": 2, "creation_date": 1242048279, "user_id": 104785, "accept_rate": 89, "view_count": 112, "reputation": 1757}, "is_answered": true, "answers": [{"question_id": 1753886, "owner": {"user_id": 8362, "accept_rate": 89, "link": "http://stackoverflow.com/users/8362/don", "user_type": "registered", "reputation": 2807}, "body": "<p>Directories under /home/ are usually well protected against others, check the permissions to make sure that the absolute path reference can indeed navigate all the way to nutch.</p>\n", "creation_date": 1258524275, "is_accepted": true, "score": 2, "last_activity_date": 1258524275, "answer_id": 1753962}, {"question_id": 1753886, "owner": {"user_id": 365719, "accept_rate": 70, "link": "http://stackoverflow.com/users/365719/mike-baranczak", "user_type": "registered", "reputation": 5678}, "body": "<p>There are several problems here.</p>\n\n<ol>\n<li><p>When executing from a script, it's hard to predict what the current working directory will be, so it's best to use the full path. </p></li>\n<li><p>Remember that the PHP script is being run by the user that Apache runs under, not by you. So make sure that the Apache user (which may have a different name depending on the distro and the Apache version) has permission to run the Nutch script, and to read/write all the Nutch data files.</p></li>\n<li><p>'all' isn't a valid option for the nutch command, so I'm not sure what it is you're trying to do.</p></li>\n</ol>\n", "creation_date": 1276441182, "is_accepted": false, "score": 0, "last_activity_date": 1276441182, "answer_id": 3032655}], "question_id": 1753886, "tags": ["php", "exec", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/1753886/problem-with-running-the-nutch-command-from-php-exec", "last_activity_date": 1276441182, "accepted_answer_id": 1753962, "body": "<p>My Nutch directory lies in /home/myserv/nutch/nutch-1.0/</p>\n\n<p>My php applictaion is in the diretcory /home/myserv/www/</p>\n\n<p>Theres a a php file in my /home/myserv/www/ diretcory that runs a exec command to run a nutch command.PHP code is like :</p>\n\n<blockquote>\n  <p>$output = exec(\"bin/nutch all\");</p>\n</blockquote>\n\n<p>When I run the command from the command line I need to be in the \"/home/myserv/nutch/nutch-1.0/\"  directory </p>\n\n<p>When i'm trying to run it through the php exec() ,I just can seems to make it execute.</p>\n\n<p>I have tried giving the ful path like (below) but nothing works :(</p>\n\n<blockquote>\n  <p>\n  \n  <p>$output =\n  exec(\"/home/myserv/nutch/nutch-1.0/bin/nutch\n  all\");</p>\n</blockquote>\n\n<p>Desperately looking for help</p>\n", "creation_date": 1258522940, "score": 1},
{"title": "How to develop Nutch for better Arabic searching technology?", "view_count": 209, "is_answered": false, "answers": [{"question_id": 2752471, "owner": {"user_id": 129031, "link": "http://stackoverflow.com/users/129031/a-rashad", "user_type": "registered", "reputation": 706}, "body": "<p>Arabic language has 29 alphabets, some of these alphabets are having sub alphabets like the Alif (\u0623) which can come in different forms.</p>\n\n<p>if you managed to be sub alphabet tolerant i.e. to allow spelling mistakes on these characters</p>\n\n<p>e.g. \u0623\u062d\u0645\u062f  and \u0627\u062d\u0645\u062f and \u0625\u062d\u0645\u062f  and \u0622\u062d\u0645\u062f although they have different UTF8 values, you can take them as close results.</p>\n\n<p>moreover, if you can derive roots from words to allow searching for singulars, plurals, verbs, nouns, etc.</p>\n\n<p>so if someone typed \u0642\u0627\u0644  (said) you can include in the searched terms the words \u0642\u0648\u0644 (saying) and (\u064a\u0642\u0648\u0644) (to say) and \u0645\u0642\u0627\u0644  (a saying), etc. \nit will require a complicated engine to do such thing</p>\n\n<p>finally, if you consider tashkeel (decorating vowels) that are optional in typing where you could take as a more specific search but would allow ignoring it</p>\n\n<p>e.g. \u0631\u062c\u0644 could match \u0631\u064e\u062c\u064f\u0644\u064c (meaning a man) or \u0631\u064e\u062c\u064e\u0644\u064e (meaning walked on feet)  or \u0631\u0650\u0650\u0650\u0650\u0650\u062c\u0652\u0644 (leg)</p>\n\n<p>I hope this would help</p>\n", "creation_date": 1274518987, "is_accepted": false, "score": 0, "last_activity_date": 1274518987, "answer_id": 2887516}], "question_id": 2752471, "tags": ["java", "algorithm", "search", "arabic", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/2752471/how-to-develop-nutch-for-better-arabic-searching-technology", "last_activity_date": 1274518987, "owner": {"age": 32, "answer_count": 1, "creation_date": 1272774518, "user_id": 330670, "accept_rate": 25, "view_count": 9, "location": "Germany", "reputation": 21}, "body": "<p>I am a Computer Science student and working on a project based on the Nutch search engine. I want to develop Java algorithms to better index and search Arabic websites. How can I optimize for this purpose, any ideas?</p>\n", "creation_date": 1272775394, "score": 0},
{"title": "how to do OR search in nutch?", "view_count": 427, "is_answered": false, "answers": [{"question_id": 937130, "owner": {"user_id": 77097, "accept_rate": 78, "link": "http://stackoverflow.com/users/77097/pugmarx", "user_type": "registered", "reputation": 3824}, "body": "<p>Never worked with Nutch actively, but since it's based on Lucene, shouldn't Lucene's rules apply? That is to say, the <a href=\"http://lucene.apache.org/java/2%5F3%5F2/queryparsersyntax.html#Boolean%20operators\" rel=\"nofollow\">Query Parser Syntax</a> should be applicable. See if this helps.</p>\n", "creation_date": 1243898920, "is_accepted": false, "score": 0, "last_activity_date": 1243898920, "answer_id": 937301}, {"question_id": 937130, "owner": {"user_id": 301473, "link": "http://stackoverflow.com/users/301473/user301473", "user_type": "registered", "reputation": 1}, "body": "<p>i was recently started working with nutch .you need to modify the query.java in nutch to get OR query exicuted.\nAdd the code in Query.java </p>\n\n<p>public void addShouldTerm(String term, String field) {\n        clauses.add(new Clause(new Term(term), field, false, false, this.conf));\n    }</p>\n\n<p>public void addShouldTerm(String term) {\n        addShouldTerm(term, Clause.DEFAULT_FIELD);\n    }</p>\n\n<p>and form your query like</p>\n\n<p>Query query= new Query(conf);\nquery.addNotRequiredTerm(\"A\");\nquery.addNotRequiredTerm(\"B\");\nyou will get the results for A Or B.</p>\n\n<p>Please correct me if any other way of doing or better way. </p>\n", "creation_date": 1269501858, "is_accepted": false, "score": 0, "last_activity_date": 1269501858, "answer_id": 2513706}, {"question_id": 937130, "owner": {"user_id": 184223, "accept_rate": 86, "link": "http://stackoverflow.com/users/184223/svenkubiak", "user_type": "registered", "reputation": 318}, "body": "<p>Never used nutch for querying (just for indexing), but the schmea.xml should conatin a defaultOperator which can be set to AND or OR.</p>\n", "creation_date": 1273573505, "is_accepted": false, "score": 0, "last_activity_date": 1273573505, "answer_id": 2809820}], "question_id": 937130, "tags": ["search", "nutch"], "answer_count": 3, "link": "http://stackoverflow.com/questions/937130/how-to-do-or-search-in-nutch", "last_activity_date": 1273573505, "owner": {"user_id": 104015, "answer_count": 1, "creation_date": 1241863152, "accept_rate": 44, "view_count": 8383, "reputation": 34802}, "body": "<p>Say,search for results whose Field is 'A' or 'B'?</p>\n\n<p>it seems the default is AND.</p>\n", "creation_date": 1243895416, "score": 0},
{"title": "What is the best way to freshen a Nutch index?", "view_count": 1665, "owner": {"age": 42, "answer_count": 6, "creation_date": 1225902264, "user_id": 34746, "accept_rate": 100, "view_count": 116, "location": "Massachusetts", "reputation": 1114}, "is_answered": true, "answers": [{"question_id": 640235, "owner": {"user_id": 3333, "accept_rate": 83, "link": "http://stackoverflow.com/users/3333/paul-tomblin", "user_type": "registered", "reputation": 113417}, "body": "<p>This script is based loosely on the one in the Nutch FAQ, which didn't work for me at first:</p>\n\n<pre><code>#!/bin/sh\n#\n# Automate crawling my site\n#\ncrawldir=./crawl\nurldir=./urls\nNUTCH_HOME=${NUTCH_HOME:=.}\n\nnutch=$NUTCH_HOME/bin/nutch\n\n# Make sure the crawl directories exist\nmkdir -p $crawldir/crawldb $crawldir/segments $crawldir/linkdb\n\n# Inject the initial urls\n$nutch inject $crawldir/crawldb $urldir\n\ndepth=1\nwhile(true) ; do\n  echo \"beginning crawl at depth $depth\"\n  echo \"-generate\"\n  $nutch generate $crawldir/crawldb $crawldir/segments\n  if [ $? -ne 0 ] ; then\n    echo \"finishing at depth $depth - no more urls\"\n    break\n  fi\n\n  segment=`/bin/ls -rtd $crawldir/segments/*|tail -1`\n\n  echo \"$nutch fetch $segment\"\n  $nutch fetch $segment\n  if [ $? -ne 0 ] ; then\n    echo \"fetch failed at depth $depth, deleting segment\"\n    rm -rf $segment\n    continue;\n  fi\n\n  echo \"$nutch updatedb $crawldir/crawldb $segment\"\n  $nutch updatedb $crawldir/crawldb $segment\n  depth=`expr $depth + 1`\ndone\n\necho \"$nutch mergesegs $crawldir/MERGEDsegs $crawldir/segments/*\"\n$nutch mergesegs $crawldir/MERGEDsegs $crawldir/segments/*\nif [ $? -eq 0 ] ; then\n  rm -rf $crawldir/segments/*\n  mv $crawldir/MERGEDsegs/* $crawldir/segments\n  rmdir $crawldir/MERGEDsegs\nelse\n  echo \"Something went wrong\"\n  exit\nfi\n\necho \"$nutch invertlinks $crawldir/linkdb -dir $crawldir/segments\"\n$nutch invertlinks $crawldir/linkdb -dir $crawldir/segments\n\necho \"$nutch index $crawldir/NEWindexes $crawldir/crawldb $crawldir/linkdb $crawldir/segments/*\"\n$nutch index $crawldir/NEWindexes $crawldir/crawldb $crawldir/linkdb \\\n$crawldir/segments/*\n\necho \"$nutch dedup $crawldir/NEWindexes\"\n$nutch dedup $crawldir/NEWindexes\n\necho \"$nutch merge $crawldir/MERGEDindexes $crawldir/NEWindexes\"\n$nutch merge $crawldir/MERGEDindexes $crawldir/NEWindexes\n\nmv $crawldir/index $crawldir/OLDindexes\nmv $crawldir/MERGEDindexes $crawldir/index\n</code></pre>\n", "creation_date": 1248898582, "is_accepted": true, "score": 2, "last_activity_date": 1248898582, "answer_id": 1202771}, {"question_id": 640235, "owner": {"user_id": 184223, "accept_rate": 86, "link": "http://stackoverflow.com/users/184223/svenkubiak", "user_type": "registered", "reputation": 318}, "body": "<p>We are using nutch in combination with solr. Our Nutch index is appr. 80 MB conatin arround 5000 Websites. So far, the best way to recrawl is to delete the index and create it from scratch.</p>\n", "creation_date": 1273573117, "is_accepted": false, "score": 0, "last_activity_date": 1273573117, "answer_id": 2809782}], "question_id": 640235, "tags": ["indexing", "nutch", "full-text-indexing"], "answer_count": 2, "link": "http://stackoverflow.com/questions/640235/what-is-the-best-way-to-freshen-a-nutch-index", "last_activity_date": 1273573117, "accepted_answer_id": 1202771, "body": "<p>I haven't looked at Nutch for a year or so and it looks like it has changed significantly. The documentation on re-crawling isn't clear. What is the best way to update an existing Nutch index?</p>\n", "creation_date": 1236887981, "score": 1},
{"title": "Nutch - how to crawl by small patches?", "view_count": 1995, "owner": {"age": 29, "answer_count": 15, "creation_date": 1256026237, "user_id": 192919, "accept_rate": 75, "view_count": 117, "location": "Riga, Republic of Latvia", "reputation": 758}, "is_answered": true, "answers": [{"question_id": 2537874, "owner": {"user_id": 183100, "link": "http://stackoverflow.com/users/183100/pascal-dimassimo", "user_type": "registered", "reputation": 5766}, "body": "<p>You have to understand the Nutch generate/fetch/update cycles. </p>\n\n<p>The generate step of the cycle will take urls (you can set a max number with the <em>topN</em> parameter) from the crawl db and generate a new segment. Initially, the crawl db will only contain the seed urls.</p>\n\n<p>The fetch step does the actual crawling. The actual content of the pages are stored in the segment.</p>\n\n<p>Finally, the update step updates the crawl db with the results from the fetch (add new urls, set the last fetch time for an url, set the http status code of the fetch for an url, etc).</p>\n\n<p>The <em>crawl</em> tool will run this cycle <em>n</em> times, configurable with the <em>depth</em> parameter.</p>\n\n<p>After all cycles are complete, the <em>crawl</em> tool will delete all indexes in the folder from which it is launch and create a new one from all the segments and the crawl db.</p>\n\n<p>So in order to do what you are asking, you should probably not use the <em>crawl</em> tool but instead call the individual Nutch commands, which is what the <em>crawl</em> tool is doing behind the scene. With that, you will be able to control how many times you crawl and also make sure that the indexes are always merge and not delete at each iteration.</p>\n\n<p>I suggest you start with the script define <a href=\"http://wiki.apache.org/nutch/Crawl\">here</a> and change it to your needs.</p>\n", "creation_date": 1271430641, "is_accepted": true, "score": 6, "last_activity_date": 1271430641, "answer_id": 2654019}], "question_id": 2537874, "tags": ["lucene", "web-crawler", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/2537874/nutch-how-to-crawl-by-small-patches", "last_activity_date": 1271430641, "accepted_answer_id": 2654019, "body": "<p>I am stuck! Can`t get Nutch to crawl for me by small patches. I start it by <strong>bin/nutch crawl</strong> command with parameters -depth 7 and -topN 10000. And it never ends. Ends only when my HDD is empty. What i need to do:</p>\n\n<ol>\n<li>Start to crawl my seeds with\npossibility to go further on\noutlinks.  </li>\n<li>Crawl 20000 pages, then\n    index them.</li>\n<li>Crawl another 20000\n    pages, index them and merge with\n    first index.</li>\n<li>Loop step 3 n times.</li>\n</ol>\n\n<p>Tried also with scripts found in wiki, but all scripts i found don't go further. If i run them again, they do everything from beginning. And in the end of script i have the same index i had, when started to crawl. But, i need to continue my crawl.</p>\n\n<p>Some help would be very usefull!</p>\n", "creation_date": 1269866401, "score": 2},
{"title": "how to parse (only text) web sites while crawling", "view_count": 302, "owner": {"age": 29, "answer_count": 10, "creation_date": 1270585341, "user_id": 310370, "accept_rate": 82, "view_count": 3473, "location": "Turkey", "reputation": 6256}, "is_answered": true, "answers": [{"question_id": 2588019, "owner": {"user_id": 183100, "link": "http://stackoverflow.com/users/183100/pascal-dimassimo", "user_type": "registered", "reputation": 5766}, "body": "<p>The crawled pages are stored in the segments. You can have access to them by dumping the segment content:</p>\n\n<pre><code>nutch readseg -dump crawl/segments/20100104113507/ dump\n</code></pre>\n\n<p>You will have to do this for each segment.</p>\n", "creation_date": 1271337745, "is_accepted": true, "score": 1, "last_activity_date": 1271337745, "answer_id": 2645591}], "question_id": 2588019, "tags": ["parsing", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/2588019/how-to-parse-only-text-web-sites-while-crawling", "last_activity_date": 1271337745, "accepted_answer_id": 2645591, "body": "<p>i can succesfully run crawl command via cygwin on windows xp. and i can also make web search via using tomcat.</p>\n\n<p>but i also want to save parsed pages during crawling event</p>\n\n<p>so when i start crawling with like this</p>\n\n<p>bin/nutch crawl urls -dir crawled -depth 3</p>\n\n<p>i also want save parsed html files to text files</p>\n\n<p>i mean during this period which i started with above command</p>\n\n<p>nutch when fetched a page it will also automaticly save that page parsed (only text) to text files</p>\n\n<p>these files names could be fetched url</p>\n\n<p>i really need help about this</p>\n\n<p>this will be used at my university language detection project</p>\n\n<p>ty</p>\n", "creation_date": 1270585417, "score": -1},
{"title": "Crawling engine architecture - Java/ Perl integration", "view_count": 1288, "is_answered": true, "answers": [{"question_id": 1944599, "owner": {"user_id": 139985, "accept_rate": 73, "link": "http://stackoverflow.com/users/139985/stephen-c", "user_type": "registered", "reputation": 391719}, "body": "<blockquote>\n  <p>how solid is integration between Java and Perl specifically from calling perl from java</p>\n</blockquote>\n\n<p>IMO, the best way to call Perl from Java is to have Java launch Perl programs in separate processes.  You could try calling Perl directly from Java using JNI / JNA, but it is hard to get right.  And if you get it wrong you'll be dealing with crashed JVMs.</p>\n\n<blockquote>\n  <p>Open to any all suggestions and opinions. </p>\n</blockquote>\n\n<p>IMO you'll get a more maintainable solution if you go pure Perl or pure Java.  If that means you have to learn Perl, then so be it.  (It is possible to write well-structured, maintainable apps in Perl.  You just need to be disciplined about it.)</p>\n", "creation_date": 1261466298, "is_accepted": false, "score": 1, "last_activity_date": 1261466298, "answer_id": 1944679}, {"question_id": 1944599, "owner": {"user_id": 44523, "accept_rate": 95, "link": "http://stackoverflow.com/users/44523/esko", "user_type": "registered", "reputation": 19970}, "body": "<p>I've had my fair share of creating crawlers with Java using Lucene and in fact I've answered to a related question before about the actual creation process and structure of a web crawler <a href=\"http://stackoverflow.com/questions/1580882/lucene-crawler-it-needs-to-build-lucene-index/1581864#1581864\">here</a>. This isn't directly applicable to your question but I do think it's worth mentioning here.</p>\n\n<p>Anyway, I have to agree with <a href=\"http://stackoverflow.com/questions/1944599/crawling-engine-architecture-java-perl-integration/1944679#1944679\">Stephen C</a>, you're better off with pure Java or pure perl solution instead of a mix of both, however my opinion is based on the fact that they're completely different from each other and hammering two (<em>or more</em>) different mindsets together isn't usually the most optimal thing one could do.</p>\n\n<p>What you described also got me thinking on improving my own crawler (<em>the one I reference in my other answer I linked in the first paragraph</em>), mainly the part about the actual crawling pattern. While I do believe it will take significantly more time to develop a way to manually instruct a Java application to crawl some URL in a specific pattern as the same would take in perl, doing that in Java would eventually lead up to a lot more usable piece of software with all sorts interesting small features which wouldn't be a pain to maintain.</p>\n\n<p>On the other hand, the scripting side of Java is a bit meh, there is a scripting API but since scripting is about loosely defining what you want to do and Java can be annoyingly strict at times, it's not as flexible as one would hope.</p>\n\n<p>To really give an opinion, I think you should minimize the part of programming language which is harder to maintain. I don't know which one it is for you but I'd assume perl. Basically commit to one of the languages and use it to its full extent, don't use the other language as a shortcut.</p>\n", "creation_date": 1261467385, "is_accepted": false, "score": 1, "last_activity_date": 1261467385, "answer_id": 1944755}, {"question_id": 1944599, "owner": {"user_id": 287727, "accept_rate": 100, "link": "http://stackoverflow.com/users/287727/navi", "user_type": "registered", "reputation": 4328}, "body": "<p>You can try webcrawling with HtmlUnit or Selenium and do scheduling using Quartz or put the whole project in application server like Glassfish. If you would like to stick with Perl, you could probably use crontab. The Perl APIs which can be used for webcrawling, may not have proper cookie handling. I hope that is not a problem for you. The only hack I know for this, is calling wget.</p>\n", "creation_date": 1268502015, "is_accepted": false, "score": 0, "last_activity_date": 1268502015, "answer_id": 2439305}], "question_id": 1944599, "tags": ["java", "perl", "hadoop", "nutch", "web-crawler"], "answer_count": 3, "link": "http://stackoverflow.com/questions/1944599/crawling-engine-architecture-java-perl-integration", "last_activity_date": 1268502015, "owner": {"user_id": 236630, "view_count": 2, "answer_count": 1, "creation_date": 1261464955, "reputation": 36}, "body": "<p>I am looking to develop a management and administration solution around our webcrawling perl scripts. Basically, right now our scripts are saved in SVN and are manually kicked off by SysAdmin/devs etc. Everytime we need to retrieve data from new sources we have to create a ticket with business instructions and goals. As you can imagine, not an optimal solution. </p>\n\n<p>There are 3 consistent themes with this system:</p>\n\n<ol>\n<li>the retrieval of data has a \"conceptual structure\" for lack of a better phrase i.e. the retrieval of information follows a particular path</li>\n<li>we are only looking for very specific information so we dont have to really worry about extensive crawling for awhile (think thousands-tens of thousands of pages vs millions)</li>\n<li>crawls are url-based instead of site-based. </li>\n</ol>\n\n<p>As I enhance this alpha version to a more production-level beta I am looking to add automation and management of the retrieval of data. Additionally our other systems are Java (which I'm more proficient in) and I'd like to compartmentalize the perl aspects so we dont have to lean heavily on outside help. </p>\n\n<p>I've evaluated the usual suspects <a href=\"http://lucene.apache.org/nutch/\" rel=\"nofollow\">Nutch</a>, <a href=\"http://incubator.apache.org/droids/\" rel=\"nofollow\">Droid</a> etc but the time spent on modifying those frameworks to suit our specific information retrieval cant be justified.  </p>\n\n<p>So I'd like your thoughts regarding the following architecture. </p>\n\n<p>I want to create a solution which </p>\n\n<ul>\n<li>use Java as the interface for managing and execution of the perl scripts </li>\n<li>use Java for configuration and data access</li>\n<li>stick with perl for retrieval</li>\n</ul>\n\n<p>An example use case would be </p>\n\n<ol>\n<li>a data analyst delivers us a requirement for crawling</li>\n<li>perl developer creates the required script and uses this webapp to submit the script (which gets saved to the filesystem) </li>\n<li>the script gets kicked off from the webapp with specific parameters \n....</li>\n</ol>\n\n<p>Webapp should be able to create multiple threads of the perl script to initiate multiple crawlers. </p>\n\n<p>So questions are</p>\n\n<ol>\n<li>what do you think</li>\n<li>how solid is integration between Java and Perl specifically from calling perl from java </li>\n<li>has someone used such a system which actually is part perl repository</li>\n</ol>\n\n<p>The goal really is to not have a whole bunch of unorganized perl scripts and put some management and organization on our information retrieval. Also, I know I can use perl do do the web part of what we want - but as I mentioned before - trying to keep perl focused. But it seems assbackwards I'm not adverse to making it an all perl solution. </p>\n\n<p>Open to any all suggestions and opinions. </p>\n\n<p>Thanks</p>\n", "creation_date": 1261464955, "score": 2},
{"title": "solr admin gives 404 errors after integrating nutch", "view_count": 757, "owner": {"user_id": 148225, "answer_count": 6, "creation_date": 1249007612, "accept_rate": 73, "view_count": 92, "reputation": 175}, "is_answered": true, "answers": [{"question_id": 2343810, "owner": {"user_id": 284488, "link": "http://stackoverflow.com/users/284488/thedjuke", "user_type": "unregistered", "reputation": 16}, "body": "<p>well , right is:</p>\n\n<p>&lt;bool name=\"hl\"&gt;true&lt;/bool&gt; not &lt;bool hl=\"true\"/&gt;\nthat works </p>\n", "creation_date": 1267541055, "is_accepted": true, "score": 0, "last_activity_date": 1267541055, "answer_id": 2363898}], "question_id": 2343810, "tags": ["solr", "jetty", "nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/2343810/solr-admin-gives-404-errors-after-integrating-nutch", "last_activity_date": 1267541055, "accepted_answer_id": 2363898, "body": "<p>I've followed the instructions from <a href=\"http://www.lucidimagination.com/blog/2009/03/09/nutch-solr/\" rel=\"nofollow\">http://www.lucidimagination.com/blog/2009/03/09/nutch-solr/</a></p>\n\n<p>Had solr up and running before that, could handle test cases, access admin pages, etc.</p>\n\n<p>Copied the nutch schema.xml over to solr as per instructions. Worked, could access admin.</p>\n\n<p>When I added in the requesthandler snippet (see 5d on the website) in solrconfig.xml going to the admin page suddenly tossed off \"HTTP ERROR: 404 missing core name in path RequestURI=/solr/admin/index.jsp\"</p>\n\n<p>I can't see what in the requesthandler snippet could be causing the admin to fail. Using the feb. 26 build of solr.</p>\n", "creation_date": 1267208145, "score": 0},
{"title": "how to create a custom field in nuch search engine?", "view_count": 667, "is_answered": false, "answers": [{"question_id": 1998444, "owner": {"user_id": 154146, "link": "http://stackoverflow.com/users/154146/brian", "user_type": "registered", "reputation": 1203}, "body": "<p>I would think your best option is to develop a Nutch Plugin which handles the logic for adding that field on a per crawled page basis.</p>\n\n<p>Please see the <a href=\"http://wiki.apache.org/nutch/WritingPluginExample\" rel=\"nofollow\">Writing Plugin Example</a> wiki article on the Nutch site.</p>\n", "creation_date": 1262810077, "is_accepted": false, "score": 0, "last_activity_date": 1262810077, "answer_id": 2016026}], "question_id": 1998444, "tags": ["nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/1998444/how-to-create-a-custom-field-in-nuch-search-engine", "last_activity_date": 1262810077, "owner": {"age": 30, "answer_count": 3, "creation_date": 1258953861, "user_id": 216772, "accept_rate": 4, "view_count": 249, "location": "Bangalore, India", "reputation": 158}, "body": "<p>hi\ni want to create a custom field in nutch search engine?\nwhat are the steps i will follow?</p>\n", "creation_date": 1262597573, "score": 0},
{"title": "configuring nutch regex-normalize.xml", "view_count": 2788, "owner": {"user_id": 213245, "view_count": 3, "answer_count": 0, "creation_date": 1258490280, "reputation": 3}, "is_answered": true, "answers": [{"question_id": 1751597, "owner": {"user_id": 122428, "link": "http://stackoverflow.com/users/122428/jitter", "user_type": "registered", "reputation": 43557}, "body": "<p>What version of Nutch are you using? I'm not familiar with Nutch but the default download of Nutch 1.0 already contains a rule in <em>regex-normalize.xml</em> which seems to handle this problem.</p>\n\n<pre><code>&lt;!-- removes session ids from urls (such as jsessionid and PHPSESSID) --&gt;\n&lt;regex&gt;\n  &lt;pattern&gt;([;_]?((?i)l|j|bv_)?((?i)sid|phpsessid|sessionid)=.*?)(\\?|&amp;amp;|#|$)&lt;/pattern&gt;\n  &lt;substitution&gt;$4&lt;/substitution&gt;\n&lt;/regex&gt;\n</code></pre>\n\n<p>Btw. <em>regex-urlfilter.txt</em> seems to contain something of relevance too</p>\n\n<pre><code># skip URLs containing certain characters as probable queries, etc.\n-[?*!@=]\n</code></pre>\n\n<p>Then there are some settings in <em>nutch-default.xml</em> which you might want to check out</p>\n\n<pre><code>urlnormalizer.order\nurlnormalizer.regex.file\nplugin.includes\n</code></pre>\n\n<p>If that all doesn't help maybe this does: <a href=\"http://wiki.apache.org/nutch/FAQ#How%5Fcan%5FI%5Fforce%5Ffetcher%5Fto%5Fuse%5Fcustom%5Fnutch-config.3F\" rel=\"nofollow\">How can I force fetcher to use custom nutch-config?</a></p>\n", "creation_date": 1258499955, "is_accepted": true, "score": 3, "last_activity_date": 1258499955, "answer_id": 1752572}], "question_id": 1751597, "tags": ["java", "lucene", "search-engine", "nutch", "web-crawler"], "answer_count": 1, "link": "http://stackoverflow.com/questions/1751597/configuring-nutch-regex-normalize-xml", "last_activity_date": 1258499955, "accepted_answer_id": 1752572, "body": "<p>I am using the Java-based Nutch web-search software. In order to prevent duplicate (url) results from being returned in my search query results, I am trying to remove (a.k.a. normalize) the expressions of 'jsessionid' from the urls being indexed when running the Nutch crawler to index my intranet. However my modifications to $NUTCH_HOME/conf/regex-normalize.xml (prior to running my crawl) do not seem to be having any effect. </p>\n\n<ol>\n<li><p>How can I ensure that my regex-normalize.xml configuration is being engaged for my crawl? and, </p></li>\n<li><p>What regular expression will successfully remove/normalize expressions of 'jsessionid' from the url during the crawl/indexing?</p></li>\n</ol>\n\n<p>The following is the contents of my current regex-normalize.xml:</p>\n\n<pre><code>&lt;?xml version=\"1.0\"?&gt;\n&lt;regex-normalize&gt;\n&lt;regex&gt;\n &lt;pattern&gt;(.*);jsessionid=(.*)$&lt;/pattern&gt;\n &lt;substitution&gt;$1&lt;/substitution&gt;\n&lt;/regex&gt;\n&lt;regex&gt;\n &lt;pattern&gt;(.*);jsessionid=(.*)(\\&amp;amp;|\\&amp;amp;amp;)&lt;/pattern&gt;\n &lt;substitution&gt;$1$3&lt;/substitution&gt;\n&lt;/regex&gt;\n&lt;regex&gt;\n &lt;pattern&gt;;jsessionid=(.*)&lt;/pattern&gt;\n &lt;substitution&gt;&lt;/substitution&gt;\n&lt;/regex&gt;\n&lt;/regex-normalize&gt;\n</code></pre>\n\n<p>Here is the command that I am issuing to run my (test) 'crawl':</p>\n\n<pre><code>bin/nutch crawl urls -dir /tmp/test/crawl_test -depth 3 -topN 500\n</code></pre>\n", "creation_date": 1258490280, "score": 0},
{"title": "How to enable follow Redirect in Nutch-1.0", "view_count": 581, "is_answered": true, "answers": [{"question_id": 1723745, "owner": {"user_id": 160577, "accept_rate": 95, "link": "http://stackoverflow.com/users/160577/prakash-panjwani", "user_type": "registered", "reputation": 1070}, "body": "<p>I have used Nutch-1.0.Please check the Nutch-1.0 server-site.xml file and set all property as defaut of Nutch-1.0.\nHere is the link for reference\n<a href=\"http://svn.apache.org/viewvc/lucene/nutch/tags/release-0.8/CHANGES.txt?view=markup\" rel=\"nofollow\">http://svn.apache.org/viewvc/lucene/nutch/tags/release-0.8/CHANGES.txt?view=markup</a>\nI think this works.</p>\n", "creation_date": 1258385798, "is_accepted": false, "score": 1, "last_activity_date": 1258385798, "answer_id": 1742945}], "question_id": 1723745, "tags": ["nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/1723745/how-to-enable-follow-redirect-in-nutch-1-0", "last_activity_date": 1258385798, "owner": {"age": 33, "answer_count": 1, "creation_date": 1250677525, "user_id": 159136, "accept_rate": 77, "view_count": 216, "location": "Bhilai Nagar, India", "reputation": 1173}, "body": "<p>Hello \nI am using Nutch-1.0 and I am getting this log entry\n2009-11-12 22:13:11,093 INFO  httpclient.HttpMethodDirector - Redirect requested but followRedirects is disabled.\nHow to enable Follow Redirect.\nThanks in advance..</p>\n", "creation_date": 1258045188, "score": 0},
{"title": "Nutch issues with crwaling website where the url differes only in termes of parameters passes", "view_count": 348, "owner": {"age": 35, "answer_count": 2, "creation_date": 1242048279, "user_id": 104785, "accept_rate": 89, "view_count": 112, "reputation": 1757}, "is_answered": true, "answers": [{"question_id": 1705808, "owner": {"user_id": 104785, "accept_rate": 89, "link": "http://stackoverflow.com/users/104785/annibigi", "user_type": "registered", "reputation": 1757}, "body": "<p>I got the issue fixed.\nIt had everything to do with the url filter set as</p>\n\n<h1>skip URLs containing certain characters as probable queries, etc</h1>\n\n<p>-[?*!@=]</p>\n\n<p>I commented this filter and Nutch crawle dall urls :)</p>\n", "creation_date": 1257832930, "is_accepted": true, "score": 1, "last_activity_date": 1257832930, "answer_id": 1705906}], "question_id": 1705808, "tags": ["nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/1705808/nutch-issues-with-crwaling-website-where-the-url-differes-only-in-termes-of-para", "last_activity_date": 1257832930, "accepted_answer_id": 1705906, "body": "<p>I am using Nutch to crawl webistes and strangely for one of my webistes, the Nutch crawl returns only two urls, the home page url (<a href=\"http://mysite.com/\" rel=\"nofollow\">http://mysite.com/</a>) and one other.</p>\n\n<p>The urls on my webiste are basically of this format</p>\n\n<p><a href=\"http://mysite.com/index.php?main%5Fpage=index&amp;params=12\" rel=\"nofollow\">http://mysite.com/index.php?main%5Fpage=index&amp;params=12</a></p>\n\n<p><a href=\"http://mysite.com/index.php?main%5Fpage=index&amp;category=tub&amp;param=17\" rel=\"nofollow\">http://mysite.com/index.php?main%5Fpage=index&amp;category=tub&amp;param=17</a></p>\n\n<p>i.e. the urls differ only in terms of parameters appened to the url (the part \"http://mysite.com/index.php?\" is common to all urls)</p>\n\n<p>Is Nutch unable to crawl such webistes?</p>\n\n<p>What Nutch settings should I do in order to crawl such websites?</p>\n", "creation_date": 1257830942, "score": 0},
{"title": "Why doesn&#39;t Nutch seem to know about &quot;Last-Modified&quot;?", "view_count": 1157, "owner": {"age": 55, "answer_count": 1209, "creation_date": 1219883520, "user_id": 3333, "accept_rate": 83, "view_count": 12303, "location": "Rochester, NY", "reputation": 113417}, "is_answered": true, "answers": [{"question_id": 1252289, "owner": {"user_id": 94173, "accept_rate": 86, "link": "http://stackoverflow.com/users/94173/sorantis", "user_type": "registered", "reputation": 8956}, "body": "<p>I think you are mistaken with an option name - db.fetch.interval.default. It should be.</p>\n\n<p><strong>db.default.fetch.interval</strong></p>\n\n<p>The number of days after each page injected is fetched that it should next be fetched. 30 by default. </p>\n\n<p>I just read change log of the latest version, and found following</p>\n\n<blockquote>\n  <ol>\n  <li>NUTCH-61 - Support for adaptive re-fetch interval and detection of\n  unmodified content. (ab)</li>\n  </ol>\n</blockquote>\n\n<p>If you don't have latest version installed, I suggest you to do that.</p>\n\n<p>Also, are you using -adddays option for crawling?</p>\n", "creation_date": 1249852419, "is_accepted": false, "score": 0, "last_activity_date": 1249852419, "answer_id": 1252310}, {"question_id": 1252289, "owner": {"user_id": 3333, "accept_rate": 83, "link": "http://stackoverflow.com/users/3333/paul-tomblin", "user_type": "registered", "reputation": 113417}, "body": "<p>I found the problem.  It's a bug in Nutch.  I've emailed the Nutch developer list about it, but here's my fix:</p>\n\n<pre><code>Index: src/plugin/protocol-http/src/java/org/apache/nutch/protocol/http/HttpResponse.java\n===================================================================\n--- src/plugin/protocol-http/src/java/org/apache/nutch/protocol/http/HttpResponse.java  (revision 802632)\n+++ src/plugin/protocol-http/src/java/org/apache/nutch/protocol/http/HttpResponse.java  (working copy)\n@@ -124,11 +124,15 @@\n         reqStr.append(\"\\r\\n\");\n       }\n\n-      reqStr.append(\"\\r\\n\");\n       if (datum.getModifiedTime() &gt; 0) {\n         reqStr.append(\"If-Modified-Since: \" + HttpDateFormat.toString(datum.getModifiedTime()));\n         reqStr.append(\"\\r\\n\");\n       }\n+      else if (datum.getFetchTime() &gt; 0) {\n+          reqStr.append(\"If-Modified-Since: \" + HttpDateFormat.toString(datum.getFetchTime()));\n+          reqStr.append(\"\\r\\n\");\n+      }\n+      reqStr.append(\"\\r\\n\");     \n\n       byte[] reqBytes= reqStr.toString().getBytes();\n</code></pre>\n\n<p>Now I'm seeing 304s in my Apache logs where I'm supposed to be seeing them.</p>\n", "creation_date": 1249938245, "is_accepted": true, "score": 6, "last_activity_date": 1249938245, "answer_id": 1257202}], "question_id": 1252289, "tags": ["web-crawler", "nutch"], "answer_count": 2, "link": "http://stackoverflow.com/questions/1252289/why-doesnt-nutch-seem-to-know-about-last-modified", "last_activity_date": 1249938282, "accepted_answer_id": 1257202, "body": "<p>I setup Nutch with a db.fetch.interval.default of 60000 so that I can crawl every day.  If I don't, it won't even look at my site when I crawl the next day.  But when I do crawl the next day, every page that it fetched yesterday gets fetched with a 200 response code, indicating that it's not using the previous day's date in the \"If-Modified-Since\".  Shouldn't it skip fetching pages that haven't changed?  Is there a way to make it do that?  I noticed a ProtocolStatus.NOT_MODIFIED in Fetcher.java, so I think it should be able to do this, shouldn't it? </p>\n\n<p>By the way, this is cut and pasted from conf/nutch-default.xml from the current trunk:</p>\n\n<pre><code>&lt;!-- web db properties --&gt;\n\n&lt;property&gt;\n  &lt;name&gt;db.default.fetch.interval&lt;/name&gt;\n  &lt;value&gt;30&lt;/value&gt;\n  &lt;description&gt;(DEPRECATED) The default number of days between re-fetches of a page.\n  &lt;/description&gt;\n&lt;/property&gt;\n\n&lt;property&gt;\n  &lt;name&gt;db.fetch.interval.default&lt;/name&gt;\n  &lt;value&gt;2592000&lt;/value&gt;\n  &lt;description&gt;The default number of seconds between re-fetches of a page (30 days).\n  &lt;/description&gt;\n&lt;/property&gt;\n</code></pre>\n", "creation_date": 1249851955, "score": 1},
{"title": "Nutch Multithreading", "view_count": 1899, "owner": {"user_id": 56150, "answer_count": 43, "creation_date": 1232187860, "accept_rate": 98, "view_count": 768, "reputation": 1728}, "is_answered": true, "answers": [{"question_id": 990996, "owner": {"user_type": "does_not_exist"}, "body": "<p>I think your issue is related to a known bug w/the new Nutch fetcher. See <a href=\"http://issues.apache.org/jira/browse/NUTCH-721\" rel=\"nofollow\">NUTCH-721</a>.</p>\n\n<p>You can try using OldFetcher (if you have Nutch 1.0) to see if that solves your problem.</p>\n\n<p>-- Ken</p>\n", "creation_date": 1245006078, "is_accepted": true, "score": 2, "last_activity_date": 1245006078, "answer_id": 993494}], "question_id": 990996, "tags": ["nutch"], "answer_count": 1, "link": "http://stackoverflow.com/questions/990996/nutch-multithreading", "last_activity_date": 1245006078, "accepted_answer_id": 993494, "body": "<p>Iam trying to configure nutch for running multi-threaded crawling. </p>\n\n<p>However , Iam facing an issue. I am not able to run crawl with multiple threads , I have modified the nutch-site.xml to use 25 threads but still I can see only 1 Threads running.</p>\n\n<pre><code>&lt;property&gt;\n  &lt;name&gt;fetcher.threads.fetch&lt;/name&gt;\n  &lt;value&gt;25&lt;/value&gt;\n  &lt;description&gt;The number of FetcherThreads the fetcher should use.\n    This is also determines the maximum number of requests that are \n    made at once (each FetcherThread handles one connection).&lt;/description&gt;\n&lt;/property&gt;\n\n&lt;property&gt;\n  &lt;name&gt;fetcher.threads.per.host&lt;/name&gt;\n  &lt;value&gt;25&lt;/value&gt;\n  &lt;description&gt;This number is the maximum number of threads that\n    should be allowed to access a host at one time.&lt;/description&gt;\n&lt;/property&gt;\n</code></pre>\n\n<p>I always get the value of \nactiveThreads=25, spinWaiting=24, fetchQueues.totalSize=some value.</p>\n\n<p>Whats the meaning of this, can you please explain whats the issue and how can I solve it.</p>\n\n<p>I will highly appreciate your help.</p>\n\n<p>Thanks,\nSumit</p>\n", "creation_date": 1244911180, "score": 1},
{"title": "Problem running Java .war on Tomcat", "view_count": 4983, "owner": {"user_id": 37379, "answer_count": 333, "creation_date": 1226591687, "accept_rate": 89, "view_count": 757, "reputation": 6017}, "is_answered": true, "answers": [{"question_id": 567810, "owner": {"user_id": 41619, "accept_rate": 92, "link": "http://stackoverflow.com/users/41619/adam-paynter", "user_type": "registered", "reputation": 29921}, "body": "<p>In Java, applications sometimes rely on third party libraries. In this case, it appears that your Tomcat installation does not include one such library. Judging by the error you received, it appears that you are missing the <a href=\"http://commons.apache.org/logging/\" rel=\"nofollow\">Apache Commons Logging</a> library (a commonly used library in the Java world that just so happens to not come bundled with Tomcat).</p>\n\n<p>The typical way to distribute a library in Java is via a JAR (Java Archive) file. Simply put, a JAR file is simply a bunch of Java classes that have been zipped into a file that has been renamed from *.zip to *.jar.</p>\n\n<p>To obtain the Commons Logging JAR file, you can download it from the <a href=\"http://commons.apache.org/downloads/download_logging.cgi\" rel=\"nofollow\">Apache Commons download site</a>. You will want the binary version, not the source version. Should you happen to download version 1.1.1 (for example), you should unzip the <code>commons-logging-1.1.1-bin.zip</code> file. Inside, you will find a file named <code>commons-logging-1.1.1.jar</code>. Copy this JAR file to the <code>lib</code> directory wherever your Tomcat software is installed. You may be required to restart Tomcat before it notices this new file.</p>\n\n<p>Hopefully, the next time you try to use the application, you may or may not receive yet another error indicating that yet another class cannot be found. In that case, I welcome you to the wonderful world of JAR hunting! :) Hopefully the application will not require too many libraries above and beyond Commons Logging, but we will see (considering you're trying to run Nutch, I can foresee it requiring <a href=\"http://www.apache.org/dyn/closer.cgi/lucene/java/\" rel=\"nofollow\">Lucene</a>, so be prepared for that).</p>\n\n<p>Have fun with Nutch!</p>\n", "creation_date": 1235112742, "is_accepted": false, "score": 2, "last_activity_date": 1235112742, "answer_id": 568562}, {"question_id": 567810, "owner": {"user_id": 44523, "accept_rate": 95, "link": "http://stackoverflow.com/users/44523/esko", "user_type": "registered", "reputation": 19970}, "body": "<p>For me that tells that it can't find the logger which is reported as a parse error itself. A bit odd or disinformant way to express it, I guess. Anyway, I think you need to add the <a href=\"http://commons.apache.org/logging/\" rel=\"nofollow\">Commons Logging .jar</a> to your libraries (<code>WEB-INF/lib</code>) and restart Tomcat and then it should work.</p>\n\n<p>Also your Tomcat seems to be ancient, if possible I'd recommend getting 5.5.x or 6.x.</p>\n", "creation_date": 1235112759, "is_accepted": true, "score": 1, "last_activity_date": 1235112759, "answer_id": 568563}, {"last_edit_date": 1235119313, "owner": {"user_type": "does_not_exist"}, "body": "<p>if you found to have required .jar existing in application, than solution to your problem might be to: </p>\n\n<ol>\n<li>Stop Tomcat</li>\n<li>go to %TOMCAT_HOME%/work  an erase everything inside</li>\n</ol>\n\n<p>this 'work' folder, as its name says, contains classes used for work, sometimes access to some of these files can be blocked for any reason. When I get such message this is first thing to do. Most of the time it works... </p>\n", "question_id": 567810, "creation_date": 1235115360, "is_accepted": false, "score": 0, "last_activity_date": 1235119313, "answer_id": 568607}], "question_id": 567810, "tags": ["java", "tomcat", "nutch"], "answer_count": 3, "link": "http://stackoverflow.com/questions/567810/problem-running-java-war-on-tomcat", "last_activity_date": 1235119313, "accepted_answer_id": 568563, "body": "<p>I am following the tutorial here:  </p>\n\n<p><a href=\"http://nutch.sourceforge.net/docs/en/tutorial.html\" rel=\"nofollow\">http://nutch.sourceforge.net/docs/en/tutorial.html</a></p>\n\n<p>Crawling works fine, as does the test search from the command line.  </p>\n\n<p>When I try to fire up tomcat after moving ROOT.war into place(and it unarchiving and creating a new ROOT folder during startup), I get a page with the 500 error and some errors in the Tomcat logs. </p>\n\n<p>HTTP Status 500 - No Context configured to process this request</p>\n\n<pre><code>2009-02-19 15:55:46 WebappLoader[]: Deploy JAR /WEB-INF/lib/xerces-2_6_2.jar to C:\\Program Files\\Apache Software Foundation\\Tomcat 4.1\\webapps\\ROOT\\WEB-INF\\lib\\xerces-2_6_2.jar\n2009-02-19 15:55:47 ContextConfig[] Parse error in default web.xml\norg.apache.commons.logging.LogConfigurationException: User-specified log class 'org.apache.commons.logging.impl.Log4JLogger' cannot be found or is not useable.\n    at org.apache.commons.digester.Digester.createSAXException(Digester.java:3181)\n    at org.apache.commons.digester.Digester.createSAXException(Digester.java:3207)\n    at org.apache.commons.digester.Digester.endElement(Digester.java:1225) ............ etc.\n</code></pre>\n\n<p>So it looks like the root of the error is default web.xml, not in the Log4JLogger - although I know very little about Java.  I did not edit the web.xml in the tomcat dir.  </p>\n\n<p>Anyone know what is going on here?  </p>\n\n<p>versions/info:</p>\n\n<p>nutch 0.9</p>\n\n<p>Tomcat 4.1</p>\n\n<p>jre1.5.0_08</p>\n\n<p>jdk1.6.0_12</p>\n\n<p>NUTCH_JAVA_HOME=C:\\Program Files\\Java\\jdk1.6.0_12</p>\n\n<p>JAVA_HOME=C:\\Program Files\\Java\\jdk1.6.0_12</p>\n", "creation_date": 1235088220, "score": 1}
]